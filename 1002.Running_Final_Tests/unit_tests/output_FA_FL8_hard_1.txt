dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 41]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 41]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
channels:3
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'F' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'H']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'H']
 ['F' 'F' 'F' 'F' 'H' 'F' 'H' 'H']
 ['F' 'F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'H' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'F' 'H' 'H' 'F']
 ['F' 'F' 'F' 'F' 'F' 'H' 'H' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:255000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
timesteps_in_obs:2
store_prev_actions:True
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
state_size:[6, 6]
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'H']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'H']
 ['F' 'F' 'F' 'F' 'H' 'F' 'H' 'H']
 ['F' 'F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'H' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'F' 'H' 'H' 'F']
 ['F' 'F' 'F' 'F' 'F' 'H' 'H' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.009]
 [0.016]
 [0.016]
 [0.016]] [[-0.]
 [ 0.]
 [-0.]
 [-0.]
 [-0.]] [[0.032]
 [0.019]
 [0.032]
 [0.032]
 [0.032]]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
from probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.014233866621824827
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  835 train batches done:  22 episodes:  87
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.07546441863448909
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.22284645
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.40058616
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.038]
 [0.045]
 [0.058]
 [0.063]] [[-1.5]
 [ 0. ]
 [-1.5]
 [-1.5]
 [-1.5]] [[0.03 ]
 [2.05 ]
 [0.064]
 [0.089]
 [0.099]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5053479
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.32867822
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
7 147
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.20907992
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.39517838
siam score:  -0.42469123
22 270
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
25 281
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
29 317
siam score:  -0.50117683
29 329
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
31 372
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.30290672
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
35 409
siam score:  -0.2935467
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.12112856311440669
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.291531
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.35188714
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.3423232
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.11513283388956574
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.16312015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
42 481
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.40734977
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5243039
siam score:  -0.5197354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.10638286676001739
siam score:  -0.5130095
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus -0.026584573581781273
siam score:  -0.47549373
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.125 0.125 0.667]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.156]
 [ 0.   ]
 [-0.001]
 [ 0.1  ]
 [ 0.141]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4339062
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.10251716631888025
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus -0.011762209236621857
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.32916656
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.1  ]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.09179197737684712
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.099]
 [ 0.   ]
 [-0.005]
 [ 0.005]
 [-0.034]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.346239
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.42668685
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.432528
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.281]
 [-0.281]
 [-0.281]
 [-0.281]
 [-0.281]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
UNIT TEST: sample policy line 217 mcts : [0.458 0.083 0.167 0.083 0.208]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.39251816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.36962035
siam score:  -0.36553276
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
91 708
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.07]
 [-0.07]
 [-0.07]
 [-0.07]
 [-0.07]] [[0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
93 711
93 713
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  -0.0817518539313933
siam score:  -0.40465888
93 721
siam score:  -0.4213253
siam score:  -0.42595252
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5680232
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
101 763
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.208 0.292 0.167 0.125 0.208]
siam score:  -0.5796379
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
106 811
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.134]
 [ 0.002]
 [ 0.002]
 [-0.014]
 [ 0.009]] [[0.   ]
 [0.136]
 [0.136]
 [0.12 ]
 [0.143]]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08577354510440593
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5863009
114 849
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.084]
 [ 0.004]
 [ 0.004]
 [ 0.004]
 [ 0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
114 866
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6005384
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6157643
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
118 897
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
deleting a thread, now have 2 threads
Frames:  10900 train batches done:  1274 episodes:  1020
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.61627704
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07789273682680727
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [ 0.001]
 [ 0.007]
 [ 0.003]
 [ 0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.595366
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.011]
 [-0.01 ]
 [-0.033]
 [-0.01 ]
 [-0.011]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 2.060185498515993e-07
0.0 1.7776477857986178e-07
0.0 2.0126966108446646e-07
0.0 2.0871385808494056e-07
0.0 0.0
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.001]
 [-0.006]
 [-0.002]
 [-0.001]
 [-0.002]] [[0.046]
 [0.041]
 [0.045]
 [0.045]
 [0.044]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.083 0.208 0.25  0.25  0.208]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
147 1074
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.006]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6915106
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.125 0.417 0.208 0.083 0.167]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
163 1172
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
163 1176
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [-0.008]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.004503302829502797
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 3.339517389556643e-07
0.0 0.0
0.0 4.720269415627457e-07
0.0 0.0
0.0 4.772310811886476e-07
0.0 0.0
0.0 0.0
0.0 0.0
0.0 3.575965872059293e-07
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.006]
 [-0.005]
 [-0.006]
 [-0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.06 ]
 [-0.005]
 [-0.06 ]
 [-0.005]
 [-0.06 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
first move QE:  -0.060020285443072426
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.208 0.083 0.167 0.417 0.125]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.004520599255708548
siam score:  -0.6401388
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
176 1279
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
177 1286
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
177 1289
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.60095704
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.053399541532684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6832207
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6999895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
189 1402
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.029]
 [0.064]
 [0.046]
 [0.03 ]
 [0.027]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.68661886
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6693828
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.701141
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.691958
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6902554
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6763431
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
218 1544
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66818273
siam score:  -0.6647684
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
221 1554
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.011]
 [-0.008]
 [-0.009]
 [-0.01 ]
 [-0.009]] [[0.   ]
 [0.001]
 [0.001]
 [0.   ]
 [0.001]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6896194
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.69947183
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.04505991907422359
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.01 ]
 [0.011]
 [0.011]
 [0.011]] [[0.006]
 [0.005]
 [0.006]
 [0.006]
 [0.006]]
230 1610
231 1614
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.04423250866819453
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
STARTED EXPV TRAINING ON FRAME NO.  20026
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.179]
 [-0.179]
 [-0.088]
 [-0.179]
 [-0.049]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7040407
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
244 1665
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70969373
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.05005779823115141
249 1698
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.032]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.   ]
 [0.021]
 [0.021]
 [0.021]
 [0.021]]
siam score:  -0.7017884
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69100386
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
249 1750
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
249 1763
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
250 1781
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.027]
 [ 0.059]
 [-0.042]
 [ 0.004]
 [ 0.004]] [[0.091]
 [0.134]
 [0.   ]
 [0.061]
 [0.061]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69689924
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.3653956696640375
siam score:  -0.6994312
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.037]
 [3.654]
 [3.263]
 [3.026]
 [3.366]] [[1.709]
 [1.924]
 [1.788]
 [1.706]
 [1.824]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.343]
 [1.54 ]
 [1.16 ]
 [1.366]
 [2.074]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69194716
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.15]
 [6.15]
 [6.15]
 [6.15]
 [6.15]] [[1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.095]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.032]
 [ 0.296]
 [ 0.296]
 [ 0.296]
 [ 0.299]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.922]
 [0.246]
 [0.385]
 [0.182]
 [0.83 ]] [[0.261]
 [0.036]
 [0.082]
 [0.014]
 [0.231]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
first move QE:  -0.038512956207522125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.037896433898561746
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6818387
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.03338223858067253
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.281135157504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
287 1883
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.464]
 [5.464]
 [5.981]
 [5.771]
 [5.752]] [[1.737]
 [1.737]
 [2.   ]
 [1.893]
 [1.884]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.765]
 [5.103]
 [5.017]
 [4.874]
 [4.676]] [[1.608]
 [1.797]
 [1.749]
 [1.669]
 [1.558]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6718292
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.021044725453199915
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.65788484
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
301 1897
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.628]
 [7.338]
 [7.403]
 [7.235]
 [7.355]] [[1.316]
 [1.642]
 [1.671]
 [1.594]
 [1.649]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.082]
 [7.082]
 [8.155]
 [7.082]
 [8.077]] [[1.241]
 [1.241]
 [1.697]
 [1.241]
 [1.664]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.562]
 [6.562]
 [6.896]
 [6.562]
 [6.623]] [[1.513]
 [1.513]
 [1.659]
 [1.513]
 [1.539]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.033]
 [0.376]
 [1.143]
 [1.174]
 [1.034]] [[0.542]
 [0.104]
 [0.615]
 [0.635]
 [0.543]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.343276778328355
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.13 ]
 [1.139]
 [3.561]
 [2.85 ]
 [3.211]] [[0.   ]
 [0.003]
 [0.812]
 [0.574]
 [0.695]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  328
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.773]
 [2.714]
 [2.808]
 [2.948]
 [2.958]] [[1.191]
 [1.139]
 [1.222]
 [1.347]
 [1.355]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66559625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.006172832689518427
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.013210639336942038
first move QE:  0.013210639336942038
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.679]
 [2.679]
 [2.679]
 [2.679]
 [2.679]] [[0.47]
 [0.47]
 [0.47]
 [0.47]
 [0.47]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.88 ]
 [5.88 ]
 [5.88 ]
 [5.88 ]
 [6.468]] [[0.997]
 [0.997]
 [0.997]
 [0.997]
 [1.193]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72832835
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.894]
 [2.263]
 [3.005]
 [3.102]
 [3.148]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
355 1960
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.03 ]
 [-0.036]
 [ 0.015]
 [ 0.203]
 [ 0.407]] [[0.008]
 [0.   ]
 [0.068]
 [0.318]
 [0.59 ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.907]
 [2.907]
 [2.907]
 [2.907]
 [3.223]] [[0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [1.131]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.488]
 [-0.235]
 [-0.079]
 [-0.097]
 [-0.421]] [[0.   ]
 [0.084]
 [0.137]
 [0.131]
 [0.022]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72892517
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.73232925
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.178]
 [ 0.088]
 [ 1.057]
 [ 0.33 ]
 [ 0.601]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73270255
378 1986
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.429]
 [5.429]
 [5.429]
 [5.429]
 [5.926]] [[1.448]
 [1.448]
 [1.448]
 [1.448]
 [1.613]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.029860916568243537
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.641]
 [3.463]
 [3.463]
 [3.463]
 [3.958]] [[1.33 ]
 [1.211]
 [1.211]
 [1.211]
 [1.54 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7300096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7284567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.166]
 [3.341]
 [3.341]
 [3.341]
 [3.341]] [[1.533]
 [1.118]
 [1.118]
 [1.118]
 [1.118]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 5.411276132132626
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.843]
 [0.594]
 [1.217]
 [0.657]
 [0.948]] [[0.298]
 [0.093]
 [0.605]
 [0.145]
 [0.384]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.902]
 [1.134]
 [3.47 ]
 [3.584]
 [4.235]] [[0.925]
 [0.   ]
 [0.78 ]
 [0.818]
 [1.036]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
407 2005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.347]
 [8.347]
 [7.892]
 [6.314]
 [7.753]] [[1.464]
 [1.464]
 [1.312]
 [0.785]
 [1.265]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.293]
 [5.462]
 [5.38 ]
 [6.293]
 [6.868]] [[1.103]
 [0.826]
 [0.799]
 [1.103]
 [1.296]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.967]
 [3.967]
 [3.967]
 [3.967]
 [4.841]] [[1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.329]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 4.021182462033581
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7232704
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.349]
 [3.349]
 [3.654]
 [3.349]
 [3.802]] [[1.334]
 [1.334]
 [1.537]
 [1.334]
 [1.636]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73350936
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[9.596]
 [9.596]
 [9.596]
 [9.596]
 [8.95 ]] [[1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.541]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[9.231]
 [9.231]
 [9.231]
 [9.231]
 [9.231]] [[1.798]
 [1.798]
 [1.798]
 [1.798]
 [1.798]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
first move QE:  0.06442306983732528
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.73178846
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.444]
 [6.444]
 [6.444]
 [6.444]
 [6.444]] [[1.61]
 [1.61]
 [1.61]
 [1.61]
 [1.61]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.227]
 [8.227]
 [8.227]
 [8.227]
 [8.227]] [[1.458]
 [1.458]
 [1.458]
 [1.458]
 [1.458]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.735726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7423049
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.7608330764802407
455 2037
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.622]
 [4.622]
 [4.622]
 [4.399]
 [5.298]] [[1.478]
 [1.478]
 [1.478]
 [1.348]
 [1.872]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74307555
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.905]
 [0.905]
 [2.453]
 [0.905]
 [0.905]] [[0.428]
 [0.428]
 [0.946]
 [0.428]
 [0.428]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.494]
 [4.494]
 [4.494]
 [4.494]
 [4.401]] [[1.898]
 [1.898]
 [1.898]
 [1.898]
 [1.824]]
first move QE:  0.07153085051597632
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.674]
 [3.818]
 [3.894]
 [3.002]
 [3.63 ]] [[1.245]
 [1.336]
 [1.384]
 [0.821]
 [1.217]]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.627]
 [4.627]
 [4.627]
 [5.714]
 [6.452]] [[0.752]
 [0.752]
 [0.752]
 [1.318]
 [1.702]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7298028
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7426031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7463085
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.614]
 [5.67 ]
 [5.103]
 [5.207]
 [5.462]] [[1.447]
 [1.474]
 [1.198]
 [1.249]
 [1.373]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7496922
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7437706
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.578]
 [4.578]
 [4.704]
 [4.578]
 [4.331]] [[1.922]
 [1.922]
 [2.   ]
 [1.922]
 [1.769]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.153]
 [6.153]
 [6.153]
 [6.153]
 [6.153]] [[8.208]
 [8.208]
 [8.208]
 [8.208]
 [8.208]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.762]
 [7.762]
 [7.762]
 [7.762]
 [9.011]] [[1.247]
 [1.247]
 [1.247]
 [1.247]
 [1.739]]
siam score:  -0.7419978
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.458 0.    0.    0.125 0.417]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.026]
 [5.026]
 [5.026]
 [5.026]
 [5.026]] [[0.959]
 [0.959]
 [0.959]
 [0.959]
 [0.959]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.328]
 [3.328]
 [3.328]
 [3.328]
 [4.015]] [[1.304]
 [1.304]
 [1.304]
 [1.304]
 [1.673]]
first move QE:  0.08841480690975359
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.419]
 [3.419]
 [3.419]
 [3.419]
 [4.167]] [[0.862]
 [0.862]
 [0.862]
 [0.862]
 [1.359]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.146]
 [5.146]
 [5.146]
 [5.146]
 [5.664]] [[1.658]
 [1.658]
 [1.658]
 [1.658]
 [1.899]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.182]
 [7.182]
 [7.182]
 [7.182]
 [7.182]] [[1.912]
 [1.912]
 [1.912]
 [1.912]
 [1.912]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
507 2075
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.927]
 [0.927]
 [1.914]
 [0.94 ]
 [1.317]] [[0.559]
 [0.559]
 [1.193]
 [0.568]
 [0.81 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.673]
 [6.673]
 [6.673]
 [7.004]
 [7.437]] [[0.944]
 [0.944]
 [0.944]
 [1.081]
 [1.26 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
UNIT TEST: sample policy line 217 mcts : [0.333 0.    0.083 0.333 0.25 ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  522
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.454]
 [4.454]
 [4.454]
 [4.454]
 [4.454]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75434357
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.551]
 [3.551]
 [3.551]
 [3.551]
 [4.226]] [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [2.   ]]
siam score:  -0.75241196
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
527 2087
siam score:  -0.75013155
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7474257
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.793]
 [5.813]
 [4.793]
 [5.439]
 [5.364]] [[1.181]
 [1.693]
 [1.181]
 [1.506]
 [1.468]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.365]
 [6.365]
 [7.358]
 [6.9  ]
 [7.491]] [[0.802]
 [0.802]
 [1.134]
 [0.981]
 [1.178]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.967]
 [0.967]
 [2.898]
 [0.967]
 [1.922]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.7478442
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.7489907
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7459534
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.093]
 [2.044]
 [1.301]
 [3.247]
 [6.123]] [[0.398]
 [0.381]
 [0.117]
 [0.808]
 [1.828]]
first move QE:  0.10148743735165375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 6.088009445562386
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  552
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.11067587535243796
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7691487
siam score:  -0.7681456
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75963926
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.744]
 [2.744]
 [3.11 ]
 [2.744]
 [2.744]] [[1.463]
 [1.463]
 [1.747]
 [1.463]
 [1.463]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.287879155443513
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.108]
 [2.299]
 [3.2  ]
 [2.299]
 [2.299]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
in main func line 156:  558
siam score:  -0.75470185
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75528014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.133]
 [4.896]
 [4.896]
 [3.38 ]
 [3.865]] [[1.445]
 [1.953]
 [1.953]
 [0.944]
 [1.266]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[10.   ]
 [10.   ]
 [10.   ]
 [10.   ]
 [ 9.136]] [[2.   ]
 [2.   ]
 [2.   ]
 [2.   ]
 [1.671]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
564 2116
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.834]
 [0.258]
 [1.9  ]
 [1.375]
 [1.67 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.048]
 [0.259]
 [0.998]
 [1.048]
 [1.463]] [[1.032]
 [0.152]
 [0.977]
 [1.032]
 [1.495]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.292 0.333 0.333]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.667]
 [4.667]
 [4.667]
 [4.667]
 [4.667]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
line 256 mcts: sample exp_bonus 1.5051203068421963
siam score:  -0.75377053
siam score:  -0.75217706
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.74762374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7326319
UNIT TEST: sample policy line 217 mcts : [0.583 0.083 0.125 0.042 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.1260486728330674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.742]
 [6.895]
 [6.895]
 [6.936]
 [7.685]] [[1.334]
 [0.991]
 [0.991]
 [1.007]
 [1.311]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.71007246
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[9.846]
 [9.641]
 [7.93 ]
 [7.93 ]
 [8.541]] [[1.943]
 [1.868]
 [1.238]
 [1.238]
 [1.463]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
601 2136
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.087]
 [3.087]
 [3.087]
 [3.087]
 [3.087]] [[1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.155]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.49 ]
 [2.49 ]
 [2.49 ]
 [2.49 ]
 [2.653]] [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.491]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 6.264732803648712
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.2594793031839253
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus -0.02993856126356692
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.706699835503467
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.75816655
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.12]
 [3.12]
 [3.12]
 [3.12]
 [3.12]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.756]
 [4.354]
 [4.354]
 [3.21 ]
 [4.022]] [[0.916]
 [0.781]
 [0.781]
 [0.399]
 [0.67 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.717]
 [6.717]
 [6.717]
 [6.717]
 [6.717]] [[1.732]
 [1.732]
 [1.732]
 [1.732]
 [1.732]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.545]
 [5.829]
 [5.829]
 [5.829]
 [7.226]] [[1.653]
 [1.287]
 [1.287]
 [1.287]
 [2.   ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.113]
 [-0.249]
 [ 0.145]
 [-0.667]
 [-0.262]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.12050540757754034
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.713]
 [1.328]
 [1.548]
 [3.559]
 [4.901]] [[1.375]
 [0.285]
 [0.356]
 [1.003]
 [1.435]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.734]
 [4.734]
 [4.734]
 [4.734]
 [4.734]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75887513
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.372]
 [8.372]
 [8.372]
 [8.372]
 [8.185]] [[1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.867]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.219]
 [4.606]
 [5.219]
 [5.219]
 [5.153]] [[1.901]
 [1.476]
 [1.901]
 [1.901]
 [1.855]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.862]
 [-0.213]
 [ 0.516]
 [-0.133]
 [ 0.05 ]] [[0.   ]
 [0.24 ]
 [0.51 ]
 [0.27 ]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.827]
 [-0.274]
 [ 0.846]
 [-0.068]
 [ 0.004]] [[0.   ]
 [0.185]
 [0.559]
 [0.253]
 [0.278]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.402]
 [3.577]
 [3.577]
 [3.577]
 [3.577]] [[1.841]
 [1.463]
 [1.463]
 [1.463]
 [1.463]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.806]
 [5.21 ]
 [6.104]
 [5.21 ]
 [6.015]] [[1.366]
 [1.087]
 [1.506]
 [1.087]
 [1.464]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.507]
 [6.012]
 [6.102]
 [5.257]
 [5.915]] [[1.236]
 [1.47 ]
 [1.511]
 [1.121]
 [1.425]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 3.6087291733978737e-19
0.0 4.188703504836818e-20
0.0 4.188703504836818e-20
0.0 0.0
0.0 4.8331194286578665e-20
0.0 2.4487805105199856e-19
0.0 1.0310654781136781e-18
0.0 4.8331194286578665e-20
0.0 5.155327390568391e-20
0.0 3.060975638149982e-20
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.207]
 [4.732]
 [4.255]
 [3.207]
 [4.05 ]] [[0.654]
 [0.829]
 [0.67 ]
 [0.32 ]
 [0.601]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.048]
 [6.048]
 [6.048]
 [6.048]
 [6.048]] [[1.741]
 [1.741]
 [1.741]
 [1.741]
 [1.741]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.13392859688096778
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.245]
 [7.245]
 [9.264]
 [7.245]
 [8.611]] [[0.833]
 [0.833]
 [1.508]
 [0.833]
 [1.289]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75988007
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.819]
 [0.485]
 [1.102]
 [0.495]
 [1.331]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
first move QE:  0.13682108118590364
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.174]
 [6.174]
 [6.174]
 [6.174]
 [7.794]] [[1.164]
 [1.164]
 [1.164]
 [1.164]
 [1.862]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 8.096278190061282
656 2200
656 2201
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.7508278
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 6.779778088095767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.1457179442955176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76218504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 4.8331194286578665e-20
0.0 4.510911466747342e-20
0.0 1.5465982171705174e-19
0.0 0.0
0.0 0.0
0.0 1.0310654781136782e-19
0.0 0.0
0.0 0.0
0.0 7.732991085852587e-20
0.0 3.8664955429262935e-20
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.301]
 [5.301]
 [5.518]
 [5.301]
 [5.452]] [[1.654]
 [1.654]
 [1.737]
 [1.654]
 [1.712]]
line 256 mcts: sample exp_bonus 4.961808157529199
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.    0.417 0.    0.125 0.458]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.204]
 [-0.054]
 [ 0.203]
 [-0.475]
 [ 0.168]] [[0.14 ]
 [0.218]
 [0.35 ]
 [0.   ]
 [0.332]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.77096605
684 2224
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.081]
 [5.804]
 [5.886]
 [5.625]
 [5.868]] [[1.336]
 [1.662]
 [1.699]
 [1.582]
 [1.691]]
siam score:  -0.76746935
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76647204
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76811045
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7684839
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.1474868899282661
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76493126
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.315]
 [0.36 ]
 [0.411]
 [0.087]
 [0.313]] [[0.304]
 [0.364]
 [0.433]
 [0.   ]
 [0.301]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.150368051102961
siam score:  -0.7589758
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.982]
 [5.982]
 [5.982]
 [5.982]
 [8.   ]] [[0.677]
 [0.677]
 [0.677]
 [0.677]
 [1.351]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7650842
line 256 mcts: sample exp_bonus 3.7611139775125224
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.444]
 [2.975]
 [4.513]
 [4.083]
 [4.067]] [[1.361]
 [0.467]
 [1.403]
 [1.141]
 [1.132]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
719 2279
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
726 2294
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7786601
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.3465792962287924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.773]
 [3.773]
 [3.773]
 [3.773]
 [3.773]] [[1.044]
 [1.044]
 [1.044]
 [1.044]
 [1.044]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.42 ]
 [3.257]
 [3.42 ]
 [3.42 ]
 [3.42 ]] [[2.191]
 [2.   ]
 [2.191]
 [2.191]
 [2.191]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76598746
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
739 2309
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7856268
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.29909974465227734
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
744 2318
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.77417356
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]] [[1.05]
 [1.05]
 [1.05]
 [1.05]
 [1.05]]
Starting evaluation
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
755 2327
line 256 mcts: sample exp_bonus 5.209572838373676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7739076
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.412]
 [5.055]
 [4.412]
 [4.412]
 [4.947]] [[1.578]
 [1.992]
 [1.578]
 [1.578]
 [1.922]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]] [[0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7769302
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7808036
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.549]
 [3.549]
 [3.549]
 [3.549]
 [3.549]] [[3.549]
 [3.549]
 [3.549]
 [3.549]
 [3.549]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7834555
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.174]
 [4.174]
 [4.174]
 [4.174]
 [4.242]] [[1.085]
 [1.085]
 [1.085]
 [1.085]
 [1.108]]
first move QE:  0.1603756044545784
in main func line 156:  766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
767 2340
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.682]
 [5.072]
 [4.697]
 [5.072]
 [6.391]] [[1.119]
 [1.297]
 [1.127]
 [1.297]
 [1.898]]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.914]
 [4.914]
 [4.914]
 [4.914]
 [4.914]] [[1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8043526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  780
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7992965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.955]
 [2.366]
 [2.154]
 [2.023]
 [2.264]] [[0.349]
 [0.487]
 [0.416]
 [0.372]
 [0.452]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.648]
 [2.648]
 [2.648]
 [2.648]
 [2.639]] [[1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.946]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.1686235368279876
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
789 2355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7998378
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8011806
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
795 2362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.531]
 [3.531]
 [3.531]
 [3.531]
 [3.531]] [[1.741]
 [1.741]
 [1.741]
 [1.741]
 [1.741]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20026
first move QE:  0.16010058083182335
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.163]
 [3.163]
 [3.163]
 [3.163]
 [3.401]] [[1.106]
 [1.106]
 [1.106]
 [1.106]
 [1.264]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.147]
 [4.247]
 [3.309]
 [3.677]
 [4.099]] [[1.159]
 [1.878]
 [1.265]
 [1.506]
 [1.782]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.394]
 [3.394]
 [3.394]
 [3.394]
 [3.394]] [[0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.765]
 [2.225]
 [1.645]
 [0.765]
 [1.772]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.936]
 [4.936]
 [4.936]
 [4.936]
 [4.936]] [[0.9]
 [0.9]
 [0.9]
 [0.9]
 [0.9]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.1563114865809019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7987347
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.825]
 [1.825]
 [1.825]
 [1.825]
 [2.262]] [[1.383]
 [1.383]
 [1.383]
 [1.383]
 [1.676]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
812 2381
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.714]
 [3.714]
 [3.714]
 [3.714]
 [3.714]] [[1.95]
 [1.95]
 [1.95]
 [1.95]
 [1.95]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.15681897540660833
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.269]
 [4.269]
 [4.269]
 [4.269]
 [4.269]] [[1.352]
 [1.352]
 [1.352]
 [1.352]
 [1.352]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.123]
 [1.123]
 [1.123]
 [1.123]
 [1.123]] [[1.342]
 [1.342]
 [1.342]
 [1.342]
 [1.342]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  822
siam score:  -0.80799377
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.689]
 [4.591]
 [6.513]
 [7.285]
 [8.66 ]] [[1.442]
 [0.693]
 [1.157]
 [1.344]
 [1.676]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.174]
 [0.217]
 [1.316]
 [1.   ]
 [0.888]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.113]
 [0.155]
 [0.217]
 [0.174]
 [0.151]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.523]
 [3.209]
 [3.329]
 [3.329]
 [3.463]] [[1.293]
 [1.084]
 [1.164]
 [1.164]
 [1.253]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8160273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8118216
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.15595158898201933
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8137528
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.1566148128113572
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.137]
 [7.137]
 [7.137]
 [7.137]
 [8.574]] [[0.985]
 [0.985]
 [0.985]
 [0.985]
 [1.465]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.1568724212484503
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.395]
 [2.627]
 [2.395]
 [2.395]
 [2.395]] [[1.088]
 [1.24 ]
 [1.088]
 [1.088]
 [1.088]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.15699218652771216
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
856 2425
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81026703
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.914]
 [2.914]
 [2.914]
 [2.914]
 [3.248]] [[0.854]
 [0.854]
 [0.854]
 [0.854]
 [1.076]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.157728786653579
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.958]] [[1.24]
 [1.24]
 [1.24]
 [1.24]
 [1.24]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81707305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8129164
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.672]
 [3.475]
 [3.672]
 [3.672]
 [3.676]] [[1.795]
 [1.656]
 [1.795]
 [1.795]
 [1.798]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.206]
 [4.206]
 [4.206]
 [4.206]
 [4.206]] [[1.893]
 [1.893]
 [1.893]
 [1.893]
 [1.893]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.014]
 [8.014]
 [8.014]
 [8.014]
 [8.014]] [[1.959]
 [1.959]
 [1.959]
 [1.959]
 [1.959]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.83422333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  881
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.264]
 [-0.259]
 [-0.319]
 [-0.259]
 [-0.114]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
first move QE:  0.15909365202600245
siam score:  -0.8332343
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.82895553
siam score:  -0.8258218
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8277149
890 2455
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.902]
 [3.3  ]
 [3.3  ]
 [2.913]
 [2.854]] [[0.899]
 [1.031]
 [1.031]
 [0.902]
 [0.882]]
siam score:  -0.8274259
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.125]
 [6.125]
 [6.125]
 [6.125]
 [7.08 ]] [[1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.787]]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.435]
 [3.51 ]
 [3.435]
 [3.435]
 [3.612]] [[1.833]
 [1.903]
 [1.833]
 [1.833]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.152]
 [3.152]
 [3.152]
 [3.152]
 [3.582]] [[1.45 ]
 [1.45 ]
 [1.45 ]
 [1.45 ]
 [1.881]]
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.917 0.    0.083]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.79317397
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.79528356
siam score:  -0.7957504
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.923]
 [4.923]
 [4.923]
 [4.923]
 [5.964]] [[1.446]
 [1.446]
 [1.446]
 [1.446]
 [1.849]]
siam score:  -0.8005066
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
first move QE:  0.15739793153479748
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.394]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 1.9262710250859727
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.681]
 [3.577]
 [3.768]
 [3.289]
 [4.299]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
start point for exploration sampling:  20026
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.8481676980693935
rdn probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.304]
 [2.304]
 [2.304]
 [2.304]
 [2.93 ]] [[0.766]
 [0.766]
 [0.766]
 [0.766]
 [1.393]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.225]
 [3.225]
 [3.225]
 [3.225]
 [3.225]] [[1.826]
 [1.826]
 [1.826]
 [1.826]
 [1.826]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.607]
 [0.151]
 [1.352]
 [1.043]
 [1.165]] [[ 0.086]
 [-0.066]
 [ 0.335]
 [ 0.232]
 [ 0.273]]
first move QE:  0.1572081779254446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8030219
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8006701
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.3  ]
 [2.102]
 [2.3  ]
 [2.3  ]
 [2.347]] [[1.29 ]
 [1.092]
 [1.29 ]
 [1.29 ]
 [1.336]]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8026843
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
first move QE:  0.1566357114803943
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
929 2500
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
in main func line 156:  931
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 7.64746969503573
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.586]
 [2.076]
 [1.786]
 [1.786]
 [1.786]] [[0.833]
 [1.323]
 [1.033]
 [1.033]
 [1.033]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.711]
 [4.711]
 [4.27 ]
 [4.158]
 [4.857]] [[1.576]
 [1.576]
 [1.278]
 [1.202]
 [1.675]]
first move QE:  0.15668256473907635
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.525]
 [2.01 ]
 [2.266]
 [1.525]
 [1.525]] [[1.218]
 [1.604]
 [1.809]
 [1.218]
 [1.218]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.92 ]
 [2.621]
 [3.311]
 [2.302]
 [2.698]] [[1.499]
 [1.222]
 [1.861]
 [0.927]
 [1.293]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.675]
 [-0.058]
 [ 1.157]
 [ 0.675]
 [ 0.675]] [[0.682]
 [0.   ]
 [1.131]
 [0.682]
 [0.682]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.79896235
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.80041075
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8081343
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
955 2515
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.359]
 [1.185]
 [1.512]
 [1.359]
 [1.433]] [[0.796]
 [0.622]
 [0.949]
 [0.796]
 [0.87 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.643]
 [2.931]
 [2.732]
 [1.971]
 [3.428]] [[0.175]
 [1.164]
 [1.011]
 [0.427]
 [1.545]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
965 2521
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.626]
 [2.26 ]
 [1.626]
 [1.626]
 [1.626]] [[0.552]
 [1.187]
 [0.552]
 [0.552]
 [0.552]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.34 ]
 [2.34 ]
 [2.34 ]
 [2.34 ]
 [2.624]] [[1.229]
 [1.229]
 [1.229]
 [1.229]
 [1.513]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.036]
 [-0.036]
 [ 0.173]
 [-0.036]
 [-0.242]] [[0.334]
 [0.334]
 [0.543]
 [0.334]
 [0.127]]
siam score:  -0.8017804
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.78787595
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.159]
 [2.996]
 [3.098]
 [2.564]
 [3.554]] [[0.537]
 [1.117]
 [1.188]
 [0.818]
 [1.504]]
siam score:  -0.79029953
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
978 2531
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.186]
 [2.186]
 [2.186]
 [2.186]
 [2.186]] [[1.281]
 [1.281]
 [1.281]
 [1.281]
 [1.281]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.028]
 [3.028]
 [2.617]
 [3.059]
 [3.304]] [[0.928]
 [0.928]
 [0.791]
 [0.938]
 [1.02 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.042 0.042 0.792]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.81739485
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
line 256 mcts: sample exp_bonus -0.2802747288381303
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[1.385]
 [1.839]
 [2.099]
 [1.839]
 [2.105]] [[0.288]
 [0.742]
 [1.002]
 [0.742]
 [1.008]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
first move QE:  0.14649949503156268
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8336165
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.84331024
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.84309095
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.852]
 [2.852]
 [2.852]
 [2.852]
 [3.328]] [[1.602]
 [1.602]
 [1.602]
 [1.602]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.101]
 [0.711]
 [1.114]
 [0.711]
 [0.894]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.097]
 [2.097]
 [2.097]
 [2.097]
 [2.097]] [[1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
1009 2586
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
line 256 mcts: sample exp_bonus 1.5024290927268935
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
first move QE:  0.1424243629249249
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8277585
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.248]
 [-0.068]
 [ 0.835]
 [ 0.628]
 [ 0.505]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.83400595
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8340729
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
first move QE:  0.14124927059603798
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1018 2618
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.923]
 [1.098]
 [1.176]
 [0.923]
 [0.923]] [[1.041]
 [1.197]
 [1.267]
 [1.041]
 [1.041]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.139]
 [2.139]
 [2.139]
 [2.139]
 [2.728]] [[1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.853]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.48632555076002
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.453]
 [2.308]
 [2.525]
 [2.453]
 [2.474]] [[1.553]
 [1.407]
 [1.624]
 [1.553]
 [1.573]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.188]
 [2.414]
 [2.433]
 [2.188]
 [2.188]] [[0.972]
 [1.199]
 [1.218]
 [0.972]
 [0.972]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.665]
 [2.665]
 [2.665]
 [2.665]
 [2.932]] [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.572]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.82245207
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.338961765778053
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1033 2657
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
first move QE:  0.13449167215371188
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.238]
 [2.293]
 [2.437]
 [2.325]
 [2.777]] [[1.062]
 [1.117]
 [1.261]
 [1.149]
 [1.601]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.82578397
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
first move QE:  0.1343969985031516
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.292]
 [-0.292]
 [ 0.526]
 [-0.292]
 [-0.292]] [[0.501]
 [0.501]
 [1.059]
 [0.501]
 [0.501]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1059 2730
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.83250624
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
1062 2744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.62 ]
 [2.808]
 [1.633]
 [2.163]
 [1.875]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.12 ]
 [2.12 ]
 [2.12 ]
 [2.12 ]
 [2.181]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
siam score:  -0.83555204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
1066 2778
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.976]
 [0.815]
 [3.562]
 [0.976]
 [0.976]] [[0.714]
 [0.661]
 [1.57 ]
 [0.714]
 [0.714]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.086]
 [2.086]
 [2.086]
 [2.086]
 [2.086]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1077 2815
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
start point for exploration sampling:  20026
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
first move QE:  0.10682668962358467
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.312]
 [2.312]
 [2.312]
 [2.312]
 [2.373]] [[1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.099]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
line 256 mcts: sample exp_bonus 0.10325748089259404
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8280767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
in main func line 156:  1088
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-1.434]
 [-0.547]
 [-1.095]
 [-1.425]
 [-1.432]] [[0.   ]
 [0.887]
 [0.338]
 [0.008]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1090 2877
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
1091 2907
first move QE:  0.09596857066505694
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
using another actor
using another actor
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1094 2947
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.724]
 [-0.406]
 [ 0.015]
 [-0.823]
 [-0.788]] [[0.1  ]
 [0.417]
 [0.838]
 [0.   ]
 [0.036]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8341282
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.229]
 [0.115]
 [0.229]
 [0.229]
 [0.103]] [[0.885]
 [0.772]
 [0.885]
 [0.885]
 [0.76 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.578]
 [0.123]
 [0.24 ]
 [0.253]
 [0.089]] [[1.657]
 [1.203]
 [1.319]
 [1.333]
 [1.169]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8244482
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.296]
 [3.296]
 [3.296]
 [3.296]
 [4.023]] [[1.419]
 [1.419]
 [1.419]
 [1.419]
 [1.799]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
using another actor
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
1105 3001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [ 0.086]] [[0.  ]
 [0.  ]
 [0.  ]
 [0.  ]
 [0.09]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.202]
 [2.202]
 [2.202]
 [2.202]
 [2.587]] [[1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.637]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.831]
 [1.831]
 [1.831]
 [1.831]
 [1.777]] [[1.272]
 [1.272]
 [1.272]
 [1.272]
 [1.2  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.838106
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.281]
 [1.578]
 [2.281]
 [2.281]
 [2.281]] [[1.531]
 [0.829]
 [1.531]
 [1.531]
 [1.531]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.8651279564420706
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8347662
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.3556355639915196
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1121 3067
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8351132
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.833657
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.181]
 [1.384]
 [1.653]
 [1.547]
 [1.658]] [[0.301]
 [0.505]
 [0.773]
 [0.667]
 [0.778]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.225]
 [1.225]
 [1.225]
 [1.517]
 [1.737]] [[0.711]
 [0.711]
 [0.711]
 [1.101]
 [1.395]]
siam score:  -0.8314219
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8328595
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.791]
 [1.22 ]
 [1.826]
 [1.471]
 [1.59 ]] [[0.382]
 [0.191]
 [0.394]
 [0.275]
 [0.315]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.569]
 [2.44 ]
 [1.453]
 [1.327]
 [2.851]] [[0.845]
 [1.629]
 [0.741]
 [0.627]
 [2.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.217]
 [ 0.505]
 [ 0.364]
 [-0.331]
 [ 0.74 ]] [[0.489]
 [0.745]
 [0.62 ]
 [0.   ]
 [0.954]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.553]
 [2.553]
 [2.553]
 [2.553]
 [2.66 ]] [[1.577]
 [1.577]
 [1.577]
 [1.577]
 [1.663]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.556]
 [ 0.662]
 [-0.556]
 [-0.556]
 [-0.556]] [[0.327]
 [1.139]
 [0.327]
 [0.327]
 [0.327]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.82621765
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.1563944114865146
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  1151
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1151 3156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
1151 3160
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8389892
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.656]
 [1.403]
 [1.377]
 [1.203]
 [1.233]] [[1.053]
 [0.8  ]
 [0.774]
 [0.601]
 [0.63 ]]
siam score:  -0.8291078
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.8289926
siam score:  -0.82884634
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.088]
 [1.088]
 [1.52 ]
 [1.015]
 [1.288]] [[0.381]
 [0.381]
 [0.668]
 [0.332]
 [0.514]]
in main func line 156:  1163
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8268447
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.542 0.125 0.208]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.343]] [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.752]
 [-0.043]
 [-0.192]
 [-0.666]
 [-0.428]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.845]
 [5.308]
 [3.599]
 [2.734]
 [3.112]] [[0.691]
 [1.179]
 [0.609]
 [0.32 ]
 [0.446]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8061279
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.79 ]
 [2.792]
 [3.197]
 [3.217]
 [3.549]] [[0.652]
 [0.654]
 [0.98 ]
 [0.996]
 [1.263]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.029790667013645578
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8347306
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8389195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.929]
 [1.929]
 [1.929]
 [1.929]
 [1.929]] [[0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.80075485
siam score:  -0.7997171
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.731]
 [2.731]
 [2.731]
 [2.731]
 [2.469]] [[0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.577]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.684]
 [2.684]
 [2.684]
 [2.684]
 [2.684]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.02 ]
 [2.02 ]
 [2.02 ]
 [2.02 ]
 [2.064]] [[1.483]
 [1.483]
 [1.483]
 [1.483]
 [1.541]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.249]
 [2.105]
 [2.105]
 [1.219]
 [2.005]] [[0.17 ]
 [0.456]
 [0.456]
 [0.16 ]
 [0.422]]
siam score:  -0.7916327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
first move QE:  0.06597622481619693
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
1195 3227
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.773]
 [1.773]
 [2.258]
 [1.569]
 [1.673]] [[0.62 ]
 [0.62 ]
 [0.943]
 [0.484]
 [0.554]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.499]
 [0.979]
 [1.176]
 [0.637]
 [1.015]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.137]
 [2.137]
 [2.137]
 [2.137]
 [2.137]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.722]
 [1.613]
 [2.305]
 [1.985]
 [2.178]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
line 256 mcts: sample exp_bonus 2.6186118113280616
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.387]
 [2.387]
 [2.182]
 [1.953]
 [1.483]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.127]
 [1.283]
 [1.127]
 [1.127]
 [1.815]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.508]
 [0.508]
 [0.998]
 [0.508]
 [0.508]] [[0.901]
 [0.901]
 [1.227]
 [0.901]
 [0.901]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.191]
 [ 0.634]
 [ 1.126]
 [ 0.634]
 [ 0.488]] [[0.   ]
 [0.55 ]
 [0.877]
 [0.55 ]
 [0.452]]
siam score:  -0.7895549
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.7917766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
siam score:  -0.79359025
siam score:  -0.79478294
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8036511
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.80248094
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.25  0.375 0.292]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.162]
 [2.162]
 [2.449]
 [2.162]
 [2.162]] [[0.601]
 [0.601]
 [0.792]
 [0.601]
 [0.601]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
siam score:  -0.8097643
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[1.674]
 [1.624]
 [0.879]
 [1.442]
 [1.534]] [[0.706]
 [0.672]
 [0.175]
 [0.55 ]
 [0.612]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.066]
 [2.163]
 [2.258]
 [2.066]
 [2.35 ]] [[1.179]
 [1.243]
 [1.306]
 [1.179]
 [1.367]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.003]
 [0.002]] [[2.379]
 [2.379]
 [2.379]
 [2.43 ]
 [2.379]] [[1.564]
 [1.564]
 [1.564]
 [1.617]
 [1.564]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
1229 3272
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.751]
 [2.751]
 [2.751]
 [2.751]
 [2.672]] [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.871]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.164]
 [2.164]
 [2.164]
 [2.164]
 [3.21 ]] [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.671]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.211]
 [2.219]
 [2.343]
 [2.545]
 [2.196]] [[0.785]
 [0.791]
 [0.873]
 [1.008]
 [0.775]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[3.241]
 [3.241]
 [3.241]
 [3.241]
 [3.241]] [[1.262]
 [1.262]
 [1.262]
 [1.262]
 [1.262]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.519]
 [2.519]
 [2.519]
 [2.519]
 [2.829]] [[1.494]
 [1.494]
 [1.494]
 [1.494]
 [1.789]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.4201414527418703
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
1244 3281
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.716]
 [2.716]
 [1.889]
 [2.324]
 [2.716]] [[1.25 ]
 [1.25 ]
 [0.699]
 [0.989]
 [1.25 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.265]
 [1.432]
 [3.265]
 [2.383]
 [1.97 ]] [[1.722]
 [0.501]
 [1.722]
 [1.134]
 [0.859]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.81175303
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.811454
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8108995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.8078742
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  1255
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.292 0.417 0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.29 ]
 [0.564]
 [0.325]
 [0.929]
 [1.582]] [[0.338]
 [0.551]
 [0.365]
 [0.836]
 [1.345]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.555]
 [1.555]
 [1.555]
 [1.555]
 [1.555]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
using explorer policy with actor:  1
using another actor
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.801]
 [1.801]
 [1.801]
 [1.801]
 [1.801]] [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.089]
 [1.089]
 [1.089]
 [1.089]
 [1.084]] [[0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.378]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.305]
 [1.305]
 [1.305]
 [1.305]
 [1.305]] [[0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]] [[0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.189]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]]
siam score:  -0.8214763
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.81961024
siam score:  -0.8172386
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.708 0.    0.292]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.915]
 [1.915]
 [1.915]
 [1.915]
 [2.03 ]] [[0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.875]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.239]
 [2.239]
 [2.234]
 [2.239]
 [2.149]] [[0.698]
 [0.698]
 [0.694]
 [0.698]
 [0.638]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
1282 3337
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[3.305]
 [3.305]
 [1.808]
 [3.305]
 [1.897]] [[1.533]
 [1.533]
 [0.536]
 [1.533]
 [0.595]]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4969717424195885
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8299847
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[1.274]
 [1.372]
 [1.419]
 [1.26 ]
 [4.282]] [[0.158]
 [0.217]
 [0.247]
 [0.149]
 [2.   ]]
siam score:  -0.8216114
1291 3348
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
first move QE:  0.07560912350673507
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8123578
siam score:  -0.8117239
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.235]
 [1.235]
 [2.763]
 [1.235]
 [1.235]] [[0.747]
 [0.747]
 [1.764]
 [0.747]
 [0.747]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8104457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Sims:  25 1 epoch:  87271 pick best:  False frame count:  87271
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.382]
 [2.382]
 [2.484]
 [2.382]
 [2.265]] [[0.745]
 [0.745]
 [0.812]
 [0.745]
 [0.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.83235526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.392]
 [2.392]
 [2.392]
 [2.057]
 [2.454]] [[0.752]
 [0.752]
 [0.752]
 [0.529]
 [0.793]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[2.008]
 [1.588]
 [2.199]
 [2.008]
 [2.249]] [[0.497]
 [0.217]
 [0.624]
 [0.497]
 [0.658]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.855]
 [0.421]
 [0.6  ]
 [0.6  ]
 [0.159]] [[0.854]
 [0.565]
 [0.684]
 [0.684]
 [0.391]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
first move QE:  0.08156910461380039
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]] [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.84503996
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
1315 3383
siam score:  -0.8424394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.8423438
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.227602425002261
line 256 mcts: sample exp_bonus 0.2658101318679224
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.3145913975375304
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.6156951213326205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
1320 3395
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.167]
 [1.392]
 [0.961]
 [0.425]
 [0.736]] [[1.   ]
 [1.149]
 [0.862]
 [0.505]
 [0.712]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.4532620292819716
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.473]
 [1.172]
 [0.352]
 [0.473]
 [0.973]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[ 0.052]
 [ 0.   ]
 [ 0.044]
 [ 0.04 ]
 [-0.081]] [[0.111]
 [0.076]
 [0.106]
 [0.103]
 [0.023]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
1338 3452
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
1341 3468
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8597589
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.001]
 [0.001]
 [0.001]] [[0.883]
 [1.054]
 [0.902]
 [0.746]
 [1.087]] [[0.002]
 [0.002]
 [0.001]
 [0.001]
 [0.001]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.214]
 [1.214]
 [1.18 ]
 [1.088]
 [1.026]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.865]
 [0.939]
 [1.61 ]
 [2.081]
 [1.612]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
1344 3479
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.645]
 [1.645]
 [1.499]
 [1.645]
 [1.645]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.989]
 [2.049]
 [3.825]
 [2.049]
 [2.819]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.002]
 [0.002]] [[1.586]
 [1.441]
 [1.738]
 [2.182]
 [2.966]] [[0.508]
 [0.412]
 [0.611]
 [0.907]
 [1.428]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.   ]
 [0.005]
 [0.003]
 [0.002]] [[2.232]
 [2.624]
 [4.464]
 [2.412]
 [4.033]] [[0.003]
 [0.   ]
 [0.005]
 [0.003]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
rdn probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
line 256 mcts: sample exp_bonus 2.6846077032088043
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
1348 3482
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.053]
 [0.053]
 [0.583]
 [0.053]
 [0.053]] [[0.613]
 [0.613]
 [0.966]
 [0.613]
 [0.613]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
1348 3488
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.8562631763784685
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
1352 3497
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
siam score:  -0.8743518
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
1356 3514
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.87676
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.871]
 [1.871]
 [1.871]
 [1.871]
 [1.871]] [[1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.118]
 [1.118]
 [1.118]
 [1.118]
 [1.118]] [[0.83]
 [0.83]
 [0.83]
 [0.83]
 [0.83]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.82 ]
 [1.684]
 [1.684]
 [1.684]
 [1.684]] [[1.53 ]
 [1.394]
 [1.394]
 [1.394]
 [1.394]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.08 ]
 [0.231]
 [0.253]
 [0.17 ]
 [0.08 ]] [[0.283]
 [0.383]
 [0.398]
 [0.343]
 [0.283]]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
siam score:  -0.8823164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
UNIT TEST: sample policy line 217 mcts : [0.167 0.083 0.083 0.458 0.208]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
line 256 mcts: sample exp_bonus 1.0026811956951012
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
first move QE:  0.09027445228693111
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.068]
 [1.17 ]
 [0.895]
 [1.17 ]
 [1.17 ]] [[0.001]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[1.198]
 [0.56 ]
 [0.837]
 [0.56 ]
 [0.231]] [[0.828]
 [0.404]
 [0.588]
 [0.404]
 [0.185]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[2.852]
 [2.852]
 [2.852]
 [2.852]
 [2.812]] [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.554]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.2179038434093137
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.002]
 [0.002]] [[1.825]
 [1.946]
 [2.25 ]
 [3.857]
 [2.417]] [[0.453]
 [0.533]
 [0.736]
 [1.807]
 [0.848]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.9045792
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.042 0.042 0.792]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.8931923
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
first move QE:  0.09707809918721513
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0053],
        [0.0068],
        [0.0049],
        [0.0050],
        [0.0186],
        [0.0067],
        [0.0011],
        [0.0068],
        [0.0015],
        [0.0016]], dtype=torch.float64)
0.0 0.005349616819118033
0.0 0.006792973124358146
0.0 0.004875471084271284
0.0 0.005042184420823834
0.0 0.018580990333031837
0.0 0.006682412702255212
0.0 0.0011198044872048904
0.0 0.006792973124358146
0.0 0.0014793171925380673
0.0 0.0016493963289897127
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.09723755387304454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[2.516]
 [3.765]
 [3.765]
 [3.936]
 [3.999]] [[0.448]
 [1.281]
 [1.281]
 [1.395]
 [1.438]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.003]
 [0.005]
 [0.005]] [[1.959]
 [1.959]
 [2.647]
 [4.081]
 [1.959]] [[0.66 ]
 [0.66 ]
 [1.091]
 [2.004]
 [0.66 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[2.951]
 [2.951]
 [2.986]
 [2.808]
 [2.98 ]] [[1.158]
 [1.158]
 [1.182]
 [1.063]
 [1.178]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
siam score:  -0.9160159
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[2.341]
 [2.341]
 [2.341]
 [2.341]
 [2.341]] [[0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[2.555]
 [2.555]
 [2.492]
 [2.555]
 [2.472]] [[0.914]
 [0.914]
 [0.871]
 [0.914]
 [0.858]]
siam score:  -0.9291243
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.013]
 [0.014]
 [0.015]
 [0.015]] [[1.541]
 [1.66 ]
 [2.   ]
 [1.744]
 [1.776]] [[0.444]
 [0.521]
 [0.749]
 [0.582]
 [0.602]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.25  0.333 0.375]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.9469007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
line 256 mcts: sample exp_bonus 1.705715206452867
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
1401 3577
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.10343413821345647
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
first move QE:  0.10247256729434215
1407 3592
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.003]
 [0.   ]] [[0.245]
 [0.377]
 [0.417]
 [1.274]
 [1.171]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.003]
 [0.   ]]
using another actor
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
1409 3599
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
1410 3621
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.316]
 [1.   ]
 [0.968]
 [1.14 ]
 [0.25 ]] [[0.208]
 [0.892]
 [0.859]
 [1.032]
 [0.142]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[1.486]
 [1.486]
 [1.486]
 [1.486]
 [1.486]] [[0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.008]] [[1.786]
 [1.786]
 [1.786]
 [1.786]
 [2.249]] [[0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.713]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.006]] [[2.709]
 [2.709]
 [2.709]
 [2.709]
 [2.763]] [[0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.991]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
first move QE:  0.10790504132997147
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.007]
 [0.008]
 [0.008]
 [0.008]] [[1.078]
 [1.309]
 [2.051]
 [1.23 ]
 [1.35 ]] [[0.225]
 [0.377]
 [0.873]
 [0.325]
 [0.405]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[1.479]
 [1.479]
 [2.045]
 [1.479]
 [1.627]] [[0.703]
 [0.703]
 [1.081]
 [0.703]
 [0.803]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.009]
 [0.01 ]
 [0.01 ]
 [0.009]] [[0.433]
 [0.728]
 [0.433]
 [0.433]
 [0.492]] [[0.211]
 [0.406]
 [0.211]
 [0.211]
 [0.249]]
siam score:  -0.94813555
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.4978777866501566
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
1430 3697
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
line 256 mcts: sample exp_bonus -0.22538352760823407
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.002]
 [0.002]
 [0.002]] [[-0.205]
 [ 0.761]
 [-0.205]
 [-0.205]
 [-0.205]] [[0.472]
 [1.112]
 [0.472]
 [0.472]
 [0.472]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
1432 3782
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.09843842802712541
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[1.307]
 [1.463]
 [1.981]
 [1.67 ]
 [1.718]] [[0.48 ]
 [0.636]
 [1.153]
 [0.843]
 [0.891]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.004]
 [0.006]
 [0.006]] [[1.137]
 [1.137]
 [1.131]
 [1.194]
 [0.968]] [[1.787]
 [1.787]
 [1.778]
 [1.845]
 [1.616]]
siam score:  -0.92504156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9270433
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.92676353
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.0964806773862345
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[1.254]
 [1.254]
 [1.638]
 [1.254]
 [1.875]] [[0.376]
 [0.376]
 [0.76 ]
 [0.376]
 [0.997]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.005]
 [0.004]
 [0.005]] [[1.947]
 [1.947]
 [1.892]
 [1.947]
 [1.963]] [[1.148]
 [1.148]
 [1.094]
 [1.148]
 [1.165]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.001]
 [0.005]
 [0.003]
 [0.004]] [[-0.847]
 [-0.153]
 [-0.504]
 [-0.843]
 [-1.52 ]] [[0.45 ]
 [0.907]
 [0.682]
 [0.453]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[2.636]
 [2.636]
 [2.636]
 [2.636]
 [2.639]] [[0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.703]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.007]] [[1.942]
 [1.942]
 [1.796]
 [1.942]
 [2.162]] [[1.125]
 [1.125]
 [0.982]
 [1.125]
 [1.339]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.2387715921839333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.003]
 [0.003]] [[-0.239]
 [-0.121]
 [-0.021]
 [ 0.   ]
 [ 0.   ]] [[0.001]
 [0.078]
 [0.145]
 [0.163]
 [0.163]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.1  ]
 [0.094]
 [0.097]
 [0.231]
 [0.091]] [[0.881]
 [0.875]
 [0.878]
 [1.012]
 [0.872]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.1980739644131853
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
1449 3872
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.09177950302356244
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.7204377729595817
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.006]
 [0.006]
 [0.005]] [[1.095]
 [1.095]
 [1.628]
 [1.529]
 [1.095]] [[0.245]
 [0.245]
 [0.602]
 [0.536]
 [0.245]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1451 3921
1451 3923
from probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.08939641040318982
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.9191524
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[2.262]
 [1.715]
 [1.715]
 [1.715]
 [1.715]] [[1.867]
 [1.541]
 [1.541]
 [1.541]
 [1.541]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.8014675834920141
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1461 3953
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.006]
 [0.007]
 [0.007]
 [0.007]] [[0.971]
 [0.331]
 [0.97 ]
 [0.701]
 [1.222]] [[0.218]
 [0.004]
 [0.219]
 [0.128]
 [0.303]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.004]
 [0.004]
 [0.005]] [[1.944]
 [1.54 ]
 [1.54 ]
 [1.54 ]
 [1.91 ]] [[0.341]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.0885359470202625
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1464 3967
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.027]
 [0.108]
 [0.027]
 [0.027]
 [0.027]] [[0.315]
 [0.34 ]
 [0.315]
 [0.315]
 [0.315]]
1466 3974
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1467 3983
siam score:  -0.89940935
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.208 0.042 0.333 0.083 0.333]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
1472 4005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[1.927]
 [1.927]
 [1.927]
 [1.927]
 [1.927]] [[1.212]
 [1.212]
 [1.212]
 [1.212]
 [1.212]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.125 0.125 0.583 0.083 0.083]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[1.125]
 [1.455]
 [1.125]
 [1.125]
 [1.606]] [[0.382]
 [0.709]
 [0.382]
 [0.382]
 [0.862]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1487 4049
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.8763849
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.004]] [[1.046]
 [1.048]
 [0.969]
 [1.017]
 [1.248]] [[0.754]
 [0.755]
 [0.651]
 [0.716]
 [1.025]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[1.76]
 [1.76]
 [1.76]
 [1.76]
 [1.76]] [[1.72]
 [1.72]
 [1.72]
 [1.72]
 [1.72]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[1.901]
 [1.901]
 [1.901]
 [1.901]
 [1.901]] [[1.32]
 [1.32]
 [1.32]
 [1.32]
 [1.32]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[1.969]
 [1.969]
 [1.969]
 [1.969]
 [1.969]] [[0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]]
line 256 mcts: sample exp_bonus 2.6430870064754948
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.005]
 [0.004]
 [0.004]] [[2.023]
 [2.429]
 [2.596]
 [2.533]
 [2.429]] [[0.534]
 [0.805]
 [0.918]
 [0.875]
 [0.805]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.003]
 [0.003]
 [0.004]
 [0.003]] [[2.374]
 [2.388]
 [2.01 ]
 [2.099]
 [1.986]] [[0.956]
 [0.964]
 [0.713]
 [0.774]
 [0.697]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.003]
 [0.004]
 [0.004]
 [0.004]] [[2.377]
 [2.152]
 [3.043]
 [2.688]
 [3.861]] [[0.452]
 [0.301]
 [0.895]
 [0.658]
 [1.44 ]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.003]
 [0.003]
 [0.003]] [[2.209]
 [1.468]
 [3.954]
 [2.049]
 [1.94 ]] [[0.836]
 [0.429]
 [1.798]
 [0.748]
 [0.688]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.001]
 [0.003]
 [0.003]
 [0.003]] [[ 0.052]
 [ 0.586]
 [ 0.128]
 [-0.074]
 [-0.21 ]] [[0.263]
 [0.794]
 [0.341]
 [0.138]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.08685248020854779
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.08690382978035122
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8707169
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.006]
 [0.003]
 [0.003]] [[-0.157]
 [-0.157]
 [ 0.587]
 [-0.157]
 [-0.157]] [[0.14 ]
 [0.14 ]
 [0.394]
 [0.14 ]
 [0.14 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.029]] [[5.744]
 [5.744]
 [5.744]
 [5.744]
 [5.595]] [[1.029]
 [1.029]
 [1.029]
 [1.029]
 [0.934]]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.002]] [[2.155]
 [2.155]
 [2.173]
 [2.155]
 [2.583]] [[0.98 ]
 [0.98 ]
 [0.996]
 [0.98 ]
 [1.349]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
using explorer policy with actor:  1
1503 4091
siam score:  -0.8707241
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.003]
 [0.004]
 [0.004]
 [0.004]] [[2.282]
 [1.888]
 [2.122]
 [1.953]
 [2.013]] [[1.141]
 [0.827]
 [1.014]
 [0.88 ]
 [0.928]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using another actor
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
1506 4093
1506 4094
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.89553565
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.9015889
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[1.314]
 [1.314]
 [1.314]
 [1.314]
 [1.314]] [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.012]
 [0.013]
 [0.012]] [[1.102]
 [1.102]
 [1.331]
 [1.102]
 [1.219]] [[0.403]
 [0.403]
 [0.553]
 [0.403]
 [0.48 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]] [[1.74 ]
 [1.74 ]
 [1.842]
 [1.74 ]
 [1.74 ]] [[1.212]
 [1.212]
 [1.314]
 [1.212]
 [1.212]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
1515 4136
siam score:  -0.938302
siam score:  -0.9394117
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.037]
 [0.036]
 [0.037]
 [0.039]] [[1.314]
 [1.342]
 [0.993]
 [1.342]
 [1.596]] [[0.486]
 [0.527]
 [0.294]
 [0.527]
 [0.7  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]] [[0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]] [[0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.022]
 [0.024]
 [0.024]] [[1.233]
 [1.233]
 [1.064]
 [1.233]
 [1.233]] [[0.277]
 [0.277]
 [0.217]
 [0.277]
 [0.277]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.027]
 [0.022]
 [0.022]] [[0.288]
 [0.288]
 [0.776]
 [0.288]
 [0.288]] [[0.338]
 [0.338]
 [0.509]
 [0.338]
 [0.338]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
line 256 mcts: sample exp_bonus 0.9991811262196931
siam score:  -0.93854815
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
1526 4168
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
first move QE:  0.08462738967053932
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
1528 4176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.9318697
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[ 1.767]
 [ 0.011]
 [-0.401]
 [-0.401]
 [-0.401]] [[0.978]
 [0.395]
 [0.258]
 [0.258]
 [0.258]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
first move QE:  0.08344520342835302
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.001]
 [0.014]
 [0.009]
 [0.009]] [[-1.114]
 [-0.25 ]
 [-0.395]
 [-0.856]
 [-1.051]] [[0.006]
 [0.001]
 [0.014]
 [0.009]
 [0.009]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.   ]
 [0.012]
 [0.007]
 [0.007]] [[-1.102]
 [-0.348]
 [-0.461]
 [-0.723]
 [-0.932]] [[0.003]
 [0.245]
 [0.231]
 [0.134]
 [0.063]]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.014]
 [0.013]] [[1.511]
 [1.511]
 [1.511]
 [1.55 ]
 [1.511]] [[0.331]
 [0.331]
 [0.331]
 [0.345]
 [0.331]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
1534 4220
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using another actor
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.40155710009777523
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.001]
 [0.004]
 [0.006]
 [0.006]] [[1.133]
 [2.029]
 [1.161]
 [1.629]
 [1.394]] [[0.182]
 [0.474]
 [0.192]
 [0.351]
 [0.272]]
siam score:  -0.9163685
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.004]
 [0.004]
 [0.004]] [[1.633]
 [0.978]
 [1.345]
 [1.199]
 [1.201]] [[0.333]
 [0.113]
 [0.235]
 [0.187]
 [0.187]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
using explorer policy with actor:  1
using explorer policy with actor:  1
first move QE:  0.07353368216818265
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[1.59 ]
 [1.59 ]
 [1.59 ]
 [1.801]
 [1.59 ]] [[0.208]
 [0.208]
 [0.208]
 [0.279]
 [0.208]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.006]
 [0.007]
 [0.007]
 [0.007]] [[1.47 ]
 [1.151]
 [2.212]
 [2.154]
 [1.989]] [[0.13 ]
 [0.021]
 [0.377]
 [0.358]
 [0.303]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20026
siam score:  -0.89862776
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[2.495]
 [2.495]
 [2.958]
 [2.495]
 [2.495]] [[1.189]
 [1.189]
 [1.709]
 [1.189]
 [1.189]]
siam score:  -0.8998503
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.014]
 [0.015]
 [0.015]] [[0.497]
 [0.497]
 [1.077]
 [0.497]
 [0.497]] [[0.638]
 [0.638]
 [0.991]
 [0.638]
 [0.638]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.003]
 [0.001]] [[-0.209]
 [ 0.209]
 [-0.053]
 [-0.518]
 [-0.088]] [[0.101]
 [0.24 ]
 [0.152]
 [0.002]
 [0.143]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[-0.723]
 [-0.723]
 [-0.723]
 [-0.723]
 [-0.723]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0162],
        [0.0165],
        [0.0112],
        [0.0078],
        [0.0416],
        [0.0093],
        [0.0188],
        [0.0305],
        [0.0188],
        [0.0003]], dtype=torch.float64)
0.0 0.016182718753157976
0.0 0.016510708355440024
0.0 0.011196724676041598
0.0 0.007819864723945188
0.0 0.04157313817263755
0.0 0.009290868819745127
0.0 0.01880757180762786
0.0 0.030536936728710044
0.0 0.01880757180762786
0.0 0.000253348604549368
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.013]
 [0.014]
 [0.013]
 [0.01 ]] [[-0.414]
 [ 0.438]
 [ 0.843]
 [ 0.438]
 [-0.068]] [[0.003]
 [0.744]
 [1.094]
 [0.744]
 [0.305]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using another actor
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.91178465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
first move QE:  0.07403898721025313
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7854939608539441, 0.07150201304868534, 0.07150201304868534, 0.07150201304868534]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.001]
 [0.016]
 [0.014]
 [0.01 ]] [[0.984]
 [1.6  ]
 [1.041]
 [1.013]
 [1.492]] [[0.012]
 [0.001]
 [0.016]
 [0.014]
 [0.01 ]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
line 256 mcts: sample exp_bonus 1.3871135780469175
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[1.889]
 [1.889]
 [1.889]
 [1.889]
 [1.889]] [[0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]]
rdn probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
first move QE:  0.07444056088932553
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
first move QE:  0.07453994443029979
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.01 ]
 [0.012]
 [0.015]
 [0.013]] [[1.651]
 [2.077]
 [2.115]
 [1.648]
 [2.016]] [[0.38 ]
 [0.658]
 [0.689]
 [0.385]
 [0.625]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.91070575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
1584 4383
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.07400751500198124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.003]
 [0.005]
 [0.004]] [[1.583]
 [1.583]
 [1.872]
 [1.896]
 [1.857]] [[0.217]
 [0.217]
 [0.312]
 [0.323]
 [0.309]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
from probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.006]
 [0.004]
 [0.004]] [[0.485]
 [0.485]
 [1.026]
 [0.789]
 [0.866]] [[0.004]
 [0.004]
 [0.006]
 [0.004]
 [0.004]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
siam score:  -0.90141815
from probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.003]
 [0.002]
 [0.012]
 [0.014]] [[ 0.004]
 [ 0.195]
 [-0.14 ]
 [ 0.314]
 [ 2.188]] [[0.057]
 [0.115]
 [0.001]
 [0.173]
 [0.802]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.014]] [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [1.608]] [[0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.556]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
1612 4410
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.417 0.167 0.333]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
siam score:  -0.91617984
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.088]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.007]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
siam score:  -0.92076284
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
siam score:  -0.92084223
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
first move QE:  0.0731454604846254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855123173962325, 0.07149589420125589, 0.07149589420125589, 0.07149589420125589]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.023]
 [0.018]
 [0.018]] [[-0.215]
 [-0.215]
 [ 0.111]
 [-0.215]
 [-0.215]] [[0.2  ]
 [0.2  ]
 [0.319]
 [0.2  ]
 [0.2  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.039]
 [0.035]
 [0.035]
 [0.036]] [[1.372]
 [1.456]
 [1.183]
 [1.183]
 [1.455]] [[0.338]
 [0.355]
 [0.254]
 [0.254]
 [0.348]]
siam score:  -0.92131895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.221]
 [0.261]
 [0.261]
 [0.245]] [[4.107]
 [5.535]
 [5.309]
 [5.309]
 [4.235]] [[0.84 ]
 [1.484]
 [1.489]
 [1.489]
 [1.098]]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.024]
 [0.026]
 [0.026]] [[0.658]
 [0.658]
 [1.143]
 [0.658]
 [0.658]] [[0.405]
 [0.405]
 [0.562]
 [0.405]
 [0.405]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0047376318826493
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
first move QE:  0.07195548958203367
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.02 ]
 [0.022]
 [0.023]
 [0.026]] [[0.683]
 [1.185]
 [1.321]
 [1.025]
 [1.048]] [[0.093]
 [0.253]
 [0.3  ]
 [0.205]
 [0.219]]
from probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
line 256 mcts: sample exp_bonus -0.2100884599677591
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
siam score:  -0.91086215
siam score:  -0.9109131
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.002]
 [0.015]
 [0.016]
 [0.013]] [[1.076]
 [0.296]
 [1.25 ]
 [1.584]
 [1.202]] [[0.341]
 [0.055]
 [0.398]
 [0.513]
 [0.38 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
siam score:  -0.90706193
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
siam score:  -0.90471923
maxi score, test score, baseline:  0.0001 0.0 0.0001
1638 4474
1638 4475
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.002]
 [0.012]
 [0.01 ]
 [0.01 ]] [[-0.388]
 [ 0.288]
 [-0.375]
 [-0.668]
 [-0.588]] [[0.124]
 [0.337]
 [0.136]
 [0.034]
 [0.061]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
first move QE:  0.06977871486876566
siam score:  -0.90734464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
1641 4502
line 256 mcts: sample exp_bonus 1.4598839179932903
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
siam score:  -0.9075194
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[-0.538]
 [-0.409]
 [-0.409]
 [-0.409]
 [-0.409]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
from probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
using another actor
from probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.   ]
 [0.026]
 [0.008]
 [0.01 ]] [[-0.68 ]
 [-0.206]
 [ 0.106]
 [-0.43 ]
 [-0.451]] [[0.006]
 [0.148]
 [0.304]
 [0.089]
 [0.086]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.008]
 [0.011]
 [0.013]
 [0.011]] [[1.313]
 [1.59 ]
 [1.512]
 [1.419]
 [1.534]] [[0.383]
 [0.561]
 [0.516]
 [0.456]
 [0.53 ]]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
using explorer policy with actor:  1
1646 4526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855184365233342, 0.07149385449222193, 0.07149385449222193, 0.07149385449222193]
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.039]
 [0.015]
 [0.023]
 [0.026]] [[0.314]
 [1.002]
 [1.119]
 [0.575]
 [0.681]] [[0.002]
 [0.306]
 [0.298]
 [0.133]
 [0.174]]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.027]
 [0.023]
 [0.027]
 [0.027]] [[0.461]
 [0.461]
 [0.756]
 [0.461]
 [0.461]] [[0.322]
 [0.322]
 [0.412]
 [0.322]
 [0.322]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4802316545125058
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
using another actor
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
siam score:  -0.9322798
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
in main func line 156:  1657
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
siam score:  -0.9307161
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
from probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
siam score:  -0.9282791
from probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2784497828400323
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.032]
 [0.036]
 [0.032]
 [0.032]] [[1.026]
 [1.026]
 [1.162]
 [1.026]
 [1.026]] [[0.193]
 [0.193]
 [0.246]
 [0.193]
 [0.193]]
siam score:  -0.92798644
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
siam score:  -0.9273003
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.01 ]
 [0.018]
 [0.02 ]
 [0.019]] [[1.195]
 [0.745]
 [1.223]
 [1.245]
 [1.114]] [[0.019]
 [0.01 ]
 [0.018]
 [0.02 ]
 [0.019]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9176063407547348
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.017]
 [0.019]
 [0.02 ]
 [0.019]] [[0.951]
 [0.953]
 [0.905]
 [1.006]
 [0.799]] [[0.02 ]
 [0.017]
 [0.019]
 [0.02 ]
 [0.019]]
from probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.005]
 [0.013]
 [0.01 ]
 [0.016]] [[2.3  ]
 [1.989]
 [1.927]
 [1.834]
 [1.814]] [[0.482]
 [0.362]
 [0.359]
 [0.321]
 [0.326]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
siam score:  -0.92069304
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
first move QE:  0.06372088236609268
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
from probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
siam score:  -0.9242038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[2.409]
 [2.409]
 [2.409]
 [2.409]
 [2.409]] [[0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.083 0.208 0.625 0.042 0.042]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855214961393265, 0.07149283462022454, 0.07149283462022454, 0.07149283462022454]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855233319257033, 0.07149222269143225, 0.07149222269143225, 0.07149222269143225]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855233319257033, 0.07149222269143225, 0.07149222269143225, 0.07149222269143225]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855233319257033, 0.07149222269143225, 0.07149222269143225, 0.07149222269143225]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855245557902804, 0.07149181473657326, 0.07149181473657326, 0.07149181473657326]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855245557902804, 0.07149181473657326, 0.07149181473657326, 0.07149181473657326]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855245557902804, 0.07149181473657326, 0.07149181473657326, 0.07149181473657326]
in main func line 156:  1679
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7855245557902804, 0.07149181473657326, 0.07149181473657326, 0.07149181473657326]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
siam score:  -0.9184066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.001]
 [0.004]
 [0.004]
 [0.004]] [[-0.07 ]
 [ 0.286]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]] [[0.231]
 [0.581]
 [0.231]
 [0.231]
 [0.231]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.185]
 [0.184]
 [0.184]
 [0.184]] [[4.993]
 [4.586]
 [4.993]
 [4.993]
 [4.993]] [[1.569]
 [1.436]
 [1.569]
 [1.569]
 [1.569]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
using another actor
first move QE:  0.06109828902943817
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.259]
 [0.246]
 [0.246]
 [0.2  ]] [[5.785]
 [6.7  ]
 [7.178]
 [7.178]
 [6.043]] [[1.153]
 [1.536]
 [1.653]
 [1.653]
 [1.242]]
siam score:  -0.9226865
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
siam score:  -0.92309767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]] [[1.208]
 [1.208]
 [1.208]
 [1.208]
 [1.208]] [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.038]
 [0.042]
 [0.038]
 [0.04 ]] [[1.71 ]
 [1.71 ]
 [2.113]
 [1.71 ]
 [1.783]] [[0.68 ]
 [0.68 ]
 [1.204]
 [0.68 ]
 [0.776]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.91605246
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.051]
 [0.036]
 [0.036]] [[0.85 ]
 [0.85 ]
 [0.105]
 [0.85 ]
 [0.85 ]] [[0.75 ]
 [0.75 ]
 [0.035]
 [0.75 ]
 [0.75 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
1692 4656
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
siam score:  -0.92263395
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
from probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
siam score:  -0.9240362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
from probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
from probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855254299826886, 0.07149152333910376, 0.07149152333910376, 0.07149152333910376]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
from probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.93226075
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
from probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855260856288679, 0.07149130479037734, 0.07149130479037734, 0.07149130479037734]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.232]
 [0.282]
 [0.197]
 [0.225]] [[2.687]
 [3.906]
 [2.882]
 [0.994]
 [7.368]] [[0.698]
 [1.088]
 [0.863]
 [0.124]
 [2.141]]
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.112]
 [0.278]
 [0.187]
 [0.222]] [[2.867]
 [2.103]
 [2.81 ]
 [0.935]
 [7.322]] [[0.753]
 [0.337]
 [0.858]
 [0.117]
 [2.139]]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9324368
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855265955770063, 0.07149113480766459, 0.07149113480766459, 0.07149113480766459]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855265955770063, 0.07149113480766459, 0.07149113480766459, 0.07149113480766459]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855265955770063, 0.07149113480766459, 0.07149113480766459, 0.07149113480766459]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855265955770063, 0.07149113480766459, 0.07149113480766459, 0.07149113480766459]
UNIT TEST: sample policy line 217 mcts : [0.083 0.292 0.417 0.083 0.125]
1710 4692
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855265955770063, 0.07149113480766459, 0.07149113480766459, 0.07149113480766459]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855265955770063, 0.07149113480766459, 0.07149113480766459, 0.07149113480766459]
Starting evaluation
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 0.5654533360248188
siam score:  -0.9379454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.7855265955770063, 0.07149113480766459, 0.07149113480766459, 0.07149113480766459]
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.134]
 [0.131]
 [0.131]
 [0.131]] [[6.322]
 [6.899]
 [6.322]
 [6.322]
 [6.322]] [[0.131]
 [0.134]
 [0.131]
 [0.131]
 [0.131]]
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.225]
 [0.23 ]
 [0.23 ]
 [0.209]] [[9.176]
 [9.204]
 [9.176]
 [9.176]
 [7.874]] [[2.08 ]
 [2.078]
 [2.08 ]
 [2.08 ]
 [1.602]]
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.7855265461728106, 0.07149115127572975, 0.07149115127572975, 0.07149115127572975]
siam score:  -0.9410754
Printing some Q and Qe and total Qs values:  [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]] [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]]
first move QE:  0.057151476399133365
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.7855265461728106, 0.07149115127572975, 0.07149115127572975, 0.07149115127572975]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0953],
        [0.0707],
        [0.0201],
        [0.0663],
        [0.0355],
        [0.0263],
        [0.0000],
        [0.0799],
        [0.0929]], dtype=torch.float64)
0.0 0.0
0.0 0.09525883835744206
0.0 0.07065978451273242
0.0 0.020074423089527317
0.0 0.06631773770430785
0.0 0.03551751501809995
0.0 0.026273024291921125
0.0 0.0
0.0 0.07992479686405199
0.0 0.09293002970798184
maxi score, test score, baseline:  0.0061 0.0 0.0061
rdn probs:  [0.7855265461728106, 0.07149115127572975, 0.07149115127572975, 0.07149115127572975]
from probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
siam score:  -0.9346202
maxi score, test score, baseline:  0.0061 0.15 0.15
from probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0745585422271462
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
from probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]] [[1.481]
 [1.481]
 [1.481]
 [1.481]
 [1.481]] [[1.107]
 [1.107]
 [1.107]
 [1.107]
 [1.107]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
from probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 6.1991467876177015
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
from probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
siam score:  -0.92741996
siam score:  -0.91847837
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
line 256 mcts: sample exp_bonus 2.4311638228790904
maxi score, test score, baseline:  0.0061 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.06 ]] [[0.846]
 [0.846]
 [0.846]
 [0.846]
 [2.082]] [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [1.937]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
siam score:  -0.916436
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855243241448313, 0.07149189195172291, 0.07149189195172291, 0.07149189195172291]
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
first move QE:  0.05558121844023051
using another actor
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.242]
 [0.229]
 [0.229]
 [0.216]] [[5.364]
 [5.245]
 [5.364]
 [5.364]
 [5.493]] [[1.263]
 [1.249]
 [1.263]
 [1.263]
 [1.279]]
first move QE:  0.055268250787907126
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0061 0.15 0.15
from probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
Printing some Q and Qe and total Qs values:  [[0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]] [[2.095]
 [2.095]
 [2.095]
 [2.095]
 [2.095]] [[1.311]
 [1.311]
 [1.311]
 [1.311]
 [1.311]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
start point for exploration sampling:  20026
using another actor
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]] [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
UNIT TEST: sample policy line 217 mcts : [0.    0.25  0.25  0.167 0.333]
maxi score, test score, baseline:  0.0061 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]] [[1.557]
 [1.557]
 [1.557]
 [1.557]
 [1.557]] [[0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
siam score:  -0.91973794
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855250298098638, 0.07149165673004537, 0.07149165673004537, 0.07149165673004537]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0061 0.15 0.15
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855255943432289, 0.07149146855225702, 0.07149146855225702, 0.07149146855225702]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855255943432289, 0.07149146855225702, 0.07149146855225702, 0.07149146855225702]
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.268]
 [0.252]
 [0.252]
 [0.251]] [[4.515]
 [4.772]
 [4.515]
 [4.515]
 [4.192]] [[1.449]
 [1.567]
 [1.449]
 [1.449]
 [1.339]]
maxi score, test score, baseline:  0.0061 0.15 0.15
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855260562350491, 0.07149131458831688, 0.07149131458831688, 0.07149131458831688]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855260562350491, 0.07149131458831688, 0.07149131458831688, 0.07149131458831688]
using another actor
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855260562350491, 0.07149131458831688, 0.07149131458831688, 0.07149131458831688]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855260562350491, 0.07149131458831688, 0.07149131458831688, 0.07149131458831688]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855260562350491, 0.07149131458831688, 0.07149131458831688, 0.07149131458831688]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.034]
 [0.067]
 [0.042]
 [0.056]] [[ 0.   ]
 [ 0.   ]
 [ 0.414]
 [-0.111]
 [ 0.215]] [[0.052]
 [0.052]
 [0.257]
 [0.032]
 [0.169]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855260562350491, 0.07149131458831688, 0.07149131458831688, 0.07149131458831688]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855260562350491, 0.07149131458831688, 0.07149131458831688, 0.07149131458831688]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855260562350491, 0.07149131458831688, 0.07149131458831688, 0.07149131458831688]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855260562350491, 0.07149131458831688, 0.07149131458831688, 0.07149131458831688]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
siam score:  -0.9195963
line 256 mcts: sample exp_bonus 5.081025657582117
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
using another actor
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
siam score:  -0.9185997
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
UNIT TEST: sample policy line 217 mcts : [0.083 0.208 0.292 0.125 0.292]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
using another actor
from probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
rdn beta is 0 so we're just using the maxi policy
first move QE:  0.05271524857316912
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.042 0.042 0.75 ]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855264411455083, 0.0714911862848306, 0.0714911862848306, 0.0714911862848306]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855267668394057, 0.07149107772019812, 0.07149107772019812, 0.07149107772019812]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855267668394057, 0.07149107772019812, 0.07149107772019812, 0.07149107772019812]
using another actor
1769 4785
line 256 mcts: sample exp_bonus 1.9121311380478159
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855267668394057, 0.07149107772019812, 0.07149107772019812, 0.07149107772019812]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855267668394057, 0.07149107772019812, 0.07149107772019812, 0.07149107772019812]
siam score:  -0.90362597
first move QE:  0.05429362387035485
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855267668394057, 0.07149107772019812, 0.07149107772019812, 0.07149107772019812]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855267668394057, 0.07149107772019812, 0.07149107772019812, 0.07149107772019812]
start point for exploration sampling:  20026
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]] [[0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]] [[0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9170096346357617
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855270460059187, 0.07149098466469375, 0.07149098466469375, 0.07149098466469375]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.025]
 [0.028]
 [0.028]
 [0.027]] [[0.836]
 [1.   ]
 [0.751]
 [1.028]
 [0.8  ]] [[0.027]
 [0.025]
 [0.028]
 [0.028]
 [0.027]]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
1777 4789
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
from probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.293]
 [0.236]
 [0.236]
 [0.279]] [[2.978]
 [2.952]
 [2.978]
 [2.978]
 [2.9  ]] [[1.338]
 [1.442]
 [1.338]
 [1.338]
 [1.396]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
UNIT TEST: sample policy line 217 mcts : [0.917 0.042 0.    0.    0.042]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.001]
 [0.071]
 [0.037]
 [0.037]] [[ 0.   ]
 [-0.554]
 [ 0.122]
 [ 0.   ]
 [ 0.   ]] [[0.259]
 [0.002]
 [0.369]
 [0.259]
 [0.259]]
siam score:  -0.9094581
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
1784 4806
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
siam score:  -0.90743124
siam score:  -0.9064342
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
siam score:  -0.9053792
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]] [[2.831]
 [2.831]
 [2.831]
 [2.831]
 [2.628]] [[0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.683]]
maxi score, test score, baseline:  0.0061 0.15 0.15
first move QE:  0.05233374296897717
siam score:  -0.90025175
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.042]] [[3.132]
 [3.132]
 [3.132]
 [3.132]
 [3.61 ]] [[1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.724]]
siam score:  -0.893598
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
siam score:  -0.89510095
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.293]
 [0.273]
 [0.273]
 [0.267]] [[5.99 ]
 [4.994]
 [5.99 ]
 [5.99 ]
 [5.306]] [[1.416]
 [1.123]
 [1.416]
 [1.416]
 [1.176]]
siam score:  -0.8948526
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
siam score:  -0.8672785
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
siam score:  -0.86769235
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855272879504657, 0.0714909040165115, 0.0714909040165115, 0.0714909040165115]
siam score:  -0.86749816
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.022]
 [0.047]
 [0.047]
 [0.039]] [[3.079]
 [0.471]
 [3.079]
 [3.079]
 [2.806]] [[1.066]
 [0.144]
 [1.066]
 [1.066]
 [0.958]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.   ]
 [0.   ]
 [0.018]
 [0.011]] [[-0.408]
 [-0.176]
 [-0.308]
 [-0.235]
 [-0.299]] [[0.047]
 [0.269]
 [0.137]
 [0.245]
 [0.167]]
siam score:  -0.8824005
from probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
from probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[-0.308]
 [-0.308]
 [-0.308]
 [-0.308]
 [-0.308]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
from probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
siam score:  -0.88358694
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
using explorer policy with actor:  0
first move QE:  0.05443273667669605
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.031]
 [0.031]
 [0.032]
 [0.031]] [[1.091]
 [1.069]
 [1.24 ]
 [1.131]
 [1.069]] [[0.402]
 [0.373]
 [0.546]
 [0.439]
 [0.373]]
maxi score, test score, baseline:  0.0061 0.15 0.15
actor:  0 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
1810 4841
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.014]
 [0.012]
 [0.014]
 [0.013]] [[1.506]
 [1.451]
 [1.611]
 [1.451]
 [1.647]] [[0.704]
 [0.643]
 [0.799]
 [0.643]
 [0.837]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
from probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.206]
 [0.257]
 [0.206]
 [0.221]] [[2.137]
 [2.137]
 [3.085]
 [2.137]
 [2.391]] [[0.562]
 [0.562]
 [0.979]
 [0.562]
 [0.676]]
siam score:  -0.8769226
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855276864478428, 0.07149077118405245, 0.07149077118405245, 0.07149077118405245]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.002]
 [0.005]
 [0.005]
 [0.005]] [[0.158]
 [0.106]
 [0.158]
 [0.158]
 [0.158]] [[0.315]
 [0.276]
 [0.315]
 [0.315]
 [0.315]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855278524885915, 0.07149071583713615, 0.07149071583713615, 0.07149071583713615]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.1395],
        [0.1019],
        [0.0218],
        [0.0000],
        [0.0000],
        [0.0350],
        [0.0204],
        [0.1043],
        [0.0434],
        [0.0300]], dtype=torch.float64)
0.0 0.13952788783696393
0.0 0.10194297559336929
0.0 0.021768489482779388
0.0 0.0
0.0 0.0
0.0 0.03496543512028076
0.0 0.020441133365730245
0.0 0.10427272460609724
0.0 0.043365427326669745
0.0 0.030029948534556095
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855278524885915, 0.07149071583713615, 0.07149071583713615, 0.07149071583713615]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528001051454, 0.07149066631618195, 0.07149066631618195, 0.07149066631618195]
maxi score, test score, baseline:  0.0081 0.15 0.15
siam score:  -0.87394595
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528001051454, 0.07149066631618195, 0.07149066631618195, 0.07149066631618195]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528001051454, 0.07149066631618195, 0.07149066631618195, 0.07149066631618195]
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.297]
 [0.264]
 [0.355]
 [0.258]] [[4.545]
 [4.313]
 [4.092]
 [4.545]
 [5.374]] [[1.3  ]
 [1.105]
 [0.967]
 [1.3  ]
 [1.382]]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528001051454, 0.07149066631618195, 0.07149066631618195, 0.07149066631618195]
UNIT TEST: sample policy line 217 mcts : [0.25  0.333 0.    0.042 0.375]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528001051454, 0.07149066631618195, 0.07149066631618195, 0.07149066631618195]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528001051454, 0.07149066631618195, 0.07149066631618195, 0.07149066631618195]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528134758101, 0.07149062174729971, 0.07149062174729971, 0.07149062174729971]
siam score:  -0.88117236
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528134758101, 0.07149062174729971, 0.07149062174729971, 0.07149062174729971]
start point for exploration sampling:  20026
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528134758101, 0.07149062174729971, 0.07149062174729971, 0.07149062174729971]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528134758101, 0.07149062174729971, 0.07149062174729971, 0.07149062174729971]
maxi score, test score, baseline:  0.0081 0.15 0.15
using another actor
first move QE:  0.05479128453592326
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528134758101, 0.07149062174729971, 0.07149062174729971, 0.07149062174729971]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528134758101, 0.07149062174729971, 0.07149062174729971, 0.07149062174729971]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528134758101, 0.07149062174729971, 0.07149062174729971, 0.07149062174729971]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528134758101, 0.07149062174729971, 0.07149062174729971, 0.07149062174729971]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528134758101, 0.07149062174729971, 0.07149062174729971, 0.07149062174729971]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528134758101, 0.07149062174729971, 0.07149062174729971, 0.07149062174729971]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.785528134758101, 0.07149062174729971, 0.07149062174729971, 0.07149062174729971]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.001]
 [0.023]
 [0.017]
 [0.013]] [[-0.751]
 [-0.464]
 [-0.55 ]
 [-0.303]
 [-0.512]] [[0.009]
 [0.001]
 [0.023]
 [0.017]
 [0.013]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.017]
 [0.022]
 [0.021]
 [0.023]] [[2.021]
 [1.929]
 [2.092]
 [2.021]
 [1.936]] [[1.09 ]
 [0.99 ]
 [1.163]
 [1.09 ]
 [1.009]]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.047]
 [0.044]
 [0.045]
 [0.046]] [[1.343]
 [1.206]
 [1.387]
 [1.329]
 [1.344]] [[0.653]
 [0.519]
 [0.693]
 [0.637]
 [0.654]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.14 ]
 [0.134]
 [0.134]
 [0.101]] [[1.956]
 [1.948]
 [1.556]
 [1.556]
 [1.618]] [[0.607]
 [0.512]
 [0.369]
 [0.369]
 [0.323]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
first move QE:  0.053315095874503636
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
siam score:  -0.88937986
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.885177
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.199]
 [ 0.04 ]
 [-0.07 ]
 [-0.019]
 [-0.042]] [[0.   ]
 [0.08 ]
 [0.043]
 [0.06 ]
 [0.052]]
1838 4867
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
from probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.37 ]
 [0.352]
 [0.352]
 [0.352]] [[4.1  ]
 [3.646]
 [4.1  ]
 [4.1  ]
 [4.1  ]] [[1.691]
 [1.499]
 [1.691]
 [1.691]
 [1.691]]
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[4.048]
 [4.048]
 [4.048]
 [4.048]
 [4.048]] [[1.834]
 [1.834]
 [1.834]
 [1.834]
 [1.834]]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.56257605822964
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
first move QE:  0.05736814095801644
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
in main func line 156:  1842
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
maxi score, test score, baseline:  0.0081 0.15 0.15
siam score:  -0.8901259
maxi score, test score, baseline:  0.0081 0.15 0.15
first move QE:  0.057193947751004615
siam score:  -0.89079237
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855282557308388, 0.07149058142305371, 0.07149058142305371, 0.07149058142305371]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.32036765971249737
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855283657061026, 0.07149054476463247, 0.07149054476463247, 0.07149054476463247]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855283657061026, 0.07149054476463247, 0.07149054476463247, 0.07149054476463247]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855283657061026, 0.07149054476463247, 0.07149054476463247, 0.07149054476463247]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855283657061026, 0.07149054476463247, 0.07149054476463247, 0.07149054476463247]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855283657061026, 0.07149054476463247, 0.07149054476463247, 0.07149054476463247]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855283657061026, 0.07149054476463247, 0.07149054476463247, 0.07149054476463247]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855283657061026, 0.07149054476463247, 0.07149054476463247, 0.07149054476463247]
actor:  1 policy actor:  1  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8844195
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
from probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.099]
 [0.05 ]
 [0.073]
 [0.061]] [[0.433]
 [0.621]
 [0.543]
 [0.555]
 [0.599]] [[0.16 ]
 [0.268]
 [0.144]
 [0.194]
 [0.184]]
UNIT TEST: sample policy line 217 mcts : [0.208 0.458 0.042 0.042 0.25 ]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
first move QE:  0.05560658927990337
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
from probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
siam score:  -0.87957215
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
using explorer policy with actor:  1
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.006]
 [0.04 ]
 [0.025]
 [0.024]] [[-0.132]
 [ 0.171]
 [ 0.031]
 [ 0.289]
 [ 0.172]] [[0.018]
 [0.006]
 [0.04 ]
 [0.025]
 [0.024]]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.003]
 [0.06 ]
 [0.061]
 [0.057]] [[-0.007]
 [-0.093]
 [ 0.329]
 [ 0.309]
 [ 0.217]] [[0.048]
 [0.003]
 [0.06 ]
 [0.061]
 [0.057]]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.009]
 [0.046]
 [0.024]
 [0.026]] [[-0.131]
 [ 0.028]
 [ 0.021]
 [ 0.267]
 [ 0.17 ]] [[0.023]
 [0.009]
 [0.046]
 [0.024]
 [0.026]]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.024]] [[1.794]
 [1.794]
 [1.794]
 [1.794]
 [1.875]] [[1.437]
 [1.437]
 [1.437]
 [1.437]
 [1.547]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.119]
 [0.139]
 [0.123]
 [0.145]] [[2.313]
 [2.848]
 [2.305]
 [1.862]
 [4.143]] [[0.12 ]
 [0.119]
 [0.139]
 [0.123]
 [0.145]]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0101 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.261]
 [0.266]
 [0.266]
 [0.266]] [[3.239]
 [3.167]
 [3.239]
 [3.239]
 [3.239]] [[0.266]
 [0.261]
 [0.266]
 [0.266]
 [0.266]]
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.202]
 [0.258]
 [0.306]
 [0.189]
 [0.251]] [[3.097]
 [2.96 ]
 [3.197]
 [3.151]
 [3.292]] [[0.202]
 [0.258]
 [0.306]
 [0.189]
 [0.251]]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
actor:  0 policy actor:  0  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[2.947]
 [2.947]
 [2.947]
 [2.947]
 [2.947]] [[1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.201]]
actor:  0 policy actor:  0  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7492000906757972
actor:  0 policy actor:  0  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.7855284661183393, 0.07149051129388687, 0.07149051129388687, 0.07149051129388687]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
using explorer policy with actor:  1
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.195]
 [0.11 ]
 [0.195]
 [0.114]] [[1.382]
 [1.166]
 [1.466]
 [1.166]
 [1.796]] [[0.778]
 [0.474]
 [0.405]
 [0.474]
 [0.522]]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
from probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.24160078442540156
first move QE:  0.054580395655008974
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.042]
 [0.042]
 [0.042]
 [0.042]] [[0.121]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.187]
 [0.162]
 [0.162]
 [0.162]
 [0.162]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.014]
 [0.026]
 [0.026]
 [0.026]] [[0.022]
 [0.038]
 [0.806]
 [0.806]
 [0.806]] [[0.101]
 [0.09 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
using another actor
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
siam score:  -0.88628685
using explorer policy with actor:  1
from probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
using another actor
from probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
from probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
siam score:  -0.88422424
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
siam score:  -0.8844128
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.553]
 [0.447]
 [0.447]
 [0.404]] [[3.493]
 [3.638]
 [3.493]
 [3.493]
 [5.088]] [[1.638]
 [1.899]
 [1.638]
 [1.638]
 [2.085]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
line 256 mcts: sample exp_bonus 1.8143527178642016
from probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
1886 4960
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855260401971256, 0.07149131993429152, 0.07149131993429152, 0.07149131993429152]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855260401971256, 0.07149131993429152, 0.07149131993429152, 0.07149131993429152]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855260401971256, 0.07149131993429152, 0.07149131993429152, 0.07149131993429152]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855260401971256, 0.07149131993429152, 0.07149131993429152, 0.07149131993429152]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855260401971256, 0.07149131993429152, 0.07149131993429152, 0.07149131993429152]
Printing some Q and Qe and total Qs values:  [[0.742]
 [0.708]
 [0.742]
 [0.742]
 [0.506]] [[2.677]
 [2.46 ]
 [2.677]
 [2.677]
 [2.91 ]] [[1.437]
 [1.298]
 [1.437]
 [1.437]
 [1.043]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855260401971256, 0.07149131993429152, 0.07149131993429152, 0.07149131993429152]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855260401971256, 0.07149131993429152, 0.07149131993429152, 0.07149131993429152]
siam score:  -0.88048875
siam score:  -0.8790521
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855260401971256, 0.07149131993429152, 0.07149131993429152, 0.07149131993429152]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855260401971256, 0.07149131993429152, 0.07149131993429152, 0.07149131993429152]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855260401971256, 0.07149131993429152, 0.07149131993429152, 0.07149131993429152]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855260401971256, 0.07149131993429152, 0.07149131993429152, 0.07149131993429152]
using another actor
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.7333009663074668
siam score:  -0.8869237
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855260401971256, 0.07149131993429152, 0.07149131993429152, 0.07149131993429152]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
from probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855262417171942, 0.0714912527609353, 0.0714912527609353, 0.0714912527609353]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855264264440568, 0.07149119118531434, 0.07149119118531434, 0.07149119118531434]
using another actor
from probs:  [0.7855264264440568, 0.07149119118531434, 0.07149119118531434, 0.07149119118531434]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855264264440568, 0.07149119118531434, 0.07149119118531434, 0.07149119118531434]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855264264440568, 0.07149119118531434, 0.07149119118531434, 0.07149119118531434]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855264264440568, 0.07149119118531434, 0.07149119118531434, 0.07149119118531434]
1906 4997
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855264264440568, 0.07149119118531434, 0.07149119118531434, 0.07149119118531434]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2878648803395922
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855264264440568, 0.07149119118531434, 0.07149119118531434, 0.07149119118531434]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855265963928831, 0.07149113453570556, 0.07149113453570556, 0.07149113453570556]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855265963928831, 0.07149113453570556, 0.07149113453570556, 0.07149113453570556]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855265963928831, 0.07149113453570556, 0.07149113453570556, 0.07149113453570556]
siam score:  -0.8978663
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855265963928831, 0.07149113453570556, 0.07149113453570556, 0.07149113453570556]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855265963928831, 0.07149113453570556, 0.07149113453570556, 0.07149113453570556]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855265963928831, 0.07149113453570556, 0.07149113453570556, 0.07149113453570556]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855265963928831, 0.07149113453570556, 0.07149113453570556, 0.07149113453570556]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855265963928831, 0.07149113453570556, 0.07149113453570556, 0.07149113453570556]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855267532688188, 0.07149108224372712, 0.07149108224372712, 0.07149108224372712]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855267532688188, 0.07149108224372712, 0.07149108224372712, 0.07149108224372712]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855267532688188, 0.07149108224372712, 0.07149108224372712, 0.07149108224372712]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855267532688188, 0.07149108224372712, 0.07149108224372712, 0.07149108224372712]
start point for exploration sampling:  20026
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855268985243964, 0.0714910338252012, 0.0714910338252012, 0.0714910338252012]
siam score:  -0.9056999
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.785527158982871, 0.07149094700570965, 0.07149094700570965, 0.07149094700570965]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.785527158982871, 0.07149094700570965, 0.07149094700570965, 0.07149094700570965]
from probs:  [0.785527158982871, 0.07149094700570965, 0.07149094700570965, 0.07149094700570965]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.785527158982871, 0.07149094700570965, 0.07149094700570965, 0.07149094700570965]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855272761892674, 0.07149090793691093, 0.07149090793691093, 0.07149090793691093]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855273858340068, 0.07149087138866435, 0.07149087138866435, 0.07149087138866435]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855273858340068, 0.07149087138866435, 0.07149087138866435, 0.07149087138866435]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855273858340068, 0.07149087138866435, 0.07149087138866435, 0.07149087138866435]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855273858340068, 0.07149087138866435, 0.07149087138866435, 0.07149087138866435]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855273858340068, 0.07149087138866435, 0.07149087138866435, 0.07149087138866435]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
in main func line 156:  1923
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.349]
 [0.443]
 [0.349]
 [0.349]] [[1.242]
 [1.242]
 [2.081]
 [1.242]
 [1.242]] [[0.755]
 [0.755]
 [1.222]
 [0.755]
 [0.755]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
Printing some Q and Qe and total Qs values:  [[0.06]
 [0.06]
 [0.06]
 [0.06]
 [0.06]] [[1.136]
 [1.136]
 [1.136]
 [1.136]
 [1.136]] [[0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855274886259912, 0.07149083712466962, 0.07149083712466962, 0.07149083712466962]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.001]
 [0.12 ]
 [0.086]
 [0.056]] [[-0.205]
 [-0.18 ]
 [ 0.467]
 [ 0.092]
 [ 0.48 ]] [[0.123]
 [0.001]
 [0.454]
 [0.261]
 [0.33 ]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.043]] [[1.743]
 [1.743]
 [1.743]
 [1.743]
 [1.765]] [[1.332]
 [1.332]
 [1.332]
 [1.332]
 [1.366]]
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.199]
 [0.128]
 [0.128]
 [0.128]] [[0.65 ]
 [0.774]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[0.508]
 [0.692]
 [0.508]
 [0.508]
 [0.508]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.89074725
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
first move QE:  0.0521309382078088
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
Printing some Q and Qe and total Qs values:  [[1.122]
 [1.134]
 [1.122]
 [1.122]
 [1.122]] [[2.947]
 [2.864]
 [2.947]
 [2.947]
 [2.947]] [[2.181]
 [2.176]
 [2.181]
 [2.181]
 [2.181]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
Printing some Q and Qe and total Qs values:  [[0.825]
 [1.015]
 [0.825]
 [0.825]
 [0.8  ]] [[2.125]
 [2.705]
 [2.125]
 [2.125]
 [2.633]] [[1.201]
 [1.775]
 [1.201]
 [1.201]
 [1.321]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855275851881942, 0.07149080493726864, 0.07149080493726864, 0.07149080493726864]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855276760702994, 0.07149077464323357, 0.07149077464323357, 0.07149077464323357]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855276760702994, 0.07149077464323357, 0.07149077464323357, 0.07149077464323357]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855276760702994, 0.07149077464323357, 0.07149077464323357, 0.07149077464323357]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855276760702994, 0.07149077464323357, 0.07149077464323357, 0.07149077464323357]
using another actor
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]] [[0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855276760702994, 0.07149077464323357, 0.07149077464323357, 0.07149077464323357]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855276760702994, 0.07149077464323357, 0.07149077464323357, 0.07149077464323357]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855277617591696, 0.0714907460802768, 0.0714907460802768, 0.0714907460802768]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855277617591696, 0.0714907460802768, 0.0714907460802768, 0.0714907460802768]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.462]
 [0.574]
 [0.462]
 [0.462]] [[2.859]
 [2.859]
 [3.08 ]
 [2.859]
 [2.859]] [[1.648]
 [1.648]
 [1.946]
 [1.648]
 [1.648]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855277617591696, 0.0714907460802768, 0.0714907460802768, 0.0714907460802768]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855278426875724, 0.07149071910414259, 0.07149071910414259, 0.07149071910414259]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855278426875724, 0.07149071910414259, 0.07149071910414259, 0.07149071910414259]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855278426875724, 0.07149071910414259, 0.07149071910414259, 0.07149071910414259]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855278426875724, 0.07149071910414259, 0.07149071910414259, 0.07149071910414259]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855278426875724, 0.07149071910414259, 0.07149071910414259, 0.07149071910414259]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855278426875724, 0.07149071910414259, 0.07149071910414259, 0.07149071910414259]
from probs:  [0.7855278426875724, 0.07149071910414259, 0.07149071910414259, 0.07149071910414259]
using another actor
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855278426875724, 0.07149071910414259, 0.07149071910414259, 0.07149071910414259]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855278426875724, 0.07149071910414259, 0.07149071910414259, 0.07149071910414259]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855278426875724, 0.07149071910414259, 0.07149071910414259, 0.07149071910414259]
maxi score, test score, baseline:  0.0241 0.4 0.4
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.349]
 [0.352]
 [0.485]
 [0.314]] [[1.432]
 [1.853]
 [1.042]
 [1.41 ]
 [1.892]] [[0.696]
 [0.845]
 [0.58 ]
 [0.969]
 [0.788]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855278426875724, 0.07149071910414259, 0.07149071910414259, 0.07149071910414259]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855278426875724, 0.07149071910414259, 0.07149071910414259, 0.07149071910414259]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.327]
 [0.375]
 [0.375]
 [0.375]] [[0.752]
 [0.935]
 [0.997]
 [0.997]
 [0.997]] [[0.618]
 [0.616]
 [0.734]
 [0.734]
 [0.734]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.785527919241489, 0.07149069358617026, 0.07149069358617026, 0.07149069358617026]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.5832735462052993
from probs:  [0.7855279917662727, 0.0714906694112424, 0.0714906694112424, 0.0714906694112424]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855279917662727, 0.0714906694112424, 0.0714906694112424, 0.0714906694112424]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855279917662727, 0.0714906694112424, 0.0714906694112424, 0.0714906694112424]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855279917662727, 0.0714906694112424, 0.0714906694112424, 0.0714906694112424]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855280605718549, 0.07149064647604839, 0.07149064647604839, 0.07149064647604839]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0241 0.4 0.4
first move QE:  0.04998292260055187
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7855279917662727, 0.0714906694112424, 0.0714906694112424, 0.0714906694112424]
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7710351896408245, 0.07017369561068891, 0.08861741913779775, 0.07017369561068891]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7710351896408245, 0.07017369561068891, 0.08861741913779775, 0.07017369561068891]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7710351896408245, 0.07017369561068891, 0.08861741913779775, 0.07017369561068891]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7710351896408245, 0.07017369561068891, 0.08861741913779775, 0.07017369561068891]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.141]] [[0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.946]] [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [1.063]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7710351896408245, 0.07017369561068891, 0.08861741913779775, 0.07017369561068891]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7710351896408245, 0.07017369561068891, 0.08861741913779775, 0.07017369561068891]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7710351896408245, 0.07017369561068891, 0.08861741913779775, 0.07017369561068891]
maxi score, test score, baseline:  0.0241 0.4 0.4
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7714001812315114, 0.07020683405809958, 0.08818615065228962, 0.07020683405809958]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7717472435377001, 0.07023834466348151, 0.08777606713533698, 0.07023834466348151]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.079]
 [0.123]
 [0.079]
 [0.079]] [[0.194]
 [0.   ]
 [0.871]
 [0.   ]
 [0.   ]] [[0.097]
 [0.095]
 [0.475]
 [0.095]
 [0.095]]
maxi score, test score, baseline:  0.0241 0.4 0.4
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
line 256 mcts: sample exp_bonus 1.9859450006894463
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.447]
 [0.584]
 [0.369]
 [0.447]] [[1.884]
 [1.884]
 [2.28 ]
 [2.03 ]
 [1.884]] [[1.319]
 [1.319]
 [1.724]
 [1.212]
 [1.319]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
start point for exploration sampling:  20026
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.596]
 [0.581]
 [0.581]
 [0.534]] [[3.111]
 [3.417]
 [3.111]
 [3.111]
 [2.633]] [[1.448]
 [1.58 ]
 [1.448]
 [1.448]
 [1.194]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.7723926172522071, 0.07029693966524077, 0.0870135034173114, 0.07029693966524077]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7726931590790496, 0.07032422656657675, 0.08665838778779704, 0.07032422656657675]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.772980258227158, 0.07035029297540368, 0.08631915582203445, 0.07035029297540368]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.772980258227158, 0.07035029297540368, 0.08631915582203445, 0.07035029297540368]
siam score:  -0.9022552
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.001]
 [0.   ]
 [0.   ]] [[1.382]
 [1.458]
 [1.179]
 [0.787]
 [1.542]] [[0.   ]
 [0.001]
 [0.001]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7732547968667606, 0.07037521898603737, 0.08599476516116454, 0.07037521898603737]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7732547968667606, 0.07037521898603737, 0.08599476516116454, 0.07037521898603737]
maxi score, test score, baseline:  0.0241 0.4 0.4
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  1987
1988 5080
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7735175816310393, 0.07039907783460697, 0.0856842626997468, 0.07039907783460697]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7735175816310393, 0.07039907783460697, 0.0856842626997468, 0.07039907783460697]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7735175816310393, 0.07039907783460697, 0.0856842626997468, 0.07039907783460697]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7735175816310393, 0.07039907783460697, 0.0856842626997468, 0.07039907783460697]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7737693515316412, 0.07042193661772299, 0.08538677523291274, 0.07042193661772299]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7737693515316412, 0.07042193661772299, 0.08538677523291274, 0.07042193661772299]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7737693515316412, 0.07042193661772299, 0.08538677523291274, 0.07042193661772299]
from probs:  [0.7737693515316412, 0.07042193661772299, 0.08538677523291274, 0.07042193661772299]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7737693515316412, 0.07042193661772299, 0.08538677523291274, 0.07042193661772299]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.8976227
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7737693515316412, 0.07042193661772299, 0.08538677523291274, 0.07042193661772299]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7737693515316412, 0.07042193661772299, 0.08538677523291274, 0.07042193661772299]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7740107848992648, 0.07044385692262951, 0.08510150125547608, 0.07044385692262951]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7740107848992648, 0.07044385692262951, 0.08510150125547608, 0.07044385692262951]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7740107848992648, 0.07044385692262951, 0.08510150125547608, 0.07044385692262951]
siam score:  -0.89969116
maxi score, test score, baseline:  0.0241 0.4 0.4
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7740107848992648, 0.07044385692262951, 0.08510150125547608, 0.07044385692262951]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.654]
 [0.651]
 [0.651]
 [0.651]] [[2.413]
 [2.803]
 [2.413]
 [2.413]
 [2.413]] [[1.581]
 [1.718]
 [1.581]
 [1.581]
 [1.581]]
Printing some Q and Qe and total Qs values:  [[0.45]
 [0.45]
 [0.45]
 [0.45]
 [0.45]] [[3.384]
 [3.384]
 [3.384]
 [3.384]
 [3.384]] [[1.771]
 [1.771]
 [1.771]
 [1.771]
 [1.771]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.051215414468141936
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7744650878491929, 0.07048510415900111, 0.08456470383280495, 0.07048510415900111]
Printing some Q and Qe and total Qs values:  [[1.18 ]
 [1.096]
 [1.18 ]
 [1.18 ]
 [1.18 ]] [[2.723]
 [2.404]
 [2.723]
 [2.723]
 [2.723]] [[2.466]
 [2.191]
 [2.466]
 [2.466]
 [2.466]]
Printing some Q and Qe and total Qs values:  [[1.5  ]
 [0.001]
 [1.5  ]
 [1.5  ]
 [0.001]] [[1.549]
 [0.053]
 [1.549]
 [1.549]
 [0.052]] [[2.846]
 [0.001]
 [2.846]
 [2.846]
 [0.   ]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
using another actor
from probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
siam score:  -0.89314497
2003 5102
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.11 ]] [[4.216]
 [4.216]
 [4.216]
 [4.216]
 [7.957]] [[0.978]
 [0.978]
 [0.978]
 [0.978]
 [1.96 ]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.089]
 [0.089]
 [0.074]
 [0.039]] [[2.913]
 [2.913]
 [2.913]
 [2.836]
 [3.63 ]] [[1.148]
 [1.148]
 [1.148]
 [1.11 ]
 [1.398]]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.063]] [[2.017]
 [2.017]
 [2.017]
 [2.017]
 [2.422]] [[0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.835]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
siam score:  -0.886074
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7746790621021183, 0.0705045313861174, 0.08431187512564683, 0.0705045313861174]
using another actor
siam score:  -0.8847119
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7748849181351104, 0.07052322154089778, 0.08406863878309417, 0.07052322154089778]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.9683384827427268
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7750831093569545, 0.07054121578939533, 0.08383445906425493, 0.07054121578939533]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7748849181351104, 0.07052322154089778, 0.08406863878309417, 0.07052322154089778]
siam score:  -0.87978745
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7748849181351104, 0.07052322154089778, 0.08406863878309417, 0.07052322154089778]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7748849181351104, 0.07052322154089778, 0.08406863878309417, 0.07052322154089778]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7748849181351104, 0.07052322154089778, 0.08406863878309417, 0.07052322154089778]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7748849181351104, 0.07052322154089778, 0.08406863878309417, 0.07052322154089778]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7748849181351104, 0.07052322154089778, 0.08406863878309417, 0.07052322154089778]
using another actor
Printing some Q and Qe and total Qs values:  [[1.091]
 [1.108]
 [1.091]
 [1.091]
 [0.791]] [[2.202]
 [2.376]
 [2.202]
 [2.202]
 [2.936]] [[2.08 ]
 [2.171]
 [2.08 ]
 [2.08 ]
 [1.723]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.7754581482410564, 0.07057526645427334, 0.08339131885039666, 0.07057526645427334]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7756357485690957, 0.07059139120695536, 0.08318146901699358, 0.07059139120695536]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7756357485690957, 0.07059139120695536, 0.08318146901699358, 0.07059139120695536]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7756357485690957, 0.07059139120695536, 0.08318146901699358, 0.07059139120695536]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7756357485690957, 0.07059139120695536, 0.08318146901699358, 0.07059139120695536]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7758071944610687, 0.0706069571838707, 0.08297889117118996, 0.0706069571838707]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7758071944610687, 0.0706069571838707, 0.08297889117118996, 0.0706069571838707]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7758071944610687, 0.0706069571838707, 0.08297889117118996, 0.0706069571838707]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7758071944610687, 0.0706069571838707, 0.08297889117118996, 0.0706069571838707]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7759728003756682, 0.07062199293546575, 0.08278321375340028, 0.07062199293546575]
in main func line 156:  2023
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[3.385]
 [1.031]
 [1.031]
 [1.031]
 [1.031]] [[1.017]
 [0.511]
 [0.511]
 [0.511]
 [0.511]]
maxi score, test score, baseline:  0.0241 0.4 0.4
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
first move QE:  0.0501413615005325
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.833 0.042]
siam score:  -0.88448995
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]] [[0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]] [[0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
from probs:  [0.7761328597074701, 0.0706365250997261, 0.08259409009307768, 0.0706365250997261]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.776287646521617, 0.07065057855967244, 0.08241119635903818, 0.07065057855967244]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.776287646521617, 0.07065057855967244, 0.08241119635903818, 0.07065057855967244]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.776287646521617, 0.07065057855967244, 0.08241119635903818, 0.07065057855967244]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.776287646521617, 0.07065057855967244, 0.08241119635903818, 0.07065057855967244]
actor:  1 policy actor:  1  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.001]
 [0.011]
 [0.001]] [[1.57 ]
 [1.563]
 [1.5  ]
 [1.093]
 [0.812]] [[0.5  ]
 [0.496]
 [0.475]
 [0.359]
 [0.247]]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
from probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
using another actor
from probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
UNIT TEST: sample policy line 217 mcts : [0.292 0.167 0.417 0.042 0.083]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
2036 5134
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7764374171198528, 0.07066417658554465, 0.08223422970905789, 0.07066417658554465]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7765824114567329, 0.0706773409633828, 0.08206290661650134, 0.0706773409633828]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7765824114567329, 0.0706773409633828, 0.08206290661650134, 0.0706773409633828]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7765824114567329, 0.0706773409633828, 0.08206290661650134, 0.0706773409633828]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7765824114567329, 0.0706773409633828, 0.08206290661650134, 0.0706773409633828]
siam score:  -0.89014953
first move QE:  0.05313891434506778
UNIT TEST: sample policy line 217 mcts : [0.    0.125 0.708 0.125 0.042]
Starting evaluation
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.7765824114567329, 0.0706773409633828, 0.08206290661650134, 0.0706773409633828]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 3.019516529297799
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[1.794]
 [1.55 ]
 [1.55 ]
 [1.55 ]
 [1.55 ]] [[0.146]
 [0.142]
 [0.142]
 [0.142]
 [0.142]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]] [[1.512]
 [1.512]
 [1.512]
 [1.512]
 [1.512]] [[0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]]
actor:  0 policy actor:  0  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.034100000000000005 0.4 0.4
probs:  [0.7765824114567329, 0.0706773409633828, 0.08206290661650134, 0.0706773409633828]
rdn beta is 0 so we're just using the maxi policy
rdn probs:  [0.7765824114567329, 0.0706773409633828, 0.08206290661650134, 0.0706773409633828]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7764380018477549, 0.07066397739252302, 0.08223404336719896, 0.07066397739252302]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
2044 5151
from probs:  [0.7765829870194694, 0.07067714496093745, 0.08206272305865571, 0.07067714496093745]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7765829870194694, 0.07067714496093745, 0.08206272305865571, 0.07067714496093745]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7765829870194694, 0.07067714496093745, 0.08206272305865571, 0.07067714496093745]
from probs:  [0.7765829870194694, 0.07067714496093745, 0.08206272305865571, 0.07067714496093745]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7765829870194694, 0.07067714496093745, 0.08206272305865571, 0.07067714496093745]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7765829870194694, 0.07067714496093745, 0.08206272305865571, 0.07067714496093745]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7765829870194694, 0.07067714496093745, 0.08206272305865571, 0.07067714496093745]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7765829870194694, 0.07067714496093745, 0.08206272305865571, 0.07067714496093745]
from probs:  [0.7765829870194694, 0.07067714496093745, 0.08206272305865571, 0.07067714496093745]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7765829870194694, 0.07067714496093745, 0.08206272305865571, 0.07067714496093745]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7767234211028384, 0.07068989919903373, 0.08189678049909413, 0.07068989919903373]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7767234211028384, 0.07068989919903373, 0.08189678049909413, 0.07068989919903373]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7767234211028384, 0.07068989919903373, 0.08189678049909413, 0.07068989919903373]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.016]
 [0.055]
 [0.039]
 [0.045]] [[-0.183]
 [-0.173]
 [-0.665]
 [-0.198]
 [-0.762]] [[0.814]
 [0.76 ]
 [0.182]
 [0.773]
 [0.034]]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7767234211028384, 0.07068989919903373, 0.08189678049909413, 0.07068989919903373]
2047 5161
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [0.7767234211028384, 0.07068989919903373, 0.08189678049909413, 0.07068989919903373]
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2049 5163
from probs:  [0.7767234211028384, 0.07068989919903373, 0.08189678049909413, 0.07068989919903373]
maxi score, test score, baseline:  0.0361 0.25 0.25
probs:  [0.7767234211028384, 0.07068989919903373, 0.08189678049909413, 0.07068989919903373]
maxi score, test score, baseline:  0.0361 0.25 0.25
probs:  [0.7767234211028384, 0.07068989919903373, 0.08189678049909413, 0.07068989919903373]
2050 5168
from probs:  [0.7767234211028384, 0.07068989919903373, 0.08189678049909413, 0.07068989919903373]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.25 0.25
probs:  [0.7768595150747509, 0.07070225926775495, 0.08173596638973926, 0.07070225926775495]
maxi score, test score, baseline:  0.0361 0.25 0.25
probs:  [0.7768595150747509, 0.07070225926775495, 0.08173596638973926, 0.07070225926775495]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.149]
 [0.149]
 [0.149]
 [0.149]] [[0.822]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[0.844]
 [0.576]
 [0.576]
 [0.576]
 [0.576]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0361 0.25 0.25
probs:  [0.7771194633440804, 0.07072586779668333, 0.08142880106255297, 0.07072586779668333]
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.193]
 [0.149]
 [0.149]
 [0.149]] [[0.671]
 [0.867]
 [0.671]
 [0.671]
 [0.671]] [[0.723]
 [0.876]
 [0.723]
 [0.723]
 [0.723]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.25 0.25
probs:  [0.7772436791496974, 0.07073714908910289, 0.08128202267209685, 0.07073714908910289]
maxi score, test score, baseline:  0.0361 0.25 0.25
maxi score, test score, baseline:  0.0361 0.25 0.25
probs:  [0.7772436791496974, 0.07073714908910289, 0.08128202267209685, 0.07073714908910289]
maxi score, test score, baseline:  0.0361 0.25 0.25
maxi score, test score, baseline:  0.0361 0.25 0.25
probs:  [0.7772436791496974, 0.07073714908910289, 0.08128202267209685, 0.07073714908910289]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.0361 0.25 0.25
probs:  [0.777364279538025, 0.07074810202894723, 0.08113951640408072, 0.07074810202894723]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.25 0.25
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7774814200900542, 0.07075874074609811, 0.08100109841774966, 0.07075874074609811]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7774814200900542, 0.07075874074609811, 0.08100109841774966, 0.07075874074609811]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7775952475862626, 0.07076907857117391, 0.08086659527138947, 0.07076907857117391]
maxi score, test score, baseline:  0.0381 0.25 0.25
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7775952475862626, 0.07076907857117391, 0.08086659527138947, 0.07076907857117391]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7777059006201944, 0.0707791280912547, 0.0807358431972961, 0.0707791280912547]
maxi score, test score, baseline:  0.0381 0.25 0.25
from probs:  [0.7777059006201944, 0.0707791280912547, 0.0807358431972961, 0.0707791280912547]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7777059006201944, 0.0707791280912547, 0.0807358431972961, 0.0707791280912547]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.293]
 [0.205]
 [0.205]
 [0.205]] [[1.847]
 [1.217]
 [1.75 ]
 [1.75 ]
 [1.75 ]] [[0.209]
 [0.293]
 [0.205]
 [0.205]
 [0.205]]
start point for exploration sampling:  20026
start point for exploration sampling:  20026
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2068 5184
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7779182000726157, 0.07079840914966257, 0.08048498162805917, 0.07079840914966257]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7779182000726157, 0.07079840914966257, 0.08048498162805917, 0.07079840914966257]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.602]
 [0.706]
 [0.602]
 [0.602]] [[1.957]
 [1.957]
 [2.267]
 [1.957]
 [1.957]] [[1.519]
 [1.519]
 [1.83 ]
 [1.519]
 [1.519]]
siam score:  -0.8646132
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.25 0.25
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7780200875852643, 0.0708076625841863, 0.08036458724636304, 0.0708076625841863]
maxi score, test score, baseline:  0.0381 0.25 0.25
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7780200875852643, 0.0708076625841863, 0.08036458724636304, 0.0708076625841863]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.055]
 [0.054]
 [0.06 ]
 [0.062]] [[1.1  ]
 [1.195]
 [1.331]
 [1.037]
 [1.261]] [[0.586]
 [0.641]
 [0.775]
 [0.492]
 [0.72 ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.778119283737467, 0.0708166715890737, 0.08024737308438562, 0.0708166715890737]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.778119283737467, 0.0708166715890737, 0.08024737308438562, 0.0708166715890737]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.037]
 [0.043]
 [0.053]
 [0.048]] [[1.014]
 [1.12 ]
 [1.028]
 [1.118]
 [1.164]] [[0.551]
 [0.636]
 [0.556]
 [0.665]
 [0.7  ]]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7782158937776545, 0.07082544572299822, 0.08013321477634895, 0.07082544572299822]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.778310017537075, 0.07083399405264415, 0.08002199435763677, 0.07083399405264415]
using another actor
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.778310017537075, 0.07083399405264415, 0.08002199435763677, 0.07083399405264415]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.778310017537075, 0.07083399405264415, 0.08002199435763677, 0.07083399405264415]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7784017497738954, 0.07084232518395818, 0.07991359985818815, 0.07084232518395818]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7784017497738954, 0.07084232518395818, 0.07991359985818815, 0.07084232518395818]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7784017497738954, 0.07084232518395818, 0.07991359985818815, 0.07084232518395818]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.0381 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.001]
 [0.178]
 [0.121]
 [0.137]] [[-0.447]
 [-0.241]
 [-0.362]
 [-0.264]
 [-0.343]] [[0.128]
 [0.001]
 [0.313]
 [0.233]
 [0.238]]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7784017497738954, 0.07084232518395818, 0.07991359985818815, 0.07084232518395818]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7784017497738954, 0.07084232518395818, 0.07991359985818815, 0.07084232518395818]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7784017497738954, 0.07084232518395818, 0.07991359985818815, 0.07084232518395818]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7784911804914068, 0.07085044729104857, 0.07980792492649613, 0.07085044729104857]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7784911804914068, 0.07085044729104857, 0.07980792492649613, 0.07085044729104857]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7784911804914068, 0.07085044729104857, 0.07980792492649613, 0.07085044729104857]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7784911804914068, 0.07085044729104857, 0.07980792492649613, 0.07085044729104857]
first move QE:  0.04561806414986124
siam score:  -0.884449
maxi score, test score, baseline:  0.0381 0.25 0.25
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7784911804914068, 0.07085044729104857, 0.07980792492649613, 0.07085044729104857]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7785783952325714, 0.07085836814293604, 0.07970486848155647, 0.07085836814293604]
2081 5217
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7785783952325714, 0.07085836814293604, 0.07970486848155647, 0.07085836814293604]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7786634753529456, 0.07086609512834073, 0.07960433439037289, 0.07086609512834073]
siam score:  -0.88154334
maxi score, test score, baseline:  0.0381 0.25 0.25
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.   ]
 [0.082]
 [0.071]
 [0.067]] [[-0.523]
 [ 0.089]
 [-0.134]
 [-0.135]
 [-0.213]] [[0.047]
 [0.   ]
 [0.082]
 [0.071]
 [0.067]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7788275377171172, 0.07088099528936351, 0.07941047170415572, 0.07088099528936351]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7789066639239005, 0.07088818153971499, 0.07931697299666958, 0.07088818153971499]
maxi score, test score, baseline:  0.0381 0.25 0.25
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7789839438572597, 0.07089520011132155, 0.0792256559200973, 0.07089520011132155]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7789839438572597, 0.07089520011132155, 0.0792256559200973, 0.07089520011132155]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7789839438572597, 0.07089520011132155, 0.0792256559200973, 0.07089520011132155]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7789839438572597, 0.07089520011132155, 0.0792256559200973, 0.07089520011132155]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7789839438572597, 0.07089520011132155, 0.0792256559200973, 0.07089520011132155]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [0.7789839438572597, 0.07089520011132155, 0.0792256559200973, 0.07089520011132155]
from probs:  [0.7789839438572597, 0.07089520011132155, 0.0792256559200973, 0.07089520011132155]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
line 256 mcts: sample exp_bonus 2.4296848735110017
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
probs:  [0.7789839438572597, 0.07089520011132155, 0.0792256559200973, 0.07089520011132155]
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
probs:  [0.7789839438572597, 0.07089520011132155, 0.0792256559200973, 0.07089520011132155]
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
probs:  [0.7789839438572597, 0.07089520011132155, 0.0792256559200973, 0.07089520011132155]
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
probs:  [0.7789839438572597, 0.07089520011132155, 0.0792256559200973, 0.07089520011132155]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.87836486
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
probs:  [0.7791332174877106, 0.07090875715800563, 0.07904926819627808, 0.07090875715800563]
first move QE:  0.04446211031160348
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
probs:  [0.7790594413914016, 0.07090205680524535, 0.07913644499810764, 0.07090205680524535]
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
probs:  [0.7791332174877106, 0.07090875715800563, 0.07904926819627808, 0.07090875715800563]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7791332174877106, 0.07090875715800563, 0.07904926819627808, 0.07090875715800563]
using another actor
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
2101 5238
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
probs:  [0.7792053303589158, 0.07091530645648852, 0.07896405672810702, 0.07091530645648852]
actor:  0 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.7792053303589158, 0.07091530645648852, 0.07896405672810702, 0.07091530645648852]
maxi score, test score, baseline:  0.0441 0.25 0.25
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.779275835622281, 0.07092170975185978, 0.07888074487399935, 0.07092170975185978]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.779275835622281, 0.07092170975185978, 0.07888074487399935, 0.07092170975185978]
maxi score, test score, baseline:  0.0441 0.25 0.25
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.779275835622281, 0.07092170975185978, 0.07888074487399935, 0.07092170975185978]
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.779275835622281, 0.07092170975185978, 0.07888074487399935, 0.07092170975185978]
maxi score, test score, baseline:  0.0441 0.25 0.25
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.779275835622281, 0.07092170975185978, 0.07888074487399935, 0.07092170975185978]
maxi score, test score, baseline:  0.0441 0.25 0.25
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.779344786442661, 0.070927971872557, 0.07879926981222482, 0.070927971872557]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.7794122336662, 0.07093409743643234, 0.07871957146093528, 0.07093409743643234]
maxi score, test score, baseline:  0.0441 0.25 0.25
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.7794122336662, 0.07093409743643234, 0.07871957146093528, 0.07093409743643234]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
first move QE:  0.043342891931568726
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.7794122336662, 0.07093409743643234, 0.07871957146093528, 0.07093409743643234]
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.7794122336662, 0.07093409743643234, 0.07871957146093528, 0.07093409743643234]
maxi score, test score, baseline:  0.0441 0.25 0.25
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.7794122336662, 0.07093409743643234, 0.07871957146093528, 0.07093409743643234]
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.7794122336662, 0.07093409743643234, 0.07871957146093528, 0.07093409743643234]
from probs:  [0.7794122336662, 0.07093409743643234, 0.07871957146093528, 0.07093409743643234]
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [0.7794122336662, 0.07093409743643234, 0.07871957146093528, 0.07093409743643234]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7794782259453767, 0.07094009086210895, 0.07864159233040534, 0.07094009086210895]
maxi score, test score, baseline:  0.0461 0.25 0.25
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7796060300070581, 0.07095169804031848, 0.07849057391230507, 0.07095169804031848]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.323]
 [0.293]
 [0.293]
 [0.293]] [[1.147]
 [1.904]
 [0.729]
 [0.729]
 [0.729]] [[1.118]
 [1.28 ]
 [0.827]
 [0.827]
 [0.827]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7797285482406819, 0.07096282515907261, 0.0783458014411727, 0.07096282515907261]
UNIT TEST: sample policy line 217 mcts : [0.375 0.167 0.292 0.    0.167]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.774012177681453, 0.07777222132148044, 0.07777222132148044, 0.07044337967558599]
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.774012177681453, 0.07777222132148044, 0.07777222132148044, 0.07044337967558599]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7683790259000641, 0.07720699136664529, 0.08448248646699706, 0.06993149626629351]
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7683790259000641, 0.07720699136664529, 0.08448248646699706, 0.06993149626629351]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0461 0.25 0.25
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7683790259000641, 0.07720699136664529, 0.08448248646699706, 0.06993149626629351]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7683790259000641, 0.07720699136664529, 0.08448248646699706, 0.06993149626629351]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7683790259000641, 0.07720699136664529, 0.08448248646699706, 0.06993149626629351]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.25 0.25
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7685520245192906, 0.07714932516023641, 0.08435143661189322, 0.06994721370857959]
siam score:  -0.8806513
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7687215681162394, 0.0770928106279202, 0.08422300400408843, 0.06996261725175196]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7687215681162394, 0.0770928106279202, 0.08422300400408843, 0.06996261725175196]
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7687215681162394, 0.0770928106279202, 0.08422300400408843, 0.06996261725175196]
maxi score, test score, baseline:  0.0461 0.25 0.25
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7687215681162394, 0.0770928106279202, 0.08422300400408843, 0.06996261725175196]
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7687215681162394, 0.0770928106279202, 0.08422300400408843, 0.06996261725175196]
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7687215681162394, 0.0770928106279202, 0.08422300400408843, 0.06996261725175196]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.25 0.25
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7688877591699634, 0.07703741361001223, 0.0840971110136852, 0.06997771620633926]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7692104736927452, 0.07692984210241824, 0.08385264841832152, 0.07000703578651497]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7692104736927452, 0.07692984210241824, 0.08385264841832152, 0.07000703578651497]
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [0.7692104736927452, 0.07692984210241824, 0.08385264841832152, 0.07000703578651497]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
actor:  0 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.17 ]
 [-0.322]
 [-0.246]
 [-0.055]
 [ 0.045]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
2136 5252
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7690506961463663, 0.07698310128454457, 0.08397368305082559, 0.06999251951826355]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7690506961463663, 0.07698310128454457, 0.08397368305082559, 0.06999251951826355]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.507]
 [1.507]
 [1.507]
 [1.507]
 [1.507]] [[0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]] [[2.998]
 [2.998]
 [2.998]
 [2.998]
 [2.998]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7692104736927452, 0.07692984210241824, 0.08385264841832152, 0.07000703578651497]
Printing some Q and Qe and total Qs values:  [[0.773]
 [0.867]
 [1.133]
 [0.72 ]
 [0.571]] [[2.087]
 [1.574]
 [1.25 ]
 [1.846]
 [1.489]] [[1.821]
 [1.543]
 [1.721]
 [1.525]
 [0.964]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7692104736927452, 0.07692984210241824, 0.08385264841832152, 0.07000703578651497]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
using explorer policy with actor:  1
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7692104736927452, 0.07692984210241824, 0.08385264841832152, 0.07000703578651497]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7693671828211245, 0.07687760572629185, 0.08373393817277534, 0.07002127327980835]
first move QE:  0.04159664496370768
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7693671828211245, 0.07687760572629185, 0.08373393817277534, 0.07002127327980835]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7693671828211245, 0.07687760572629185, 0.08373393817277534, 0.07002127327980835]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7693671828211245, 0.07687760572629185, 0.08373393817277534, 0.07002127327980835]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7693671828211245, 0.07687760572629185, 0.08373393817277534, 0.07002127327980835]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7693671828211245, 0.07687760572629185, 0.08373393817277534, 0.07002127327980835]
using another actor
from probs:  [0.7693671828211245, 0.07687760572629185, 0.08373393817277534, 0.07002127327980835]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7693671828211245, 0.07687760572629185, 0.08373393817277534, 0.07002127327980835]
2142 5258
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.90027636
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.463181518107479
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [0.7693671828211245, 0.07687760572629185, 0.08373393817277534, 0.07002127327980835]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.45 ]
 [0.431]
 [0.431]
 [0.423]] [[1.144]
 [1.319]
 [1.142]
 [1.142]
 [1.364]] [[1.235]
 [1.165]
 [1.069]
 [1.069]
 [1.126]]
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]] [[2.357]
 [2.357]
 [2.357]
 [2.357]
 [2.357]] [[2.574]
 [2.574]
 [2.574]
 [2.574]
 [2.574]]
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [0.7696717427230797, 0.0767760857589734, 0.08350322805959581, 0.07004894345835103]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [0.7698197588519706, 0.07672674704934311, 0.08339110293206067, 0.07006239116662553]
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [0.7698197588519706, 0.07672674704934311, 0.08339110293206067, 0.07006239116662553]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [0.7699650375728827, 0.07667832080903911, 0.08328105144488523, 0.07007559017319297]
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [0.7699650375728827, 0.07667832080903911, 0.08328105144488523, 0.07007559017319297]
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [0.7699650375728827, 0.07667832080903911, 0.08328105144488523, 0.07007559017319297]
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [0.7699650375728827, 0.07667832080903911, 0.08328105144488523, 0.07007559017319297]
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [0.7699650375728827, 0.07667832080903911, 0.08328105144488523, 0.07007559017319297]
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.034]
 [0.191]
 [0.157]
 [0.158]] [[-1.011]
 [-0.326]
 [-0.502]
 [-0.396]
 [-0.384]] [[0.131]
 [0.108]
 [0.363]
 [0.33 ]
 [0.335]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.17 ]
 [0.012]
 [0.283]
 [0.18 ]
 [0.179]] [[-0.199]
 [ 0.099]
 [-0.318]
 [ 0.234]
 [ 0.162]] [[0.224]
 [0.008]
 [0.412]
 [0.39 ]
 [0.365]]
actor:  0 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0521 0.25 0.25
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [0.7699650375728827, 0.07667832080903911, 0.08328105144488523, 0.07007559017319297]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.637]
 [0.797]
 [0.637]
 [0.637]] [[1.486]
 [1.486]
 [2.098]
 [1.486]
 [1.486]] [[1.17 ]
 [1.17 ]
 [1.694]
 [1.17 ]
 [1.17 ]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.321]] [[3.101]
 [3.101]
 [3.101]
 [3.101]
 [3.006]] [[2.083]
 [2.083]
 [2.083]
 [2.083]
 [1.666]]
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [0.7702476810286462, 0.07658410632378462, 0.08306694347055903, 0.07010126917701022]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [0.7654658566991895, 0.0760502217330908, 0.08881717793616671, 0.06966674363155285]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [0.7606089615082288, 0.08191133565257068, 0.08825430411851141, 0.06922539872068924]
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [0.7606089615082288, 0.08191133565257068, 0.08825430411851141, 0.06922539872068924]
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [0.7606089615082288, 0.08191133565257068, 0.08825430411851141, 0.06922539872068924]
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [0.7606089615082288, 0.08191133565257068, 0.08825430411851141, 0.06922539872068924]
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [0.7606089615082288, 0.08191133565257068, 0.08825430411851141, 0.06922539872068924]
maxi score, test score, baseline:  0.0521 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.534]
 [0.663]
 [0.512]
 [0.526]] [[0.859]
 [1.607]
 [1.254]
 [2.091]
 [1.863]] [[0.101]
 [0.534]
 [0.663]
 [0.512]
 [0.526]]
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [0.7606089615082288, 0.08191133565257068, 0.08825430411851141, 0.06922539872068924]
actor:  0 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2162 5270
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.107]
 [0.107]
 [0.103]
 [0.107]] [[0.76 ]
 [0.76 ]
 [0.76 ]
 [0.992]
 [0.76 ]] [[0.819]
 [0.819]
 [0.819]
 [1.121]
 [0.819]]
maxi score, test score, baseline:  0.0541 0.25 0.25
probs:  [0.7608283967994777, 0.08181957397678735, 0.08810669270662709, 0.0692453365171079]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.398]
 [0.743]
 [0.62 ]
 [0.545]] [[2.276]
 [1.505]
 [1.82 ]
 [1.759]
 [1.579]] [[1.128]
 [0.654]
 [1.451]
 [1.185]
 [0.974]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]]
maxi score, test score, baseline:  0.0541 0.25 0.25
maxi score, test score, baseline:  0.0541 0.25 0.25
probs:  [0.7608283967994777, 0.08181957397678735, 0.08810669270662709, 0.0692453365171079]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0541 0.25 0.25
probs:  [0.7610440015699272, 0.08172941411721911, 0.08796165803880358, 0.06926492627405016]
maxi score, test score, baseline:  0.0541 0.25 0.25
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0541 0.25 0.25
maxi score, test score, baseline:  0.0541 0.25 0.25
probs:  [0.761255875251606, 0.08164081449418419, 0.08781913322834256, 0.06928417702586744]
using another actor
maxi score, test score, baseline:  0.0541 0.25 0.25
probs:  [0.761255875251606, 0.08164081449418419, 0.08781913322834256, 0.06928417702586744]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.2544969206518353
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7614641138646768, 0.08155373495474713, 0.08767905368366542, 0.06930309749691056]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.761668810162535, 0.08146813671204339, 0.08754135701070849, 0.06932169611471319]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.761668810162535, 0.08146813671204339, 0.08754135701070849, 0.06932169611471319]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.761668810162535, 0.08146813671204339, 0.08754135701070849, 0.06932169611471319]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.761668810162535, 0.08146813671204339, 0.08754135701070849, 0.06932169611471319]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.163]
 [0.163]
 [0.159]
 [0.163]] [[0.899]
 [0.899]
 [0.899]
 [0.722]
 [0.899]] [[0.395]
 [0.395]
 [0.395]
 [0.327]
 [0.395]]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.761668810162535, 0.08146813671204339, 0.08754135701070849, 0.06932169611471319]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7620679313119622, 0.08130123545699208, 0.08727287313993042, 0.06935796009111542]
Printing some Q and Qe and total Qs values:  [[1.332]
 [1.318]
 [1.332]
 [1.332]
 [1.286]] [[1.595]
 [1.574]
 [1.595]
 [1.595]
 [1.906]] [[2.074]
 [2.04 ]
 [2.074]
 [2.074]
 [2.087]]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7624539204567204, 0.08113982563747035, 0.08701322300660183, 0.06939303089920744]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7624539204567204, 0.08113982563747035, 0.08701322300660183, 0.06939303089920744]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7624539204567204, 0.08113982563747035, 0.08701322300660183, 0.06939303089920744]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7624539204567204, 0.08113982563747035, 0.08701322300660183, 0.06939303089920744]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7624539204567204, 0.08113982563747035, 0.08701322300660183, 0.06939303089920744]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2183 5291
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.065]
 [0.066]
 [0.065]
 [0.068]] [[1.209]
 [1.209]
 [1.383]
 [1.209]
 [1.481]] [[0.511]
 [0.511]
 [0.687]
 [0.511]
 [0.789]]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.762262526542108, 0.08121986119710893, 0.0871419713305437, 0.06937564093023939]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.762262526542108, 0.08121986119710893, 0.0871419713305437, 0.06937564093023939]
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.077]] [[1.14]
 [1.14]
 [1.14]
 [1.14]
 [1.14]] [[1.293]
 [1.293]
 [1.293]
 [1.293]
 [1.293]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2186 5295
siam score:  -0.8830705
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7624539204567204, 0.08113982563747035, 0.08701322300660183, 0.06939303089920744]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7624539204567204, 0.08113982563747035, 0.08701322300660183, 0.06939303089920744]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
from probs:  [0.7624539204567204, 0.08113982563747035, 0.08701322300660183, 0.06939303089920744]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.025]
 [0.058]
 [0.049]
 [0.048]] [[-0.144]
 [-0.325]
 [-0.113]
 [-0.293]
 [-0.154]] [[0.048]
 [0.025]
 [0.058]
 [0.049]
 [0.048]]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7624539204567204, 0.08113982563747035, 0.08701322300660183, 0.06939303089920744]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7624539204567204, 0.08113982563747035, 0.08701322300660183, 0.06939303089920744]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.9898802377758263
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7626421914092849, 0.08106109601284922, 0.086886575460682, 0.06941013711718365]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7626421914092849, 0.08106109601284922, 0.086886575460682, 0.06941013711718365]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7630096652625881, 0.08090742876819719, 0.08663938033537695, 0.06944352563383767]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7630096652625881, 0.08090742876819719, 0.08663938033537695, 0.06944352563383767]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7630096652625881, 0.08090742876819719, 0.08663938033537695, 0.06944352563383767]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[1.142]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]] [[1.282]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7633655260028177, 0.08075861780126892, 0.08639999720789328, 0.06947585898802018]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7633655260028177, 0.08075861780126892, 0.08639999720789328, 0.06947585898802018]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7633655260028177, 0.08075861780126892, 0.08639999720789328, 0.06947585898802018]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7637103155726727, 0.08061443648684848, 0.08616806152006656, 0.06950718642041233]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7638787188764555, 0.08054401495494899, 0.08605477869625146, 0.06952248747234405]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7638787188764555, 0.08054401495494899, 0.08605477869625146, 0.06952248747234405]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7642078458902243, 0.08040638328158825, 0.08583337901657744, 0.06955239181160991]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7642078458902243, 0.08040638328158825, 0.08583337901657744, 0.06955239181160991]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7642078458902243, 0.08040638328158825, 0.08583337901657744, 0.06955239181160991]
siam score:  -0.8819095
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [0.7642078458902243, 0.08040638328158825, 0.08583337901657744, 0.06955239181160991]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20026
siam score:  -0.8812021
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7646831934281741, 0.08020760629173639, 0.08551361859511963, 0.06959558168496992]
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7646831934281741, 0.08020760629173639, 0.08551361859511963, 0.06959558168496992]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7646831934281741, 0.08020760629173639, 0.08551361859511963, 0.06959558168496992]
2213 5319
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7646831934281741, 0.08020760629173639, 0.08551361859511963, 0.06959558168496992]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7646831934281741, 0.08020760629173639, 0.08551361859511963, 0.06959558168496992]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.54]
 [0.6 ]
 [0.54]
 [0.54]
 [0.54]] [[1.161]
 [1.143]
 [1.161]
 [1.161]
 [1.161]] [[1.248]
 [1.355]
 [1.248]
 [1.248]
 [1.248]]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7648369676264475, 0.08014330224089071, 0.08541017659001039, 0.06960955354265137]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  2216
from probs:  [0.7649884899113981, 0.08007993987694985, 0.08530824941919754, 0.06962332079245447]
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.123]
 [0.123]
 [0.123]
 [0.119]] [[3.812]
 [3.812]
 [3.812]
 [3.812]
 [5.022]] [[0.759]
 [0.759]
 [0.759]
 [0.759]
 [1.076]]
Printing some Q and Qe and total Qs values:  [[0.982]
 [1.196]
 [1.463]
 [0.282]
 [1.095]] [[2.196]
 [1.785]
 [0.403]
 [1.771]
 [2.412]] [[2.021]
 [2.239]
 [2.294]
 [0.861]
 [2.245]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7651378093899517, 0.08001749866477664, 0.08520780404905827, 0.06963688789621339]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7651378093899517, 0.08001749866477664, 0.08520780404905827, 0.06963688789621339]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7651378093899517, 0.08001749866477664, 0.08520780404905827, 0.06963688789621339]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7651378093899517, 0.08001749866477664, 0.08520780404905827, 0.06963688789621339]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7651378093899517, 0.08001749866477664, 0.08520780404905827, 0.06963688789621339]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7651378093899517, 0.08001749866477664, 0.08520780404905827, 0.06963688789621339]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[1.831]
 [1.798]
 [1.798]
 [1.798]
 [1.798]] [[1.752]
 [1.752]
 [1.752]
 [1.752]
 [1.752]]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7649884899113981, 0.08007993987694985, 0.08530824941919754, 0.06962332079245447]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7649884899113981, 0.08007993987694985, 0.08530824941919754, 0.06962332079245447]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7651378093899517, 0.08001749866477664, 0.08520780404905827, 0.06963688789621339]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.349]
 [0.41 ]
 [0.5  ]
 [0.426]] [[3.5  ]
 [0.852]
 [1.607]
 [1.69 ]
 [2.188]] [[0.332]
 [0.349]
 [0.41 ]
 [0.5  ]
 [0.426]]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [0.7651378093899517, 0.08001749866477664, 0.08520780404905827, 0.06963688789621339]
actor:  0 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.25 0.25
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7651378093899517, 0.08001749866477664, 0.08520780404905827, 0.06963688789621339]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7651378093899517, 0.08001749866477664, 0.08520780404905827, 0.06963688789621339]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8940002
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7652849737515209, 0.07995595866199813, 0.08510880839951335, 0.06965025918696771]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7652849737515209, 0.07995595866199813, 0.08510880839951335, 0.06965025918696771]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7652849737515209, 0.07995595866199813, 0.08510880839951335, 0.06965025918696771]
from probs:  [0.7652849737515209, 0.07995595866199813, 0.08510880839951335, 0.06965025918696771]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7652849737515209, 0.07995595866199813, 0.08510880839951335, 0.06965025918696771]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]] [[1.61 ]
 [1.61 ]
 [1.61 ]
 [1.61 ]
 [1.555]] [[1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.242]]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7652849737515209, 0.07995595866199813, 0.08510880839951335, 0.06965025918696771]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7652849737515209, 0.07995595866199813, 0.08510880839951335, 0.06965025918696771]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7652849737515209, 0.07995595866199813, 0.08510880839951335, 0.06965025918696771]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.005]
 [0.025]
 [0.025]
 [0.036]] [[-0.191]
 [ 0.152]
 [ 0.29 ]
 [ 0.29 ]
 [ 0.495]] [[0.01 ]
 [0.447]
 [0.671]
 [0.671]
 [0.967]]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7652849737515209, 0.07995595866199813, 0.08510880839951335, 0.06965025918696771]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7652849737515209, 0.07995595866199813, 0.08510880839951335, 0.06965025918696771]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7654300293187857, 0.07989530049777052, 0.08501123130986764, 0.06966343887357627]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7655730210963066, 0.07983550535245053, 0.08491504250610872, 0.06967643104513414]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7655730210963066, 0.07983550535245053, 0.08491504250610872, 0.06967643104513414]
2234 5338
maxi score, test score, baseline:  0.0601 0.25 0.25
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7654300293187857, 0.07989530049777052, 0.08501123130986764, 0.06966343887357627]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.267]] [[1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.233]] [[1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.705]]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7654300293187857, 0.07989530049777052, 0.08501123130986764, 0.06966343887357627]
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.48 ]
 [0.479]
 [0.159]
 [0.443]] [[1.049]
 [1.006]
 [0.569]
 [0.178]
 [0.972]] [[1.725]
 [1.798]
 [1.233]
 [0.109]
 [1.684]]
maxi score, test score, baseline:  0.0601 0.25 0.25
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7655730210963066, 0.07983550535245053, 0.08491504250610872, 0.06967643104513414]
maxi score, test score, baseline:  0.0601 0.25 0.25
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7655730210963066, 0.07983550535245053, 0.08491504250610872, 0.06967643104513414]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7655730210963066, 0.07983550535245053, 0.08491504250610872, 0.06967643104513414]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7655730210963066, 0.07983550535245053, 0.08491504250610872, 0.06967643104513414]
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.218]
 [0.259]
 [0.218]
 [0.218]] [[0.055]
 [0.055]
 [0.352]
 [0.055]
 [0.055]] [[0.439]
 [0.439]
 [0.719]
 [0.439]
 [0.439]]
siam score:  -0.90271056
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7655730210963066, 0.07983550535245053, 0.08491504250610872, 0.06967643104513414]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7655730210963066, 0.07983550535245053, 0.08491504250610872, 0.06967643104513414]
maxi score, test score, baseline:  0.0601 0.25 0.25
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7657139928170783, 0.07977655493812816, 0.08482021256959103, 0.06968923967520242]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.05118114790224592
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.208 0.125 0.625]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7657139928170783, 0.07977655493812816, 0.08482021256959103, 0.06968923967520242]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7657139928170783, 0.07977655493812816, 0.08482021256959103, 0.06968923967520242]
maxi score, test score, baseline:  0.0601 0.25 0.25
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  2244
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7658529869871226, 0.07971843147997895, 0.0847267129070384, 0.06970186862586007]
maxi score, test score, baseline:  0.0601 0.25 0.25
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7658529869871226, 0.07971843147997895, 0.0847267129070384, 0.06970186862586007]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7658529869871226, 0.07971843147997895, 0.0847267129070384, 0.06970186862586007]
maxi score, test score, baseline:  0.0601 0.25 0.25
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0601 0.25 0.25
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7659900449282189, 0.07966111769839522, 0.08463451572179972, 0.06971432165158617]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7659900449282189, 0.07966111769839522, 0.08463451572179972, 0.06971432165158617]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7659900449282189, 0.07966111769839522, 0.08463451572179972, 0.06971432165158617]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7661252068188612, 0.07960459679185923, 0.0845435939862981, 0.0697266024029815]
first move QE:  0.03341015485343219
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7661252068188612, 0.07960459679185923, 0.0845435939862981, 0.0697266024029815]
first move QE:  0.03318693489870928
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7661252068188612, 0.07960459679185923, 0.0845435939862981, 0.0697266024029815]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7661252068188612, 0.07960459679185923, 0.0845435939862981, 0.0697266024029815]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7661252068188612, 0.07960459679185923, 0.0845435939862981, 0.0697266024029815]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7661252068188612, 0.07960459679185923, 0.0845435939862981, 0.0697266024029815]
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [0.7661252068188612, 0.07960459679185923, 0.0845435939862981, 0.0697266024029815]
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0621 0.25 0.25
maxi score, test score, baseline:  0.0621 0.25 0.25
probs:  [0.7662585117335268, 0.07954885242052206, 0.08445392141561495, 0.06973871443033629]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0621 0.25 0.25
probs:  [0.7662585117335268, 0.07954885242052206, 0.08445392141561495, 0.06973871443033629]
maxi score, test score, baseline:  0.0621 0.25 0.25
2257 5347
siam score:  -0.9010306
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0621 0.25 0.25
probs:  [0.7663899976803371, 0.0794938686904547, 0.084365472442156, 0.06975066118705212]
actor:  0 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0641 0.25 0.25
maxi score, test score, baseline:  0.0641 0.25 0.25
maxi score, test score, baseline:  0.0641 0.25 0.25
probs:  [0.7663899976803371, 0.0794938686904547, 0.084365472442156, 0.06975066118705212]
maxi score, test score, baseline:  0.0641 0.25 0.25
probs:  [0.7663899976803371, 0.0794938686904547, 0.084365472442156, 0.06975066118705212]
maxi score, test score, baseline:  0.0641 0.25 0.25
probs:  [0.7663899976803371, 0.0794938686904547, 0.084365472442156, 0.06975066118705212]
maxi score, test score, baseline:  0.0641 0.25 0.25
probs:  [0.7663899976803371, 0.0794938686904547, 0.084365472442156, 0.06975066118705212]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.425]
 [0.38 ]
 [0.38 ]
 [0.38 ]] [[0.566]
 [0.668]
 [0.566]
 [0.566]
 [0.566]] [[1.175]
 [1.3  ]
 [1.175]
 [1.175]
 [1.175]]
maxi score, test score, baseline:  0.0641 0.25 0.25
probs:  [0.7663899976803371, 0.0794938686904547, 0.084365472442156, 0.06975066118705212]
Starting evaluation
maxi score, test score, baseline:  0.0641 0.25 0.25
probs:  [0.7663899976803371, 0.0794938686904547, 0.084365472442156, 0.06975066118705212]
maxi score, test score, baseline:  0.0641 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.262]
 [0.145]
 [0.165]
 [0.176]] [[0.826]
 [0.623]
 [0.618]
 [0.826]
 [1.102]] [[0.165]
 [0.262]
 [0.145]
 [0.165]
 [0.176]]
maxi score, test score, baseline:  0.0641 0.25 0.25
probs:  [0.7663899976803371, 0.0794938686904547, 0.084365472442156, 0.06975066118705212]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.957]
 [0.671]
 [0.671]
 [0.901]] [[1.122]
 [0.178]
 [1.122]
 [1.122]
 [1.525]] [[0.671]
 [0.957]
 [0.671]
 [0.671]
 [0.901]]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.685]
 [0.935]
 [0.672]
 [0.661]] [[1.456]
 [1.215]
 [0.63 ]
 [1.391]
 [2.735]] [[0.654]
 [0.685]
 [0.935]
 [0.672]
 [0.661]]
maxi score, test score, baseline:  0.0641 0.25 0.25
probs:  [0.7663899976803371, 0.0794938686904547, 0.084365472442156, 0.06975066118705212]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.7663899976803371, 0.0794938686904547, 0.084365472442156, 0.06975066118705212]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.615]
 [0.619]
 [0.366]
 [0.544]] [[2.057]
 [1.658]
 [2.724]
 [3.146]
 [2.606]] [[1.163]
 [1.262]
 [2.149]
 [2.08 ]
 [1.928]]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 2.6492698581260448
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20026
siam score:  -0.89255315
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[4.74 ]
 [2.847]
 [2.847]
 [2.847]
 [2.847]] [[2.059]
 [1.237]
 [1.237]
 [1.237]
 [1.237]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.541]
 [0.555]
 [0.555]
 [0.555]] [[1.755]
 [3.544]
 [1.135]
 [1.135]
 [1.135]] [[1.4  ]
 [2.248]
 [1.052]
 [1.052]
 [1.052]]
maxi score, test score, baseline:  0.1041 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.055]
 [0.199]
 [0.151]
 [0.141]] [[ 1.827]
 [-0.041]
 [-0.238]
 [-0.414]
 [-0.362]] [[1.742]
 [0.234]
 [0.301]
 [0.102]
 [0.125]]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[2.839]
 [2.183]
 [2.183]
 [2.183]
 [2.183]] [[1.935]
 [1.414]
 [1.414]
 [1.414]
 [1.414]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.093235649662882
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.14]
 [0.14]
 [0.14]
 [0.14]
 [0.14]] [[1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.449]] [[1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.236]]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.03148819323376888
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]] [[-0.562]
 [-0.562]
 [-0.562]
 [-0.562]
 [-0.562]] [[0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.198]] [[1.972]
 [1.972]
 [1.972]
 [1.972]
 [4.688]] [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [1.836]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.355]] [[1.011]
 [1.011]
 [1.011]
 [1.011]
 [1.011]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.098]] [[1.211]
 [1.211]
 [1.211]
 [1.211]
 [1.686]] [[0.556]
 [0.556]
 [0.556]
 [0.556]
 [1.016]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9143396
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1041 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.141]
 [1.141]
 [1.079]
 [1.017]
 [1.141]] [[1.783]
 [1.783]
 [1.467]
 [2.534]
 [1.783]] [[1.773]
 [1.773]
 [1.457]
 [2.133]
 [1.773]]
siam score:  -0.9101903
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.1223412714730827
siam score:  -0.9087046
siam score:  -0.9110758
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90842813
line 256 mcts: sample exp_bonus 1.570991255224398
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.276]
 [0.431]
 [0.276]
 [0.276]] [[ 0.196]
 [ 0.196]
 [-0.172]
 [ 0.196]
 [ 0.196]] [[0.853]
 [0.853]
 [0.918]
 [0.853]
 [0.853]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041 1.0 1.0
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.253]
 [0.279]
 [0.253]
 [0.252]] [[1.613]
 [1.613]
 [0.737]
 [1.613]
 [4.252]] [[0.6  ]
 [0.6  ]
 [0.319]
 [0.6  ]
 [1.486]]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.274]
 [0.198]
 [0.261]
 [0.274]] [[1.695]
 [1.695]
 [1.671]
 [1.784]
 [1.695]] [[1.929]
 [1.929]
 [1.752]
 [1.991]
 [1.929]]
siam score:  -0.9105985
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.447]
 [0.613]
 [0.493]
 [0.515]] [[1.426]
 [0.988]
 [1.204]
 [1.37 ]
 [2.84 ]] [[1.7  ]
 [0.971]
 [1.591]
 [1.571]
 [3.576]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2320 5386
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.756]
 [0.612]
 [0.612]
 [0.494]] [[0.91 ]
 [0.945]
 [0.91 ]
 [0.91 ]
 [1.224]] [[0.909]
 [1.134]
 [0.909]
 [0.909]
 [0.963]]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
first move QE:  0.02878623054267417
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1061 1.0 1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[1.048]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[2.052]
 [1.659]
 [1.659]
 [1.659]
 [1.659]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.016]
 [0.237]
 [0.441]
 [0.364]] [[-0.733]
 [ 0.045]
 [ 0.209]
 [ 0.458]
 [-0.188]] [[0.197]
 [0.191]
 [0.742]
 [1.316]
 [0.732]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.306]
 [0.432]
 [0.383]
 [0.339]] [[0.861]
 [0.861]
 [0.548]
 [0.305]
 [0.11 ]] [[1.619]
 [1.619]
 [1.558]
 [1.218]
 [0.934]]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.855]
 [1.09 ]
 [0.855]
 [0.855]
 [0.81 ]] [[1.717]
 [1.811]
 [1.717]
 [1.717]
 [2.002]] [[1.882]
 [2.383]
 [1.882]
 [1.882]
 [1.888]]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.22 ]
 [1.29 ]
 [1.22 ]
 [1.22 ]
 [1.272]] [[1.441]
 [1.234]
 [1.441]
 [1.441]
 [2.107]] [[2.093]
 [2.164]
 [2.093]
 [2.093]
 [2.42 ]]
first move QE:  0.028223185420929425
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.3  ]
 [0.189]
 [0.239]
 [0.243]] [[0.79 ]
 [1.119]
 [1.312]
 [1.273]
 [1.187]] [[0.625]
 [0.746]
 [0.653]
 [0.728]
 [0.677]]
maxi score, test score, baseline:  0.1101 1.0 1.0
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.167 0.125 0.042 0.625]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.042 0.5   0.042]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.9104682
maxi score, test score, baseline:  0.1121 1.0 1.0
siam score:  -0.90994805
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.163]
 [0.158]
 [0.161]
 [0.163]] [[1.583]
 [1.583]
 [1.429]
 [1.56 ]
 [1.494]] [[1.761]
 [1.761]
 [1.551]
 [1.728]
 [1.645]]
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[2.665]
 [2.932]
 [2.932]
 [2.932]
 [2.932]] [[1.816]
 [2.159]
 [2.159]
 [2.159]
 [2.159]]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.271]
 [0.271]
 [0.271]
 [0.271]] [[2.746]
 [2.609]
 [2.609]
 [2.609]
 [2.609]] [[1.667]
 [1.569]
 [1.569]
 [1.569]
 [1.569]]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9020497
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.02704603202781133
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2358 5419
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.31 ]
 [1.31 ]
 [1.391]
 [1.31 ]
 [1.31 ]] [[2.997]
 [2.997]
 [2.03 ]
 [2.997]
 [2.997]] [[2.665]
 [2.665]
 [2.516]
 [2.665]
 [2.665]]
Printing some Q and Qe and total Qs values:  [[1.166]
 [1.141]
 [1.374]
 [1.051]
 [1.198]] [[3.061]
 [3.305]
 [2.097]
 [3.615]
 [3.746]] [[1.94 ]
 [1.972]
 [2.034]
 [1.895]
 [2.234]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.939]
 [0.608]
 [0.608]
 [0.608]] [[2.309]
 [2.293]
 [2.309]
 [2.309]
 [2.309]] [[1.462]
 [2.008]
 [1.462]
 [1.462]
 [1.462]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89891005
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.8978155
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2370 5433
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.278]
 [0.363]
 [0.264]
 [0.278]] [[-0.609]
 [-0.609]
 [-0.247]
 [-0.39 ]
 [-0.609]] [[0.653]
 [0.653]
 [1.029]
 [0.788]
 [0.653]]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  2373
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
2375 5434
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]] [[2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91143453
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.933]
 [1.182]
 [1.175]
 [0.933]
 [0.933]] [[2.177]
 [1.637]
 [1.182]
 [2.177]
 [2.177]] [[1.957]
 [2.097]
 [1.779]
 [1.957]
 [1.957]]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.292]
 [0.23 ]
 [0.643]
 [0.593]] [[2.786]
 [2.667]
 [0.737]
 [2.04 ]
 [3.552]] [[1.423]
 [1.102]
 [0.064]
 [1.051]
 [1.788]]
siam score:  -0.9009209
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9052445
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
siam score:  -0.90673375
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[4.492]
 [4.492]
 [4.492]
 [4.492]
 [4.492]] [[0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.1  ]] [[0.802]
 [0.802]
 [0.802]
 [0.802]
 [1.21 ]] [[1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.705]]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6366086305205971
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90038574
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.6035512806989854
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8935379
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.682]
 [0.986]
 [0.682]
 [0.682]] [[2.518]
 [2.518]
 [2.688]
 [2.518]
 [2.518]] [[1.583]
 [1.583]
 [2.134]
 [1.583]
 [1.583]]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
using explorer policy with actor:  1
first move QE:  0.02631983253392804
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88895917
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.390446064867039
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.2533607342343216
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1341 1.0 1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.487]
 [0.472]
 [0.415]
 [0.43 ]] [[1.701]
 [1.471]
 [0.953]
 [2.014]
 [2.117]] [[1.175]
 [1.139]
 [0.763]
 [1.357]
 [1.456]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
2424 5461
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.164]
 [0.148]
 [0.162]
 [0.16 ]] [[1.465]
 [1.61 ]
 [1.594]
 [1.828]
 [1.615]] [[1.009]
 [1.148]
 [1.097]
 [1.421]
 [1.147]]
maxi score, test score, baseline:  0.1601 1.0 1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.88720626
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.16 ]
 [1.16 ]
 [1.361]
 [1.16 ]
 [1.16 ]] [[2.085]
 [2.085]
 [1.363]
 [2.085]
 [2.085]] [[1.942]
 [1.942]
 [2.104]
 [1.942]
 [1.942]]
maxi score, test score, baseline:  0.1601 1.0 1.0
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.712930446948426
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[2.352]
 [2.352]
 [2.352]
 [2.352]
 [2.352]] [[1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]] [[1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89193475
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.0548239868874907
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
siam score:  -0.89765745
Printing some Q and Qe and total Qs values:  [[0.869]
 [1.092]
 [1.092]
 [1.092]
 [1.059]] [[3.806]
 [5.018]
 [5.018]
 [5.018]
 [5.386]] [[1.741]
 [2.51 ]
 [2.51 ]
 [2.51 ]
 [2.563]]
maxi score, test score, baseline:  0.1621 1.0 1.0
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2460 5476
maxi score, test score, baseline:  0.1661 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[2.013]
 [1.934]
 [1.934]
 [1.934]
 [1.934]] [[1.801]
 [1.63 ]
 [1.63 ]
 [1.63 ]
 [1.63 ]]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2464 5476
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8967594
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89268446
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.477]
 [0.477]
 [0.477]
 [0.477]] [[2.607]
 [2.676]
 [2.676]
 [2.676]
 [2.676]] [[1.991]
 [2.073]
 [2.073]
 [2.073]
 [2.073]]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[2.034]
 [2.034]
 [2.034]
 [2.034]
 [2.034]] [[1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.677]]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
first move QE:  0.027547941717655604
Printing some Q and Qe and total Qs values:  [[1.37]
 [1.37]
 [1.37]
 [1.37]
 [1.37]] [[2.309]
 [2.309]
 [2.309]
 [2.309]
 [2.309]] [[2.951]
 [2.951]
 [2.951]
 [2.951]
 [2.951]]
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [1.054]
 [1.198]
 [0.915]
 [1.031]] [[1.587]
 [1.84 ]
 [0.128]
 [1.545]
 [1.801]] [[1.272]
 [1.863]
 [1.23 ]
 [1.511]
 [1.81 ]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[2.492]
 [2.278]
 [2.278]
 [2.278]
 [2.278]] [[1.855]
 [1.583]
 [1.583]
 [1.583]
 [1.583]]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8882382
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.2448],
        [0.0000],
        [0.4688],
        [0.2320],
        [0.2270],
        [0.1049],
        [0.1644],
        [0.0000],
        [0.0000],
        [0.2415]], dtype=torch.float64)
0.0 0.24476662763159177
0.0 0.0
0.0 0.4687810083029181
0.0 0.2320058442226289
0.0 0.2269592038604868
0.0 0.10491037254612516
0.0 0.16438721857850172
0.0 0.0
0.0 0.0
0.0 0.24151027104246506
maxi score, test score, baseline:  0.1681 1.0 1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.179]
 [0.31 ]
 [0.272]
 [0.288]] [[-0.208]
 [ 0.004]
 [-0.289]
 [-0.235]
 [-0.15 ]] [[0.348]
 [0.185]
 [0.349]
 [0.29 ]
 [0.352]]
maxi score, test score, baseline:  0.1681 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.25  0.25  0.375 0.083 0.042]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
actor:  0 policy actor:  0  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.353]] [[4.255]
 [4.255]
 [4.255]
 [4.255]
 [4.901]] [[0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [1.088]]
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.393]] [[3.426]
 [3.426]
 [3.426]
 [3.426]
 [5.642]] [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [1.23 ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.052024398615763
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.332]
 [0.193]
 [0.21 ]
 [0.206]] [[1.719]
 [1.394]
 [2.128]
 [1.719]
 [1.904]] [[0.21 ]
 [0.332]
 [0.193]
 [0.21 ]
 [0.206]]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
2497 5485
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.233]
 [0.233]
 [0.246]
 [0.233]] [[2.869]
 [2.869]
 [2.869]
 [3.034]
 [2.869]] [[1.849]
 [1.849]
 [1.849]
 [2.041]
 [1.849]]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.029324016130580093
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[1.621]
 [1.621]
 [1.621]
 [1.621]
 [1.621]] [[1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.699]]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.254]
 [0.254]
 [0.253]
 [0.254]] [[2.312]
 [2.312]
 [2.312]
 [2.492]
 [2.312]] [[1.965]
 [1.965]
 [1.965]
 [2.143]
 [1.965]]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.528943451016725
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.476]
 [0.488]
 [0.488]
 [0.47 ]] [[3.538]
 [3.256]
 [3.613]
 [3.613]
 [3.454]] [[2.006]
 [1.75 ]
 [2.076]
 [2.076]
 [1.915]]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87215894
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.2249125957420204
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2513 5490
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.158]
 [0.158]
 [0.158]
 [0.164]] [[0.323]
 [0.598]
 [0.598]
 [0.598]
 [0.109]] [[0.632]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.311]]
siam score:  -0.8737221
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87787217
siam score:  -0.8773987
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.452943278657574
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.03233738214671622
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.276]
 [0.194]
 [0.274]
 [0.3  ]] [[2.819]
 [3.17 ]
 [0.37 ]
 [2.972]
 [2.44 ]] [[1.684]
 [1.852]
 [0.299]
 [1.748]
 [1.517]]
siam score:  -0.8702475
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86908513
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.07 ]
 [0.969]
 [1.037]
 [0.835]
 [1.037]] [[2.017]
 [2.153]
 [1.85 ]
 [4.199]
 [1.85 ]] [[1.159]
 [1.128]
 [1.046]
 [1.991]
 [1.046]]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.164]
 [0.33 ]
 [0.224]
 [0.235]] [[ 0.17 ]
 [-0.1  ]
 [ 0.057]
 [-0.212]
 [ 0.068]] [[0.856]
 [0.371]
 [0.896]
 [0.343]
 [0.726]]
siam score:  -0.87680787
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  0.03223474755098455
siam score:  -0.8768931
Printing some Q and Qe and total Qs values:  [[1.122]
 [1.122]
 [1.095]
 [0.823]
 [0.885]] [[2.571]
 [2.146]
 [1.981]
 [2.555]
 [3.154]] [[2.427]
 [2.284]
 [2.176]
 [1.823]
 [2.148]]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
line 256 mcts: sample exp_bonus 2.074779350323277
line 256 mcts: sample exp_bonus 2.1211647575219716
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
siam score:  -0.87731254
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.09456951333234
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.292 0.    0.667 0.042 0.   ]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
siam score:  -0.88130873
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.031527169276744114
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
in main func line 156:  2540
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
siam score:  -0.8896879
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.97 ]
 [1.113]
 [0.679]
 [0.97 ]] [[2.102]
 [0.872]
 [0.331]
 [1.787]
 [0.872]] [[1.313]
 [1.416]
 [1.52 ]
 [1.139]
 [1.416]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.445]] [[1.493]
 [1.493]
 [1.493]
 [1.493]
 [1.76 ]] [[1.831]
 [1.831]
 [1.831]
 [1.831]
 [2.059]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.604249185062715
siam score:  -0.8842569
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.03037671963814506
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]] [[2.061]
 [2.061]
 [2.061]
 [2.061]
 [2.061]] [[2.639]
 [2.639]
 [2.639]
 [2.639]
 [2.639]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.156504239896996
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.57 ]
 [0.754]
 [0.786]
 [0.786]] [[1.904]
 [1.937]
 [1.264]
 [1.363]
 [1.363]] [[1.445]
 [1.541]
 [1.46 ]
 [1.591]
 [1.591]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.375 0.333 0.167]
start point for exploration sampling:  20026
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.03030887534163591
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.03034206566143162
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.8692404244308387
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[0.852]
 [1.074]
 [1.074]
 [1.074]
 [1.074]] [[1.29 ]
 [1.101]
 [1.101]
 [1.101]
 [1.101]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  2571
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.8814433
Printing some Q and Qe and total Qs values:  [[1.322]
 [1.322]
 [1.322]
 [1.322]
 [1.322]] [[1.374]
 [1.374]
 [1.374]
 [1.374]
 [1.374]] [[2.272]
 [2.272]
 [2.272]
 [2.272]
 [2.272]]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.556]
 [0.556]
 [0.556]
 [0.556]] [[0.848]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[1.444]
 [1.111]
 [1.111]
 [1.111]
 [1.111]]
maxi score, test score, baseline:  0.1881 1.0 1.0
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1881 1.0 1.0
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1881 1.0 1.0
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.81 ]
 [0.952]
 [0.878]
 [0.878]] [[1.795]
 [1.946]
 [1.046]
 [1.795]
 [1.795]] [[0.878]
 [0.81 ]
 [0.952]
 [0.878]
 [0.878]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2301 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.292]
 [1.34 ]
 [1.049]
 [1.113]
 [1.226]] [[2.119]
 [1.464]
 [1.831]
 [2.226]
 [2.186]] [[2.742]
 [2.447]
 [2.155]
 [2.493]
 [2.666]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2301 1.0 1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.028818697263554093
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.208 0.125 0.292 0.25  0.125]
maxi score, test score, baseline:  0.2301 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.216]
 [1.217]
 [1.216]
 [1.216]
 [1.216]] [[2.311]
 [1.728]
 [2.311]
 [2.311]
 [2.311]] [[2.493]
 [2.107]
 [2.493]
 [2.493]
 [2.493]]
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.523]
 [0.523]
 [0.798]
 [0.674]] [[2.783]
 [3.181]
 [3.181]
 [3.053]
 [5.324]] [[0.848]
 [1.116]
 [1.116]
 [1.295]
 [2.149]]
siam score:  -0.88332415
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.17 ]
 [1.17 ]
 [1.372]
 [1.17 ]
 [1.17 ]] [[1.597]
 [1.597]
 [1.562]
 [1.597]
 [1.597]] [[1.845]
 [1.845]
 [2.238]
 [1.845]
 [1.845]]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8899732
maxi score, test score, baseline:  0.2341 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.543]] [[5.525]
 [5.525]
 [5.525]
 [5.525]
 [6.384]] [[1.13 ]
 [1.13 ]
 [1.13 ]
 [1.13 ]
 [1.352]]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.403]
 [0.652]
 [0.489]
 [0.501]] [[3.895]
 [3.833]
 [3.354]
 [3.159]
 [8.14 ]] [[0.7  ]
 [0.669]
 [0.639]
 [0.516]
 [1.92 ]]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
first move QE:  0.028989313581413774
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.599]
 [0.812]
 [0.599]
 [0.599]] [[2.965]
 [2.965]
 [2.482]
 [2.965]
 [2.965]] [[1.643]
 [1.643]
 [1.732]
 [1.643]
 [1.643]]
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8780063
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.877249
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8753624
maxi score, test score, baseline:  0.2341 1.0 1.0
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.327]
 [0.719]
 [0.409]
 [0.272]] [[1.403]
 [1.497]
 [1.336]
 [1.636]
 [4.079]] [[0.519]
 [0.513]
 [0.722]
 [0.642]
 [1.738]]
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8772247
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.397]] [[1.546]
 [1.546]
 [1.546]
 [1.546]
 [1.879]] [[1.498]
 [1.498]
 [1.498]
 [1.498]
 [1.945]]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.862]
 [0.623]
 [0.623]
 [0.623]] [[1.214]
 [0.328]
 [1.214]
 [1.214]
 [1.214]] [[1.645]
 [1.818]
 [1.645]
 [1.645]
 [1.645]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2341 1.0 1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.87536925
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2341 1.0 1.0
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86848587
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8685797
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
start point for exploration sampling:  20026
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.436]
 [0.499]
 [0.435]
 [0.424]] [[ 0.   ]
 [ 0.   ]
 [-0.624]
 [-0.172]
 [-0.335]] [[0.672]
 [0.672]
 [0.591]
 [0.614]
 [0.537]]
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
first move QE:  0.026995312358711658
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.86481494
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2615 5565
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8623218
Printing some Q and Qe and total Qs values:  [[0.643]
 [1.07 ]
 [1.102]
 [1.038]
 [0.918]] [[1.971]
 [1.689]
 [1.832]
 [1.853]
 [1.872]] [[1.334]
 [2.002]
 [2.16 ]
 [2.048]
 [1.821]]
Printing some Q and Qe and total Qs values:  [[1.059]
 [1.059]
 [1.1  ]
 [1.015]
 [1.059]] [[1.563]
 [1.563]
 [2.091]
 [1.684]
 [1.563]] [[1.9  ]
 [1.9  ]
 [2.315]
 [1.892]
 [1.9  ]]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8700674
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.028]
 [0.23 ]
 [0.241]
 [0.208]] [[3.014]
 [2.251]
 [2.514]
 [2.698]
 [3.221]] [[1.122]
 [0.313]
 [0.736]
 [0.886]
 [1.24 ]]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.331576873511772
in main func line 156:  2624
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[0.127]
 [0.321]
 [0.321]
 [0.321]
 [0.321]] [[0.   ]
 [0.008]
 [0.008]
 [0.008]
 [0.008]]
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.583]
 [0.463]
 [0.463]
 [0.463]] [[3.517]
 [2.276]
 [3.517]
 [3.517]
 [3.517]] [[0.463]
 [0.583]
 [0.463]
 [0.463]
 [0.463]]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.87843865
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87714463
first move QE:  0.027187544425287733
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
in main func line 156:  2637
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
start point for exploration sampling:  20026
Printing some Q and Qe and total Qs values:  [[1.11 ]
 [1.188]
 [1.11 ]
 [1.11 ]
 [1.11 ]] [[2.254]
 [1.522]
 [2.254]
 [2.254]
 [2.254]] [[2.105]
 [2.018]
 [2.105]
 [2.105]
 [2.105]]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.86957276
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.61 ]
 [0.806]
 [0.589]
 [0.714]] [[0.666]
 [2.891]
 [2.037]
 [2.347]
 [2.419]] [[0.101]
 [1.926]
 [1.564]
 [1.555]
 [1.722]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.357]
 [0.227]
 [0.292]
 [0.27 ]] [[0.897]
 [1.022]
 [1.299]
 [1.106]
 [1.499]] [[0.226]
 [0.357]
 [0.227]
 [0.292]
 [0.27 ]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
line 256 mcts: sample exp_bonus 1.8466177992648234
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[1.569]
 [1.569]
 [1.569]
 [1.569]
 [1.569]] [[1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.192]]
siam score:  -0.86652863
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88029265
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.247]] [[3.356]
 [3.356]
 [3.356]
 [3.356]
 [3.267]] [[1.921]
 [1.921]
 [1.921]
 [1.921]
 [1.816]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.991]
 [0.93 ]
 [0.86 ]
 [0.901]] [[2.818]
 [1.446]
 [1.82 ]
 [1.732]
 [2.12 ]] [[2.006]
 [1.998]
 [2.001]
 [1.836]
 [2.043]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.838]
 [0.873]
 [0.838]
 [0.838]
 [0.821]] [[2.337]
 [1.754]
 [2.337]
 [2.337]
 [2.292]] [[2.436]
 [1.918]
 [2.436]
 [2.436]
 [2.368]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.826]
 [0.736]
 [0.773]
 [0.776]] [[2.842]
 [2.19 ]
 [2.66 ]
 [2.514]
 [3.123]] [[1.733]
 [1.286]
 [1.546]
 [1.475]
 [1.95 ]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.64 ]
 [0.694]
 [0.698]
 [0.694]] [[3.259]
 [3.324]
 [2.69 ]
 [3.227]
 [2.69 ]] [[2.034]
 [2.052]
 [1.661]
 [2.066]
 [1.661]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.213]
 [0.206]
 [0.215]
 [0.213]] [[3.932]
 [3.932]
 [5.19 ]
 [3.313]
 [3.932]] [[1.332]
 [1.332]
 [2.063]
 [0.972]
 [1.332]]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.027004214409979216
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]] [[2.159]
 [2.159]
 [2.159]
 [2.159]
 [2.159]] [[0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.214]
 [0.218]
 [0.214]
 [0.228]] [[3.119]
 [3.119]
 [3.318]
 [3.119]
 [4.15 ]] [[1.091]
 [1.091]
 [1.23 ]
 [1.091]
 [1.804]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89629745
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.505]] [[0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.125]] [[1.063]
 [1.063]
 [1.063]
 [1.063]
 [0.997]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
siam score:  -0.9006899
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89934385
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89821947
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 4.469453680536019
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.727]
 [0.493]
 [0.493]
 [0.493]] [[3.127]
 [1.889]
 [2.472]
 [2.472]
 [2.472]] [[1.942]
 [1.257]
 [1.425]
 [1.425]
 [1.425]]
using explorer policy with actor:  1
first move QE:  0.02703660848263604
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.02680765590731999
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.026433248651415494
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[6.762]
 [4.088]
 [4.088]
 [4.088]
 [4.088]] [[2.108]
 [1.507]
 [1.507]
 [1.507]
 [1.507]]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2481 1.0 1.0
siam score:  -0.884068
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
first move QE:  0.025344066631670225
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
first move QE:  0.025344066631670225
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8857684
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8868353
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.686]
 [0.559]
 [0.559]
 [0.559]] [[2.189]
 [2.541]
 [3.364]
 [3.364]
 [3.364]] [[1.353]
 [1.77 ]
 [2.2  ]
 [2.2  ]
 [2.2  ]]
line 256 mcts: sample exp_bonus 1.9912838423744612
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[6.683]
 [4.351]
 [4.351]
 [4.351]
 [4.351]] [[2.069]
 [1.123]
 [1.123]
 [1.123]
 [1.123]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.477]
 [0.544]
 [0.544]
 [0.544]] [[4.674]
 [4.315]
 [4.46 ]
 [4.46 ]
 [4.46 ]] [[1.66 ]
 [1.499]
 [1.689]
 [1.689]
 [1.689]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2695 5612
siam score:  -0.87709236
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8900844
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.787]
 [0.787]
 [0.787]
 [0.787]] [[2.2  ]
 [1.431]
 [1.431]
 [1.431]
 [1.431]] [[2.142]
 [1.71 ]
 [1.71 ]
 [1.71 ]
 [1.71 ]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8895203
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.555]
 [0.699]
 [0.699]
 [0.699]] [[3.225]
 [3.346]
 [3.02 ]
 [3.02 ]
 [3.02 ]] [[2.202]
 [2.142]
 [2.108]
 [2.108]
 [2.108]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.5   0.042 0.042 0.042 0.375]
2713 5620
siam score:  -0.89583105
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.625]
 [1.036]
 [0.78 ]
 [0.774]
 [0.807]] [[1.876]
 [1.188]
 [1.613]
 [1.846]
 [1.85 ]] [[1.856]
 [1.99 ]
 [1.903]
 [2.124]
 [2.194]]
start point for exploration sampling:  20026
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.718]
 [0.782]
 [0.682]
 [0.682]] [[2.828]
 [2.953]
 [2.05 ]
 [2.383]
 [2.383]] [[1.86 ]
 [1.901]
 [1.427]
 [1.449]
 [1.449]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8921286
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.61 ]
 [0.61 ]
 [0.645]
 [0.61 ]] [[2.972]
 [2.993]
 [2.993]
 [2.793]
 [2.993]] [[2.135]
 [2.166]
 [2.166]
 [2.015]
 [2.166]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89384687
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.642]
 [0.731]
 [0.642]
 [0.642]] [[1.862]
 [1.862]
 [2.074]
 [1.862]
 [1.862]] [[0.642]
 [0.642]
 [0.731]
 [0.642]
 [0.642]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]] [[1.648]
 [1.648]
 [1.648]
 [1.648]
 [1.648]] [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.594]
 [0.572]
 [0.637]
 [0.572]] [[3.564]
 [3.658]
 [3.285]
 [3.326]
 [3.285]] [[1.944]
 [2.013]
 [1.72 ]
 [1.878]
 [1.72 ]]
siam score:  -0.89015794
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [1.321]
 [1.181]
 [1.135]
 [1.163]] [[1.307]
 [2.584]
 [2.428]
 [3.96 ]
 [3.838]] [[0.155]
 [2.674]
 [2.358]
 [2.757]
 [2.772]]
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.687]
 [0.595]
 [0.681]
 [0.728]] [[2.276]
 [2.683]
 [2.16 ]
 [2.259]
 [4.46 ]] [[0.924]
 [1.066]
 [0.724]
 [0.842]
 [2.016]]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.729]] [[2.777]
 [2.777]
 [2.777]
 [2.777]
 [5.519]] [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [2.179]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.663]
 [0.748]
 [0.748]
 [0.743]] [[2.234]
 [2.444]
 [3.762]
 [3.762]
 [3.621]] [[1.346]
 [1.282]
 [1.892]
 [1.892]
 [1.835]]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2743 5631
siam score:  -0.8893187
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.256008867730883
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2745 5632
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.8536796289215483
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8903441
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.814]
 [0.913]
 [0.821]
 [0.821]] [[1.544]
 [1.891]
 [1.204]
 [1.544]
 [1.544]] [[0.821]
 [0.814]
 [0.913]
 [0.821]
 [0.821]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.861]
 [1.525]
 [0.731]
 [0.454]
 [1.488]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]] [[1.631]
 [1.631]
 [1.631]
 [1.631]
 [1.631]] [[1.982]
 [1.982]
 [1.982]
 [1.982]
 [1.982]]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.85959876140956
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8927863
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8959997
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89285123
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.173291930436354
maxi score, test score, baseline:  0.3001 1.0 1.0
first move QE:  0.02710309869119886
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.826]
 [1.134]
 [1.204]
 [1.134]
 [1.134]] [[2.11 ]
 [1.141]
 [1.08 ]
 [1.141]
 [1.141]] [[2.323]
 [1.992]
 [2.066]
 [1.992]
 [1.992]]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8994122
maxi score, test score, baseline:  0.3001 1.0 1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.887]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]] [[2.896]
 [2.665]
 [2.665]
 [2.665]
 [2.665]] [[1.761]
 [1.671]
 [1.671]
 [1.671]
 [1.671]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90381455
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.3021 1.0 1.0
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90285856
maxi score, test score, baseline:  0.3021 1.0 1.0
maxi score, test score, baseline:  0.3021 1.0 1.0
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.846]
 [0.705]
 [0.932]
 [0.803]
 [0.823]] [[2.729]
 [2.238]
 [2.33 ]
 [3.145]
 [3.419]] [[1.836]
 [1.342]
 [1.761]
 [1.991]
 [2.172]]
Printing some Q and Qe and total Qs values:  [[0.87]
 [0.87]
 [0.87]
 [0.87]
 [0.77]] [[4.711]
 [4.711]
 [4.711]
 [4.711]
 [5.87 ]] [[1.293]
 [1.293]
 [1.293]
 [1.293]
 [1.519]]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.619]
 [0.668]
 [0.722]
 [0.642]] [[3.313]
 [3.414]
 [2.464]
 [2.564]
 [3.768]] [[1.347]
 [1.313]
 [0.895]
 [1.031]
 [1.535]]
2786 5644
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90525144
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
2789 5644
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3101 1.0 1.0
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
line 256 mcts: sample exp_bonus 4.531496834820874
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 1.0 1.0
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.781]
 [0.685]
 [0.176]
 [0.67 ]] [[0.618]
 [0.98 ]
 [0.618]
 [0.976]
 [1.917]] [[0.685]
 [0.781]
 [0.685]
 [0.176]
 [0.67 ]]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8985112
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.236]
 [1.27 ]
 [1.426]
 [1.285]
 [1.219]] [[1.432]
 [0.741]
 [0.054]
 [1.023]
 [1.137]] [[2.799]
 [2.644]
 [2.722]
 [2.762]
 [2.671]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]] [[1.654]
 [1.654]
 [1.654]
 [1.654]
 [1.654]] [[1.914]
 [1.914]
 [1.914]
 [1.914]
 [1.914]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3201 1.0 1.0
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2815 5661
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.5561873901228412
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.788]] [[2.132]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [2.091]] [[2.185]
 [1.941]
 [1.941]
 [1.941]
 [2.143]]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8915628
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.4990702309773045
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.604]
 [0.546]
 [0.546]
 [0.546]] [[1.32 ]
 [1.424]
 [1.589]
 [1.589]
 [1.589]] [[1.485]
 [1.509]
 [1.558]
 [1.558]
 [1.558]]
siam score:  -0.88659424
maxi score, test score, baseline:  0.3221 1.0 1.0
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[2.599]
 [2.599]
 [2.599]
 [2.599]
 [2.599]] [[1.856]
 [1.856]
 [1.856]
 [1.856]
 [1.856]]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.023863183193495414
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8772127
first move QE:  0.023863183193495414
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.3164887205344478
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.604]] [[2.284]
 [2.351]
 [2.351]
 [2.351]
 [2.171]] [[1.75 ]
 [1.797]
 [1.797]
 [1.797]
 [1.625]]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.79 ]] [[3.198]
 [2.875]
 [2.875]
 [2.875]
 [2.875]] [[2.326]
 [2.115]
 [2.115]
 [2.115]
 [2.115]]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.268]
 [0.855]
 [0.645]
 [0.63 ]] [[2.268]
 [0.866]
 [0.535]
 [0.409]
 [0.322]] [[2.148]
 [0.544]
 [1.498]
 [0.995]
 [0.906]]
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[3.557]
 [3.557]
 [3.557]
 [3.557]
 [3.557]] [[1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.559]
 [0.539]
 [0.576]
 [0.553]] [[1.953]
 [2.015]
 [2.191]
 [2.546]
 [1.686]] [[1.964]
 [1.894]
 [1.991]
 [2.302]
 [1.647]]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8803631
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3241 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.5  ]
 [1.5  ]
 [0.578]
 [1.5  ]
 [0.486]] [[ 0.   ]
 [ 0.   ]
 [-0.87 ]
 [ 0.   ]
 [-0.558]] [[3.275]
 [3.275]
 [0.851]
 [3.275]
 [0.875]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.781]
 [0.617]
 [0.617]
 [0.661]] [[0.477]
 [0.298]
 [0.477]
 [0.477]
 [0.746]] [[0.914]
 [1.124]
 [0.914]
 [0.914]
 [1.181]]
siam score:  -0.8844041
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.88134277
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.87789744
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.602]
 [0.667]
 [0.667]
 [0.699]] [[2.209]
 [2.522]
 [1.846]
 [1.846]
 [2.067]] [[1.766]
 [1.932]
 [1.43 ]
 [1.43 ]
 [1.692]]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[2.926]
 [2.926]
 [2.926]
 [2.926]
 [2.926]]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8847076
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8866596
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
siam score:  -0.8856787
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.316]
 [1.316]
 [1.316]
 [1.316]
 [1.316]] [[1.99]
 [1.99]
 [1.99]
 [1.99]
 [1.99]] [[2.289]
 [2.289]
 [2.289]
 [2.289]
 [2.289]]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.698]
 [0.945]
 [0.776]
 [0.721]] [[2.754]
 [2.59 ]
 [2.017]
 [2.12 ]
 [3.132]] [[1.921]
 [1.703]
 [1.641]
 [1.46 ]
 [2.158]]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  2875
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]] [[3.235]
 [3.235]
 [3.235]
 [3.235]
 [3.235]] [[2.356]
 [2.356]
 [2.356]
 [2.356]
 [2.356]]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.025701377014636585
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.02575674577659462
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  0.025732727696906522
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.581]
 [0.766]
 [0.581]
 [0.581]] [[1.706]
 [1.706]
 [1.582]
 [1.706]
 [1.706]] [[0.581]
 [0.581]
 [0.766]
 [0.581]
 [0.581]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.919]
 [0.855]
 [0.948]
 [0.919]
 [0.919]] [[1.986]
 [2.201]
 [0.64 ]
 [1.986]
 [1.986]] [[0.919]
 [0.855]
 [0.948]
 [0.919]
 [0.919]]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.591]] [[2.35 ]
 [2.35 ]
 [2.35 ]
 [2.35 ]
 [2.226]] [[2.223]
 [2.223]
 [2.223]
 [2.223]
 [2.059]]
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.908]
 [0.919]
 [0.844]
 [0.823]] [[1.511]
 [2.169]
 [1.635]
 [2.219]
 [2.742]] [[0.82 ]
 [0.908]
 [0.919]
 [0.844]
 [0.823]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
maxi score, test score, baseline:  0.3641 1.0 1.0
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.713]] [[3.436]
 [3.436]
 [3.436]
 [3.436]
 [4.829]] [[1.682]
 [1.682]
 [1.682]
 [1.682]
 [2.046]]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.629]] [[3.686]
 [3.686]
 [3.686]
 [3.686]
 [7.014]] [[1.258]
 [1.258]
 [1.258]
 [1.258]
 [2.001]]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2902 5686
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[2.405]
 [2.405]
 [2.405]
 [2.405]
 [2.405]] [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.778]
 [0.778]
 [0.778]
 [0.778]] [[4.297]
 [4.411]
 [4.411]
 [4.411]
 [4.411]] [[2.375]
 [2.477]
 [2.477]
 [2.477]
 [2.477]]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.794]
 [0.794]
 [0.794]
 [0.794]] [[3.361]
 [3.665]
 [3.665]
 [3.665]
 [3.665]] [[2.183]
 [2.394]
 [2.394]
 [2.394]
 [2.394]]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3681 1.0 1.0
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.008]
 [0.78 ]
 [0.58 ]
 [0.718]] [[-0.18 ]
 [-0.374]
 [-0.2  ]
 [-0.12 ]
 [ 0.   ]] [[1.191]
 [0.015]
 [1.582]
 [1.218]
 [1.526]]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
in main func line 156:  2916
siam score:  -0.87699217
UNIT TEST: sample policy line 217 mcts : [0.292 0.292 0.    0.    0.417]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.3701 1.0 1.0
maxi score, test score, baseline:  0.3701 1.0 1.0
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8761994
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.823]
 [0.783]
 [0.842]
 [0.842]
 [0.842]] [[1.967]
 [1.984]
 [1.636]
 [1.636]
 [1.636]] [[2.296]
 [2.235]
 [2.013]
 [2.013]
 [2.013]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
siam score:  -0.86782914
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
siam score:  -0.86558473
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.569]
 [0.809]
 [0.569]
 [0.569]] [[0.266]
 [0.266]
 [1.134]
 [0.266]
 [0.266]] [[1.125]
 [1.125]
 [2.124]
 [1.125]
 [1.125]]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[2.878]
 [2.455]
 [2.455]
 [2.455]
 [2.455]] [[1.194]
 [0.984]
 [0.984]
 [0.984]
 [0.984]]
Printing some Q and Qe and total Qs values:  [[0.806]
 [1.054]
 [0.746]
 [0.9  ]
 [0.746]] [[3.326]
 [1.622]
 [4.225]
 [2.519]
 [4.225]] [[2.391]
 [1.566]
 [2.925]
 [1.968]
 [2.925]]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.566]
 [0.519]
 [0.504]] [[1.641]
 [1.641]
 [0.872]
 [3.668]
 [1.641]] [[0.958]
 [0.958]
 [0.677]
 [1.854]
 [0.958]]
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.442]
 [0.439]
 [0.514]
 [0.453]] [[1.397]
 [1.484]
 [1.766]
 [1.549]
 [3.317]] [[0.844]
 [0.812]
 [1.018]
 [0.941]
 [2.178]]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.938]
 [0.938]
 [0.938]
 [0.938]] [[1.439]
 [1.269]
 [1.269]
 [1.269]
 [1.269]] [[2.571]
 [2.491]
 [2.491]
 [2.491]
 [2.491]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  2931
2931 5693
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.169]
 [1.23 ]
 [1.335]
 [1.214]
 [1.152]] [[1.756]
 [2.278]
 [1.463]
 [2.374]
 [2.503]] [[1.777]
 [2.074]
 [2.011]
 [2.074]
 [1.994]]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20026
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3741 1.0 1.0
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.064]
 [1.064]
 [1.054]
 [1.064]
 [1.064]] [[0.739]
 [0.739]
 [0.918]
 [0.739]
 [0.739]] [[1.616]
 [1.616]
 [1.656]
 [1.616]
 [1.616]]
2941 5694
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8649363
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8648295
siam score:  -0.8648323
maxi score, test score, baseline:  0.3741 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3741 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3761 1.0 1.0
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.732]] [[1.65 ]
 [1.65 ]
 [1.65 ]
 [1.65 ]
 [1.947]] [[1.755]
 [1.755]
 [1.755]
 [1.755]
 [2.051]]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.696]
 [0.983]
 [0.696]
 [0.696]] [[1.277]
 [1.277]
 [1.294]
 [1.277]
 [1.277]] [[1.675]
 [1.675]
 [2.271]
 [1.675]
 [1.675]]
maxi score, test score, baseline:  0.3781 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8637756
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.748]
 [0.721]
 [0.164]
 [0.624]] [[ 1.227]
 [ 1.107]
 [ 1.262]
 [-0.043]
 [ 1.238]] [[1.621]
 [1.647]
 [1.715]
 [0.084]
 [1.569]]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.972]
 [0.972]
 [0.972]
 [0.972]] [[1.97]
 [1.29]
 [1.29]
 [1.29]
 [1.29]] [[1.77 ]
 [1.432]
 [1.432]
 [1.432]
 [1.432]]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.697246801906183
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85652566
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
2956 5697
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8613751
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20026
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.79 ]
 [0.914]
 [0.957]
 [0.888]] [[1.511]
 [1.347]
 [2.205]
 [1.02 ]
 [1.4  ]] [[1.294]
 [1.057]
 [1.78 ]
 [1.062]
 [1.22 ]]
Printing some Q and Qe and total Qs values:  [[0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.76]] [[2.17 ]
 [2.17 ]
 [2.17 ]
 [2.17 ]
 [2.413]] [[2.315]
 [2.315]
 [2.315]
 [2.315]
 [2.269]]
siam score:  -0.8745062
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.924]
 [0.916]
 [0.916]
 [0.732]] [[1.111]
 [1.039]
 [1.111]
 [1.111]
 [2.392]] [[1.449]
 [1.397]
 [1.449]
 [1.449]
 [2.328]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86893725
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8723984
2972 5705
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
2979 5709
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
