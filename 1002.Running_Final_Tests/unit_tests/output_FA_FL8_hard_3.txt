dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 41]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 41]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
channels:3
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'F' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'H']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'H']
 ['F' 'F' 'F' 'F' 'H' 'F' 'H' 'H']
 ['F' 'F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'H' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'F' 'H' 'H' 'F']
 ['F' 'F' 'F' 'F' 'F' 'H' 'H' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:255000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
timesteps_in_obs:2
store_prev_actions:True
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
state_size:[6, 6]
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'H']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'H']
 ['F' 'F' 'F' 'F' 'H' 'F' 'H' 'H']
 ['F' 'F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'H' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'F' 'H' 'H' 'F']
 ['F' 'F' 'F' 'F' 'F' 'H' 'H' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
from probs:  [0.25, 0.25, 0.25, 0.25]
4 52
Starting evaluation
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.0020340723514726215
rdn probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  694 train batches done:  21 episodes:  83
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.2016912
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.2996303
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 3 threads
7 108
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  1325 train batches done:  119 episodes:  152
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.2526751913540757
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.33337343
siam score:  -0.33806583
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.3569324
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
16 202
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  20
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
23 242
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
28 283
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.3357856
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
43 409
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.39292032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.37758797
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
56 532
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
57 536
57 539
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
57 545
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.092]
 [-0.007]
 [-0.01 ]
 [-0.001]
 [ 0.005]] [[0.   ]
 [0.057]
 [0.054]
 [0.061]
 [0.064]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
57 561
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.47982228
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]] [[0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.199]
 [-0.053]
 [-0.096]
 [-0.144]
 [-0.062]] [[0.055]
 [0.103]
 [0.089]
 [0.073]
 [0.1  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.49978715
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.44628808
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  61
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.40474254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.292 0.083 0.167 0.25  0.208]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.084]
 [-0.001]
 [ 0.051]
 [-0.042]
 [ 0.074]] [[0.   ]
 [0.112]
 [0.181]
 [0.057]
 [0.211]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  65
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.45470357
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.023]
 [0.064]
 [0.292]
 [0.235]
 [0.266]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
68 699
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5131724
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.011710420670219993
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.047]
 [-0.047]
 [-0.016]
 [-0.047]
 [-0.047]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5642266
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.56817365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
73 755
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
74 768
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
74 774
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.177]
 [0.177]
 [0.177]
 [0.177]
 [0.177]] [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.58551794
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.46 ]
 [-0.029]
 [-0.182]
 [-0.011]
 [-0.057]] [[0.23 ]
 [0.518]
 [0.415]
 [0.529]
 [0.499]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
82 860
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.625]
 [-0.41 ]
 [-0.303]
 [-0.219]
 [-0.223]] [[0.   ]
 [0.143]
 [0.215]
 [0.27 ]
 [0.268]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
84 912
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]] [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
85 921
first move QE:  0.004066452139994585
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6283375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
first move QE:  0.004114765039381373
85 937
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
86 940
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.60430366
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [ 0.002]
 [ 0.004]
 [ 0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
88 989
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
90 1006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
91 1026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.61992794
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
91 1046
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  10973 train batches done:  1280 episodes:  1162
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6489075
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
97 1106
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
99 1120
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
101 1137
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
101 1141
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
101 1150
from probs:  [0.25, 0.25, 0.25, 0.25]
102 1153
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
102 1159
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5954127
siam score:  -0.58727366
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
107 1208
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5896113
107 1209
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5841965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.004797934463319802
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.005]
 [0.007]
 [0.007]
 [0.007]] [[0.006]
 [0.001]
 [0.004]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.003]
 [0.004]
 [0.004]
 [0.003]] [[0.003]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.64415556
113 1327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.63654125
maxi score, test score, baseline:  0.0001 0.0 0.0001
113 1335
siam score:  -0.6349258
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.208 0.083 0.125 0.125 0.458]
from probs:  [0.25, 0.25, 0.25, 0.25]
114 1342
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.64475995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
115 1362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.5]
 [0. ]
 [0. ]
 [0. ]
 [0. ]] [[0.   ]
 [0.003]
 [0.005]
 [0.003]
 [0.003]] [[2.999]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -2.8437069529954912e-09
0.0 0.0
0.0 0.0
0.0 0.0
0.0 -7.019737308358347e-09
0.0 -2.668690307662257e-09
0.0 -4.4477451027353045e-09
0.0 -4.664796880078375e-09
0.0 -6.9015891801125326e-09
0.0 -3.030054055718852e-09
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5810424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.004625740794655474
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
118 1482
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5867697
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
122 1524
using explorer policy with actor:  1
siam score:  -0.6176653
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.004382242411285868
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [-0.   ]
 [-0.001]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6315345
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6327948
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
125 1582
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
126 1590
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.0002999918096054065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
126 1601
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.004128890548616624
siam score:  -0.6333605
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
129 1623
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.004010740277035225
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.65481234
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.012]
 [-0.012]
 [-0.011]
 [-0.012]
 [-0.012]] [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6630563
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
136 1693
siam score:  -0.6643448
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6671553
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.003738591876628806
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.0036684456880905726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.0010450530499811464
siam score:  -0.66161996
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.0036373388085996212
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
143 1745
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.208 0.25  0.375 0.083 0.083]
144 1758
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6532135
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  149
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6404956
siam score:  -0.6363353
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  152
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.58173865
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.012]
 [0.012]
 [0.012]
 [0.011]] [[0.008]
 [0.006]
 [0.005]
 [0.005]
 [0.004]]
154 1830
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5793615
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.58223855
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6481485
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
159 1929
159 1934
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.66749245
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [ 0.001]
 [-0.001]
 [-0.006]
 [-0.006]] [[0.005]
 [0.01 ]
 [0.007]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 1.0482835399927783e-10
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 1.530909130150786e-10
0.0 6.538798320597997e-11
0.0 1.6641068735586125e-10
0.0 3.1067941203888623e-10
0.0 1.6865948039566864e-10
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  -0.6544471
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
STARTED EXPV TRAINING ON FRAME NO.  20031
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.64961183
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.002561552958525725
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.069]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]] [[0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.158]
 [-0.125]
 [-0.012]
 [ 0.005]
 [-0.134]] [[0.01 ]
 [0.043]
 [0.156]
 [0.173]
 [0.034]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.1  ]
 [-0.041]
 [-0.051]
 [ 0.008]
 [-0.161]] [[0.04 ]
 [0.08 ]
 [0.073]
 [0.113]
 [0.   ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
171 2060
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.004039665395459352
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.148]
 [-0.   ]
 [-0.03 ]
 [ 0.011]
 [-0.043]] [[0.   ]
 [0.099]
 [0.079]
 [0.106]
 [0.07 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
175 2106
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.057]
 [ 0.052]
 [ 0.055]
 [-0.033]
 [-0.197]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
178 2114
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
180 2135
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
180 2170
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
185 2207
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.006074753883678982
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
186 2214
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6511534
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.658145
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.8513735336268597
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.038]
 [3.038]
 [3.038]
 [3.181]
 [3.038]] [[1.708]
 [1.708]
 [1.708]
 [1.841]
 [1.708]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.255]
 [2.228]
 [2.9  ]
 [3.921]
 [2.26 ]] [[0.112]
 [0.717]
 [1.135]
 [1.77 ]
 [0.738]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
193 2256
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.12932657116168775
siam score:  -0.6660746
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.676]
 [2.676]
 [3.084]
 [2.676]
 [2.676]] [[0.936]
 [0.936]
 [1.207]
 [0.936]
 [0.936]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.6536243
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.882]
 [2.882]
 [3.003]
 [3.42 ]
 [2.882]] [[1.322]
 [1.322]
 [1.428]
 [1.792]
 [1.322]]
197 2263
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]] [[0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]]
siam score:  -0.66333205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
siam score:  -0.6432027
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.536]
 [1.882]
 [1.922]
 [1.633]
 [1.724]] [[1.801]
 [1.241]
 [1.274]
 [1.027]
 [1.105]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
212 2367
siam score:  -0.6571487
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.86 ]
 [3.596]
 [3.489]
 [3.297]
 [4.86 ]] [[2.   ]
 [1.239]
 [1.175]
 [1.059]
 [2.   ]]
siam score:  -0.6548254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6504509
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.054]
 [0.434]
 [0.434]
 [0.434]
 [0.434]] [[0.726]
 [1.105]
 [1.105]
 [1.105]
 [1.105]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.6481268
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.211]
 [0.453]
 [0.216]
 [0.158]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.91 ]
 [7.778]
 [5.91 ]
 [5.91 ]
 [5.91 ]] [[0.502]
 [1.233]
 [0.502]
 [0.502]
 [0.502]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[9.163]
 [9.163]
 [4.682]
 [5.765]
 [9.163]] [[2.   ]
 [2.   ]
 [0.194]
 [0.63 ]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6631221
siam score:  -0.6631005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.2]
 [8.2]
 [8.2]
 [8.2]
 [8.2]] [[1.915]
 [1.915]
 [1.915]
 [1.915]
 [1.915]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.679]
 [8.799]
 [7.72 ]
 [8.298]
 [8.409]] [[1.548]
 [1.594]
 [1.177]
 [1.4  ]
 [1.444]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.083 0.167 0.333 0.125 0.292]
245 2437
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  247
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6729173
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.009286090365684853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
248 2451
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  -0.011024032957639814
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6632711
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.483]
 [1.483]
 [1.995]
 [1.483]
 [1.483]] [[1.185]
 [1.185]
 [1.583]
 [1.185]
 [1.185]]
257 2462
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.239]
 [2.222]
 [2.367]
 [2.239]
 [2.167]] [[1.65 ]
 [1.635]
 [1.758]
 [1.65 ]
 [1.588]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.531086089325254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6670784897015025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6545517
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6638824
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.010565113577160438
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
277 2493
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.778]
 [0.778]
 [2.797]
 [0.778]
 [0.778]] [[0.442]
 [0.442]
 [1.123]
 [0.442]
 [0.442]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.566]
 [3.54 ]
 [2.566]
 [2.566]
 [2.566]] [[1.018]
 [1.344]
 [1.018]
 [1.018]
 [1.018]]
siam score:  -0.6894185
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 5.79974331438944e-20
0.0 2.2833898948163595e-11
0.0 7.784283761180757e-12
0.0 0.0
0.0 8.303236019229521e-12
0.0 6.227427064364374e-12
0.0 0.0
0.0 1.6606471951462893e-11
0.0 0.0
0.0 9.021822933494684e-20
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6777057
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  287
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6845412
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.161]
 [5.161]
 [5.161]
 [5.161]
 [5.161]] [[1.805]
 [1.805]
 [1.805]
 [1.805]
 [1.805]]
293 2527
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.682008
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.992]
 [2.708]
 [2.708]
 [2.708]
 [2.708]] [[1.184]
 [0.461]
 [0.461]
 [0.461]
 [0.461]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6669508
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
using explorer policy with actor:  1
siam score:  -0.67135024
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.06 ]
 [0.014]
 [0.778]
 [0.632]
 [0.492]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6788823
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6800679
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
306 2547
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
310 2549
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0008968787177892856
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6853076
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 6.394986043467487
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6906735
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.487]
 [6.578]
 [4.487]
 [4.487]
 [4.487]] [[0.789]
 [2.   ]
 [0.789]
 [0.789]
 [0.789]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6830475
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
326 2572
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.4743331153104973
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
331 2577
332 2578
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
335 2580
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.700561
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.454]
 [2.807]
 [3.168]
 [3.454]
 [2.367]] [[1.758]
 [1.38 ]
 [1.591]
 [1.758]
 [1.122]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.989]
 [6.894]
 [6.894]
 [6.894]
 [6.894]] [[2.   ]
 [1.535]
 [1.535]
 [1.535]
 [1.535]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.722]
 [7.814]
 [5.722]
 [5.722]
 [5.722]] [[0.98]
 [2.  ]
 [0.98]
 [0.98]
 [0.98]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.9  ]
 [6.976]
 [5.9  ]
 [6.212]
 [5.9  ]] [[1.145]
 [1.589]
 [1.145]
 [1.274]
 [1.145]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.010993903239958094
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.2716052958991976
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.387]
 [2.075]
 [1.387]
 [1.387]
 [1.387]] [[0.95 ]
 [1.408]
 [0.95 ]
 [0.95 ]
 [0.95 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
345 2593
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0. ]
 [1.5]
 [0. ]
 [1.5]
 [1.5]] [[0.288]
 [0.   ]
 [0.321]
 [0.   ]
 [0.   ]] [[0.101]
 [2.909]
 [0.122]
 [2.909]
 [2.909]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.061679370700819
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
354 2604
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68314433
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.518]
 [3.544]
 [3.   ]
 [2.569]
 [2.825]] [[0.727]
 [1.482]
 [1.082]
 [0.765]
 [0.953]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68208426
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70977265
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.01571274412247109
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7081616
366 2618
siam score:  -0.70814013
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.2651600985132125
372 2622
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.017470127527515537
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6991572
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69669116
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7011703
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.821]
 [0.588]
 [1.401]
 [1.429]
 [1.04 ]] [[0.155]
 [0.   ]
 [0.541]
 [0.56 ]
 [0.301]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.02238142814564805
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.456]
 [0.063]
 [0.457]
 [0.722]
 [0.204]] [[0.648]
 [0.255]
 [0.649]
 [0.914]
 [0.396]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.023]
 [ 0.175]
 [ 1.219]
 [ 0.367]
 [ 1.219]] [[0.   ]
 [0.199]
 [1.242]
 [0.39 ]
 [1.242]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.175]
 [0.175]
 [0.963]
 [0.175]
 [0.175]] [[0.201]
 [0.201]
 [0.745]
 [0.201]
 [0.201]]
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.8765950749861755
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.942]
 [0.942]
 [1.28 ]
 [0.605]
 [0.914]] [[0.579]
 [0.579]
 [0.923]
 [0.237]
 [0.551]]
siam score:  -0.71777844
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.937]
 [4.08 ]
 [3.897]
 [3.156]
 [3.755]] [[1.291]
 [1.383]
 [1.265]
 [0.786]
 [1.173]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.408]
 [4.984]
 [3.911]
 [3.911]
 [3.911]] [[1.492]
 [1.788]
 [1.237]
 [1.237]
 [1.237]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.5099993947328856
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.696]
 [4.755]
 [3.803]
 [2.964]
 [3.356]] [[0.609]
 [1.696]
 [1.194]
 [0.75 ]
 [0.957]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.792]
 [2.94 ]
 [2.792]
 [2.792]
 [2.792]] [[1.101]
 [1.2  ]
 [1.101]
 [1.101]
 [1.101]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71639526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.27858986102190136
start point for exploration sampling:  20031
siam score:  -0.6940461
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.686]
 [3.686]
 [3.686]
 [3.686]
 [3.686]] [[1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.58 ]
 [3.886]
 [4.001]
 [3.844]
 [4.448]] [[0.635]
 [0.738]
 [0.776]
 [0.724]
 [0.925]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
418 2674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.321]
 [0.321]
 [0.414]
 [0.45 ]
 [0.505]] [[0.5  ]
 [0.5  ]
 [0.613]
 [0.657]
 [0.723]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6050954938124284
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 6.361176290632712
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]] [[0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.343]
 [-0.343]
 [-0.343]
 [ 0.416]
 [-0.343]] [[0.038]
 [0.038]
 [0.038]
 [0.866]
 [0.038]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7119809
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.424]
 [0.424]
 [0.738]
 [0.385]
 [0.424]] [[0.064]
 [0.064]
 [0.272]
 [0.038]
 [0.064]]
siam score:  -0.71606684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
435 2694
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.832]
 [1.092]
 [2.958]
 [2.837]
 [2.487]] [[1.159]
 [0.   ]
 [1.243]
 [1.162]
 [0.929]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  442
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  447
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.838]
 [0.838]
 [1.279]
 [0.973]
 [0.838]] [[0.855]
 [0.855]
 [1.203]
 [0.961]
 [0.855]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.683]
 [4.196]
 [4.18 ]
 [3.047]
 [3.938]] [[1.373]
 [1.776]
 [1.764]
 [0.873]
 [1.573]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73879653
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
457 2723
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.071]
 [1.681]
 [1.644]
 [1.071]
 [1.105]] [[0.665]
 [0.869]
 [0.856]
 [0.665]
 [0.677]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.932]
 [3.185]
 [3.062]
 [3.99 ]
 [3.248]] [[0.994]
 [1.155]
 [1.077]
 [1.67 ]
 [1.196]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.0976298461091414
siam score:  -0.71648026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.914]
 [3.078]
 [1.914]
 [1.914]
 [1.914]] [[0.336]
 [0.724]
 [0.336]
 [0.336]
 [0.336]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.347]
 [1.764]
 [1.935]
 [1.089]
 [1.913]] [[0.239]
 [0.378]
 [0.435]
 [0.152]
 [0.427]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
467 2744
468 2745
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.71940863
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
Starting evaluation
siam score:  -0.7234955
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.563]
 [2.494]
 [2.55 ]
 [2.331]
 [2.395]] [[1.287]
 [1.225]
 [1.276]
 [1.077]
 [1.135]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7246739
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
475 2762
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
480 2781
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.049026728045045875
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
488 2789
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7319919
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
in main func line 156:  492
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
493 2798
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74093276
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.739271
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
502 2813
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.392]
 [3.392]
 [3.392]
 [3.392]
 [3.797]] [[1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.616]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.027]
 [2.696]
 [2.6  ]
 [3.231]
 [3.027]] [[0.583]
 [0.473]
 [0.441]
 [0.652]
 [0.583]]
first move QE:  0.054711609388214975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7461692
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.217]
 [0.251]
 [0.958]
 [0.669]
 [0.703]] [[0.   ]
 [0.028]
 [0.609]
 [0.371]
 [0.399]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.749]
 [0.005]
 [0.005]] [[0.002]
 [0.002]
 [0.497]
 [0.002]
 [0.002]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.688]
 [2.876]
 [2.797]
 [2.524]
 [2.827]] [[0.42 ]
 [0.483]
 [0.457]
 [0.365]
 [0.467]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.725641
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7224501
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  516
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
518 2829
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  523
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.0627601186078754
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.039]
 [ 0.31 ]
 [ 0.683]
 [ 0.207]
 [ 0.603]] [[0.   ]
 [0.233]
 [0.481]
 [0.164]
 [0.428]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 1.0471758762092045e-20
0.0 0.0
0.0 0.0
0.0 8.649204182669818e-13
0.0 0.0
0.0 1.4595531997841325e-12
0.0 9.666238857315734e-21
0.0 1.7514638345856316e-11
0.0 0.0
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7386061
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
532 2845
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.798]
 [1.877]
 [1.898]
 [1.746]
 [1.85 ]] [[1.204]
 [1.27 ]
 [1.289]
 [1.16 ]
 [1.248]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.331]
 [2.578]
 [2.504]
 [2.416]
 [2.502]] [[1.295]
 [1.599]
 [1.508]
 [1.4  ]
 [1.505]]
first move QE:  0.059810922627942194
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73023224
first move QE:  0.059378102245661
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.546]
 [2.436]
 [2.436]
 [2.621]
 [2.436]] [[0.925]
 [0.852]
 [0.852]
 [0.975]
 [0.852]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7255649
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.   ]
 [2.112]
 [2.259]
 [2.04 ]
 [2.141]] [[0.62 ]
 [0.695]
 [0.793]
 [0.647]
 [0.714]]
siam score:  -0.74472547
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.942]
 [3.942]
 [2.697]
 [2.66 ]
 [3.942]] [[1.378]
 [1.378]
 [0.549]
 [0.525]
 [1.378]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.305]
 [2.305]
 [2.305]
 [2.305]
 [2.305]] [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.029]
 [3.029]
 [1.522]
 [1.937]
 [3.029]] [[2.109]
 [2.109]
 [0.602]
 [1.017]
 [2.109]]
siam score:  -0.74510545
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.06446091092570884
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
552 2864
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.088]
 [6.592]
 [6.592]
 [6.592]
 [4.822]] [[0.805]
 [2.   ]
 [2.   ]
 [2.   ]
 [1.156]]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.426]
 [4.426]
 [4.426]
 [4.426]
 [4.426]] [[1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7511504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.446]
 [2.933]
 [2.933]
 [3.232]
 [3.33 ]] [[1.385]
 [0.926]
 [0.926]
 [1.194]
 [1.281]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.997]
 [2.902]
 [2.578]
 [2.133]
 [2.907]] [[0.861]
 [0.829]
 [0.721]
 [0.572]
 [0.831]]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7395569
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 5.0395603506170294
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.586]
 [2.11 ]
 [2.863]
 [2.868]
 [2.36 ]] [[0.921]
 [0.604]
 [1.105]
 [1.109]
 [0.77 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.356]
 [2.476]
 [2.533]
 [2.192]
 [2.192]] [[1.138]
 [1.258]
 [1.315]
 [0.974]
 [0.974]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.854]
 [3.608]
 [1.385]
 [3.682]
 [3.312]] [[1.244]
 [1.162]
 [0.419]
 [1.186]
 [1.063]]
start point for exploration sampling:  20031
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.639]
 [2.639]
 [2.639]
 [2.641]
 [2.639]] [[1.386]
 [1.386]
 [1.386]
 [1.388]
 [1.386]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73399
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
578 2895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73555595
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.114]
 [2.691]
 [2.861]
 [2.625]
 [2.742]] [[0.905]
 [1.295]
 [1.409]
 [1.25 ]
 [1.329]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.995]
 [2.956]
 [3.024]
 [2.543]
 [2.771]] [[1.033]
 [1.007]
 [1.052]
 [0.732]
 [0.884]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.729]
 [2.374]
 [2.563]
 [2.496]
 [2.125]] [[1.279]
 [1.043]
 [1.169]
 [1.124]
 [0.877]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
584 2896
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75580126
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.639]
 [3.639]
 [3.639]
 [3.145]
 [3.639]] [[1.77 ]
 [1.77 ]
 [1.77 ]
 [1.389]
 [1.77 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.0815450331344897
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7512001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.747937
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.258]
 [3.075]
 [4.013]
 [4.013]
 [4.013]] [[0.818]
 [0.293]
 [0.709]
 [0.709]
 [0.709]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.585]
 [3.506]
 [3.925]
 [3.662]
 [3.698]] [[1.196]
 [1.143]
 [1.422]
 [1.247]
 [1.271]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.705]
 [3.723]
 [2.297]
 [3.037]
 [3.81 ]] [[1.086]
 [1.092]
 [0.616]
 [0.863]
 [1.121]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.191]
 [2.388]
 [2.241]
 [2.311]
 [2.465]] [[0.567]
 [0.698]
 [0.6  ]
 [0.647]
 [0.749]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.384599996588774
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.084]
 [3.958]
 [3.512]
 [3.228]
 [4.239]] [[1.193]
 [1.112]
 [0.824]
 [0.64 ]
 [1.293]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.478]
 [3.478]
 [3.478]
 [4.828]
 [3.478]] [[1.237]
 [1.237]
 [1.237]
 [1.768]
 [1.237]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.987]
 [2.987]
 [2.987]
 [2.987]
 [2.987]] [[1.763]
 [1.763]
 [1.763]
 [1.763]
 [1.763]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
608 2917
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.529]
 [1.809]
 [2.529]
 [2.529]
 [2.529]] [[0.443]
 [0.203]
 [0.443]
 [0.443]
 [0.443]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
start point for exploration sampling:  20031
rdn probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76098114
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.4992997002903765
siam score:  -0.7633406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.09486535423508981
627 2937
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7715898
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.653]
 [1.322]
 [0.377]
 [1.322]
 [0.71 ]] [[0.24 ]
 [0.464]
 [0.148]
 [0.464]
 [0.259]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.708]
 [2.636]
 [3.061]
 [3.442]
 [2.778]] [[0.407]
 [0.383]
 [0.525]
 [0.653]
 [0.431]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
636 2952
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7692794
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.659]
 [1.833]
 [3.001]
 [2.625]
 [2.428]] [[1.468]
 [0.609]
 [1.825]
 [1.433]
 [1.229]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.803]
 [3.008]
 [2.803]
 [2.803]
 [2.803]] [[1.845]
 [2.   ]
 [1.845]
 [1.845]
 [1.845]]
siam score:  -0.7713797
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.11158057908363478
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78322047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.77695465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.771671
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.42 ]
 [2.392]
 [1.882]
 [2.462]
 [2.83 ]] [[1.393]
 [1.362]
 [0.803]
 [1.439]
 [1.842]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.731]
 [1.954]
 [1.921]
 [1.905]
 [1.83 ]] [[1.233]
 [1.456]
 [1.423]
 [1.407]
 [1.332]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.404]
 [3.404]
 [3.404]
 [3.404]
 [3.404]] [[3.404]
 [3.404]
 [3.404]
 [3.404]
 [3.404]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.394]
 [3.559]
 [3.48 ]
 [4.362]
 [3.887]] [[0.899]
 [0.994]
 [0.948]
 [1.457]
 [1.183]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.1152995447207688
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.858]
 [3.683]
 [3.857]
 [3.204]
 [4.256]] [[1.391]
 [1.269]
 [1.39 ]
 [0.936]
 [1.668]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.914]
 [2.154]
 [2.087]
 [1.992]
 [1.973]] [[0.824]
 [1.064]
 [0.997]
 [0.902]
 [0.883]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7829164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78505814
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7856639
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.12156624186177159
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.12175012308649508
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]] [[1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.258]
 [1.269]
 [1.904]
 [1.876]
 [2.294]] [[0.485]
 [0.492]
 [0.915]
 [0.896]
 [1.174]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 6.205187035062865
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
siam score:  -0.77011603
start point for exploration sampling:  20031
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.12781995586763245
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7745645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78022647
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.424]
 [1.424]
 [1.726]
 [1.258]
 [1.424]] [[0.526]
 [0.526]
 [0.828]
 [0.361]
 [0.526]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7911371
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.79398924
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.32 ]
 [3.32 ]
 [3.158]
 [3.329]
 [3.32 ]] [[1.891]
 [1.891]
 [1.734]
 [1.899]
 [1.891]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7985376
first move QE:  0.13574646049729408
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.763788073743696
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.749]
 [2.749]
 [2.749]
 [2.789]
 [2.749]] [[1.411]
 [1.411]
 [1.411]
 [1.451]
 [1.411]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78555125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.308]
 [ 0.308]
 [ 0.219]
 [-0.135]
 [ 0.308]] [[0.582]
 [0.582]
 [0.522]
 [0.282]
 [0.582]]
siam score:  -0.7933396
siam score:  -0.7976595
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
709 3015
siam score:  -0.8002617
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.277]
 [2.928]
 [2.928]
 [2.928]
 [2.948]] [[1.087]
 [0.855]
 [0.855]
 [0.855]
 [0.868]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
713 3016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.80446714
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80255586
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.922]
 [3.532]
 [3.427]
 [3.427]
 [3.427]] [[0.801]
 [0.671]
 [0.636]
 [0.636]
 [0.636]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8012488
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.473]
 [3.473]
 [3.473]
 [3.473]
 [3.473]] [[1.478]
 [1.478]
 [1.478]
 [1.478]
 [1.478]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
723 3020
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.1451846897882422
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8082633
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.188]
 [2.197]
 [2.179]
 [2.22 ]
 [2.188]] [[0.384]
 [0.387]
 [0.381]
 [0.394]
 [0.384]]
first move QE:  0.14609388384661662
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.432]
 [2.432]
 [2.432]
 [2.432]
 [2.432]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.265]
 [2.946]
 [2.946]
 [3.085]
 [2.946]] [[1.775]
 [0.996]
 [0.996]
 [1.078]
 [0.996]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.896]
 [5.116]
 [4.535]
 [2.718]
 [4.76 ]] [[1.704]
 [1.285]
 [0.974]
 [0.   ]
 [1.094]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.8065858595650144
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.8151426
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.492]
 [2.571]
 [2.157]
 [2.436]
 [2.492]] [[1.316]
 [1.369]
 [1.094]
 [1.28 ]
 [1.316]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.016]
 [0.593]
 [0.55 ]
 [0.756]
 [0.756]] [[1.149]
 [0.585]
 [0.527]
 [0.802]
 [0.802]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.577286998314032
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
750 3040
siam score:  -0.8099092
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81673646
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.732]
 [3.916]
 [3.732]
 [4.055]
 [4.11 ]] [[1.403]
 [1.543]
 [1.403]
 [1.648]
 [1.69 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.692]
 [5.692]
 [5.692]
 [5.692]
 [5.692]] [[1.99]
 [1.99]
 [1.99]
 [1.99]
 [1.99]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8171057
line 256 mcts: sample exp_bonus 3.972497955786763
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81798446
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.753]
 [4.483]
 [3.502]
 [3.427]
 [5.363]] [[1.214]
 [1.042]
 [0.418]
 [0.37 ]
 [1.602]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.511]
 [-0.011]
 [ 1.077]
 [ 0.4  ]
 [ 0.451]] [[0.463]
 [0.115]
 [0.84 ]
 [0.389]
 [0.423]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.836]
 [0.6  ]
 [1.263]
 [1.226]
 [0.862]] [[0.377]
 [0.219]
 [0.661]
 [0.636]
 [0.394]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.819476
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.15956705460824522
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.049]
 [2.049]
 [2.262]
 [2.049]
 [2.049]] [[1.435]
 [1.435]
 [1.72 ]
 [1.435]
 [1.435]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
779 3057
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.16271711030026584
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.198]
 [3.198]
 [3.198]
 [3.198]
 [3.404]] [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.813]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
790 3062
siam score:  -0.81648475
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
790 3066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.199]
 [5.199]
 [5.199]
 [5.199]
 [5.199]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.46678129226899
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8304242
siam score:  -0.83126456
first move QE:  0.16471708888185863
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5468640986312616
siam score:  -0.8272433
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.82955813
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.209]
 [1.209]
 [2.102]
 [1.209]
 [1.209]] [[0.551]
 [0.551]
 [0.849]
 [0.551]
 [0.551]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
816 3095
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
line 256 mcts: sample exp_bonus 1.1370499067332238
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.099]
 [2.099]
 [2.099]
 [1.903]
 [2.099]] [[1.057]
 [1.057]
 [1.057]
 [0.86 ]
 [1.057]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
825 3105
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.333]
 [2.993]
 [2.993]
 [2.993]
 [2.993]] [[2.   ]
 [0.469]
 [0.469]
 [0.469]
 [0.469]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8474095
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8477919
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 6.0460907354453495
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.4486983393383402
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84641236
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84725624
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.053]
 [ 0.231]
 [ 0.761]
 [ 0.315]
 [-0.051]] [[0.071]
 [0.192]
 [0.553]
 [0.249]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.634]
 [3.584]
 [3.584]
 [3.584]
 [3.584]] [[1.838]
 [1.796]
 [1.796]
 [1.796]
 [1.796]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.26 ]
 [4.123]
 [4.123]
 [4.123]
 [5.566]] [[1.66 ]
 [1.191]
 [1.191]
 [1.191]
 [1.786]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84872437
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84833497
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.538]
 [0.804]
 [0.772]
 [1.078]
 [0.889]] [[0.025]
 [0.114]
 [0.103]
 [0.205]
 [0.142]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.1798547885330359
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85626733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.363]
 [3.633]
 [2.511]
 [3.553]
 [3.215]] [[1.375]
 [1.539]
 [0.857]
 [1.49 ]
 [1.285]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8519014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
siam score:  -0.8525693
siam score:  -0.8516764
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
869 3152
siam score:  -0.8573683
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.51 ]
 [3.496]
 [3.264]
 [3.582]
 [3.264]] [[1.668]
 [1.654]
 [1.424]
 [1.739]
 [1.424]]
siam score:  -0.8450586
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.836]
 [4.127]
 [4.069]
 [3.811]
 [3.774]] [[1.912]
 [1.537]
 [1.506]
 [1.369]
 [1.349]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.153]
 [2.153]
 [2.153]
 [2.153]
 [2.153]] [[0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.8716082208994815
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8618128
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
885 3166
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.254]
 [3.367]
 [1.463]
 [3.322]
 [3.844]] [[1.388]
 [1.449]
 [0.417]
 [1.425]
 [1.708]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.1876694984423904
siam score:  -0.862454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.426]
 [1.601]
 [1.59 ]
 [1.777]
 [1.407]] [[0.296]
 [0.413]
 [0.405]
 [0.529]
 [0.283]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  890
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8689227
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.086]
 [2.3  ]
 [2.098]
 [2.129]
 [2.97 ]] [[0.539]
 [0.611]
 [0.543]
 [0.554]
 [0.834]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.567]
 [-0.524]
 [ 0.12 ]
 [-0.371]
 [-0.919]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.86530375
UNIT TEST: sample policy line 217 mcts : [0.083 0.75  0.083 0.042 0.042]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.095]
 [0.79 ]
 [1.393]
 [0.581]
 [0.114]] [[0.026]
 [0.463]
 [0.844]
 [0.332]
 [0.038]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.185]
 [2.127]
 [2.191]
 [2.32 ]
 [1.899]] [[0.632]
 [0.592]
 [0.636]
 [0.721]
 [0.441]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8646117
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.336834969353452
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
910 3190
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.36 ]
 [3.36 ]
 [3.296]
 [3.36 ]
 [3.36 ]] [[1.509]
 [1.509]
 [1.458]
 [1.509]
 [1.509]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.825]
 [3.665]
 [2.895]
 [2.273]
 [3.006]] [[1.006]
 [1.588]
 [1.054]
 [0.623]
 [1.13 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.763]
 [3.343]
 [3.343]
 [3.343]
 [3.343]] [[2.   ]
 [1.629]
 [1.629]
 [1.629]
 [1.629]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.003]
 [3.767]
 [3.767]
 [3.767]
 [3.767]] [[1.07 ]
 [1.456]
 [1.456]
 [1.456]
 [1.456]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.435]
 [3.717]
 [3.386]
 [2.985]
 [3.253]] [[1.322]
 [1.538]
 [1.285]
 [0.979]
 [1.184]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.467]
 [3.369]
 [3.939]
 [2.975]
 [3.939]] [[1.056]
 [0.993]
 [1.358]
 [0.741]
 [1.358]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.311]
 [0.15 ]
 [0.146]
 [0.037]
 [0.615]] [[0.995]
 [0.088]
 [0.086]
 [0.   ]
 [0.451]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.953]
 [3.355]
 [1.809]
 [2.99 ]
 [3.754]] [[1.391]
 [1.003]
 [0.   ]
 [0.766]
 [1.262]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.19397359159616623
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8694197
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8699634
siam score:  -0.87099516
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.402]
 [1.97 ]
 [1.94 ]
 [2.245]
 [3.449]] [[1.162]
 [0.907]
 [0.889]
 [1.069]
 [1.782]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
line 256 mcts: sample exp_bonus 3.900520909282842
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.789]
 [2.97 ]
 [0.788]
 [3.233]
 [2.97 ]] [[1.665]
 [1.265]
 [0.202]
 [1.394]
 [1.265]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8695517
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8662646
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.257]
 [2.446]
 [2.446]
 [2.446]
 [2.446]] [[1.128]
 [0.588]
 [0.588]
 [0.588]
 [0.588]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.333 0.458 0.042]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.154]
 [0.154]
 [1.454]
 [0.154]
 [0.154]] [[-0.536]
 [-0.536]
 [ 0.763]
 [-0.536]
 [-0.536]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.599]
 [3.599]
 [3.599]
 [3.523]
 [5.773]] [[0.923]
 [0.923]
 [0.923]
 [0.886]
 [1.964]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87008035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.485]
 [4.503]
 [4.03 ]
 [4.284]
 [3.888]] [[1.287]
 [1.899]
 [1.615]
 [1.767]
 [1.529]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.5353616904244074
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  950
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8724455
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.313]
 [2.008]
 [1.313]
 [1.313]
 [1.313]] [[0.814]
 [1.286]
 [0.814]
 [0.814]
 [0.814]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.14 ]
 [ 0.905]
 [ 0.388]
 [-0.255]] [[0.085]
 [0.132]
 [0.387]
 [0.214]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.189]
 [1.461]
 [1.189]
 [1.189]
 [1.189]] [[0.216]
 [0.307]
 [0.216]
 [0.216]
 [0.216]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.619]
 [2.288]
 [1.619]
 [1.619]
 [1.619]] [[1.035]
 [1.481]
 [1.035]
 [1.035]
 [1.035]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88286597
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 1.0471758762092045e-20
0.0 7.423671442418483e-18
0.0 3.0931964343410346e-18
0.0 0.0
0.0 9.341140459608179e-12
0.0 1.752811312793253e-18
0.0 1.0310654781136782e-19
0.0 0.0
0.0 4.865177326975136e-12
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8725454
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.539]
 [2.267]
 [2.267]
 [2.267]
 [2.267]] [[2.   ]
 [0.967]
 [0.967]
 [0.967]
 [0.967]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8746157
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.87476075
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8842708
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.088]
 [3.216]
 [3.216]
 [3.216]
 [3.216]] [[1.627]
 [1.756]
 [1.756]
 [1.756]
 [1.756]]
first move QE:  0.19176735454142785
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.678]
 [0.33 ]
 [1.471]
 [0.659]
 [0.217]] [[0.341]
 [0.084]
 [0.928]
 [0.327]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.89 ]
 [2.041]
 [1.772]
 [1.267]
 [1.49 ]] [[0.   ]
 [1.157]
 [0.886]
 [0.379]
 [0.603]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.234]
 [3.085]
 [3.838]
 [3.085]
 [3.224]] [[1.168]
 [1.049]
 [1.648]
 [1.049]
 [1.16 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
siam score:  -0.8861809
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.798]
 [3.798]
 [3.981]
 [3.798]
 [3.798]] [[1.342]
 [1.342]
 [1.475]
 [1.342]
 [1.342]]
996 3282
996 3283
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.112]
 [ 0.073]
 [ 0.252]
 [-0.149]
 [-0.362]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0961940872750886
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.319]
 [1.725]
 [1.319]
 [1.275]
 [1.319]] [[0.692]
 [1.099]
 [0.692]
 [0.649]
 [0.692]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8783024
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8785912
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.973]
 [3.42 ]
 [3.169]
 [2.89 ]
 [3.169]] [[1.167]
 [1.522]
 [1.322]
 [1.1  ]
 [1.322]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8754093
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
1009 3318
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
1011 3320
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.358]
 [2.358]
 [2.358]
 [1.716]
 [2.358]] [[1.542]
 [1.542]
 [1.542]
 [0.901]
 [1.542]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.818]
 [0.818]
 [1.287]
 [0.818]
 [0.818]] [[0.149]
 [0.149]
 [0.579]
 [0.149]
 [0.149]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.041]
 [1.25 ]
 [1.002]
 [1.041]
 [1.041]] [[0.376]
 [0.515]
 [0.35 ]
 [0.376]
 [0.376]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.244]
 [3.246]
 [3.093]
 [3.253]
 [3.261]] [[1.47 ]
 [1.472]
 [1.332]
 [1.478]
 [1.485]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.211]
 [0.138]
 [0.138]
 [0.149]
 [0.172]] [[1.335]
 [1.238]
 [1.238]
 [1.253]
 [1.284]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.1918785047651901
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.035]
 [0.035]
 [0.428]
 [0.035]
 [0.035]] [[0.418]
 [0.418]
 [0.549]
 [0.418]
 [0.418]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.19281442362873208
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.479]
 [1.479]
 [1.479]
 [1.454]
 [1.479]] [[0.574]
 [0.574]
 [0.574]
 [0.557]
 [0.574]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.187]
 [2.987]
 [3.91 ]
 [2.987]
 [2.987]] [[1.409]
 [1.247]
 [2.   ]
 [1.247]
 [1.247]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87342995
1034 3335
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8775335
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.428]
 [2.428]
 [2.428]
 [2.428]
 [2.428]] [[1.275]
 [1.275]
 [1.275]
 [1.275]
 [1.275]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]] [[0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87478477
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.309410438159355
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.941]
 [1.557]
 [1.144]
 [1.357]
 [1.012]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.79 ]
 [2.439]
 [2.439]
 [2.439]
 [2.439]] [[1.062]
 [0.828]
 [0.828]
 [0.828]
 [0.828]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88861006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1052 3353
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.551]
 [3.794]
 [3.794]
 [3.794]
 [3.794]] [[1.78]
 [2.  ]
 [2.  ]
 [2.  ]
 [2.  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5362297910624214
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.916]
 [1.916]
 [1.916]
 [1.916]
 [1.916]] [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1058 3362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8844176
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.88105184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.37 ]
 [2.867]
 [1.117]
 [2.937]
 [3.844]] [[0.753]
 [0.585]
 [0.   ]
 [0.608]
 [0.911]]
start point for exploration sampling:  20031
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.551]
 [2.504]
 [2.46 ]
 [2.606]
 [2.232]] [[1.15 ]
 [1.108]
 [1.069]
 [1.199]
 [0.864]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 0.4594063340772363
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.611316772473657
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  1072
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.20002713649064446
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]] [[0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.8866564
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8863806
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.795]
 [1.937]
 [1.795]
 [1.003]
 [1.366]] [[1.146]
 [1.288]
 [1.146]
 [0.354]
 [0.717]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.865]
 [1.865]
 [1.865]
 [1.865]
 [1.865]] [[0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[1.625]
 [1.625]
 [1.625]
 [1.625]
 [1.625]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8801484
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.88210374
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.171]
 [2.209]
 [1.833]
 [0.549]
 [1.835]] [[1.622]
 [1.661]
 [1.285]
 [0.   ]
 [1.286]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.25  0.583 0.042]
siam score:  -0.8876635
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8496609190191715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.194]
 [1.42 ]
 [0.504]
 [1.194]
 [1.325]] [[0.258]
 [0.334]
 [0.027]
 [0.258]
 [0.302]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8846826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89138776
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.357]
 [0.958]
 [0.958]
 [0.958]
 [0.958]] [[0.238]
 [0.105]
 [0.105]
 [0.105]
 [0.105]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.111]
 [2.111]
 [2.111]
 [2.111]
 [2.111]] [[0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.353]
 [3.353]
 [3.353]
 [2.973]
 [3.559]] [[1.428]
 [1.428]
 [1.428]
 [1.12 ]
 [1.596]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.908]
 [3.126]
 [3.126]
 [3.126]
 [3.105]] [[1.824]
 [2.   ]
 [2.   ]
 [2.   ]
 [1.983]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.5115337071571318
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8934613
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.821]
 [2.285]
 [2.314]
 [2.084]
 [2.407]] [[0.947]
 [1.554]
 [1.591]
 [1.291]
 [1.713]]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8863506
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.515]
 [2.024]
 [1.864]
 [2.753]
 [1.783]] [[0.487]
 [0.996]
 [0.836]
 [1.725]
 [0.755]]
line 256 mcts: sample exp_bonus 0.47277306895583293
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.19920678961763919
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.282]
 [2.282]
 [2.282]
 [1.558]
 [2.282]] [[0.99 ]
 [0.99 ]
 [0.99 ]
 [0.508]
 [0.99 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8953847
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.324]
 [1.742]
 [1.688]
 [1.553]
 [1.531]] [[0.208]
 [0.348]
 [0.33 ]
 [0.285]
 [0.277]]
in main func line 156:  1137
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.373]
 [3.105]
 [3.389]
 [3.389]
 [3.789]] [[1.221]
 [1.086]
 [1.229]
 [1.229]
 [1.431]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.19871219540323545
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.162638403568712
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88326627
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8846562
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.19928050328816097
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8892793
1154 3474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.8050687437766336
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
1165 3489
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.394]
 [2.88 ]
 [2.394]
 [2.394]
 [2.394]] [[1.195]
 [1.641]
 [1.195]
 [1.195]
 [1.195]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.19948327900308227
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1169 3495
1170 3497
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8863026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8818042
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.20016225808158591
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.749]
 [0.749]
 [1.304]
 [0.416]
 [0.224]] [[0.527]
 [0.527]
 [0.712]
 [0.416]
 [0.351]]
line 256 mcts: sample exp_bonus 1.5468057641435433
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88583255
siam score:  -0.88580024
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8885382
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8829382
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.2013454064251138
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.653840526714516
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.601]
 [2.647]
 [1.644]
 [2.317]
 [2.647]] [[1.832]
 [0.9  ]
 [0.422]
 [0.743]
 [0.9  ]]
start point for exploration sampling:  20031
line 256 mcts: sample exp_bonus 2.096157549885786
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1192 3527
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8896601
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.964]
 [1.964]
 [1.964]
 [1.964]
 [1.964]] [[1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.636]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.556]
 [2.201]
 [1.754]
 [1.61 ]
 [1.556]] [[0.647]
 [1.31 ]
 [0.851]
 [0.702]
 [0.647]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6972179309515916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8894279
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.187]
 [1.846]
 [1.187]
 [1.187]
 [1.524]] [[0.134]
 [0.355]
 [0.134]
 [0.134]
 [0.247]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.595]
 [2.707]
 [3.347]
 [2.512]
 [2.847]] [[0.933]
 [1.008]
 [1.434]
 [0.878]
 [1.101]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.492]
 [1.415]
 [1.492]
 [1.492]
 [1.492]] [[2.  ]
 [1.91]
 [2.  ]
 [2.  ]
 [2.  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.89458954
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.259]
 [1.819]
 [1.483]
 [2.259]
 [1.653]] [[1.441]
 [1.001]
 [0.665]
 [1.441]
 [0.834]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.108]
 [2.059]
 [1.766]
 [1.189]
 [2.009]] [[1.506]
 [1.461]
 [1.19 ]
 [0.658]
 [1.415]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.369]
 [0.369]
 [0.733]
 [0.369]
 [0.369]] [[0.16 ]
 [0.16 ]
 [0.524]
 [0.16 ]
 [0.16 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.43 ]
 [3.43 ]
 [3.43 ]
 [2.402]
 [3.43 ]] [[1.744]
 [1.744]
 [1.744]
 [1.056]
 [1.744]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8932177
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8968008
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.226]
 [ 0.032]
 [ 0.301]
 [-0.427]
 [-0.286]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.19889878586669524
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8982456
1231 3583
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.849]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]] [[1.477]
 [1.36 ]
 [1.36 ]
 [1.36 ]
 [1.36 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.42 ]
 [0.42 ]
 [0.488]
 [0.42 ]
 [0.42 ]] [[0.141]
 [0.141]
 [0.164]
 [0.141]
 [0.141]]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.132]
 [ 0.282]
 [ 0.469]
 [ 0.125]
 [ 0.414]] [[0.   ]
 [0.276]
 [0.401]
 [0.171]
 [0.364]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.2032197489229017
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.975]
 [-0.975]
 [-0.975]
 [-0.975]
 [-0.975]] [[0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8913948
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.193]
 [0.196]
 [0.198]
 [0.201]
 [0.196]] [[0.   ]
 [0.002]
 [0.003]
 [0.006]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8882195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8865947
first move QE:  0.20057699886094216
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.564]
 [1.41 ]
 [1.41 ]
 [1.41 ]
 [1.41 ]] [[1.193]
 [0.988]
 [0.988]
 [0.988]
 [0.988]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.447]
 [0.447]
 [0.447]
 [0.993]
 [0.447]] [[0.012]
 [0.012]
 [0.012]
 [0.376]
 [0.012]]
first move QE:  0.20000002507341802
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88111967
siam score:  -0.8809969
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.607]
 [1.658]
 [1.57 ]
 [1.585]
 [2.304]] [[0.574]
 [0.608]
 [0.549]
 [0.559]
 [1.037]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.886665
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1259 3686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8872055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8871668
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  1261
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.175]
 [1.056]
 [0.21 ]
 [0.158]
 [0.746]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.413]
 [-0.058]
 [ 0.159]
 [-0.634]
 [-0.817]] [[0.538]
 [1.013]
 [1.302]
 [0.244]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.577]
 [3.577]
 [3.577]
 [3.577]
 [4.129]] [[1.494]
 [1.494]
 [1.494]
 [1.494]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88386077
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.88085467
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
1271 3717
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8459505802590478
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.275]
 [6.275]
 [6.275]
 [6.275]
 [7.985]] [[1.255]
 [1.255]
 [1.255]
 [1.255]
 [1.857]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1285 3730
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.967]
 [-0.608]
 [ 0.746]
 [-0.519]
 [-0.947]] [[0.   ]
 [0.239]
 [1.141]
 [0.298]
 [0.013]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1287 3731
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86741674
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8663427
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1287 3738
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.816]
 [1.576]
 [1.474]
 [1.751]
 [1.377]] [[1.658]
 [0.668]
 [0.586]
 [0.808]
 [0.509]]
first move QE:  0.20072363535324309
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.635]
 [3.461]
 [1.633]
 [2.553]
 [3.94 ]] [[1.394]
 [1.296]
 [0.271]
 [0.787]
 [1.564]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  0.20068795218828456
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.4743350229825475
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1292 3751
siam score:  -0.88251525
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8857016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1297 3757
siam score:  -0.8860052
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.185]
 [1.519]
 [1.528]
 [1.185]
 [1.185]] [[0.224]
 [0.447]
 [0.453]
 [0.224]
 [0.224]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.531]
 [0.531]
 [0.784]
 [0.65 ]
 [0.531]] [[0.901]
 [0.901]
 [1.069]
 [0.98 ]
 [0.901]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1305 3769
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8852308
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88687193
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89079684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89180267
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1321 3789
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8871292
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.083]
 [3.93 ]
 [3.991]
 [3.991]
 [4.178]] [[1.699]
 [1.598]
 [1.639]
 [1.639]
 [1.762]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88774186
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.086]
 [1.225]
 [1.179]
 [1.215]
 [1.157]] [[0.145]
 [0.191]
 [0.176]
 [0.188]
 [0.169]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.6089526936304197
siam score:  -0.8791932
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88061446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  1337
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88245547
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8893116
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.317]
 [3.074]
 [2.356]
 [2.317]
 [2.317]] [[1.322]
 [1.921]
 [1.353]
 [1.322]
 [1.322]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.469]
 [1.416]
 [1.46 ]
 [1.204]
 [1.181]] [[0.519]
 [0.483]
 [0.512]
 [0.342]
 [0.326]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.375]
 [2.249]
 [4.038]
 [2.249]
 [2.249]] [[0.686]
 [0.613]
 [1.649]
 [0.613]
 [0.613]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  1347
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.649]
 [1.847]
 [1.847]
 [1.847]
 [1.847]] [[0.535]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.681]
 [1.671]
 [1.669]
 [1.276]
 [1.545]] [[0.965]
 [0.955]
 [0.954]
 [0.56 ]
 [0.829]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8648213
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]] [[0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.406]
 [1.327]
 [1.067]
 [1.48 ]
 [1.564]] [[0.   ]
 [0.308]
 [0.221]
 [0.359]
 [0.387]]
siam score:  -0.8649334
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1353 3817
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88577306
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.658]
 [1.25 ]
 [0.863]
 [1.25 ]
 [1.25 ]] [[1.105]
 [0.618]
 [0.157]
 [0.618]
 [0.618]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88049513
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0020296803400384
1360 3834
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.688]
 [2.895]
 [2.688]
 [2.688]
 [2.639]] [[1.54 ]
 [1.737]
 [1.54 ]
 [1.54 ]
 [1.493]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.19598507653672065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.113]
 [ 0.258]
 [ 0.301]
 [-0.113]
 [-0.113]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8813953
siam score:  -0.881637
siam score:  -0.8800948
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.507]
 [0.507]
 [1.139]
 [0.507]
 [0.507]] [[0.823]
 [0.823]
 [1.42 ]
 [0.823]
 [0.823]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.229]
 [2.356]
 [1.989]
 [1.989]
 [2.497]] [[1.239]
 [1.399]
 [0.937]
 [0.937]
 [1.577]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.277915351331107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1388 3865
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88343185
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 1.3132137121018823
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.774]
 [4.355]
 [4.355]
 [3.889]
 [4.231]] [[1.318]
 [1.705]
 [1.705]
 [1.394]
 [1.623]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8735142
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.13 ]
 [ 0.   ]
 [ 0.   ]
 [-0.751]
 [-0.2  ]] [[0.208]
 [0.251]
 [0.251]
 [0.   ]
 [0.184]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.839]
 [3.839]
 [3.839]
 [3.839]
 [3.839]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.326]
 [0.289]
 [0.316]
 [0.193]
 [0.187]] [[0.104]
 [0.079]
 [0.097]
 [0.015]
 [0.011]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.998]
 [2.526]
 [1.998]
 [1.998]
 [2.353]] [[0.801]
 [1.304]
 [0.801]
 [0.801]
 [1.139]]
siam score:  -0.8950621
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.393]
 [3.435]
 [3.001]
 [3.427]
 [3.467]] [[1.564]
 [1.604]
 [1.196]
 [1.596]
 [1.634]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8864774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1423 3904
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.933]
 [1.498]
 [0.933]
 [0.933]
 [0.933]] [[0.637]
 [1.013]
 [0.637]
 [0.637]
 [0.637]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8843281
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.422]
 [2.422]
 [2.422]
 [2.829]
 [2.422]] [[1.438]
 [1.438]
 [1.438]
 [1.981]
 [1.438]]
siam score:  -0.8871035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.762]
 [3.762]
 [3.762]
 [3.762]
 [4.214]] [[0.894]
 [0.894]
 [0.894]
 [0.894]
 [1.052]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.941]
 [2.51 ]
 [3.941]
 [3.941]
 [4.973]] [[1.199]
 [0.37 ]
 [1.199]
 [1.199]
 [1.797]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.19386822657200037
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6027483833049954
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8884116
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.19383017890572726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]] [[0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.1  ]
 [0.223]
 [0.187]
 [0.166]
 [0.193]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.480991587410015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1444 3925
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.19458410559374117
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7188927398983838
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.644]
 [2.659]
 [2.819]
 [2.644]
 [2.644]] [[1.359]
 [1.369]
 [1.475]
 [1.359]
 [1.359]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]] [[0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.183]
 [-0.183]
 [ 0.09 ]
 [-0.398]
 [-0.183]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.8765743
siam score:  -0.8705836
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.19464627173125554
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.49 ]
 [1.587]
 [1.584]
 [1.175]
 [1.419]] [[0.534]
 [0.598]
 [0.596]
 [0.324]
 [0.487]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.502]
 [1.596]
 [1.685]
 [1.374]
 [1.363]] [[0.454]
 [0.516]
 [0.576]
 [0.369]
 [0.361]]
siam score:  -0.87061363
1464 3942
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88595366
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.542 0.292 0.042]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8788283
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  1482
1482 3967
siam score:  -0.8756629
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.87870604
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  1488
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
1490 3973
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.251]
 [1.038]
 [0.982]
 [0.777]
 [1.018]] [[0.   ]
 [0.263]
 [0.244]
 [0.176]
 [0.256]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.315]
 [0.315]
 [0.663]
 [0.315]
 [0.315]] [[0.365]
 [0.365]
 [0.829]
 [0.365]
 [0.365]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8716726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.825]
 [3.825]
 [2.773]
 [3.825]
 [4.568]] [[1.403]
 [1.403]
 [0.799]
 [1.403]
 [1.83 ]]
siam score:  -0.8774968
siam score:  -0.87650454
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.835696101019289
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.671]
 [2.554]
 [1.232]
 [2.345]
 [2.616]] [[1.561]
 [1.434]
 [0.   ]
 [1.208]
 [1.501]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.647377381573148
1515 4006
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87241846
first move QE:  0.1905781059580182
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.772]
 [6.772]
 [6.772]
 [6.772]
 [8.287]] [[1.465]
 [1.465]
 [1.465]
 [1.465]
 [1.997]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1518 4008
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8627533
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.682]
 [6.682]
 [2.393]
 [6.252]
 [6.262]] [[1.756]
 [1.756]
 [0.   ]
 [1.58 ]
 [1.584]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.878]
 [6.878]
 [6.878]
 [6.878]
 [7.084]] [[1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.778]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87394124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.481]
 [1.481]
 [1.587]
 [1.481]
 [1.726]] [[0.637]
 [0.637]
 [0.744]
 [0.637]
 [0.882]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.876869
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.18898104589383055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.075]
 [1.202]
 [1.202]
 [1.202]
 [1.202]] [[0.504]
 [0.631]
 [0.631]
 [0.631]
 [0.631]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.328]
 [0.631]
 [0.693]
 [0.491]
 [0.328]] [[0.367]
 [0.772]
 [0.854]
 [0.584]
 [0.367]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.763]
 [3.5  ]
 [3.763]
 [4.354]
 [4.126]] [[0.997]
 [0.858]
 [0.997]
 [1.31 ]
 [1.189]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
1538 4034
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.148]
 [1.092]
 [1.181]
 [1.148]
 [1.148]] [[1.908]
 [1.844]
 [1.946]
 [1.908]
 [1.908]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
1548 4043
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.164]
 [0.164]
 [0.19 ]
 [0.164]
 [0.164]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8767479
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.18659177368289667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.775]
 [1.805]
 [3.013]
 [1.758]
 [1.832]] [[0.282]
 [0.292]
 [0.695]
 [0.276]
 [0.301]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.877]
 [3.877]
 [3.877]
 [3.107]
 [3.714]] [[1.656]
 [1.656]
 [1.656]
 [1.268]
 [1.573]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.638]
 [0.913]
 [0.92 ]
 [0.744]
 [0.638]] [[0.178]
 [0.361]
 [0.366]
 [0.249]
 [0.178]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87684387
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.649]
 [1.913]
 [2.038]
 [1.913]
 [2.162]] [[0.825]
 [1.011]
 [1.099]
 [1.011]
 [1.186]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.583]
 [-0.127]
 [ 0.277]
 [-0.607]
 [-0.56 ]] [[0.031]
 [0.619]
 [1.141]
 [0.   ]
 [0.061]]
using explorer policy with actor:  1
siam score:  -0.8118306
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.326]
 [1.266]
 [1.266]
 [1.266]
 [1.266]] [[1.032]
 [0.952]
 [0.952]
 [0.952]
 [0.952]]
line 256 mcts: sample exp_bonus 1.228828808292351
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.149]
 [2.149]
 [2.149]
 [2.149]
 [2.207]] [[1.262]
 [1.262]
 [1.262]
 [1.262]
 [1.33 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.79421127
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.166]
 [1.475]
 [1.166]
 [1.166]
 [1.166]] [[0.599]
 [0.805]
 [0.599]
 [0.599]
 [0.599]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.837]
 [1.837]
 [1.992]
 [1.837]
 [1.837]] [[1.19 ]
 [1.19 ]
 [1.397]
 [1.19 ]
 [1.19 ]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.689]
 [1.689]
 [1.689]
 [1.689]
 [1.839]] [[1.088]
 [1.088]
 [1.088]
 [1.088]
 [1.288]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.79086715
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
in main func line 156:  1594
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.79659534
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.558]
 [1.453]
 [1.453]
 [1.453]
 [1.453]] [[1.227]
 [1.087]
 [1.087]
 [1.087]
 [1.087]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
start point for exploration sampling:  20031
line 256 mcts: sample exp_bonus 4.075006293681235
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.40855846352525377
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
UNIT TEST: sample policy line 217 mcts : [0.042 0.417 0.25  0.125 0.167]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.76 ]
 [0.872]
 [1.757]
 [1.584]
 [1.584]] [[1.186]
 [0.001]
 [1.182]
 [0.95 ]
 [0.95 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.533]
 [ 0.076]
 [ 0.098]
 [-0.203]
 [-0.238]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.85190827
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.8520246
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.002]
 [0.002]
 [0.002]] [[1.942]
 [2.057]
 [1.942]
 [1.942]
 [1.942]] [[1.554]
 [1.71 ]
 [1.554]
 [1.554]
 [1.554]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]] [[0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]]
siam score:  -0.8570308
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  0
siam score:  -0.8747351
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.909]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[1.174]
 [1.679]
 [1.215]
 [1.215]
 [1.215]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
1622 4135
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
line 256 mcts: sample exp_bonus 2.0015254104792297
siam score:  -0.87994105
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.008]
 [0.012]
 [0.008]
 [0.011]] [[3.742]
 [2.432]
 [5.094]
 [2.416]
 [5.083]] [[1.236]
 [0.498]
 [2.004]
 [0.489]
 [1.997]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
1632 4165
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[0.166]
 [0.373]
 [0.   ]
 [0.135]
 [0.219]] [[ 0.02 ]
 [ 0.089]
 [-0.034]
 [ 0.009]
 [ 0.038]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
first move QE:  0.1756687191807154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
UNIT TEST: sample policy line 217 mcts : [0.375 0.375 0.125 0.    0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.88209933
siam score:  -0.88087183
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
UNIT TEST: sample policy line 217 mcts : [0.    0.083 0.25  0.    0.667]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
start point for exploration sampling:  20031
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.007]
 [0.006]
 [0.006]
 [0.006]] [[1.515]
 [1.581]
 [1.515]
 [1.515]
 [1.515]] [[1.638]
 [1.727]
 [1.638]
 [1.638]
 [1.638]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.9004459
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]] [[7.647]
 [7.647]
 [7.647]
 [7.647]
 [7.647]] [[1.826]
 [1.826]
 [1.826]
 [1.826]
 [1.826]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
first move QE:  0.17141569001139373
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
in main func line 156:  1654
siam score:  -0.8984989
line 256 mcts: sample exp_bonus 3.8759142312682022
line 256 mcts: sample exp_bonus 2.0378939873398427
start point for exploration sampling:  20031
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
siam score:  -0.9063421
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.044]
 [0.03 ]
 [0.035]] [[3.101]
 [3.101]
 [3.718]
 [3.303]
 [3.485]] [[0.837]
 [0.837]
 [1.275]
 [0.968]
 [1.101]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
siam score:  -0.9159838
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.011]
 [0.012]
 [0.01 ]
 [0.01 ]] [[0.137]
 [0.485]
 [0.42 ]
 [0.137]
 [0.137]] [[1.028]
 [1.332]
 [1.277]
 [1.028]
 [1.028]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[-0.269]
 [-0.269]
 [-0.269]
 [-0.269]
 [-0.269]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
using another actor
start point for exploration sampling:  20031
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.011]
 [0.006]
 [0.002]
 [0.009]] [[0.997]
 [0.914]
 [0.756]
 [0.243]
 [0.83 ]] [[0.013]
 [0.011]
 [0.006]
 [0.002]
 [0.009]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.041]
 [0.053]
 [0.041]
 [0.043]] [[2.087]
 [3.435]
 [3.639]
 [3.435]
 [3.545]] [[0.787]
 [1.746]
 [1.902]
 [1.746]
 [1.826]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.097]
 [0.157]
 [0.137]
 [0.089]
 [0.104]] [[0.036]
 [0.075]
 [0.064]
 [0.033]
 [0.041]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
line 256 mcts: sample exp_bonus 1.040150715096895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.012]
 [0.013]
 [0.014]] [[1.797]
 [1.797]
 [1.61 ]
 [1.613]
 [1.721]] [[1.277]
 [1.277]
 [1.025]
 [1.03 ]
 [1.176]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
start point for exploration sampling:  20031
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.013]
 [0.008]
 [0.008]
 [0.008]] [[0.455]
 [0.639]
 [0.455]
 [0.455]
 [0.455]] [[0.281]
 [0.474]
 [0.281]
 [0.281]
 [0.281]]
using explorer policy with actor:  1
siam score:  -0.9230692
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.005]
 [0.004]
 [0.004]] [[-0.274]
 [-0.274]
 [ 0.007]
 [-0.274]
 [-0.274]] [[0.004]
 [0.004]
 [0.005]
 [0.004]
 [0.004]]
using another actor
1681 4293
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.011]
 [0.01 ]
 [0.008]] [[ 0.   ]
 [ 0.   ]
 [ 0.18 ]
 [-0.094]
 [ 0.   ]] [[0.129]
 [0.129]
 [0.374]
 [0.007]
 [0.129]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.9309703
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.099]
 [0.099]
 [0.09 ]
 [0.099]] [[2.488]
 [2.488]
 [2.488]
 [2.256]
 [2.488]] [[2.027]
 [2.027]
 [2.027]
 [1.701]
 [2.027]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.93103075
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
1692 4322
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
UNIT TEST: sample policy line 217 mcts : [0.042 0.542 0.333 0.042 0.042]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.005]] [[0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.978]] [[0.702]
 [0.702]
 [0.702]
 [0.702]
 [1.107]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
first move QE:  0.14988505378208192
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.9303006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]] [[1.314]
 [1.334]
 [1.334]
 [1.334]
 [1.334]] [[0.009]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[1.247]
 [1.298]
 [1.389]
 [1.389]
 [1.251]] [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]]
1700 4452
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.014]
 [0.034]
 [0.036]
 [0.037]] [[-1.81 ]
 [-1.048]
 [-0.806]
 [-1.573]
 [-1.58 ]] [[0.031]
 [0.633]
 [0.861]
 [0.221]
 [0.216]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
start point for exploration sampling:  20031
siam score:  -0.9363039
siam score:  -0.93773025
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.036]] [[2.212]
 [2.212]
 [2.212]
 [2.212]
 [2.495]] [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.903]]
start point for exploration sampling:  20031
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
using explorer policy with actor:  0
1705 4502
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using another actor
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
1707 4521
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using another actor
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.   ]
 [0.01 ]
 [0.012]
 [0.013]] [[-1.705]
 [-0.445]
 [-0.475]
 [-1.321]
 [-1.245]] [[0.011]
 [0.   ]
 [0.01 ]
 [0.012]
 [0.013]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.018]
 [0.015]
 [0.015]
 [0.015]] [[0.895]
 [1.099]
 [0.88 ]
 [0.88 ]
 [0.88 ]] [[0.015]
 [0.018]
 [0.015]
 [0.015]
 [0.015]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8600645393368893
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
line 256 mcts: sample exp_bonus 1.5774138809106188
siam score:  -0.9306044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.035]] [[1.906]
 [1.906]
 [1.906]
 [1.906]
 [1.763]] [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.035]]
siam score:  -0.92833054
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.031]] [[2.333]
 [2.333]
 [2.333]
 [2.333]
 [1.357]] [[0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.031]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.033]] [[2.001]
 [2.001]
 [2.001]
 [2.001]
 [1.476]] [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.033]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.038]] [[1.972]
 [1.972]
 [1.972]
 [1.972]
 [1.292]] [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.038]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
line 256 mcts: sample exp_bonus 1.9922160417031824
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[3.306]
 [3.306]
 [3.306]
 [3.306]
 [3.306]] [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
line 256 mcts: sample exp_bonus 3.181856859344508
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.056]] [[2.565]
 [2.565]
 [2.565]
 [2.565]
 [2.535]] [[0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.056]]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.088498191834361
line 256 mcts: sample exp_bonus 0.07194734932929917
rdn probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.9267763
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
1715 4557
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
1716 4560
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.038]
 [0.037]
 [0.035]
 [0.035]] [[0.169]
 [0.696]
 [0.261]
 [0.169]
 [0.169]] [[1.148]
 [1.633]
 [1.235]
 [1.148]
 [1.148]]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.032]
 [0.032]
 [0.032]
 [0.032]] [[1.03 ]
 [0.805]
 [0.604]
 [0.604]
 [0.604]] [[1.821]
 [1.531]
 [1.287]
 [1.287]
 [1.287]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.04 ]
 [0.035]
 [0.035]
 [0.035]] [[0.479]
 [0.808]
 [0.479]
 [0.479]
 [0.479]] [[0.582]
 [1.031]
 [0.582]
 [0.582]
 [0.582]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
1724 4572
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]] [[1.658]
 [1.658]
 [1.658]
 [1.658]
 [1.658]] [[1.547]
 [1.547]
 [1.547]
 [1.547]
 [1.547]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.9309536
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  0
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
1737 4620
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
siam score:  -0.9413749
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
1738 4641
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[1.539]
 [1.539]
 [1.539]
 [1.539]
 [1.539]]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.028]
 [0.033]
 [0.028]
 [0.028]] [[-0.284]
 [-0.284]
 [ 0.101]
 [-0.284]
 [-0.284]] [[1.236]
 [1.236]
 [1.742]
 [1.236]
 [1.236]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
1740 4668
siam score:  -0.9321212
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]] [[1.903]
 [1.903]
 [1.903]
 [1.903]
 [1.903]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.   ]
 [0.003]
 [0.003]
 [0.003]] [[-0.256]
 [ 0.131]
 [-0.256]
 [-0.256]
 [-0.256]] [[1.044]
 [1.446]
 [1.044]
 [1.044]
 [1.044]]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.029]
 [0.029]
 [0.029]
 [0.029]] [[0.622]
 [0.512]
 [0.512]
 [0.512]
 [0.512]] [[2.021]
 [1.892]
 [1.892]
 [1.892]
 [1.892]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.9242734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
first move QE:  0.12105097117747358
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
using another actor
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
line 256 mcts: sample exp_bonus 0.4151437708193977
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
siam score:  -0.9296308
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.03 ]
 [0.033]
 [0.03 ]
 [0.03 ]] [[-0.412]
 [-0.412]
 [-0.209]
 [-0.412]
 [-0.412]] [[0.911]
 [0.911]
 [1.188]
 [0.911]
 [0.911]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.002]
 [0.02 ]
 [0.014]
 [0.012]] [[-0.892]
 [-0.32 ]
 [-0.46 ]
 [-0.874]
 [-0.892]] [[0.012]
 [0.002]
 [0.02 ]
 [0.014]
 [0.012]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]] [[0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]] [[0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
1746 4769
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
using another actor
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.1  ]
 [0.031]
 [0.055]
 [0.08 ]] [[3.341]
 [2.763]
 [1.994]
 [1.875]
 [2.673]] [[0.912]
 [0.739]
 [0.344]
 [0.351]
 [0.668]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
1749 4782
1749 4784
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
using another actor
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.013]
 [0.014]
 [0.014]
 [0.014]] [[0.407]
 [0.46 ]
 [0.407]
 [0.407]
 [0.407]] [[0.014]
 [0.013]
 [0.014]
 [0.014]
 [0.014]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
siam score:  -0.93389595
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
start point for exploration sampling:  20031
using another actor
1753 4825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.02 ]
 [0.016]
 [0.016]] [[0.022]
 [0.022]
 [0.649]
 [0.022]
 [0.022]] [[1.197]
 [1.197]
 [1.623]
 [1.197]
 [1.197]]
siam score:  -0.92680895
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20031
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.003]
 [0.009]
 [0.006]
 [0.006]] [[-0.595]
 [-0.008]
 [-0.196]
 [-0.595]
 [-0.595]] [[0.006]
 [0.003]
 [0.009]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.023]
 [0.018]
 [0.022]
 [0.019]] [[-0.133]
 [-0.018]
 [ 0.222]
 [-0.177]
 [-0.133]] [[0.019]
 [0.023]
 [0.018]
 [0.022]
 [0.019]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.93097335
using another actor
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using another actor
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
1756 4884
using another actor
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using another actor
using explorer policy with actor:  0
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.9295238
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
1760 4905
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
line 256 mcts: sample exp_bonus 5.168231913441558
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]] [[1.816]
 [1.816]
 [1.816]
 [1.816]
 [1.816]] [[1.491]
 [1.491]
 [1.491]
 [1.491]
 [1.491]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.016]
 [0.027]
 [0.034]
 [0.027]] [[1.204]
 [1.04 ]
 [0.991]
 [0.726]
 [0.991]] [[1.06 ]
 [0.774]
 [0.73 ]
 [0.391]
 [0.73 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
line 256 mcts: sample exp_bonus 1.4176969915039155
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.114]
 [0.148]
 [0.114]
 [0.114]] [[4.807]
 [4.807]
 [4.148]
 [4.807]
 [4.807]] [[1.824]
 [1.824]
 [1.543]
 [1.824]
 [1.824]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.012]
 [0.009]
 [0.014]
 [0.014]] [[0.629]
 [0.552]
 [0.769]
 [0.465]
 [0.774]] [[0.017]
 [0.012]
 [0.009]
 [0.014]
 [0.014]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
rdn beta is 0 so we're just using the maxi policy
1786 5001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
1787 5005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.9024694
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.001]
 [0.011]
 [0.008]
 [0.012]] [[-1.072]
 [-0.059]
 [-0.178]
 [-0.451]
 [-0.408]] [[0.009]
 [0.001]
 [0.011]
 [0.008]
 [0.012]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5374743284154799
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.8987383
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
first move QE:  0.09839880956288684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.8988652
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.01 ]
 [0.009]
 [0.009]
 [0.009]] [[1.387]
 [1.781]
 [1.387]
 [1.387]
 [1.387]] [[0.946]
 [1.345]
 [0.946]
 [0.946]
 [0.946]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.014]
 [0.007]
 [0.007]] [[-0.963]
 [-0.963]
 [-0.238]
 [-0.963]
 [-0.963]] [[0.003]
 [0.003]
 [0.604]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
1805 5067
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
1806 5081
siam score:  -0.9054544
line 256 mcts: sample exp_bonus 0.5813751401611366
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.005]
 [0.028]
 [0.018]
 [0.018]] [[-0.29 ]
 [-0.043]
 [ 0.079]
 [-0.29 ]
 [-0.29 ]] [[0.679]
 [0.922]
 [1.086]
 [0.679]
 [0.679]]
using another actor
line 256 mcts: sample exp_bonus 0.1773821563551612
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
first move QE:  0.09437531106152074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.001]
 [0.018]
 [0.015]
 [0.014]] [[-0.679]
 [-0.043]
 [-0.335]
 [-0.594]
 [-0.679]] [[0.276]
 [0.461]
 [0.398]
 [0.306]
 [0.276]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
line 256 mcts: sample exp_bonus 0.16394079883943477
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
siam score:  -0.88725626
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.017]
 [0.016]
 [0.016]] [[0.27 ]
 [0.27 ]
 [0.431]
 [0.27 ]
 [0.27 ]] [[0.346]
 [0.346]
 [0.562]
 [0.346]
 [0.346]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.   ]
 [0.005]
 [0.005]
 [0.   ]] [[-0.155]
 [-0.015]
 [-0.155]
 [-0.155]
 [ 0.125]] [[1.047]
 [1.224]
 [1.047]
 [1.047]
 [1.411]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.013]
 [0.015]
 [0.015]] [[1.413]
 [1.201]
 [1.477]
 [1.201]
 [1.201]] [[0.932]
 [0.65 ]
 [1.013]
 [0.65 ]
 [0.65 ]]
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
line 256 mcts: sample exp_bonus 0.18737540513306924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.001]
 [0.012]
 [0.009]
 [0.011]] [[-1.073]
 [-0.094]
 [ 0.107]
 [-0.539]
 [-0.422]] [[0.006]
 [0.001]
 [0.012]
 [0.009]
 [0.011]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using another actor
siam score:  -0.9120247
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[1.116]
 [1.116]
 [1.116]
 [1.116]
 [1.116]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
using another actor
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.004]
 [0.017]
 [0.012]
 [0.011]] [[-1.168]
 [-0.455]
 [-0.136]
 [-1.036]
 [-1.049]] [[0.01 ]
 [0.699]
 [1.032]
 [0.14 ]
 [0.126]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.084]
 [0.079]
 [0.084]
 [0.086]
 [0.086]] [[0.541]
 [0.534]
 [0.541]
 [0.544]
 [0.544]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
using explorer policy with actor:  1
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07149283462022453, 0.07149283462022453, 0.07149283462022453, 0.7855214961393264]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
siam score:  -0.9147799
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.91189986
using another actor
from probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
from probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
1833 5253
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
line 256 mcts: sample exp_bonus 2.9165796978307714
siam score:  -0.91107696
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.059]
 [0.05 ]
 [0.05 ]
 [0.05 ]] [[0.472]
 [0.711]
 [0.484]
 [0.484]
 [0.484]] [[1.284]
 [1.599]
 [1.279]
 [1.279]
 [1.279]]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.234]
 [0.226]
 [0.234]
 [0.234]] [[2.611]
 [2.611]
 [2.478]
 [2.611]
 [2.611]] [[1.989]
 [1.989]
 [1.849]
 [1.989]
 [1.989]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.02 ]
 [0.039]
 [0.02 ]
 [0.02 ]] [[-0.433]
 [-0.433]
 [-0.337]
 [-0.433]
 [-0.433]] [[0.996]
 [0.996]
 [1.112]
 [0.996]
 [0.996]]
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
line 256 mcts: sample exp_bonus 0.8519324453686404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
1840 5310
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.9111014
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.  ]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[0.08]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[0.  ]
 [0.01]
 [0.01]
 [0.01]
 [0.01]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]] [[3.16]
 [3.16]
 [3.16]
 [3.16]
 [3.16]] [[2.077]
 [2.077]
 [2.077]
 [2.077]
 [2.077]]
1843 5334
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.9143311
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
first move QE:  0.07758127289202026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
1846 5431
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.911665
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.9125625
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.023]
 [0.012]
 [0.012]] [[ 0.   ]
 [ 0.   ]
 [-0.208]
 [ 0.   ]
 [ 0.   ]] [[0.012]
 [0.012]
 [0.023]
 [0.012]
 [0.012]]
using another actor
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 0.4305381874823757
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.6864448019170932
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.087]] [[3.318]
 [3.318]
 [3.318]
 [3.318]
 [2.131]] [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.087]]
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.107]
 [0.167]
 [0.107]
 [0.097]] [[2.865]
 [2.281]
 [2.375]
 [2.281]
 [2.098]] [[0.082]
 [0.107]
 [0.167]
 [0.107]
 [0.097]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
line 256 mcts: sample exp_bonus 1.257819252795331
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.082]] [[2.65 ]
 [2.65 ]
 [2.65 ]
 [2.65 ]
 [1.961]] [[0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.082]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.052]
 [0.071]
 [0.052]
 [0.052]
 [0.052]] [[1.704]
 [1.727]
 [1.704]
 [1.704]
 [1.704]]
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
1847 5468
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.067]] [[1.459]
 [1.459]
 [1.459]
 [1.459]
 [1.483]] [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.067]]
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.059]
 [0.087]
 [0.081]
 [0.069]] [[2.665]
 [1.788]
 [2.492]
 [1.897]
 [1.39 ]] [[0.061]
 [0.059]
 [0.087]
 [0.081]
 [0.069]]
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.08 ]
 [0.069]
 [0.081]
 [0.068]] [[2.665]
 [2.487]
 [1.879]
 [1.897]
 [1.63 ]] [[0.061]
 [0.08 ]
 [0.069]
 [0.081]
 [0.068]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.068]
 [0.092]
 [0.077]
 [0.064]] [[1.815]
 [2.54 ]
 [2.45 ]
 [1.815]
 [1.067]] [[0.077]
 [0.068]
 [0.092]
 [0.077]
 [0.064]]
actor:  0 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 1.8139274666509713
actor:  0 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 1.0362596554482306
line 256 mcts: sample exp_bonus 1.1900678887253424
from probs:  [0.0714918394384817, 0.0714918394384817, 0.0714918394384817, 0.7855244816845549]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.0714918394384817, 0.0714918394384817, 0.0714918394384817, 0.7855244816845549]
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.068]
 [0.077]
 [0.077]
 [0.066]] [[1.808]
 [2.197]
 [1.808]
 [1.808]
 [1.072]] [[0.077]
 [0.068]
 [0.077]
 [0.077]
 [0.066]]
using explorer policy with actor:  0
using another actor
1848 5488
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.0714918394384817, 0.0714918394384817, 0.0714918394384817, 0.7855244816845549]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]] [[1.286]
 [1.286]
 [1.286]
 [1.286]
 [1.286]] [[0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]]
rdn probs:  [0.0714918394384817, 0.0714918394384817, 0.0714918394384817, 0.7855244816845549]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
from probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
maxi score, test score, baseline:  0.0061 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.002]
 [0.002]
 [0.002]] [[-0.349]
 [-0.055]
 [-0.349]
 [-0.349]
 [-0.349]] [[1.249]
 [1.59 ]
 [1.249]
 [1.249]
 [1.249]]
using explorer policy with actor:  1
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.034]
 [0.029]
 [0.03 ]
 [0.03 ]] [[0.154]
 [0.076]
 [0.219]
 [0.154]
 [0.154]] [[0.03 ]
 [0.034]
 [0.029]
 [0.03 ]
 [0.03 ]]
using another actor
from probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
siam score:  -0.9021503
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
1851 5528
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149259761303607, 0.07149259761303607, 0.07149259761303607, 0.7855222071608918]
siam score:  -0.90252364
maxi score, test score, baseline:  0.0061 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.023]
 [0.036]
 [0.036]
 [0.046]] [[1.348]
 [1.343]
 [1.084]
 [1.084]
 [1.423]] [[1.171]
 [1.167]
 [0.846]
 [0.846]
 [1.318]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
from probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
1857 5586
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
from probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
from probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
Printing some Q and Qe and total Qs values:  [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]] [[0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
from probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
from probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
using another actor
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.056]
 [0.056]
 [0.536]
 [0.536]
 [0.056]] [[0.  ]
 [0.  ]
 [0.64]
 [0.64]
 [0.  ]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.187]] [[0.939]
 [0.939]
 [0.939]
 [0.939]
 [1.113]] [[0.712]
 [0.712]
 [0.712]
 [0.712]
 [1.156]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.181]] [[1.115]
 [1.115]
 [1.115]
 [1.115]
 [1.743]] [[0.709]
 [0.709]
 [0.709]
 [0.709]
 [1.271]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
siam score:  -0.9128
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.038]
 [0.044]
 [0.044]
 [0.044]] [[1.54 ]
 [1.857]
 [1.54 ]
 [1.54 ]
 [1.54 ]] [[1.118]
 [1.527]
 [1.118]
 [1.118]
 [1.118]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.27 ]
 [0.338]
 [0.319]
 [0.275]] [[2.454]
 [3.688]
 [2.723]
 [2.814]
 [2.677]] [[1.037]
 [1.953]
 [1.298]
 [1.346]
 [1.192]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
1873 5627
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
from probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
using explorer policy with actor:  1
from probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
start point for exploration sampling:  20031
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
1878 5636
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
using another actor
start point for exploration sampling:  20031
siam score:  -0.92034334
1878 5641
using another actor
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
1879 5647
using another actor
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.038]
 [0.038]
 [0.034]
 [0.04 ]] [[1.287]
 [2.526]
 [2.526]
 [3.208]
 [1.593]] [[0.276]
 [1.114]
 [1.114]
 [1.572]
 [0.484]]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
1881 5650
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.116]] [[3.176]
 [3.176]
 [3.176]
 [3.176]
 [6.31 ]] [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [1.587]]
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.111]] [[2.89 ]
 [2.89 ]
 [2.89 ]
 [2.89 ]
 [4.968]] [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [1.005]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.011]
 [0.071]
 [0.049]
 [0.039]] [[0.364]
 [0.66 ]
 [0.494]
 [0.445]
 [0.082]] [[0.412]
 [0.746]
 [0.643]
 [0.534]
 [0.03 ]]
from probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.191]
 [0.226]
 [0.181]
 [0.177]] [[2.925]
 [3.495]
 [2.138]
 [2.567]
 [1.854]] [[0.656]
 [0.977]
 [0.273]
 [0.468]
 [0.079]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
using explorer policy with actor:  1
from probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
line 256 mcts: sample exp_bonus 0.9808369668556609
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
siam score:  -0.9199379
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
siam score:  -0.91994476
line 256 mcts: sample exp_bonus 2.817497392520081
from probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.889419996877477
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
1892 5678
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149316213806967, 0.07149316213806967, 0.07149316213806967, 0.785520513585791]
maxi score, test score, baseline:  0.0061 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]] [[2.356]
 [2.356]
 [2.356]
 [2.356]
 [2.356]] [[1.97]
 [1.97]
 [1.97]
 [1.97]
 [1.97]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.118]
 [0.158]
 [0.263]
 [0.124]] [[0.007]
 [0.075]
 [0.129]
 [0.267]
 [0.082]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
using explorer policy with actor:  1
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
siam score:  -0.90814954
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.059]
 [0.059]
 [0.054]
 [0.059]] [[0.447]
 [0.447]
 [0.447]
 [2.052]
 [0.447]] [[0.368]
 [0.368]
 [0.368]
 [2.023]
 [0.368]]
Printing some Q and Qe and total Qs values:  [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]] [[0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
from probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
using another actor
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]] [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]] [[0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[0.008]
 [0.136]
 [0.136]
 [0.136]
 [0.136]] [[0.058]
 [0.135]
 [0.135]
 [0.135]
 [0.135]]
from probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
first move QE:  0.05399131510522025
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.02 ]
 [0.015]
 [0.015]
 [0.015]] [[1.543]
 [0.965]
 [1.543]
 [1.543]
 [1.543]] [[1.463]
 [0.703]
 [1.463]
 [1.463]
 [1.463]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
siam score:  -0.8932202
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
from probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149400891892521, 0.07149400891892521, 0.07149400891892521, 0.7855179732432244]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.052]
 [0.051]
 [0.052]
 [0.013]
 [0.013]] [[0.077]
 [0.075]
 [0.077]
 [0.025]
 [0.025]]
using another actor
using another actor
maxi score, test score, baseline:  0.0061 0.15 0.15
using explorer policy with actor:  1
1917 5772
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.002]
 [0.002]
 [0.002]] [[-0.122]
 [ 0.038]
 [ 0.053]
 [ 0.053]
 [ 0.053]] [[1.487]
 [1.686]
 [1.709]
 [1.709]
 [1.709]]
using explorer policy with actor:  1
from probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.048]
 [0.022]
 [0.039]
 [0.047]
 [0.001]] [[0.063]
 [0.029]
 [0.051]
 [0.061]
 [0.   ]]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
siam score:  -0.906832
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
1917 5827
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
from probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
siam score:  -0.90746504
from probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
siam score:  -0.9031487
start point for exploration sampling:  20031
using another actor
from probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
siam score:  -0.8995103
first move QE:  0.04002608933312458
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
using explorer policy with actor:  1
from probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
from probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
from probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
from probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.05 ]
 [0.035]
 [0.035]] [[-0.263]
 [-0.263]
 [ 0.19 ]
 [-0.263]
 [-0.263]] [[1.411]
 [1.411]
 [1.88 ]
 [1.411]
 [1.411]]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
from probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.081]
 [0.075]
 [0.056]
 [0.075]] [[0.473]
 [0.881]
 [0.544]
 [0.511]
 [0.52 ]] [[0.086]
 [0.081]
 [0.075]
 [0.056]
 [0.075]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6034994032823425
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
using another actor
maxi score, test score, baseline:  0.0061 0.15 0.15
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
actor:  0 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]] [[-0.414]
 [-0.414]
 [-0.414]
 [-0.414]
 [-0.414]] [[1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.476]]
1926 5977
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.066]
 [0.096]
 [0.066]
 [0.066]] [[-0.594]
 [-0.594]
 [-0.486]
 [-0.594]
 [-0.594]] [[1.679]
 [1.679]
 [1.883]
 [1.679]
 [1.679]]
from probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
using another actor
from probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.035]
 [0.048]
 [0.032]
 [0.041]] [[1.168]
 [1.202]
 [1.264]
 [0.827]
 [1.105]] [[0.037]
 [0.035]
 [0.048]
 [0.032]
 [0.041]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
from probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
line 256 mcts: sample exp_bonus -0.9133860851821002
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
from probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
using another actor
from probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.599898993591072
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
1932 6057
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
siam score:  -0.8976476
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
from probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
from probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
from probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
1936 6090
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07150670966777964, 0.07150670966777964, 0.07150670966777964, 0.785479870996661]
siam score:  -0.8934347
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
from probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
using another actor
1937 6106
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]] [[0.093]
 [0.093]
 [0.093]
 [0.093]
 [0.093]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
using another actor
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6509554051637949
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.029]
 [0.02 ]
 [0.02 ]
 [0.02 ]] [[0.336]
 [0.513]
 [0.682]
 [0.933]
 [0.393]] [[0.15 ]
 [0.392]
 [0.601]
 [0.935]
 [0.216]]
using another actor
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
from probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
using another actor
using explorer policy with actor:  1
1945 6167
1945 6168
line 256 mcts: sample exp_bonus 1.1904630790284683
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
siam score:  -0.87866175
maxi score, test score, baseline:  0.0081 0.15 0.15
first move QE:  0.0320014922976975
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
using another actor
from probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
1946 6218
1946 6227
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0081 0.15 0.15
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.002]
 [0.002]
 [0.002]] [[ 0.015]
 [-0.003]
 [ 0.015]
 [ 0.015]
 [ 0.015]] [[1.029]
 [1.002]
 [1.029]
 [1.029]
 [1.029]]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.051]
 [0.03 ]
 [0.037]
 [0.036]] [[1.976]
 [1.976]
 [1.61 ]
 [1.278]
 [2.21 ]] [[1.717]
 [1.717]
 [1.232]
 [0.837]
 [1.977]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.034]
 [0.036]
 [0.038]
 [0.036]] [[1.427]
 [1.424]
 [1.88 ]
 [1.799]
 [1.824]] [[0.765]
 [0.767]
 [1.226]
 [1.15 ]
 [1.171]]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.036]] [[2.609]
 [2.609]
 [2.609]
 [2.609]
 [2.526]] [[2.016]
 [2.016]
 [2.016]
 [2.016]
 [1.91 ]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
first move QE:  0.03141678643698931
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149824270269992, 0.07149824270269992, 0.07149824270269992, 0.7855052718919002]
maxi score, test score, baseline:  0.0081 0.15 0.15
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.07149542020249855, 0.07149542020249855, 0.07149542020249855, 0.7855137393925044]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.25, 0.05776819755373657, 0.05776819755373657, 0.6344636048925268]
maxi score, test score, baseline:  0.0081 0.15 0.15
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.25, 0.05776819755373657, 0.05776819755373657, 0.6344636048925268]
siam score:  -0.87876666
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.32 ]] [[3.473]
 [3.473]
 [3.473]
 [3.473]
 [3.517]] [[1.981]
 [1.981]
 [1.981]
 [1.981]
 [2.019]]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.319]] [[3.367]
 [3.367]
 [3.367]
 [3.367]
 [3.367]] [[2.163]
 [2.163]
 [2.163]
 [2.163]
 [2.163]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.25, 0.05776819755373657, 0.05776819755373657, 0.6344636048925268]
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.019]] [[1.727]
 [1.727]
 [1.727]
 [1.727]
 [4.944]] [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [1.839]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.25, 0.05776819755373657, 0.05776819755373657, 0.6344636048925268]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.022]] [[1.995]
 [1.995]
 [1.995]
 [1.995]
 [2.093]] [[1.639]
 [1.639]
 [1.639]
 [1.639]
 [1.772]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.25, 0.05776819755373657, 0.05776819755373657, 0.6344636048925268]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.25, 0.05776819755373657, 0.05776819755373657, 0.6344636048925268]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.25, 0.05776819755373657, 0.05776819755373657, 0.6344636048925268]
first move QE:  0.029071678200333555
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0081 0.15 0.15
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.25, 0.05776819755373657, 0.05776819755373657, 0.6344636048925268]
from probs:  [0.25, 0.05776819755373657, 0.05776819755373657, 0.6344636048925268]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
siam score:  -0.88959193
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
from probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
from probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
from probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
1972 6341
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
from probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
UNIT TEST: sample policy line 217 mcts : [0.042 0.625 0.25  0.042 0.042]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.001]
 [0.017]
 [0.024]
 [0.015]] [[ 0.   ]
 [-0.069]
 [ 0.407]
 [-0.505]
 [ 0.   ]] [[0.672]
 [0.553]
 [1.219]
 [0.016]
 [0.672]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
from probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
siam score:  -0.89026815
from probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
using another actor
using another actor
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
first move QE:  0.022896071940961348
from probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.401]
 [-0.401]
 [-0.401]
 [-0.401]
 [-0.401]] [[1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]]
maxi score, test score, baseline:  0.0081 0.15 0.15
1976 6400
from probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0562854831712296
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0141 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8930154
actor:  0 policy actor:  0  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.152]
 [0.152]
 [0.169]
 [0.182]] [[2.799]
 [2.799]
 [2.799]
 [2.52 ]
 [2.498]] [[0.739]
 [0.739]
 [0.739]
 [0.68 ]
 [0.699]]
actor:  0 policy actor:  0  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
rdn probs:  [0.31576225378794226, 0.05271323863617332, 0.05271323863617332, 0.5788112689397111]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
1983 6437
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.011]
 [0.056]
 [0.028]
 [0.028]] [[-0.869]
 [-0.257]
 [ 0.012]
 [-0.869]
 [-0.869]] [[0.321]
 [0.493]
 [0.672]
 [0.321]
 [0.321]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.212]] [[1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.804]] [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.819]]
maxi score, test score, baseline:  0.0201 0.3 0.3
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
from probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
siam score:  -0.89330566
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
siam score:  -0.89750594
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
1990 6484
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
siam score:  -0.8935354
from probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
using another actor
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.001]
 [0.008]
 [0.016]
 [0.016]] [[0.127]
 [1.51 ]
 [0.44 ]
 [1.441]
 [1.441]] [[0.132]
 [1.56 ]
 [0.445]
 [1.512]
 [1.512]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
using another actor
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
from probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
from probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]] [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]] [[0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
siam score:  -0.87885576
maxi score, test score, baseline:  0.0201 0.3 0.3
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
using another actor
in main func line 156:  2000
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0201 0.3 0.3
from probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.001]
 [0.046]
 [0.033]
 [0.032]] [[-0.69 ]
 [ 0.231]
 [-0.241]
 [-0.485]
 [-0.357]] [[0.018]
 [0.275]
 [0.208]
 [0.1  ]
 [0.142]]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]] [[0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
line 256 mcts: sample exp_bonus 0.3644650911597979
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
using explorer policy with actor:  1
2011 6571
maxi score, test score, baseline:  0.0201 0.3 0.3
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
first move QE:  0.010700758000001476
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0201 0.3 0.3
using another actor
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
2015 6590
from probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.003]
 [0.004]
 [0.001]
 [0.001]] [[1.482]
 [0.522]
 [0.945]
 [0.489]
 [0.37 ]] [[1.789]
 [0.618]
 [1.137]
 [0.573]
 [0.429]]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7848162877185813
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.03 ]
 [0.036]
 [0.041]
 [0.04 ]] [[3.372]
 [0.977]
 [1.377]
 [1.044]
 [1.302]] [[2.015]
 [0.278]
 [0.572]
 [0.337]
 [0.522]]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
from probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.057]
 [0.041]
 [0.058]
 [0.057]] [[ 0.   ]
 [ 0.168]
 [ 0.   ]
 [-0.182]
 [-0.009]] [[0.067]
 [0.155]
 [0.067]
 [0.039]
 [0.096]]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
actor:  0 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
UNIT TEST: sample policy line 217 mcts : [0.833 0.042 0.042 0.042 0.042]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
using another actor
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
from probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
using another actor
UNIT TEST: sample policy line 217 mcts : [0.042 0.75  0.125 0.042 0.042]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.   ]
 [0.029]
 [0.027]
 [0.029]] [[-0.562]
 [-0.075]
 [-0.179]
 [-0.432]
 [-0.15 ]] [[0.016]
 [0.619]
 [0.538]
 [0.197]
 [0.576]]
2025 6668
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
in main func line 156:  2027
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
siam score:  -0.88612974
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.033]
 [0.029]
 [0.029]
 [0.029]] [[2.752]
 [0.998]
 [2.752]
 [2.752]
 [2.752]] [[2.013]
 [0.488]
 [2.013]
 [2.013]
 [2.013]]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
using another actor
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.009]
 [0.023]
 [0.022]
 [0.017]] [[-0.604]
 [-0.492]
 [-0.318]
 [-0.619]
 [-0.376]] [[0.026]
 [0.009]
 [0.023]
 [0.022]
 [0.017]]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[-0.156]
 [-0.156]
 [-0.156]
 [-0.156]
 [-0.156]] [[0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.3157611631856017, 0.0527165104431948, 0.0527165104431948, 0.5788058159280086]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.997030299297638
siam score:  -0.89534795
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.061]
 [0.103]
 [0.061]
 [0.061]] [[-0.267]
 [-0.267]
 [ 0.004]
 [-0.267]
 [-0.267]] [[0.12 ]
 [0.12 ]
 [0.566]
 [0.12 ]
 [0.12 ]]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.042]
 [0.035]
 [0.031]
 [0.077]] [[1.065]
 [0.52 ]
 [1.306]
 [1.259]
 [1.014]] [[1.093]
 [0.588]
 [1.359]
 [1.305]
 [1.152]]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
using another actor
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
first move QE:  0.003857125291736387
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
from probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.45824153706393367, 0.04175846293606627, 0.04175846293606627, 0.45824153706393367]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.003]
 [0.074]
 [0.052]
 [0.052]] [[0.172]
 [0.244]
 [0.196]
 [0.172]
 [0.172]] [[0.107]
 [0.058]
 [0.168]
 [0.107]
 [0.107]]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.162]
 [0.202]
 [0.162]
 [0.149]] [[1.142]
 [1.142]
 [1.402]
 [1.142]
 [1.324]] [[0.704]
 [0.704]
 [1.048]
 [0.704]
 [0.878]]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
using another actor
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.022099999999999998 0.3 0.3
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
using another actor
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
using another actor
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.062]
 [0.051]
 [0.062]
 [0.062]] [[0.648]
 [0.648]
 [0.639]
 [0.648]
 [0.648]] [[0.218]
 [0.218]
 [0.193]
 [0.218]
 [0.218]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
siam score:  -0.8758224
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
2074 6794
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
siam score:  -0.87759936
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
2075 6810
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.008]
 [0.032]
 [0.025]
 [0.029]] [[-0.375]
 [-0.728]
 [-0.344]
 [-0.695]
 [-0.924]] [[0.028]
 [0.008]
 [0.032]
 [0.025]
 [0.029]]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.75  0.125]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
using another actor
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.272]
 [-0.272]
 [-0.272]
 [-0.272]
 [-0.272]] [[1.248]
 [1.248]
 [1.248]
 [1.248]
 [1.248]]
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
2081 6858
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
using another actor
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
using another actor
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
siam score:  -0.8831068
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
using another actor
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
siam score:  -0.883033
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
siam score:  -0.8772687
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.02 ]
 [0.017]
 [0.018]
 [0.018]] [[0.625]
 [0.253]
 [0.369]
 [0.625]
 [0.625]] [[0.269]
 [0.148]
 [0.181]
 [0.269]
 [0.269]]
using another actor
2095 6968
siam score:  -0.879855
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]] [[0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.001]
 [0.025]
 [0.039]
 [0.038]] [[1.778]
 [1.791]
 [5.563]
 [1.421]
 [1.986]] [[0.367]
 [0.272]
 [1.58 ]
 [0.224]
 [0.412]]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
line 256 mcts: sample exp_bonus 1.012443906307637
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
start point for exploration sampling:  20031
2100 7006
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
siam score:  -0.89143455
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.018]
 [0.014]
 [0.015]
 [0.015]] [[0.543]
 [0.468]
 [0.672]
 [0.365]
 [1.326]] [[0.55 ]
 [0.469]
 [0.735]
 [0.326]
 [1.608]]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[-0.846]
 [-0.846]
 [-0.846]
 [-0.846]
 [-0.846]] [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]]
maxi score, test score, baseline:  0.0241 0.3 0.3
using another actor
from probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.5321399335103933, 0.04847147606400481, 0.04847147606400481, 0.37091711436159713]
2103 7041
siam score:  -0.88463575
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.125 0.208 0.125 0.5   0.042]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.021]
 [0.021]
 [0.026]] [[1.42 ]
 [1.42 ]
 [1.093]
 [0.932]
 [1.241]] [[0.473]
 [0.473]
 [0.361]
 [0.306]
 [0.421]]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
siam score:  -0.8835498
using explorer policy with actor:  1
first move QE:  -0.002392293405582201
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
from probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
siam score:  -0.8852053
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.004]
 [0.026]
 [0.029]
 [0.023]] [[-1.041]
 [-0.636]
 [-0.978]
 [-1.372]
 [-1.434]] [[0.025]
 [0.004]
 [0.026]
 [0.029]
 [0.023]]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.458 0.042 0.25  0.083 0.167]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
using another actor
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
siam score:  -0.88845736
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
from probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.792 0.083 0.042 0.042]
using another actor
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
using another actor
maxi score, test score, baseline:  0.0241 0.3 0.3
2110 7116
from probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
from probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.013]
 [0.038]
 [0.011]
 [0.015]] [[ 0.18 ]
 [ 0.063]
 [-0.092]
 [-0.067]
 [-0.002]] [[0.1  ]
 [0.081]
 [0.079]
 [0.033]
 [0.062]]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
using another actor
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6344589454338203, 0.05777052728308983, 0.05777052728308983, 0.25]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]] [[0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2113 7159
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
using explorer policy with actor:  1
siam score:  -0.86498564
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
UNIT TEST: sample policy line 217 mcts : [0.167 0.167 0.042 0.292 0.333]
in main func line 156:  2116
from probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
siam score:  -0.867272
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.126]
 [0.162]
 [0.126]
 [0.126]] [[1.613]
 [1.613]
 [1.264]
 [1.613]
 [1.613]] [[0.126]
 [0.126]
 [0.162]
 [0.126]
 [0.126]]
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
2119 7175
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
2119 7182
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.109]
 [0.109]
 [0.109]
 [0.109]] [[0.487]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[0.272]
 [0.252]
 [0.252]
 [0.252]
 [0.252]]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6665028473649202, 0.0606805239250363, 0.0606805239250363, 0.21213610478500722]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6873318503661041, 0.06257206412881253, 0.06257206412881253, 0.18752402137627086]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6873318503661041, 0.06257206412881253, 0.06257206412881253, 0.18752402137627086]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6873318503661041, 0.06257206412881253, 0.06257206412881253, 0.18752402137627086]
using another actor
from probs:  [0.6873318503661041, 0.06257206412881253, 0.06257206412881253, 0.18752402137627086]
from probs:  [0.6873318503661041, 0.06257206412881253, 0.06257206412881253, 0.18752402137627086]
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6873318503661041, 0.06257206412881253, 0.06257206412881253, 0.18752402137627086]
2122 7200
siam score:  -0.86552703
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6873318503661041, 0.06257206412881253, 0.06257206412881253, 0.18752402137627086]
using another actor
maxi score, test score, baseline:  0.0241 0.3 0.3
maxi score, test score, baseline:  0.0241 0.3 0.3
probs:  [0.6873318503661041, 0.06257206412881253, 0.06257206412881253, 0.18752402137627086]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]] [[0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]] [[0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]]
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]] [[1.453]
 [1.453]
 [1.453]
 [1.453]
 [1.453]] [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.026099999999999998 0.3 0.3
probs:  [0.6873318503661041, 0.06257206412881253, 0.06257206412881253, 0.18752402137627086]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.042100000000000005 0.3 0.3
probs:  [0.6873318503661041, 0.06257206412881253, 0.06257206412881253, 0.18752402137627086]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8677196
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.36666675462174675
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.3 0.3
probs:  [0.6873318503661041, 0.06257206412881253, 0.06257206412881253, 0.18752402137627086]
rdn probs:  [0.6873318503661041, 0.06257206412881253, 0.06257206412881253, 0.18752402137627086]
using another actor
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
2123 7232
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
start point for exploration sampling:  20031
from probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.043]
 [0.035]
 [0.037]
 [0.039]] [[0.279]
 [0.497]
 [0.474]
 [0.341]
 [0.396]] [[0.05 ]
 [0.043]
 [0.035]
 [0.037]
 [0.039]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
2127 7257
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]] [[0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]] [[0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]]
UNIT TEST: sample policy line 217 mcts : [0.125 0.417 0.25  0.042 0.167]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.6872114404493035, 0.06262366837886996, 0.06262366837886996, 0.1875412227929567]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7018507807981493, 0.06394379614193857, 0.06394379614193857, 0.17026162691797367]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.044]
 [0.047]
 [0.049]
 [0.029]] [[0.812]
 [0.975]
 [1.033]
 [0.968]
 [0.516]] [[0.252]
 [0.312]
 [0.339]
 [0.321]
 [0.129]]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
siam score:  -0.8494015
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7018507807981493, 0.06394379614193857, 0.06394379614193857, 0.17026162691797367]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7018507807981493, 0.06394379614193857, 0.06394379614193857, 0.17026162691797367]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7018507807981493, 0.06394379614193857, 0.06394379614193857, 0.17026162691797367]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.027]
 [0.027]
 [0.027]
 [0.027]] [[-0.277]
 [-0.277]
 [-0.277]
 [-0.277]
 [-0.277]] [[0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7018507807981493, 0.06394379614193857, 0.06394379614193857, 0.17026162691797367]
siam score:  -0.8630194
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7018507807981493, 0.06394379614193857, 0.06394379614193857, 0.17026162691797367]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7126956277113963, 0.06492174891544145, 0.06492174891544145, 0.15746087445772072]
from probs:  [0.7126956277113963, 0.06492174891544145, 0.06492174891544145, 0.15746087445772072]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.721052010695748, 0.06567530016253344, 0.06567530016253344, 0.14759738897918528]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.721052010695748, 0.06567530016253344, 0.06567530016253344, 0.14759738897918528]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.228]
 [0.203]
 [0.247]
 [0.247]] [[-0.151]
 [ 0.265]
 [ 0.027]
 [-0.151]
 [-0.151]] [[0.349]
 [0.59 ]
 [0.382]
 [0.349]
 [0.349]]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.721052010695748, 0.06567530016253344, 0.06567530016253344, 0.14759738897918528]
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]] [[0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]] [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]]
line 256 mcts: sample exp_bonus 1.4434139951885754
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7276882826738784, 0.06627373743312363, 0.06627373743312363, 0.13976424245987418]
Printing some Q and Qe and total Qs values:  [[0.567]
 [1.112]
 [0.643]
 [0.643]
 [0.629]] [[2.382]
 [2.504]
 [2.118]
 [2.118]
 [1.719]] [[1.196]
 [2.327]
 [1.259]
 [1.259]
 [1.098]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7375622860820469, 0.06716414271923236, 0.06716414271923236, 0.12810942847948825]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7375622860820469, 0.06716414271923236, 0.06716414271923236, 0.12810942847948825]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
siam score:  -0.8921909
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7375622860820469, 0.06716414271923236, 0.06716414271923236, 0.12810942847948825]
using another actor
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7375622860820469, 0.06716414271923236, 0.06716414271923236, 0.12810942847948825]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7375622860820469, 0.06716414271923236, 0.06716414271923236, 0.12810942847948825]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7375622860820469, 0.06716414271923236, 0.06716414271923236, 0.12810942847948825]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7375622860820469, 0.06716414271923236, 0.06716414271923236, 0.12810942847948825]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7413345432329477, 0.06750431251347662, 0.06750431251347662, 0.12365683174009923]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7413345432329477, 0.06750431251347662, 0.06750431251347662, 0.12365683174009923]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7413345432329477, 0.06750431251347662, 0.06750431251347662, 0.12365683174009923]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
from probs:  [0.7413345432329477, 0.06750431251347662, 0.06750431251347662, 0.12365683174009923]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
using another actor
from probs:  [0.7413345432329477, 0.06750431251347662, 0.06750431251347662, 0.12365683174009923]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7413345432329477, 0.06750431251347662, 0.06750431251347662, 0.12365683174009923]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7145231896257297
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.744556752209367, 0.06779488076496996, 0.06779488076496996, 0.11985348626069281]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.744556752209367, 0.06779488076496996, 0.06779488076496996, 0.11985348626069281]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.744556752209367, 0.06779488076496996, 0.06779488076496996, 0.11985348626069281]
siam score:  -0.90800476
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.744556752209367, 0.06779488076496996, 0.06779488076496996, 0.11985348626069281]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.744556752209367, 0.06779488076496996, 0.06779488076496996, 0.11985348626069281]
first move QE:  -0.01495333138835949
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.744556752209367, 0.06779488076496996, 0.06779488076496996, 0.11985348626069281]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.744556752209367, 0.06779488076496996, 0.06779488076496996, 0.11985348626069281]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.744556752209367, 0.06779488076496996, 0.06779488076496996, 0.11985348626069281]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7473410449499264, 0.06804595916466114, 0.06804595916466114, 0.11656703672075153]
using another actor
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7473410449499264, 0.06804595916466114, 0.06804595916466114, 0.11656703672075153]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7473410449499264, 0.06804595916466114, 0.06804595916466114, 0.11656703672075153]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.3135885016411529
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7497710140081271, 0.06826508581522647, 0.06826508581522647, 0.11369881436141986]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7497710140081271, 0.06826508581522647, 0.06826508581522647, 0.11369881436141986]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7497710140081271, 0.06826508581522647, 0.06826508581522647, 0.11369881436141986]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7497710140081271, 0.06826508581522647, 0.06826508581522647, 0.11369881436141986]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.056]
 [0.061]
 [0.068]
 [0.069]] [[0.149]
 [0.677]
 [0.568]
 [0.619]
 [0.437]] [[0.087]
 [0.056]
 [0.061]
 [0.068]
 [0.069]]
using another actor
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7519102490500014, 0.06845799502446759, 0.06845799502446759, 0.11117376090106346]
first move QE:  -0.01592515432885739
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7519102490500014, 0.06845799502446759, 0.06845799502446759, 0.11117376090106346]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
actor:  0 policy actor:  0  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.   ]
 [0.104]
 [0.068]
 [0.068]] [[-0.294]
 [-0.022]
 [-0.112]
 [-0.294]
 [-0.294]] [[0.939]
 [1.168]
 [1.255]
 [0.939]
 [0.939]]
2162 7321
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7519102490500014, 0.06845799502446759, 0.06845799502446759, 0.11117376090106346]
siam score:  -0.91292953
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7519102490500014, 0.06845799502446759, 0.06845799502446759, 0.11117376090106346]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7519102490500014, 0.06845799502446759, 0.06845799502446759, 0.11117376090106346]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7519102490500014, 0.06845799502446759, 0.06845799502446759, 0.11117376090106346]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7519102490500014, 0.06845799502446759, 0.06845799502446759, 0.11117376090106346]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7519102490500014, 0.06845799502446759, 0.06845799502446759, 0.11117376090106346]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7555029239052142, 0.06878197067548926, 0.06878197067548926, 0.10693313474380732]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7570259307547442, 0.06891931044473418, 0.06891931044473418, 0.10513544835578734]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7570259307547442, 0.06891931044473418, 0.06891931044473418, 0.10513544835578734]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7570259307547442, 0.06891931044473418, 0.06891931044473418, 0.10513544835578734]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7570259307547442, 0.06891931044473418, 0.06891931044473418, 0.10513544835578734]
Printing some Q and Qe and total Qs values:  [[1.077]
 [0.022]
 [0.03 ]
 [1.077]
 [1.077]] [[ 1.31 ]
 [ 0.024]
 [-0.002]
 [ 1.31 ]
 [ 1.31 ]] [[2.557]
 [0.016]
 [0.023]
 [2.557]
 [2.557]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2168 7345
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7584019015265154, 0.0690433909820877, 0.0690433909820877, 0.10351131650930909]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
from probs:  [0.7584019015265154, 0.0690433909820877, 0.0690433909820877, 0.10351131650930909]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7584019015265154, 0.0690433909820877, 0.0690433909820877, 0.10351131650930909]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7584019015265154, 0.0690433909820877, 0.0690433909820877, 0.10351131650930909]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7584019015265154, 0.0690433909820877, 0.0690433909820877, 0.10351131650930909]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7584019015265154, 0.0690433909820877, 0.0690433909820877, 0.10351131650930909]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7584019015265154, 0.0690433909820877, 0.0690433909820877, 0.10351131650930909]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]] [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]]
siam score:  -0.9212981
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7596511487208086, 0.06915604400229376, 0.06915604400229376, 0.10203676327460401]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7596511487208086, 0.06915604400229376, 0.06915604400229376, 0.10203676327460401]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7596511487208086, 0.06915604400229376, 0.06915604400229376, 0.10203676327460401]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7596511487208086, 0.06915604400229376, 0.06915604400229376, 0.10203676327460401]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.760790408079101, 0.06925877867970265, 0.06925877867970265, 0.1006920345614935]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.760790408079101, 0.06925877867970265, 0.06925877867970265, 0.1006920345614935]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7618335926623535, 0.06935284964858107, 0.06935284964858107, 0.09946070804048422]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7618335926623535, 0.06935284964858107, 0.06935284964858107, 0.09946070804048422]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7618335926623535, 0.06935284964858107, 0.06935284964858107, 0.09946070804048422]
siam score:  -0.92644113
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7618335926623535, 0.06935284964858107, 0.06935284964858107, 0.09946070804048422]
siam score:  -0.92669684
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7618335926623535, 0.06935284964858107, 0.06935284964858107, 0.09946070804048422]
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.146]
 [0.175]
 [0.175]
 [0.175]] [[1.415]
 [1.66 ]
 [1.415]
 [1.415]
 [1.415]] [[0.504]
 [0.527]
 [0.504]
 [0.504]
 [0.504]]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7618335926623535, 0.06935284964858107, 0.06935284964858107, 0.09946070804048422]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7627923638716835, 0.06943930849588612, 0.06943930849588612, 0.09832901913654436]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7636765692402695, 0.06951904323990529, 0.06951904323990529, 0.09728534427991987]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7644945819043302, 0.06959280894263747, 0.06959280894263747, 0.09631980021039488]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9324975089023502
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7659596899964988, 0.0697249275915847, 0.0697249275915847, 0.09459045482033163]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7666182887679974, 0.06978431787162877, 0.06978431787162877, 0.09381307548874492]
siam score:  -0.9220494
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7666182887679974, 0.06978431787162877, 0.06978431787162877, 0.09381307548874492]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.241]
 [0.241]
 [0.241]
 [0.241]] [[0.807]
 [0.973]
 [0.973]
 [0.973]
 [0.973]] [[0.79 ]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7666182887679974, 0.06978431787162877, 0.06978431787162877, 0.09381307548874492]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7672340045674669, 0.06983984110571381, 0.06983984110571381, 0.09308631322110557]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7509000789110278, 0.09037250232505707, 0.06835491643885805, 0.09037250232505707]
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7509000789110278, 0.09037250232505707, 0.06835491643885805, 0.09037250232505707]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7519359777456539, 0.08980766667691899, 0.06844868890050818, 0.08980766667691899]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7519359777456539, 0.08980766667691899, 0.06844868890050818, 0.08980766667691899]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7519359777456539, 0.08980766667691899, 0.06844868890050818, 0.08980766667691899]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7362063773547027, 0.1088433098002476, 0.06701910529661727, 0.08793120754843244]
using another actor
line 256 mcts: sample exp_bonus -0.33302593592077545
from probs:  [0.7362063773547027, 0.1088433098002476, 0.06701910529661727, 0.08793120754843244]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7362063773547026, 0.1088433098002476, 0.06701910529661727, 0.08793120754843242]
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.7362063773547026, 0.1088433098002476, 0.06701910529661727, 0.08793120754843242]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.7376098341151344, 0.10778046504975246, 0.06714631220682457, 0.08746338862828854]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2202 7378
from probs:  [0.7389356335499533, 0.10677643057627632, 0.06726648039042152, 0.08702145548334893]
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.7389356335499533, 0.10677643057627632, 0.06726648039042152, 0.08702145548334893]
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.7389356335499533, 0.10677643057627632, 0.06726648039042152, 0.08702145548334893]
2202 7389
maxi score, test score, baseline:  0.0541 0.8 0.8
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.7389356335499533, 0.10677643057627632, 0.06726648039042152, 0.08702145548334893]
maxi score, test score, baseline:  0.0541 0.8 0.8
maxi score, test score, baseline:  0.0541 0.8 0.8
2203 7390
maxi score, test score, baseline:  0.0541 0.8 0.8
from probs:  [0.7389356335499533, 0.10677643057627632, 0.06726648039042152, 0.08702145548334893]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.740190047642628, 0.10582645657569761, 0.06738017832921701, 0.08660331745245732]
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.740190047642628, 0.10582645657569761, 0.06738017832921701, 0.08660331745245732]
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.740190047642628, 0.10582645657569761, 0.06738017832921701, 0.08660331745245732]
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.740190047642628, 0.10582645657569761, 0.06738017832921701, 0.08660331745245732]
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.740190047642628, 0.10582645657569761, 0.06738017832921701, 0.08660331745245732]
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.740190047642628, 0.10582645657569761, 0.06738017832921701, 0.08660331745245732]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.7413786906751487, 0.10492629132447989, 0.06748791489208762, 0.08620710310828376]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.7413786906751487, 0.10492629132447989, 0.06748791489208762, 0.08620710310828376]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.7425066032366805, 0.10407211755950212, 0.06759014694937762, 0.08583113225443986]
using another actor
from probs:  [0.7425066032366805, 0.10407211755950212, 0.06759014694937762, 0.08583113225443986]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.7445979491365882, 0.10248833095926319, 0.06777970294967804, 0.08513401695447062]
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.7445979491365882, 0.10248833095926319, 0.06777970294967804, 0.08513401695447062]
maxi score, test score, baseline:  0.0541 0.8 0.8
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.762]
 [0.87 ]
 [0.076]
 [0.762]] [[1.385]
 [1.385]
 [2.049]
 [2.177]
 [1.385]] [[1.16 ]
 [1.16 ]
 [1.597]
 [0.052]
 [1.16 ]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9203017
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0541 0.8 0.8
Printing some Q and Qe and total Qs values:  [[0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]] [[2.161]
 [2.161]
 [2.161]
 [2.161]
 [2.161]] [[2.401]
 [2.401]
 [2.401]
 [2.401]
 [2.401]]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]] [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]] [[0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0541 0.8 0.8
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.7354795455529437, 0.11470242173114682, 0.06695033528331631, 0.08286769743259316]
maxi score, test score, baseline:  0.0541 0.8 0.8
probs:  [0.7354795455529437, 0.11470242173114682, 0.06695033528331631, 0.08286769743259316]
actor:  0 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.004]
 [0.001]
 [0.001]
 [0.007]] [[0.233]
 [0.14 ]
 [0.263]
 [0.637]
 [0.077]] [[0.265]
 [0.169]
 [0.285]
 [0.66 ]
 [0.112]]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7354795455529437, 0.11470242173114682, 0.06695033528331631, 0.08286769743259316]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.736570953099854, 0.11376013313204088, 0.06704932163445489, 0.08261959213365021]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.736570953099854, 0.11376013313204088, 0.06704932163445489, 0.08261959213365021]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.736570953099854, 0.11376013313204088, 0.06704932163445489, 0.08261959213365021]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.736570953099854, 0.11376013313204088, 0.06704932163445489, 0.08261959213365021]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.736570953099854, 0.11376013313204088, 0.06704932163445489, 0.08261959213365021]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7265412620193463, 0.1261743177430045, 0.06613762331537036, 0.0811467969222789]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7265412620193463, 0.1261743177430045, 0.06613762331537036, 0.0811467969222789]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7265412620193463, 0.1261743177430045, 0.06613762331537036, 0.0811467969222789]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7265412620193463, 0.1261743177430045, 0.06613762331537036, 0.0811467969222789]
siam score:  -0.92287534
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7265412620193463, 0.1261743177430045, 0.06613762331537036, 0.0811467969222789]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7265412620193463, 0.1261743177430045, 0.06613762331537036, 0.0811467969222789]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7265412620193463, 0.1261743177430045, 0.06613762331537036, 0.0811467969222789]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7265412620193463, 0.1261743177430045, 0.06613762331537036, 0.0811467969222789]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7265412620193462, 0.12617431774300453, 0.06613762331537035, 0.08114679692227889]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7289211325642173, 0.12396812300941655, 0.0663535506708641, 0.08075719375550221]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7289211325642173, 0.12396812300941655, 0.0663535506708641, 0.08075719375550221]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7289211325642173, 0.12396812300941655, 0.0663535506708641, 0.08075719375550221]
using another actor
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7289211325642173, 0.12396812300941655, 0.0663535506708641, 0.08075719375550221]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]] [[0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]]
maxi score, test score, baseline:  0.056100000000000004 0.8 0.8
probs:  [0.7289211325642173, 0.12396812300941655, 0.0663535506708641, 0.08075719375550221]
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.073]
 [0.064]
 [0.01 ]
 [0.062]] [[-0.133]
 [ 0.072]
 [-0.108]
 [-0.128]
 [-0.106]] [[0.004]
 [0.073]
 [0.064]
 [0.01 ]
 [0.062]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7289211325642173, 0.12396812300941655, 0.0663535506708641, 0.08075719375550221]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7289211325642173, 0.12396812300941655, 0.0663535506708641, 0.08075719375550221]
siam score:  -0.9202014
line 256 mcts: sample exp_bonus 0.0424613161625797
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7300404815314309, 0.12293046077109186, 0.06645511000268824, 0.08057394769478915]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7311164226979514, 0.12193303856241582, 0.06655273091373078, 0.08039780782590204]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.8 0.8
probs:  [0.7321514330355493, 0.12097356017358536, 0.0666466381414108, 0.08022836864945443]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7321514330355493, 0.12097356017358536, 0.0666466381414108, 0.08022836864945443]
from probs:  [0.7321514330355493, 0.12097356017358536, 0.0666466381414108, 0.08022836864945443]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.792]
 [0.623]
 [0.623]
 [0.623]] [[0.555]
 [1.054]
 [0.555]
 [0.555]
 [0.555]] [[1.594]
 [2.1  ]
 [1.594]
 [1.594]
 [1.594]]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.922]
 [0.506]
 [0.858]
 [0.649]] [[0.501]
 [1.124]
 [0.653]
 [0.58 ]
 [0.731]] [[0.698]
 [1.568]
 [0.578]
 [1.259]
 [0.89 ]]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7321514330355493, 0.12097356017358536, 0.0666466381414108, 0.08022836864945443]
siam score:  -0.9252449
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7321514330355493, 0.12097356017358536, 0.0666466381414108, 0.08022836864945443]
maxi score, test score, baseline:  0.0601 0.8 0.8
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7538274765740377, 0.10892830655926944, 0.06862210843334644, 0.06862210843334644]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7538274765740377, 0.10892830655926944, 0.06862210843334644, 0.06862210843334644]
maxi score, test score, baseline:  0.0601 0.8 0.8
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7538274765740377, 0.10892830655926944, 0.06862210843334644, 0.06862210843334644]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7538274765740377, 0.10892830655926944, 0.06862210843334644, 0.06862210843334644]
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.364]
 [0.269]
 [0.214]
 [0.352]] [[0.   ]
 [0.473]
 [0.   ]
 [0.21 ]
 [0.464]] [[0.329]
 [0.678]
 [0.329]
 [0.289]
 [0.65 ]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7544129872431647, 0.1082368728663001, 0.06867506994526758, 0.06867506994526758]
siam score:  -0.9143747
Printing some Q and Qe and total Qs values:  [[1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]] [[1.939]
 [1.939]
 [1.939]
 [1.939]
 [1.939]] [[1.989]
 [1.989]
 [1.989]
 [1.989]
 [1.989]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.8 0.8
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7549772630750268, 0.10757051554294121, 0.06872611069101607, 0.06872611069101607]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7549772630750268, 0.10757051554294121, 0.06872611069101607, 0.06872611069101607]
siam score:  -0.9123249
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7549772630750268, 0.10757051554294121, 0.06872611069101607, 0.06872611069101607]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7555214386873077, 0.10692789471113931, 0.06877533330077648, 0.06877533330077648]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7555214386873077, 0.10692789471113931, 0.06877533330077648, 0.06877533330077648]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7555214386873077, 0.10692789471113931, 0.06877533330077648, 0.06877533330077648]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7555214386873077, 0.10692789471113931, 0.06877533330077648, 0.06877533330077648]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7555214386873077, 0.10692789471113931, 0.06877533330077648, 0.06877533330077648]
maxi score, test score, baseline:  0.0601 0.8 0.8
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.02 ]
 [0.252]
 [0.167]
 [0.156]] [[-0.266]
 [-0.097]
 [ 0.255]
 [-0.321]
 [-0.351]] [[0.177]
 [0.04 ]
 [0.622]
 [0.26 ]
 [0.228]]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7555214386873077, 0.10692789471113931, 0.06877533330077648, 0.06877533330077648]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7560465692791497, 0.10630776427875992, 0.06882283322104515, 0.06882283322104515]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7560465692791497, 0.10630776427875992, 0.06882283322104515, 0.06882283322104515]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7560465692791497, 0.10630776427875992, 0.06882283322104515, 0.06882283322104515]
siam score:  -0.9164638
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7560465692791497, 0.10630776427875992, 0.06882283322104515, 0.06882283322104515]
siam score:  -0.9163266
first move QE:  -0.024561081687549415
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7560465692791497, 0.10630776427875992, 0.06882283322104515, 0.06882283322104515]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7560465692791497, 0.10630776427875992, 0.06882283322104515, 0.06882283322104515]
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7560465692791497, 0.10630776427875992, 0.06882283322104515, 0.06882283322104515]
from probs:  [0.7560465692791497, 0.10630776427875992, 0.06882283322104515, 0.06882283322104515]
siam score:  -0.91439027
maxi score, test score, baseline:  0.0601 0.8 0.8
probs:  [0.7560465692791497, 0.10630776427875992, 0.06882283322104515, 0.06882283322104515]
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0621 0.8 0.8
probs:  [0.7560465692791497, 0.10630776427875992, 0.06882283322104515, 0.06882283322104515]
maxi score, test score, baseline:  0.0621 0.8 0.8
probs:  [0.7560465692791497, 0.10630776427875992, 0.06882283322104515, 0.06882283322104515]
maxi score, test score, baseline:  0.0621 0.8 0.8
probs:  [0.7560465692791497, 0.10630776427875992, 0.06882283322104515, 0.06882283322104515]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.861]
 [0.157]
 [0.431]
 [0.486]] [[0.883]
 [0.96 ]
 [0.493]
 [0.913]
 [0.936]] [[0.858]
 [1.669]
 [0.107]
 [0.794]
 [0.913]]
Printing some Q and Qe and total Qs values:  [[1.002]
 [1.123]
 [1.058]
 [1.058]
 [1.058]] [[2.266]
 [2.357]
 [2.318]
 [2.318]
 [2.318]] [[2.071]
 [2.344]
 [2.201]
 [2.201]
 [2.201]]
Printing some Q and Qe and total Qs values:  [[0.755]
 [1.05 ]
 [0.755]
 [0.755]
 [0.755]] [[1.879]
 [2.128]
 [1.879]
 [1.879]
 [1.879]] [[1.448]
 [2.121]
 [1.448]
 [1.448]
 [1.448]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0641 0.8 0.8
probs:  [0.7480103443010917, 0.1158055958470112, 0.06809202992594855, 0.06809202992594855]
maxi score, test score, baseline:  0.0641 0.8 0.8
probs:  [0.7480103443010917, 0.1158055958470112, 0.06809202992594855, 0.06809202992594855]
first move QE:  -0.024859255343738104
maxi score, test score, baseline:  0.0641 0.8 0.8
siam score:  -0.9192252
maxi score, test score, baseline:  0.0641 0.8 0.8
probs:  [0.7480103443010917, 0.1158055958470112, 0.06809202992594855, 0.06809202992594855]
maxi score, test score, baseline:  0.0641 0.8 0.8
probs:  [0.7480103443010917, 0.1158055958470112, 0.06809202992594855, 0.06809202992594855]
maxi score, test score, baseline:  0.0641 0.8 0.8
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.101]
 [0.108]
 [0.113]
 [0.123]] [[1.49 ]
 [1.152]
 [1.508]
 [1.283]
 [0.983]] [[0.406]
 [0.298]
 [0.431]
 [0.366]
 [0.285]]
maxi score, test score, baseline:  0.0641 0.8 0.8
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0641 0.8 0.8
maxi score, test score, baseline:  0.0641 0.8 0.8
maxi score, test score, baseline:  0.0641 0.8 0.8
probs:  [0.7480103443010917, 0.1158055958470112, 0.06809202992594855, 0.06809202992594855]
actor:  0 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0661 0.8 0.8
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0661 0.8 0.8
maxi score, test score, baseline:  0.0661 0.8 0.8
maxi score, test score, baseline:  0.0661 0.8 0.8
maxi score, test score, baseline:  0.0661 0.8 0.8
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7480103443010917, 0.11580559584701122, 0.06809202992594855, 0.06809202992594855]
maxi score, test score, baseline:  0.0661 0.8 0.8
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7480103443010917, 0.11580559584701122, 0.06809202992594855, 0.06809202992594855]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7480103443010917, 0.1158055958470112, 0.06809202992594855, 0.06809202992594855]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.7480103443010917, 0.1158055958470112, 0.06809202992594855, 0.06809202992594855]
maxi score, test score, baseline:  0.0661 0.8 0.8
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7492234046089691, 0.11437283227386393, 0.06820188155858356, 0.06820188155858356]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7498009883337362, 0.11369063954534467, 0.06825418606045956, 0.06825418606045956]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7509027257915365, 0.11238936104628121, 0.06835395658109121, 0.06835395658109121]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.997]
 [0.853]
 [0.853]
 [0.871]] [[1.392]
 [1.324]
 [1.392]
 [1.392]
 [1.271]] [[1.593]
 [1.857]
 [1.593]
 [1.593]
 [1.588]]
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7509027257915365, 0.11238936104628121, 0.06835395658109121, 0.06835395658109121]
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7509027257915365, 0.11238936104628121, 0.06835395658109121, 0.06835395658109121]
siam score:  -0.91347253
siam score:  -0.91423416
maxi score, test score, baseline:  0.0661 0.8 0.8
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7509027257915365, 0.11238936104628121, 0.06835395658109121, 0.06835395658109121]
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7509027257915365, 0.11238936104628121, 0.06835395658109121, 0.06835395658109121]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.3000],
        [0.4254],
        [0.1862],
        [0.4782],
        [0.0000],
        [0.1589],
        [0.0000],
        [0.1986],
        [0.6930],
        [0.1115]], dtype=torch.float64)
0.0 0.30002157394751144
0.0 0.4254323074741988
0.0 0.18623253846658416
0.0 0.4781821675070994
0.0 0.0
0.0 0.1589337729953367
0.96059601 0.96059601
0.0 0.19864367396639987
0.0 0.6929700069514654
0.0 0.11145519419760162
line 256 mcts: sample exp_bonus 2.2712973105851857
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7509027257915365, 0.11238936104628121, 0.06835395658109121, 0.06835395658109121]
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7509027257915365, 0.11238936104628121, 0.06835395658109121, 0.06835395658109121]
first move QE:  -0.027400221089026122
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0661 0.8 0.8
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7514285020417085, 0.11176835889661009, 0.06840156953084073, 0.06840156953084073]
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7514285020417085, 0.11176835889661009, 0.06840156953084073, 0.06840156953084073]
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7514285020417085, 0.11176835889661009, 0.06840156953084073, 0.06840156953084073]
Printing some Q and Qe and total Qs values:  [[0.999]
 [0.999]
 [0.999]
 [0.999]
 [0.999]] [[1.411]
 [1.411]
 [1.411]
 [1.411]
 [1.411]] [[0.999]
 [0.999]
 [0.999]
 [0.999]
 [0.999]]
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7514285020417085, 0.11176835889661009, 0.06840156953084073, 0.06840156953084073]
maxi score, test score, baseline:  0.0661 0.8 0.8
probs:  [0.7514285020417085, 0.11176835889661009, 0.06840156953084073, 0.06840156953084073]
actor:  0 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06810000000000001 0.8 0.8
probs:  [0.7433665790905236, 0.12129567501986335, 0.06766887294480643, 0.06766887294480643]
maxi score, test score, baseline:  0.06810000000000001 0.8 0.8
probs:  [0.7433665790905236, 0.12129567501986335, 0.06766887294480643, 0.06766887294480643]
maxi score, test score, baseline:  0.06810000000000001 0.8 0.8
probs:  [0.7433665790905236, 0.12129567501986335, 0.06766887294480643, 0.06766887294480643]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06810000000000001 0.8 0.8
actor:  0 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7445963011277212, 0.11984307865059968, 0.06778031011083956, 0.06778031011083956]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7445963011277212, 0.11984307865059968, 0.06778031011083956, 0.06778031011083956]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7445963011277212, 0.11984307865059968, 0.06778031011083956, 0.06778031011083956]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7445963011277212, 0.11984307865059968, 0.06778031011083956, 0.06778031011083956]
from probs:  [0.7445963011277212, 0.11984307865059968, 0.06778031011083956, 0.06778031011083956]
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.001]
 [0.066]
 [0.069]
 [0.048]] [[-1.017]
 [-0.435]
 [-0.379]
 [-0.767]
 [-0.729]] [[0.053]
 [0.001]
 [0.066]
 [0.069]
 [0.048]]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7445963011277212, 0.11984307865059968, 0.06778031011083956, 0.06778031011083956]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.256]
 [0.14 ]
 [0.145]
 [0.204]] [[0.487]
 [0.634]
 [0.517]
 [0.27 ]
 [0.573]] [[0.362]
 [0.564]
 [0.293]
 [0.221]
 [0.44 ]]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7445963011277212, 0.11984307865059968, 0.06778031011083956, 0.06778031011083956]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.028904602979749363
siam score:  -0.91635835
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.746312027530902, 0.11781639467769947, 0.06793578889569925, 0.06793578889569925]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.747378113732566, 0.11655709143760425, 0.06803239741491486, 0.06803239741491486]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7468524153520902, 0.11717806718310458, 0.06798475873240258, 0.06798475873240258]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7395736880009728, 0.12577981050721587, 0.06732325074590571, 0.06732325074590571]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7395736880009728, 0.12577981050721587, 0.06732325074590571, 0.06732325074590571]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7395736880009728, 0.12577981050721587, 0.06732325074590571, 0.06732325074590571]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7401923091710175, 0.12504901923091707, 0.06737933579903269, 0.06737933579903269]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7401923091710175, 0.12504901923091707, 0.06737933579903269, 0.06737933579903269]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7413808982555136, 0.12364491187715362, 0.06748709493366638, 0.06748709493366638]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7413808982555136, 0.12364491187715362, 0.06748709493366638, 0.06748709493366638]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.845]
 [0.945]
 [0.845]
 [0.845]
 [0.845]] [[3.174]
 [2.726]
 [3.174]
 [3.174]
 [3.174]] [[2.506]
 [2.282]
 [2.506]
 [2.506]
 [2.506]]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7425087593594906, 0.1223125438697617, 0.06758934838537388, 0.06758934838537388]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7430513529869457, 0.12167156566093197, 0.06763854067606122, 0.06763854067606122]
siam score:  -0.9041536
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7430513529869457, 0.12167156566093197, 0.06763854067606122, 0.06763854067606122]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7435804306653785, 0.12104655415048672, 0.06768650759206743, 0.06768650759206743]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7435804306653785, 0.12104655415048672, 0.06768650759206743, 0.06768650759206743]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7435804306653785, 0.12104655415048672, 0.06768650759206743, 0.06768650759206743]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7435804306653785, 0.12104655415048672, 0.06768650759206743, 0.06768650759206743]
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.939]
 [0.82 ]
 [0.82 ]
 [0.82 ]] [[1.098]
 [2.241]
 [1.098]
 [1.098]
 [1.098]] [[1.554]
 [2.162]
 [1.554]
 [1.554]
 [1.554]]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7435804306653785, 0.12104655415048672, 0.06768650759206743, 0.06768650759206743]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7440964912034134, 0.12043692008443821, 0.06773329435607411, 0.06773329435607411]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7446000091631159, 0.11984210285181157, 0.06777894399253621, 0.06777894399253621]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7450914363155526, 0.11926156876515707, 0.06782349745964512, 0.06782349745964512]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7450914363155526, 0.11926156876515707, 0.06782349745964512, 0.06782349745964512]
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.794]
 [0.858]
 [0.816]
 [0.816]] [[2.982]
 [2.94 ]
 [2.342]
 [2.982]
 [2.982]] [[1.944]
 [1.894]
 [1.571]
 [1.944]
 [1.944]]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7450914363155526, 0.11926156876515707, 0.06782349745964512, 0.06782349745964512]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7450914363155526, 0.11926156876515707, 0.06782349745964512, 0.06782349745964512]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7325275306187949, 0.1256806798842406, 0.06668168050727001, 0.07511010898969438]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7331458727347122, 0.12504848118929854, 0.0667377724109712, 0.07506787366501795]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7331458727347122, 0.12504848118929854, 0.0667377724109712, 0.07506787366501795]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7331458727347122, 0.12504848118929854, 0.0667377724109712, 0.07506787366501795]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7331458727347122, 0.12504848118929854, 0.0667377724109712, 0.07506787366501795]
from probs:  [0.7331458727347122, 0.12504848118929854, 0.0667377724109712, 0.07506787366501795]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7337499538893661, 0.12443086303297306, 0.06679257065466561, 0.07502661242299524]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7337499538893661, 0.12443086303297306, 0.06679257065466561, 0.07502661242299524]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7337499538893661, 0.12443086303297306, 0.06679257065466561, 0.07502661242299524]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7337499538893661, 0.12443086303297306, 0.06679257065466561, 0.07502661242299524]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7337499538893661, 0.12443086303297306, 0.06679257065466561, 0.07502661242299524]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7337499538893661, 0.12443086303297306, 0.06679257065466561, 0.07502661242299524]
first move QE:  -0.03137644313673854
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7455631427072759, 0.11869694509465342, 0.06786995609903537, 0.06786995609903537]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7455631427072759, 0.11869694509465342, 0.06786995609903537, 0.06786995609903537]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7460317460317459, 0.1181434599156118, 0.06791239702632107, 0.06791239702632107]
siam score:  -0.89673096
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7460317460317459, 0.1181434599156118, 0.06791239702632107, 0.06791239702632107]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
probs:  [0.7464894883245531, 0.1176028031134525, 0.06795385428099716, 0.06795385428099716]
maxi score, test score, baseline:  0.07010000000000001 0.8 0.8
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7464894883245531, 0.1176028031134525, 0.06795385428099716, 0.06795385428099716]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7464894883245531, 0.1176028031134525, 0.06795385428099716, 0.06795385428099716]
from probs:  [0.7464894883245531, 0.1176028031134525, 0.06795385428099716, 0.06795385428099716]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0721 0.8 0.8
maxi score, test score, baseline:  0.0721 0.8 0.8
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7469367428565447, 0.11707453380380498, 0.06799436166982525, 0.06799436166982525]
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7469367428565447, 0.11707453380380498, 0.06799436166982525, 0.06799436166982525]
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7469367428565447, 0.11707453380380498, 0.06799436166982525, 0.06799436166982525]
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7469367428565447, 0.11707453380380498, 0.06799436166982525, 0.06799436166982525]
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7469367428565447, 0.11707453380380498, 0.06799436166982525, 0.06799436166982525]
maxi score, test score, baseline:  0.0721 0.8 0.8
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7469367428565447, 0.11707453380380498, 0.06799436166982525, 0.06799436166982525]
maxi score, test score, baseline:  0.0721 0.8 0.8
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7473738659875854, 0.11655823107650141, 0.0680339514679565, 0.0680339514679565]
maxi score, test score, baseline:  0.0721 0.8 0.8
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7473738659875854, 0.11655823107650141, 0.0680339514679565, 0.0680339514679565]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7478011981135214, 0.11605349287708462, 0.06807265450469702, 0.06807265450469702]
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.902]
 [0.957]
 [0.83 ]
 [0.856]] [[2.046]
 [2.474]
 [1.861]
 [2.046]
 [1.973]] [[1.756]
 [2.213]
 [1.811]
 [1.756]
 [1.74 ]]
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7478011981135214, 0.11605349287708462, 0.06807265450469702, 0.06807265450469702]
maxi score, test score, baseline:  0.0721 0.8 0.8
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0721 0.8 0.8
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7478011981135214, 0.11605349287708462, 0.06807265450469702, 0.06807265450469702]
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7478011981135214, 0.11605349287708462, 0.06807265450469702, 0.06807265450469702]
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7478011981135214, 0.11605349287708462, 0.06807265450469702, 0.06807265450469702]
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7478011981135214, 0.11605349287708462, 0.06807265450469702, 0.06807265450469702]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Starting evaluation
maxi score, test score, baseline:  0.0721 0.8 0.8
probs:  [0.7478011981135214, 0.11605349287708462, 0.06807265450469702, 0.06807265450469702]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[1.136]
 [1.136]
 [1.136]
 [1.136]
 [1.136]] [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.7478011981135214, 0.11605349287708462, 0.06807265450469702, 0.06807265450469702]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.03191918240877547
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90881485
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2352 7566
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9220222
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.111]] [[2.219]
 [2.219]
 [2.219]
 [2.219]
 [2.215]] [[1.675]
 [1.675]
 [1.675]
 [1.675]
 [1.656]]
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.123]
 [0.138]
 [0.145]
 [0.14 ]] [[0.92 ]
 [1.549]
 [0.884]
 [0.757]
 [0.839]] [[0.605]
 [1.05 ]
 [0.509]
 [0.412]
 [0.473]]
start point for exploration sampling:  20031
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  2357
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2357 7571
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91781217
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.2923],
        [0.3190],
        [0.7950],
        [0.0000],
        [0.2310],
        [0.5006],
        [0.0000],
        [0.0000],
        [0.5438],
        [0.4958]], dtype=torch.float64)
0.0 0.29233993032977057
0.0 0.3190247748169999
0.0 0.7949757013746471
0.99 0.99
0.0 0.2310063259819988
0.0 0.5005671459988855
0.0 0.0
0.0 0.0
0.0 0.5437732024116833
0.0 0.49580765838078006
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.1  ]
 [1.1  ]
 [1.159]
 [1.1  ]
 [1.1  ]] [[2.389]
 [2.389]
 [2.048]
 [2.389]
 [2.389]] [[2.538]
 [2.538]
 [2.329]
 [2.538]
 [2.538]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.92620224
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2378 7603
2378 7604
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
siam score:  -0.92960006
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.163]
 [1.199]
 [1.178]
 [1.003]
 [1.163]] [[2.129]
 [1.479]
 [1.737]
 [2.193]
 [2.129]] [[2.606]
 [2.245]
 [2.373]
 [2.327]
 [2.606]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.029]
 [0.027]
 [0.012]
 [0.028]] [[-0.009]
 [-0.011]
 [-0.033]
 [-0.038]
 [-0.021]] [[0.031]
 [0.057]
 [0.045]
 [0.016]
 [0.051]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]] [[1.203]
 [1.203]
 [1.203]
 [1.203]
 [1.203]] [[3.081]
 [3.081]
 [3.081]
 [3.081]
 [3.081]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 1.0061329882436758
line 256 mcts: sample exp_bonus 2.5962476587519996
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2396 7625
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.035781691992585
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2397 7626
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.079]
 [1.089]
 [1.103]
 [1.079]
 [0.898]] [[2.795]
 [2.726]
 [2.452]
 [2.795]
 [3.01 ]] [[2.061]
 [2.035]
 [1.908]
 [2.061]
 [1.985]]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.008]
 [0.072]
 [0.072]
 [0.072]] [[-0.441]
 [-0.179]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.034]
 [0.211]
 [0.519]
 [0.519]
 [0.519]]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.03362924634339164
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[-0.035]
 [ 0.344]
 [ 0.344]
 [ 0.344]
 [ 0.344]] [[0.   ]
 [0.401]
 [0.401]
 [0.401]
 [0.401]]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2405 7637
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2409 7640
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90972364
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9024457
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.06381170033364787
2416 7664
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.786]
 [0.891]
 [0.687]
 [0.653]] [[1.351]
 [4.63 ]
 [1.826]
 [2.484]
 [1.77 ]] [[0.051]
 [1.911]
 [0.837]
 [0.94 ]
 [0.616]]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
siam score:  -0.8886765
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2427 7671
siam score:  -0.8838991
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
siam score:  -0.8897358
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.824]
 [0.912]
 [0.824]
 [0.824]
 [0.824]] [[1.938]
 [2.453]
 [1.938]
 [1.938]
 [1.938]] [[1.652]
 [2.091]
 [1.652]
 [1.652]
 [1.652]]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.646]
 [1.124]
 [1.299]
 [0.774]
 [0.864]] [[2.785]
 [2.968]
 [2.77 ]
 [3.207]
 [3.107]] [[1.537]
 [2.179]
 [2.224]
 [1.975]
 [2.   ]]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87736493
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.068]
 [1.096]
 [1.24 ]
 [1.094]
 [1.059]] [[1.894]
 [2.351]
 [1.433]
 [2.018]
 [2.415]] [[1.688]
 [1.896]
 [1.879]
 [1.781]
 [1.844]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
siam score:  -0.8789995
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8802599
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.1181 1.0 1.0
2450 7700
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.087]
 [1.249]
 [1.249]
 [1.249]
 [1.249]] [[2.52 ]
 [2.417]
 [2.417]
 [2.417]
 [2.417]] [[2.601]
 [2.812]
 [2.812]
 [2.812]
 [2.812]]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  20031
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
2456 7709
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.1201 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8874196
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[-0.413]
 [-0.413]
 [-0.413]
 [-0.413]
 [-0.413]] [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.459]
 [0.46 ]
 [0.467]
 [0.466]] [[0.155]
 [0.255]
 [0.166]
 [0.094]
 [0.155]] [[0.461]
 [0.74 ]
 [0.684]
 [0.65 ]
 [0.689]]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.431]
 [0.431]
 [0.407]
 [0.431]] [[0.604]
 [0.604]
 [0.604]
 [0.557]
 [0.604]] [[0.431]
 [0.431]
 [0.431]
 [0.407]
 [0.431]]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9052837
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.985]
 [0.985]
 [1.198]
 [0.985]
 [0.985]] [[0.429]
 [0.429]
 [1.068]
 [0.429]
 [0.429]] [[1.411]
 [1.411]
 [2.048]
 [1.411]
 [1.411]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.09 ]
 [1.187]
 [1.378]
 [1.187]
 [1.187]] [[2.15 ]
 [1.718]
 [0.47 ]
 [1.718]
 [1.718]] [[1.953]
 [2.002]
 [1.969]
 [2.002]
 [2.002]]
siam score:  -0.9041846
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.90427095
start point for exploration sampling:  20031
Printing some Q and Qe and total Qs values:  [[1.132]
 [1.172]
 [1.45 ]
 [1.185]
 [1.185]] [[2.319]
 [2.841]
 [0.448]
 [1.87 ]
 [1.87 ]] [[2.109]
 [2.362]
 [2.118]
 [2.063]
 [2.063]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  2486
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2493 7726
start point for exploration sampling:  20031
siam score:  -0.91142535
maxi score, test score, baseline:  0.1261 1.0 1.0
maxi score, test score, baseline:  0.1261 1.0 1.0
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.9098182
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90462023
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9053118
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 3.167320958373411
Printing some Q and Qe and total Qs values:  [[0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]] [[2.158]
 [2.158]
 [2.158]
 [2.158]
 [2.158]] [[2.639]
 [2.639]
 [2.639]
 [2.639]
 [2.639]]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.164]
 [1.036]
 [1.297]
 [1.036]
 [1.036]] [[3.364]
 [2.55 ]
 [2.505]
 [2.55 ]
 [2.55 ]] [[2.365]
 [1.599]
 [1.94 ]
 [1.599]
 [1.599]]
maxi score, test score, baseline:  0.1261 1.0 1.0
maxi score, test score, baseline:  0.1261 1.0 1.0
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9013258
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
2508 7732
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.421]
 [0.827]
 [0.805]
 [0.452]] [[1.961]
 [1.963]
 [1.722]
 [1.708]
 [2.363]] [[0.909]
 [0.858]
 [1.377]
 [1.334]
 [1.12 ]]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1281 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.8902],
        [0.0000],
        [0.2118],
        [0.2523],
        [0.7415],
        [0.0191],
        [0.0000],
        [0.2366],
        [0.1170],
        [0.6444]], dtype=torch.float64)
0.0 0.890204326478435
0.0 0.0
0.0 0.21175138924476125
0.0 0.2523085375161681
0.0 0.7414828804963193
0.0 0.01914234503813053
0.0 0.0
0.0 0.23663070081282214
0.0 0.11698722857289959
0.0 0.6443930003693702
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.04169389008276478
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.90185773
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1281 1.0 1.0
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
first move QE:  -0.04203455376019163
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.179]
 [1.179]
 [1.281]
 [1.179]
 [1.179]] [[1.864]
 [1.864]
 [1.722]
 [1.864]
 [1.864]] [[2.115]
 [2.115]
 [2.126]
 [2.115]
 [2.115]]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Starting evaluation
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2530 7752
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.92097735
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2531 7752
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.9573263476799898
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.243]
 [1.27 ]
 [1.243]
 [1.243]
 [1.243]] [[2.223]
 [2.49 ]
 [2.223]
 [2.223]
 [2.223]] [[2.417]
 [2.638]
 [2.417]
 [2.417]
 [2.417]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1701 1.0 1.0
siam score:  -0.92214096
maxi score, test score, baseline:  0.1701 1.0 1.0
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.1701 1.0 1.0
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91591585
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.13 ]
 [1.13 ]
 [1.13 ]
 [0.941]
 [1.193]] [[2.584]
 [2.584]
 [2.584]
 [3.114]
 [3.583]] [[1.311]
 [1.311]
 [1.311]
 [1.455]
 [1.951]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9073558
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.217]
 [0.225]
 [0.217]
 [0.217]] [[1.414]
 [2.762]
 [2.071]
 [2.762]
 [2.762]] [[0.616]
 [1.528]
 [1.014]
 [1.528]
 [1.528]]
Printing some Q and Qe and total Qs values:  [[1.005]
 [1.165]
 [1.2  ]
 [1.008]
 [1.177]] [[2.583]
 [2.972]
 [1.272]
 [2.837]
 [2.407]] [[1.888]
 [2.387]
 [1.079]
 [2.093]
 [1.952]]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.90252364
Printing some Q and Qe and total Qs values:  [[1.211]
 [1.211]
 [1.211]
 [1.211]
 [1.211]] [[2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.006]] [[2.104]
 [2.104]
 [2.104]
 [2.104]
 [2.104]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.003]
 [0.005]
 [0.005]
 [0.005]] [[-0.036]
 [-0.032]
 [-0.036]
 [-0.036]
 [-0.036]] [[0.073]
 [0.071]
 [0.073]
 [0.073]
 [0.073]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.093]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[1.099]
 [0.817]
 [0.817]
 [0.817]
 [0.817]] [[2.041]
 [1.162]
 [1.162]
 [1.162]
 [1.162]]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.1  ]
 [0.459]
 [0.241]
 [0.157]] [[-0.453]
 [-0.317]
 [-0.113]
 [-0.338]
 [-0.46 ]] [[0.425]
 [0.269]
 [1.19 ]
 [0.528]
 [0.24 ]]
siam score:  -0.904947
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.686]
 [0.565]
 [0.507]
 [0.624]] [[0.768]
 [1.003]
 [0.598]
 [0.695]
 [1.053]] [[0.587]
 [0.686]
 [0.565]
 [0.507]
 [0.624]]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2553 7768
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.078]
 [0.319]
 [0.419]
 [0.279]] [[-0.537]
 [-0.533]
 [-0.732]
 [-1.1  ]
 [-0.689]] [[0.522]
 [0.106]
 [0.521]
 [0.6  ]
 [0.457]]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.941]
 [0.744]
 [0.744]
 [0.744]] [[1.694]
 [1.831]
 [1.694]
 [1.694]
 [1.694]] [[1.321]
 [1.552]
 [1.321]
 [1.321]
 [1.321]]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.844]
 [1.062]
 [0.844]
 [0.844]
 [0.465]] [[0.522]
 [0.587]
 [0.522]
 [0.522]
 [0.832]] [[1.243]
 [1.7  ]
 [1.243]
 [1.243]
 [0.587]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
Printing some Q and Qe and total Qs values:  [[1.044]
 [1.069]
 [1.054]
 [1.054]
 [1.057]] [[2.403]
 [2.332]
 [2.054]
 [2.054]
 [2.773]] [[2.055]
 [2.037]
 [1.782]
 [1.782]
 [2.381]]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2570 7774
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.0570827965315135
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88409317
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.345]
 [0.345]
 [0.252]
 [0.345]] [[0.851]
 [0.859]
 [0.859]
 [1.813]
 [0.859]] [[1.304]
 [1.138]
 [1.138]
 [2.165]
 [1.138]]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.9342157765219794
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1861 1.0 1.0
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2578 7778
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1861 1.0 1.0
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88097304
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.949]
 [0.828]
 [0.58 ]
 [0.633]
 [0.828]] [[1.309]
 [0.695]
 [0.885]
 [0.834]
 [0.695]] [[1.513]
 [1.007]
 [0.722]
 [0.777]
 [1.007]]
siam score:  -0.8861002
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.917 0.042 0.   ]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8834932
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1921 1.0 1.0
siam score:  -0.883964
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.055]
 [1.055]
 [1.154]
 [1.055]
 [1.055]] [[1.218]
 [1.218]
 [1.326]
 [1.218]
 [1.218]] [[1.887]
 [1.887]
 [2.122]
 [1.887]
 [1.887]]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.214]
 [1.226]
 [1.226]
 [1.226]
 [1.226]] [[2.828]
 [2.635]
 [2.635]
 [2.635]
 [2.635]] [[2.842]
 [2.737]
 [2.737]
 [2.737]
 [2.737]]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.099]
 [1.052]
 [1.194]
 [1.095]
 [1.099]] [[2.266]
 [2.225]
 [2.406]
 [2.428]
 [2.266]] [[2.215]
 [2.095]
 [2.499]
 [2.316]
 [2.215]]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.866]
 [1.027]
 [0.866]
 [0.866]] [[1.495]
 [1.495]
 [1.335]
 [1.495]
 [1.495]] [[1.303]
 [1.303]
 [1.411]
 [1.303]
 [1.303]]
Printing some Q and Qe and total Qs values:  [[1.09]
 [1.09]
 [1.09]
 [1.09]
 [1.09]] [[2.194]
 [2.194]
 [2.194]
 [2.194]
 [2.194]] [[2.601]
 [2.601]
 [2.601]
 [2.601]
 [2.601]]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
2602 7790
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1921 1.0 1.0
actor:  0 policy actor:  0  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.542]
 [0.687]
 [0.542]
 [0.542]] [[2.803]
 [2.03 ]
 [2.138]
 [2.03 ]
 [2.03 ]] [[0.479]
 [0.542]
 [0.687]
 [0.542]
 [0.542]]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1961 1.0 1.0
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.035]
 [1.083]
 [1.035]
 [1.035]
 [1.035]] [[2.1  ]
 [3.186]
 [2.1  ]
 [2.1  ]
 [2.1  ]] [[2.192]
 [2.645]
 [2.192]
 [2.192]
 [2.192]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.968]
 [1.043]
 [0.776]
 [0.776]
 [0.898]] [[2.577]
 [1.86 ]
 [2.824]
 [2.824]
 [2.611]] [[2.334]
 [2.005]
 [2.114]
 [2.114]
 [2.216]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1961 1.0 1.0
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8819729
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.126]
 [1.163]
 [1.126]
 [1.126]
 [1.126]] [[2.194]
 [2.406]
 [2.194]
 [2.194]
 [2.194]] [[2.301]
 [2.518]
 [2.301]
 [2.301]
 [2.301]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.213]
 [1.213]
 [1.211]
 [1.213]
 [1.213]] [[2.251]
 [2.251]
 [2.332]
 [2.251]
 [2.251]] [[2.437]
 [2.437]
 [2.477]
 [2.437]
 [2.437]]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.022]
 [1.022]
 [0.978]
 [1.022]
 [1.022]] [[1.727]
 [1.727]
 [1.702]
 [1.727]
 [1.727]] [[2.633]
 [2.633]
 [2.539]
 [2.633]
 [2.633]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
maxi score, test score, baseline:  0.2021 1.0 1.0
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.279]
 [0.706]
 [0.279]
 [0.279]] [[-0.726]
 [-0.726]
 [-0.868]
 [-0.726]
 [-0.726]] [[0.459]
 [0.459]
 [1.145]
 [0.459]
 [0.459]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.]
 [1.]
 [1.]
 [1.]
 [1.]] [[2.063]
 [2.063]
 [2.063]
 [2.063]
 [2.063]] [[1.886]
 [1.886]
 [1.886]
 [1.886]
 [1.886]]
using explorer policy with actor:  1
siam score:  -0.8828414
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89194304
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
siam score:  -0.8929743
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
siam score:  -0.8935486
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8914259
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.654]
 [0.444]
 [0.444]
 [0.444]] [[0.158]
 [0.243]
 [0.158]
 [0.158]
 [0.158]] [[0.901]
 [1.241]
 [0.901]
 [0.901]
 [0.901]]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.480104131127955
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
using explorer policy with actor:  1
2655 7828
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
line 256 mcts: sample exp_bonus 0.5237708676838388
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2661 7830
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.319]
 [0.713]
 [0.319]
 [0.319]] [[-0.029]
 [-0.029]
 [ 0.596]
 [-0.029]
 [-0.029]] [[0.774]
 [0.774]
 [1.771]
 [0.774]
 [0.774]]
siam score:  -0.8776395
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.023]
 [1.078]
 [1.023]
 [1.023]
 [1.017]] [[2.023]
 [2.232]
 [2.023]
 [2.023]
 [2.445]] [[1.968]
 [2.243]
 [1.968]
 [1.968]
 [2.321]]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.3495088302253224
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.993]
 [0.993]
 [1.197]
 [0.993]
 [0.993]] [[1.743]
 [1.743]
 [1.982]
 [1.743]
 [1.743]] [[1.677]
 [1.677]
 [2.203]
 [1.677]
 [1.677]]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  2672
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.1104649789142007
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8660976
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8650832
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.051049824987337386
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.195]
 [0.213]
 [0.195]
 [0.195]] [[-0.06 ]
 [-0.06 ]
 [ 0.202]
 [-0.06 ]
 [-0.06 ]] [[0.642]
 [0.642]
 [1.028]
 [0.642]
 [0.642]]
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.523593655499079
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.3112837136259872
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.103]
 [1.103]
 [1.272]
 [1.103]
 [1.103]] [[0.701]
 [0.701]
 [0.939]
 [0.701]
 [0.701]] [[2.044]
 [2.044]
 [2.39 ]
 [2.044]
 [2.044]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2697 7843
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8640599
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.863396
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.2008286365366474
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2181 1.0 1.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86618584
maxi score, test score, baseline:  0.2201 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.601]
 [0.342]
 [0.432]
 [0.432]] [[1.127]
 [1.303]
 [0.978]
 [0.916]
 [0.916]] [[0.605]
 [0.601]
 [0.342]
 [0.432]
 [0.432]]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.452]
 [0.783]
 [0.64 ]
 [0.716]] [[1.535]
 [1.833]
 [1.561]
 [1.358]
 [1.732]] [[0.641]
 [0.452]
 [0.783]
 [0.64 ]
 [0.716]]
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.829]
 [0.712]
 [0.712]
 [0.712]] [[2.203]
 [2.12 ]
 [1.726]
 [1.726]
 [1.726]] [[0.705]
 [0.829]
 [0.712]
 [0.712]
 [0.712]]
siam score:  -0.8641009
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.946]
 [0.575]
 [0.577]
 [0.577]
 [0.577]] [[1.011]
 [1.121]
 [0.752]
 [0.752]
 [0.752]] [[1.964]
 [1.33 ]
 [1.223]
 [1.223]
 [1.223]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.776]
 [0.521]
 [0.521]
 [0.521]] [[0.395]
 [0.716]
 [0.395]
 [0.395]
 [0.395]] [[1.293]
 [1.91 ]
 [1.293]
 [1.293]
 [1.293]]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [1.088]
 [1.014]
 [1.007]
 [0.909]] [[2.742]
 [3.347]
 [2.697]
 [2.966]
 [2.848]] [[0.607]
 [2.101]
 [1.639]
 [1.786]
 [1.605]]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86419314
siam score:  -0.86395806
maxi score, test score, baseline:  0.2541 1.0 1.0
start point for exploration sampling:  20031
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2732 7867
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8729685
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
2736 7876
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8739583
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.039]
 [1.039]
 [1.112]
 [1.039]
 [1.039]] [[1.811]
 [1.811]
 [2.084]
 [1.811]
 [1.811]] [[2.165]
 [2.165]
 [2.584]
 [2.165]
 [2.165]]
2738 7879
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.22 ]
 [1.091]
 [1.22 ]
 [1.22 ]
 [1.113]] [[2.077]
 [2.071]
 [2.077]
 [2.077]
 [2.184]] [[2.661]
 [2.451]
 [2.661]
 [2.661]
 [2.575]]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8865305
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8852624
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88508517
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
first move QE:  -0.05692845129943037
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2753 7885
siam score:  -0.87564886
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.147]
 [1.114]
 [1.171]
 [1.147]
 [1.116]] [[2.898]
 [2.667]
 [2.625]
 [2.898]
 [2.86 ]] [[2.685]
 [2.424]
 [2.486]
 [2.685]
 [2.596]]
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]] [[1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.269]] [[2.462]
 [2.462]
 [2.462]
 [2.462]
 [2.462]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 10.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.2641 1.0 1.0
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.176]
 [1.128]
 [1.176]
 [1.176]
 [1.176]] [[2.845]
 [3.393]
 [2.845]
 [2.845]
 [2.845]] [[1.832]
 [2.076]
 [1.832]
 [1.832]
 [1.832]]
maxi score, test score, baseline:  0.2641 1.0 1.0
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8786495
2771 7894
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
using explorer policy with actor:  1
first move QE:  -0.05694857849705288
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.869688
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.781]
 [0.814]
 [0.006]
 [0.503]] [[0.738]
 [0.781]
 [0.681]
 [0.58 ]
 [1.197]] [[1.158]
 [0.991]
 [0.971]
 [0.003]
 [0.911]]
maxi score, test score, baseline:  0.2661 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.4708],
        [0.2676],
        [0.2064],
        [0.9382],
        [0.0000],
        [0.0513],
        [0.0711],
        [0.6136],
        [0.2064]], dtype=torch.float64)
0.0 0.0
0.0 0.4708403256268456
0.0 0.26764575511295025
0.0 0.20644806520510345
0.0 0.9381970123453072
0.0 0.0
0.0 0.05128140552925155
0.0 0.07107146696860016
0.0 0.613570663917025
0.0 0.20644806520510345
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.85922503
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.6610731819057851
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.9264086648382786
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
siam score:  -0.867135
2786 7906
line 256 mcts: sample exp_bonus 2.2478957096931826
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[1.972]
 [2.016]
 [2.016]
 [2.016]
 [2.016]] [[0.783]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]]
Printing some Q and Qe and total Qs values:  [[1.083]
 [1.083]
 [1.234]
 [1.083]
 [1.083]] [[0.673]
 [0.673]
 [1.055]
 [0.673]
 [0.673]] [[1.985]
 [1.985]
 [2.413]
 [1.985]
 [1.985]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.5607368784774565
Printing some Q and Qe and total Qs values:  [[0.987]
 [0.987]
 [1.197]
 [0.987]
 [0.987]] [[1.29 ]
 [1.29 ]
 [1.646]
 [1.29 ]
 [1.29 ]] [[1.809]
 [1.809]
 [2.391]
 [1.809]
 [1.809]]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.011]
 [0.804]
 [0.804]
 [0.804]
 [0.804]] [[1.157]
 [1.359]
 [1.359]
 [1.359]
 [1.359]] [[2.016]
 [1.783]
 [1.783]
 [1.783]
 [1.783]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.11 ]
 [1.306]
 [1.11 ]
 [1.11 ]
 [1.11 ]] [[2.146]
 [1.817]
 [2.146]
 [2.146]
 [2.146]] [[2.292]
 [2.463]
 [2.292]
 [2.292]
 [2.292]]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2814 7920
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
2816 7920
Printing some Q and Qe and total Qs values:  [[0.983]
 [0.983]
 [1.212]
 [0.983]
 [0.983]] [[1.742]
 [1.742]
 [1.716]
 [1.742]
 [1.742]] [[1.928]
 [1.928]
 [2.369]
 [1.928]
 [1.928]]
siam score:  -0.86932147
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.05766289590119255
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.778]
 [0.875]
 [0.875]
 [0.875]] [[0.946]
 [1.289]
 [0.946]
 [0.946]
 [0.946]] [[1.571]
 [1.721]
 [1.571]
 [1.571]
 [1.571]]
Printing some Q and Qe and total Qs values:  [[1.296]
 [1.276]
 [1.296]
 [1.296]
 [1.296]] [[2.158]
 [2.057]
 [2.158]
 [2.158]
 [2.158]] [[2.6  ]
 [2.473]
 [2.6  ]
 [2.6  ]
 [2.6  ]]
maxi score, test score, baseline:  0.2661 1.0 1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.08]
 [1.08]
 [1.08]
 [1.08]
 [1.08]] [[1.709]
 [1.709]
 [1.709]
 [1.709]
 [1.709]] [[2.359]
 [2.359]
 [2.359]
 [2.359]
 [2.359]]
maxi score, test score, baseline:  0.2661 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.213]
 [0.29 ]
 [0.213]
 [0.213]] [[-0.438]
 [-0.438]
 [-0.429]
 [-0.438]
 [-0.438]] [[0.213]
 [0.213]
 [0.29 ]
 [0.213]
 [0.213]]
siam score:  -0.8642389
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86711717
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[2.693]
 [2.23 ]
 [2.23 ]
 [2.23 ]
 [2.23 ]] [[2.122]
 [1.851]
 [1.851]
 [1.851]
 [1.851]]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8740299
maxi score, test score, baseline:  0.2681 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]] [[0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]] [[2.679]
 [2.679]
 [2.679]
 [2.679]
 [2.679]]
siam score:  -0.87417257
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 2.1516807488971104
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2829 7929
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.2701 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.058416396589212514
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
siam score:  -0.8725415
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.87256646
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[ 0.014]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[0.912]
 [0.854]
 [0.854]
 [0.854]
 [0.854]]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2721 1.0 1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.058]
 [1.058]
 [1.216]
 [1.058]
 [1.058]] [[0.842]
 [0.842]
 [1.024]
 [0.842]
 [0.842]] [[2.148]
 [2.148]
 [2.494]
 [2.148]
 [2.148]]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2838 7938
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8694063
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2721 1.0 1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.489]
 [0.429]
 [0.429]
 [0.429]] [[0.584]
 [0.686]
 [0.584]
 [0.584]
 [0.584]] [[0.429]
 [0.489]
 [0.429]
 [0.429]
 [0.429]]
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.915]
 [0.791]
 [0.791]
 [0.791]] [[2.126]
 [1.266]
 [2.126]
 [2.126]
 [2.126]] [[0.791]
 [0.915]
 [0.791]
 [0.791]
 [0.791]]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.195]
 [1.195]
 [1.208]
 [1.195]
 [1.073]] [[1.778]
 [1.778]
 [1.427]
 [1.778]
 [1.574]] [[2.311]
 [2.311]
 [2.22 ]
 [2.311]
 [1.999]]
siam score:  -0.8616202
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.5521],
        [0.3548],
        [0.4528],
        [0.6128],
        [0.6812],
        [0.4535],
        [0.4615],
        [0.8768]], dtype=torch.float64)
0.99 0.99
0.0 0.0
0.0 0.5520922209547293
0.0 0.35476722313258174
0.0 0.4528355088154288
0.0 0.6128180603234342
0.0 0.6812440636022533
0.0 0.45354728963733
0.0 0.461491729132057
0.0 0.8768350866176408
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  20031
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8705276
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.792 0.042 0.083 0.042 0.042]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.6774],
        [0.6683],
        [0.7290],
        [0.0000],
        [0.7712],
        [0.5097],
        [0.3952],
        [0.8478],
        [0.5097],
        [0.3176]], dtype=torch.float64)
0.0 0.6774125233726431
0.0 0.668323781792258
0.0 0.7289804922157607
0.0 0.0
0.0 0.7712014934527867
0.0 0.5096982314731687
0.0 0.39519926904879454
0.0 0.8478082764104203
0.0 0.5096982314731687
0.0 0.31762182312660525
maxi score, test score, baseline:  0.2781 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.834]
 [0.036]
 [0.73 ]
 [0.73 ]] [[0.995]
 [1.733]
 [1.143]
 [0.995]
 [0.995]] [[0.73 ]
 [0.834]
 [0.036]
 [0.73 ]
 [0.73 ]]
line 256 mcts: sample exp_bonus 1.3450364935940673
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
2870 7956
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.2145592035587927
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87683684
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
2879 7961
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
siam score:  -0.8780013
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.147]
 [1.26 ]
 [1.147]
 [1.147]
 [1.218]] [[2.16 ]
 [2.307]
 [2.16 ]
 [2.16 ]
 [2.387]] [[2.016]
 [2.326]
 [2.016]
 [2.016]
 [2.322]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.001]
 [1.001]
 [0.98 ]
 [1.001]
 [1.001]] [[1.209]
 [1.209]
 [1.366]
 [1.209]
 [1.209]] [[2.497]
 [2.497]
 [2.648]
 [2.497]
 [2.497]]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.603]
 [0.457]
 [0.457]
 [0.457]] [[0.84 ]
 [0.937]
 [0.84 ]
 [0.84 ]
 [0.84 ]] [[0.457]
 [0.603]
 [0.457]
 [0.457]
 [0.457]]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.108]] [[2.496]
 [2.496]
 [2.496]
 [2.496]
 [4.922]] [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [1.268]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]] [[1.32]
 [1.32]
 [1.32]
 [1.32]
 [1.32]] [[1.93]
 [1.93]
 [1.93]
 [1.93]
 [1.93]]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[-0.196]
 [-0.196]
 [-0.196]
 [-0.196]
 [-0.196]] [[0.997]
 [0.997]
 [0.997]
 [0.997]
 [0.997]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.22839538834698386
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.86046135
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8640734
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
line 256 mcts: sample exp_bonus 2.3048595063277557
Printing some Q and Qe and total Qs values:  [[1.348]
 [1.348]
 [1.348]
 [1.348]
 [1.348]] [[1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]] [[2.371]
 [2.371]
 [2.371]
 [2.371]
 [2.371]]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
first move QE:  -0.06253423934137577
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.44 ]
 [0.284]
 [0.372]
 [0.372]] [[-0.268]
 [-0.191]
 [-0.077]
 [-0.268]
 [-0.268]] [[0.372]
 [0.44 ]
 [0.284]
 [0.372]
 [0.372]]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.583]
 [0.552]
 [0.403]
 [0.549]] [[0.458]
 [0.431]
 [0.445]
 [0.39 ]
 [0.549]] [[0.497]
 [0.583]
 [0.552]
 [0.403]
 [0.549]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.003]
 [1.003]
 [0.999]
 [0.999]
 [0.86 ]] [[0.867]
 [0.4  ]
 [0.365]
 [0.332]
 [0.728]] [[1.003]
 [1.003]
 [0.999]
 [0.999]
 [0.86 ]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.072]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[0.354]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[2.151]
 [1.415]
 [1.415]
 [1.415]
 [1.415]]
Printing some Q and Qe and total Qs values:  [[1.102]
 [0.696]
 [0.696]
 [0.696]
 [0.696]] [[0.214]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[1.789]
 [1.033]
 [1.033]
 [1.033]
 [1.033]]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]] [[1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]]
Printing some Q and Qe and total Qs values:  [[0.998]
 [0.208]
 [0.648]
 [0.678]
 [0.714]] [[0.805]
 [1.566]
 [0.902]
 [0.808]
 [1.221]] [[1.443]
 [0.812]
 [0.978]
 [0.952]
 [1.327]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.280895839276013
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85909516
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]]
siam score:  -0.8602176
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.009]
 [0.657]
 [0.657]
 [0.495]] [[-0.089]
 [ 0.469]
 [-0.197]
 [-0.403]
 [-0.028]] [[0.451]
 [0.006]
 [0.859]
 [0.722]
 [0.648]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86461014
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85853314
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3361 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3361 1.0 1.0
maxi score, test score, baseline:  0.3361 1.0 1.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.311]
 [0.442]
 [0.248]
 [0.311]] [[ 0.   ]
 [ 0.   ]
 [-0.084]
 [-0.088]
 [ 0.   ]] [[0.311]
 [0.311]
 [0.442]
 [0.248]
 [0.311]]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3421 1.0 1.0
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3421 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.448]
 [0.426]
 [0.426]
 [0.426]] [[1.174]
 [1.154]
 [0.866]
 [0.866]
 [0.866]] [[0.45 ]
 [0.448]
 [0.426]
 [0.426]
 [0.426]]
Printing some Q and Qe and total Qs values:  [[1.216]
 [1.288]
 [1.216]
 [1.216]
 [1.216]] [[2.503]
 [2.421]
 [2.503]
 [2.503]
 [2.503]] [[2.336]
 [2.358]
 [2.336]
 [2.336]
 [2.336]]
maxi score, test score, baseline:  0.3421 1.0 1.0
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.674]
 [1.205]
 [0.755]
 [0.779]] [[1.753]
 [2.185]
 [1.792]
 [1.753]
 [2.087]] [[0.88 ]
 [1.059]
 [1.291]
 [0.88 ]
 [1.093]]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.167]
 [1.167]
 [1.167]
 [1.167]
 [1.167]] [[2.107]
 [2.107]
 [2.107]
 [2.107]
 [2.107]] [[1.953]
 [1.953]
 [1.953]
 [1.953]
 [1.953]]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.011]
 [0.689]
 [0.431]
 [0.44 ]] [[0.283]
 [0.453]
 [0.709]
 [0.172]
 [0.143]] [[0.37 ]
 [0.016]
 [1.226]
 [0.347]
 [0.333]]
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.295]
 [0.702]
 [0.885]
 [0.674]] [[6.215]
 [3.306]
 [3.486]
 [2.245]
 [3.049]] [[2.135]
 [0.832]
 [1.2  ]
 [0.876]
 [1.018]]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8368582
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
first move QE:  -0.06324072473132651
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.123]
 [0.201]
 [1.036]
 [0.395]] [[5.753]
 [2.561]
 [2.164]
 [1.937]
 [3.093]] [[2.25 ]
 [0.429]
 [0.363]
 [1.284]
 [0.973]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.06316807759791934
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.726]
 [0.796]
 [0.726]
 [0.726]] [[1.53 ]
 [1.53 ]
 [1.605]
 [1.53 ]
 [1.53 ]] [[0.726]
 [0.726]
 [0.796]
 [0.726]
 [0.726]]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.149]
 [1.23 ]
 [1.149]
 [1.149]
 [1.149]] [[1.967]
 [2.018]
 [1.967]
 [1.967]
 [1.967]] [[2.384]
 [2.546]
 [2.384]
 [2.384]
 [2.384]]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06312441612095852
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.84300214
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.3750410592359844
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.373]] [[1.696]
 [1.696]
 [1.696]
 [1.696]
 [1.696]]
line 256 mcts: sample exp_bonus 2.501478169382469
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.441]
 [0.312]
 [0.314]] [[ 0.   ]
 [ 0.   ]
 [-0.047]
 [-0.242]
 [-0.132]] [[0.974]
 [0.974]
 [0.775]
 [0.323]
 [0.435]]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.858]
 [0.896]
 [0.746]
 [0.746]
 [0.734]] [[1.242]
 [0.94 ]
 [0.905]
 [0.905]
 [1.056]] [[1.758]
 [1.471]
 [1.168]
 [1.168]
 [1.323]]
siam score:  -0.8498502
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.828]
 [0.585]
 [0.585]
 [0.585]] [[0.494]
 [0.324]
 [0.494]
 [0.494]
 [0.494]] [[1.154]
 [1.527]
 [1.154]
 [1.154]
 [1.154]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84763455
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]] [[0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.820961722434178
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]] [[1.006]
 [1.006]
 [1.006]
 [1.006]
 [1.006]] [[2.479]
 [2.479]
 [2.479]
 [2.479]
 [2.479]]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8451546
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06497274880799571
first move QE:  -0.06497274880799571
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]] [[2.075]
 [2.075]
 [2.075]
 [2.075]
 [2.075]] [[2.428]
 [2.428]
 [2.428]
 [2.428]
 [2.428]]
siam score:  -0.83641404
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0650656725710984
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.291]
 [1.385]
 [1.291]
 [1.291]
 [1.291]] [[1.431]
 [1.153]
 [1.431]
 [1.431]
 [1.431]] [[2.77 ]
 [2.853]
 [2.77 ]
 [2.77 ]
 [2.77 ]]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.756]
 [0.738]
 [0.049]
 [0.738]] [[1.133]
 [1.448]
 [1.133]
 [0.487]
 [1.133]] [[1.705]
 [1.998]
 [1.705]
 [0.266]
 [1.705]]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.992]
 [0.665]
 [0.665]
 [0.665]] [[2.002]
 [1.558]
 [2.002]
 [2.002]
 [2.002]] [[1.63 ]
 [1.805]
 [1.63 ]
 [1.63 ]
 [1.63 ]]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[0.72]
 [0.26]
 [0.26]
 [0.26]
 [0.26]] [[1.927]
 [1.383]
 [1.383]
 [1.383]
 [1.383]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.385]
 [1.385]
 [1.385]
 [1.385]
 [1.385]] [[2.262]
 [2.262]
 [2.262]
 [2.262]
 [2.262]] [[2.604]
 [2.604]
 [2.604]
 [2.604]
 [2.604]]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.045]
 [0.046]
 [0.046]
 [0.046]] [[0.027]
 [0.019]
 [0.027]
 [0.027]
 [0.027]] [[0.103]
 [0.098]
 [0.102]
 [0.102]
 [0.102]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.699]
 [0.772]
 [0.692]
 [0.67 ]] [[2.178]
 [2.042]
 [2.15 ]
 [2.178]
 [1.917]] [[1.949]
 [1.827]
 [2.08 ]
 [1.949]
 [1.644]]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.5681785533816617
start point for exploration sampling:  20031
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  3051
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.422207738165381
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.4740597374843347
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]] [[2.008]
 [2.008]
 [2.008]
 [2.008]
 [2.008]] [[2.678]
 [2.678]
 [2.678]
 [2.678]
 [2.678]]
Printing some Q and Qe and total Qs values:  [[0.272]
 [1.308]
 [1.251]
 [1.084]
 [1.105]] [[2.367]
 [2.245]
 [2.666]
 [2.146]
 [2.341]] [[0.51 ]
 [2.381]
 [2.538]
 [1.898]
 [2.059]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8123697
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
siam score:  -0.81683505
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06518139157330095
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
3065 8042
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
siam score:  -0.8347923
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5105836444487222
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
3071 8046
maxi score, test score, baseline:  0.3721 1.0 1.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.6683071140827168
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.109]
 [1.218]
 [1.109]
 [1.109]
 [1.109]] [[1.834]
 [2.253]
 [1.834]
 [1.834]
 [1.834]] [[1.691]
 [2.284]
 [1.691]
 [1.691]
 [1.691]]
maxi score, test score, baseline:  0.3721 1.0 1.0
first move QE:  -0.06597744601660364
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.83883893
maxi score, test score, baseline:  0.3721 1.0 1.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.865]
 [0.681]
 [0.681]
 [0.681]] [[0.547]
 [0.391]
 [0.547]
 [0.547]
 [0.547]] [[1.651]
 [1.914]
 [1.651]
 [1.651]
 [1.651]]
maxi score, test score, baseline:  0.3721 1.0 1.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.281]
 [1.316]
 [1.281]
 [1.281]
 [1.135]] [[1.938]
 [1.651]
 [1.938]
 [1.938]
 [1.794]] [[2.761]
 [2.639]
 [2.761]
 [2.761]
 [2.372]]
Printing some Q and Qe and total Qs values:  [[1.329]
 [1.329]
 [1.421]
 [1.329]
 [1.329]] [[2.127]
 [2.127]
 [1.847]
 [2.127]
 [2.127]] [[2.605]
 [2.605]
 [2.604]
 [2.605]
 [2.605]]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.089]
 [1.125]
 [1.482]
 [1.125]
 [1.125]] [[1.968]
 [1.77 ]
 [0.864]
 [1.77 ]
 [1.77 ]] [[2.259]
 [2.221]
 [2.291]
 [2.221]
 [2.221]]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8395965
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8405261
first move QE:  -0.06683859857843338
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3741 1.0 1.0
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8455206
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.194]
 [1.194]
 [1.287]
 [1.194]
 [1.194]] [[1.337]
 [1.337]
 [1.248]
 [1.337]
 [1.337]] [[2.2  ]
 [2.2  ]
 [2.354]
 [2.2  ]
 [2.2  ]]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8491666
maxi score, test score, baseline:  0.3761 1.0 1.0
maxi score, test score, baseline:  0.3761 1.0 1.0
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3761 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8441928
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.265]
 [1.275]
 [1.275]
 [1.171]
 [1.268]] [[3.106]
 [2.487]
 [2.95 ]
 [2.384]
 [2.283]] [[2.38 ]
 [1.973]
 [2.285]
 [1.798]
 [1.828]]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.724]
 [0.655]
 [0.517]
 [0.514]] [[1.971]
 [1.609]
 [1.564]
 [1.323]
 [1.19 ]] [[0.519]
 [0.724]
 [0.655]
 [0.517]
 [0.514]]
maxi score, test score, baseline:  0.3761 1.0 1.0
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84810233
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.661]
 [0.66 ]
 [0.661]
 [0.661]] [[0.195]
 [0.195]
 [0.604]
 [0.195]
 [0.195]] [[0.819]
 [0.819]
 [1.089]
 [0.819]
 [0.819]]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.2129219995182368
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.042 0.833]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.385]
 [1.385]
 [1.385]
 [1.385]
 [1.385]] [[1.672]
 [1.672]
 [1.672]
 [1.672]
 [1.672]] [[2.541]
 [2.541]
 [2.541]
 [2.541]
 [2.541]]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.13 ]
 [1.13 ]
 [1.261]
 [1.13 ]
 [1.13 ]] [[1.318]
 [1.318]
 [1.053]
 [1.318]
 [1.318]] [[2.087]
 [2.087]
 [2.261]
 [2.087]
 [2.087]]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8434687
maxi score, test score, baseline:  0.3781 1.0 1.0
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.075]
 [1.075]
 [1.252]
 [1.075]
 [1.075]] [[1.601]
 [1.601]
 [1.356]
 [1.601]
 [1.601]] [[2.083]
 [2.083]
 [2.276]
 [2.083]
 [2.083]]
Printing some Q and Qe and total Qs values:  [[1.049]
 [1.049]
 [1.117]
 [1.049]
 [1.049]] [[1.762]
 [1.762]
 [1.605]
 [1.762]
 [1.762]] [[2.147]
 [2.147]
 [2.178]
 [2.147]
 [2.147]]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.345]
 [1.296]
 [1.3  ]
 [1.141]
 [1.212]] [[2.11 ]
 [2.004]
 [1.665]
 [2.279]
 [2.185]] [[1.023]
 [2.565]
 [2.381]
 [2.458]
 [2.525]]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.4  ]
 [0.483]
 [0.41 ]
 [0.408]] [[ 0.   ]
 [ 0.   ]
 [-0.241]
 [-0.287]
 [-0.141]] [[0.4  ]
 [0.4  ]
 [0.483]
 [0.41 ]
 [0.408]]
maxi score, test score, baseline:  0.3781 1.0 1.0
siam score:  -0.8568698
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5163932205260315
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85641754
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06763311691544757
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.193]
 [1.193]
 [1.193]
 [1.193]
 [1.193]] [[1.388]
 [1.388]
 [1.388]
 [1.388]
 [1.388]] [[2.261]
 [2.261]
 [2.261]
 [2.261]
 [2.261]]
Printing some Q and Qe and total Qs values:  [[0.994]
 [1.123]
 [0.994]
 [0.994]
 [0.994]] [[0.988]
 [0.934]
 [0.988]
 [0.988]
 [0.988]] [[2.068]
 [2.307]
 [2.068]
 [2.068]
 [2.068]]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.848562
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06755396267326896
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.617]
 [0.617]
 [0.617]
 [0.656]] [[0.291]
 [0.749]
 [0.749]
 [0.749]
 [0.928]] [[0.486]
 [1.109]
 [1.109]
 [1.109]
 [1.367]]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.668]
 [0.508]
 [0.389]] [[0.851]
 [0.851]
 [0.894]
 [0.851]
 [0.981]] [[1.846]
 [1.846]
 [2.159]
 [1.846]
 [1.793]]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.144]
 [1.323]
 [1.144]
 [1.144]
 [1.144]] [[1.581]
 [1.914]
 [1.581]
 [1.581]
 [1.581]] [[2.443]
 [2.832]
 [2.443]
 [2.443]
 [2.443]]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
3134 8080
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.018]
 [1.018]
 [1.248]
 [1.018]
 [1.018]] [[1.021]
 [1.021]
 [0.5  ]
 [1.021]
 [1.021]] [[2.121]
 [2.121]
 [2.367]
 [2.121]
 [2.121]]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.655]
 [0.655]
 [0.655]
 [0.655]] [[5.142]
 [2.188]
 [2.188]
 [2.188]
 [2.188]] [[1.777]
 [0.643]
 [0.643]
 [0.643]
 [0.643]]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]] [[2.133]
 [2.133]
 [2.133]
 [2.133]
 [2.133]] [[1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.269]]
line 256 mcts: sample exp_bonus 4.247508412285748
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.    0.167 0.    0.125 0.708]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0802356051696291
line 256 mcts: sample exp_bonus 0.03983747844127525
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
siam score:  -0.83555543
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.374337715436022
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.226]] [[2.536]
 [2.536]
 [2.536]
 [2.536]
 [2.564]] [[2.658]
 [2.658]
 [2.658]
 [2.658]
 [2.621]]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8299974
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
maxi score, test score, baseline:  0.4221 1.0 1.0
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.619]
 [0.619]
 [0.597]
 [0.653]] [[0.221]
 [0.097]
 [0.097]
 [0.197]
 [0.351]] [[0.579]
 [0.671]
 [0.671]
 [0.76 ]
 [1.079]]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.518]
 [0.666]
 [0.75 ]
 [0.702]] [[2.316]
 [2.315]
 [2.175]
 [1.792]
 [2.195]] [[1.834]
 [1.656]
 [1.737]
 [1.481]
 [1.811]]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.4241 1.0 1.0
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20031
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84109616
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.348]
 [1.348]
 [1.348]
 [1.348]
 [1.348]] [[2.27]
 [2.27]
 [2.27]
 [2.27]
 [2.27]] [[2.623]
 [2.623]
 [2.623]
 [2.623]
 [2.623]]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.678]
 [0.583]
 [0.606]
 [0.674]] [[-0.085]
 [ 0.294]
 [ 0.275]
 [ 0.007]
 [ 0.33 ]] [[0.036]
 [1.469]
 [1.272]
 [1.23 ]
 [1.473]]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]] [[0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]] [[1.366]
 [1.366]
 [1.366]
 [1.366]
 [1.366]]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.388]
 [0.434]
 [0.388]
 [0.388]] [[ 0.323]
 [ 0.274]
 [-0.018]
 [ 0.274]
 [ 0.274]] [[1.585]
 [1.57 ]
 [1.273]
 [1.57 ]
 [1.57 ]]
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4281 1.0 1.0
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4301 1.0 1.0
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4301 1.0 1.0
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4301 1.0 1.0
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.9299047609099182
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8260338
siam score:  -0.8254303
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.199]
 [1.292]
 [1.199]
 [1.199]
 [1.199]] [[2.836]
 [2.86 ]
 [2.836]
 [2.836]
 [2.836]] [[1.764]
 [1.911]
 [1.764]
 [1.764]
 [1.764]]
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
3172 8109
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.779]
 [0.713]
 [0.713]
 [0.713]] [[0.631]
 [0.952]
 [0.631]
 [0.631]
 [0.631]] [[2.226]
 [2.674]
 [2.226]
 [2.226]
 [2.226]]
maxi score, test score, baseline:  0.4341 1.0 1.0
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.544]] [[1.796]
 [1.796]
 [1.796]
 [1.796]
 [4.447]] [[0.915]
 [0.915]
 [0.915]
 [0.915]
 [2.042]]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4341 1.0 1.0
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.946]
 [1.162]
 [0.946]
 [0.946]
 [0.946]] [[1.477]
 [1.772]
 [1.477]
 [1.477]
 [1.477]] [[1.491]
 [2.016]
 [1.491]
 [1.491]
 [1.491]]
maxi score, test score, baseline:  0.4341 1.0 1.0
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8335454
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
3179 8111
maxi score, test score, baseline:  0.4341 1.0 1.0
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4341 1.0 1.0
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4341 1.0 1.0
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06711345217092342
Printing some Q and Qe and total Qs values:  [[1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]] [[2.942]
 [2.942]
 [2.942]
 [2.942]
 [2.942]] [[2.543]
 [2.543]
 [2.543]
 [2.543]
 [2.543]]
siam score:  -0.8319
using explorer policy with actor:  1
3180 8112
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4341 1.0 1.0
3185 8115
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4341 1.0 1.0
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.359]
 [1.359]
 [1.395]
 [1.359]
 [1.359]] [[1.157]
 [1.157]
 [1.002]
 [1.157]
 [1.157]] [[2.188]
 [2.188]
 [2.207]
 [2.188]
 [2.188]]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.183]
 [1.371]
 [1.379]
 [1.386]
 [1.309]] [[1.054]
 [0.974]
 [0.579]
 [1.099]
 [1.064]] [[1.657]
 [2.006]
 [1.891]
 [2.079]
 [1.913]]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.84211916
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
siam score:  -0.8392608
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
3193 8116
maxi score, test score, baseline:  0.4361 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8306519
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.198]] [[1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]] [[2.054]
 [2.054]
 [2.054]
 [2.054]
 [2.054]]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.715]
 [0.873]
 [0.715]
 [0.715]] [[1.43 ]
 [1.43 ]
 [0.762]
 [1.43 ]
 [1.43 ]] [[0.715]
 [0.715]
 [0.873]
 [0.715]
 [0.715]]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[1.299]
 [1.299]
 [1.299]
 [1.299]
 [1.299]] [[0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4381 1.0 1.0
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4381 1.0 1.0
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4381 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.4381 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.95 ]
 [0.776]
 [0.776]
 [0.728]] [[1.017]
 [0.951]
 [0.487]
 [0.487]
 [0.956]] [[1.232]
 [1.434]
 [0.81 ]
 [0.81 ]
 [1.096]]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.366]
 [1.371]
 [1.422]
 [1.244]
 [1.366]] [[1.292]
 [1.304]
 [1.096]
 [1.436]
 [1.292]] [[2.155]
 [2.169]
 [2.201]
 [1.958]
 [2.155]]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]] [[1.545]
 [1.545]
 [1.545]
 [1.545]
 [1.545]] [[1.923]
 [1.923]
 [1.923]
 [1.923]
 [1.923]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.487]
 [0.474]
 [0.474]
 [0.474]] [[0.313]
 [0.491]
 [0.313]
 [0.313]
 [0.313]] [[0.474]
 [0.487]
 [0.474]
 [0.474]
 [0.474]]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.294]
 [1.36 ]
 [1.305]
 [1.294]
 [1.294]] [[2.041]
 [2.199]
 [1.948]
 [2.041]
 [2.041]] [[2.657]
 [2.895]
 [2.618]
 [2.657]
 [2.657]]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06787956605012244
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8356647
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.222]
 [1.33 ]
 [1.222]
 [1.222]
 [1.222]] [[2.104]
 [2.171]
 [2.104]
 [2.104]
 [2.104]] [[2.405]
 [2.576]
 [2.405]
 [2.405]
 [2.405]]
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4401 1.0 1.0
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4401 1.0 1.0
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
