append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[5, 12]
observable_size:[5, 12]
game_modes:1
env_map:[['S' 'F' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']
 ['F' 'F' 'H' 'H' 'H' 'H' 'F' 'F' 'F' 'F' 'F' 'G']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'H' 'H' 'H' 'H' 'H']
 ['F' 'F' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']
 ['F' 'F' 'E' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:305000
exp_gamma:0.95
atari_env:False
memory_size:60
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 60)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']
 ['F' 'F' 'H' 'H' 'H' 'H' 'F' 'F' 'F' 'F' 'F' 'G']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'H' 'H' 'H' 'H' 'H']
 ['F' 'F' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']
 ['F' 'F' 'E' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.2 0.  0.4]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Starting evaluation
using another actor
siam score:  0.001445380865003575
from probs:  [0.09564569745443416, 0.33085225371434407, 0.4778563513767878, 0.09564569745443416]
actor:  0 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
deleting a thread, now have 2 threads
Frames:  862 train batches done:  27 episodes:  54
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
siam score:  -0.098645609351919
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
deleting a thread, now have 1 threads
Frames:  862 train batches done:  72 episodes:  54
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
maxi score, test score, baseline:  0.03135 0.0 0.03135
maxi score, test score, baseline:  0.03135 0.0 0.03135
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.08517191939029782, 0.40419401089294715, 0.4254621503264571, 0.08517191939029782]
using another actor
maxi score, test score, baseline:  0.029511764705882352 0.0 0.029511764705882352
probs:  [0.08340033445784373, 0.41659966554215627, 0.41659966554215627, 0.08340033445784373]
maxi score, test score, baseline:  0.029511764705882352 0.0 0.029511764705882352
probs:  [0.0848872484111679, 0.40618773798943575, 0.4240377651882284, 0.0848872484111679]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.029511764705882352 0.0 0.029511764705882352
probs:  [0.2234155062579651, 0.32975348122610476, 0.3722886712133606, 0.07454234130256954]
maxi score, test score, baseline:  0.029511764705882352 0.0 0.029511764705882352
probs:  [0.2234155062579651, 0.32975348122610476, 0.3722886712133606, 0.07454234130256954]
maxi score, test score, baseline:  0.029511764705882352 0.0 0.029511764705882352
probs:  [0.2307772540787813, 0.30766823776365615, 0.384559221448531, 0.07699528670903162]
maxi score, test score, baseline:  0.029511764705882352 0.0 0.029511764705882352
probs:  [0.2307772540787813, 0.30766823776365615, 0.384559221448531, 0.07699528670903162]
maxi score, test score, baseline:  0.029511764705882352 0.0 0.029511764705882352
probs:  [0.2360307567709269, 0.3167419398722379, 0.3726189127885301, 0.07460839056830494]
maxi score, test score, baseline:  0.029511764705882352 0.0 0.029511764705882352
from probs:  [0.2360307567709269, 0.3167419398722379, 0.3726189127885301, 0.07460839056830494]
maxi score, test score, baseline:  0.02867142857142857 0.0 0.02867142857142857
maxi score, test score, baseline:  0.02867142857142857 0.0 0.02867142857142857
first move QE:  -0.28791341764272493
maxi score, test score, baseline:  0.02867142857142857 0.0 0.02867142857142857
probs:  [0.2307772571353892, 0.30766822859383236, 0.38455920005227545, 0.07699531421850303]
from probs:  [0.23776288809756227, 0.3098974424698268, 0.3768795286726438, 0.07546014075996718]
siam score:  -0.36921853
maxi score, test score, baseline:  0.027877777777777776 0.0 0.027877777777777776
probs:  [0.2424844078673614, 0.31764032919374724, 0.36649167805589816, 0.07338358488299312]
maxi score, test score, baseline:  0.027877777777777776 0.0 0.027877777777777776
probs:  [0.2424844078673614, 0.31764032919374724, 0.36649167805589816, 0.07338358488299312]
maxi score, test score, baseline:  0.027127027027027028 0.0 0.027127027027027028
probs:  [0.24248440781092087, 0.31764032970171197, 0.3664916789307262, 0.07338358355664082]
maxi score, test score, baseline:  0.027127027027027028 0.0 0.027127027027027028
probs:  [0.24248440781092087, 0.31764032970171197, 0.3664916789307262, 0.07338358355664082]
maxi score, test score, baseline:  0.027127027027027028 0.0 0.027127027027027028
maxi score, test score, baseline:  0.027127027027027028 0.0 0.027127027027027028
from probs:  [0.24695253367341377, 0.32496767163402107, 0.3566613214305178, 0.07141847326204728]
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.226]
 [0.226]
 [0.114]
 [0.226]] [[-0.451]
 [-0.451]
 [-0.451]
 [-0.286]
 [-0.451]] [[0.08 ]
 [0.08 ]
 [0.08 ]
 [0.078]
 [0.08 ]]
maxi score, test score, baseline:  0.0251 0.0 0.0251
probs:  [0.25520604218597703, 0.33850271716161023, 0.3385027171616102, 0.0677885234908025]
maxi score, test score, baseline:  0.0251 0.0 0.0251
probs:  [0.25520604218597703, 0.33850271716161023, 0.3385027171616102, 0.0677885234908025]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.0251 0.0 0.0251
probs:  [0.23820202316962305, 0.2933972364111527, 0.39026483565003717, 0.07813590476918718]
maxi score, test score, baseline:  0.0251 0.0 0.0251
probs:  [0.23438147899748532, 0.29685556300754407, 0.39056668902263214, 0.07819626897233845]
maxi score, test score, baseline:  0.0251 0.0 0.0251
probs:  [0.23438147899748532, 0.29685556300754407, 0.39056668902263214, 0.07819626897233845]
maxi score, test score, baseline:  0.0251 0.0 0.0251
probs:  [0.23438147899748532, 0.29685556300754407, 0.39056668902263214, 0.07819626897233845]
siam score:  -0.5825388
maxi score, test score, baseline:  0.0251 0.0 0.0251
probs:  [0.23438147899748532, 0.29685556300754407, 0.39056668902263214, 0.07819626897233845]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.38783040768038485
maxi score, test score, baseline:  0.024490243902439025 0.0 0.024490243902439025
probs:  [0.2272822785602668, 0.3181531643191994, 0.3787337548251545, 0.0758308022953793]
maxi score, test score, baseline:  0.024490243902439025 0.0 0.024490243902439025
maxi score, test score, baseline:  0.024490243902439025 0.0 0.024490243902439025
probs:  [0.2272822785602668, 0.3181531643191994, 0.3787337548251545, 0.0758308022953793]
maxi score, test score, baseline:  0.024490243902439025 0.0 0.024490243902439025
maxi score, test score, baseline:  0.024490243902439025 0.0 0.024490243902439025
probs:  [0.23479726436458082, 0.3100508057599056, 0.3792232529010628, 0.07592867697445069]
from probs:  [0.22660591397161747, 0.3133701870577258, 0.3832835476023172, 0.07674035136833952]
maxi score, test score, baseline:  0.024490243902439025 0.0 0.024490243902439025
probs:  [0.22660591397161747, 0.3133701870577258, 0.3832835476023172, 0.07674035136833952]
maxi score, test score, baseline:  0.023909523809523808 0.0 0.023909523809523808
probs:  [0.22660591383552503, 0.3133701874263746, 0.3832835483776791, 0.07674035036042125]
maxi score, test score, baseline:  0.023909523809523808 0.0 0.023909523809523808
probs:  [0.22938831669857057, 0.31852944277141915, 0.37666495977545095, 0.07541728075455932]
maxi score, test score, baseline:  0.023909523809523808 0.0 0.023909523809523808
maxi score, test score, baseline:  0.023909523809523808 0.0 0.023909523809523808
probs:  [0.22190035391769994, 0.32162491390117437, 0.380325658239273, 0.07614907394185266]
maxi score, test score, baseline:  0.023909523809523808 0.0 0.023909523809523808
probs:  [0.22190035391769994, 0.32162491390117437, 0.380325658239273, 0.07614907394185266]
maxi score, test score, baseline:  0.023909523809523808 0.0 0.023909523809523808
probs:  [0.22190035391769994, 0.32162491390117437, 0.380325658239273, 0.07614907394185266]
siam score:  -0.652292
maxi score, test score, baseline:  0.023909523809523808 0.0 0.023909523809523808
maxi score, test score, baseline:  0.023909523809523808 0.0 0.023909523809523808
probs:  [0.22190035391769994, 0.32162491390117437, 0.380325658239273, 0.07614907394185266]
maxi score, test score, baseline:  0.023909523809523808 0.0 0.023909523809523808
probs:  [0.22190035391769994, 0.32162491390117437, 0.380325658239273, 0.07614907394185266]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.02335581395348837 0.0 0.02335581395348837
probs:  [0.2219003537565867, 0.3216249143118456, 0.3803256589865131, 0.07614907294505444]
maxi score, test score, baseline:  0.022322222222222222 0.0 0.022322222222222222
probs:  [0.2104377330723009, 0.3225862320382154, 0.38907743695871816, 0.07789859793076555]
maxi score, test score, baseline:  0.022322222222222222 0.0 0.022322222222222222
probs:  [0.2104377330723009, 0.3225862320382154, 0.38907743695871816, 0.07789859793076555]
actor:  0 policy actor:  0  step number:  63 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.032708695652173915 0.0 0.032708695652173915
siam score:  -0.6636705
maxi score, test score, baseline:  0.032708695652173915 0.0 0.032708695652173915
maxi score, test score, baseline:  0.032708695652173915 0.0 0.032708695652173915
maxi score, test score, baseline:  0.032708695652173915 0.0 0.032708695652173915
probs:  [0.21173280692553742, 0.31841710276949375, 0.3914726531843769, 0.07837743712059196]
maxi score, test score, baseline:  0.032708695652173915 0.0 0.032708695652173915
probs:  [0.21173280692553742, 0.31841710276949375, 0.3914726531843769, 0.07837743712059196]
maxi score, test score, baseline:  0.032708695652173915 0.0 0.032708695652173915
maxi score, test score, baseline:  0.032708695652173915 0.0 0.032708695652173915
probs:  [0.21173280692553742, 0.31841710276949375, 0.3914726531843769, 0.07837743712059196]
maxi score, test score, baseline:  0.032708695652173915 0.0 0.032708695652173915
probs:  [0.21173280692553742, 0.31841710276949375, 0.3914726531843769, 0.07837743712059196]
maxi score, test score, baseline:  0.032708695652173915 0.0 0.032708695652173915
probs:  [0.21173280692553742, 0.31841710276949375, 0.3914726531843769, 0.07837743712059196]
maxi score, test score, baseline:  0.032708695652173915 0.0 0.032708695652173915
probs:  [0.21173280692553742, 0.31841710276949375, 0.3914726531843769, 0.07837743712059196]
from probs:  [0.21173280692553742, 0.31841710276949375, 0.3914726531843769, 0.07837743712059196]
actor:  0 policy actor:  0  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.040916326530612245 0.0 0.040916326530612245
probs:  [0.22203355803823732, 0.3338993258852881, 0.36998505744885285, 0.07408205862762178]
maxi score, test score, baseline:  0.040916326530612245 0.0 0.040916326530612245
probs:  [0.22203355803823732, 0.3338993258852881, 0.36998505744885285, 0.07408205862762178]
maxi score, test score, baseline:  0.040916326530612245 0.0 0.040916326530612245
probs:  [0.22555391752579249, 0.3342031729667147, 0.3667979495989913, 0.07344495990850143]
siam score:  -0.7159827
maxi score, test score, baseline:  0.040916326530612245 0.0 0.040916326530612245
probs:  [0.22904770670779317, 0.3344042308258555, 0.36371865014051263, 0.07282941232583867]
maxi score, test score, baseline:  0.040916326530612245 0.0 0.040916326530612245
probs:  [0.22904770670779317, 0.3344042308258555, 0.36371865014051263, 0.07282941232583867]
siam score:  -0.7128377
siam score:  -0.7140552
actor:  1 policy actor:  1  step number:  48 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.291648513957429, 0.3471798659006677, 0.291648513957429, 0.06952310618447433]
first move QE:  -0.22250142502602138
maxi score, test score, baseline:  0.039315686274509806 0.0 0.039315686274509806
probs:  [0.29164851431456096, 0.3471798667339755, 0.2916485143145609, 0.06952310463690252]
maxi score, test score, baseline:  0.039315686274509806 0.0 0.039315686274509806
probs:  [0.29164851431456096, 0.3471798667339755, 0.2916485143145609, 0.06952310463690252]
maxi score, test score, baseline:  0.039315686274509806 0.0 0.039315686274509806
probs:  [0.29164851431456096, 0.3471798667339755, 0.2916485143145609, 0.06952310463690252]
maxi score, test score, baseline:  0.039315686274509806 0.0 0.039315686274509806
probs:  [0.29164851431456096, 0.3471798667339755, 0.2916485143145609, 0.06952310463690252]
maxi score, test score, baseline:  0.039315686274509806 0.0 0.039315686274509806
probs:  [0.29164851431456096, 0.3471798667339755, 0.2916485143145609, 0.06952310463690252]
maxi score, test score, baseline:  0.039315686274509806 0.0 0.039315686274509806
probs:  [0.29164851431456096, 0.3471798667339755, 0.2916485143145609, 0.06952310463690252]
maxi score, test score, baseline:  0.039315686274509806 0.0 0.039315686274509806
probs:  [0.29164851431456096, 0.3471798667339755, 0.2916485143145609, 0.06952310463690252]
maxi score, test score, baseline:  0.039315686274509806 0.0 0.039315686274509806
probs:  [0.29164851431456096, 0.3471798667339755, 0.2916485143145609, 0.06952310463690252]
maxi score, test score, baseline:  0.039315686274509806 0.0 0.039315686274509806
probs:  [0.286148557056452, 0.35345217680119734, 0.289622292269084, 0.07077697387326677]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.037835849056603775 0.0 0.037835849056603775
probs:  [0.3417209137683414, 0.3249889275987248, 0.2648583523016648, 0.06843180633126893]
siam score:  -0.7154182
siam score:  -0.7145806
using another actor
maxi score, test score, baseline:  0.035187719298245615 0.0 0.035187719298245615
probs:  [0.3417062871083561, 0.3262377567527298, 0.2636270386466232, 0.06842891749229082]
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
probs:  [0.34170628771106826, 0.3262377572537797, 0.2636270387361828, 0.06842891629896927]
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
probs:  [0.34170628771106826, 0.3262377572537797, 0.2636270387361828, 0.06842891629896927]
siam score:  -0.6586149
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
probs:  [0.34170628771106826, 0.3262377572537797, 0.2636270387361828, 0.06842891629896927]
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
probs:  [0.34170628771106826, 0.3262377572537797, 0.2636270387361828, 0.06842891629896927]
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
probs:  [0.34170628771106826, 0.3262377572537797, 0.2636270387361828, 0.06842891629896927]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
probs:  [0.34170628771106826, 0.3262377572537797, 0.2636270387361828, 0.06842891629896927]
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
probs:  [0.3356237522790615, 0.3305593985320954, 0.2666038454989815, 0.06721300368986162]
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
probs:  [0.3248593237396293, 0.3394003434588379, 0.26777235743458827, 0.06796797536694467]
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
probs:  [0.3248593237396293, 0.3394003434588379, 0.26777235743458827, 0.06796797536694467]
maxi score, test score, baseline:  0.033998305084745765 0.0 0.033998305084745765
maxi score, test score, baseline:  0.033998305084745765 0.0 0.033998305084745765
actor:  0 policy actor:  0  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.30415857523227313, 0.3526303511960668, 0.27259832798312, 0.07061274558853994]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.37797184413736795, 0.3083873415076661, 0.23796230131384363, 0.07567851304112237]
siam score:  -0.62421256
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.37797184413736795, 0.3083873415076661, 0.23796230131384363, 0.07567851304112237]
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.37797184413736795, 0.3083873415076661, 0.23796230131384363, 0.07567851304112237]
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.37797184413736795, 0.3083873415076661, 0.23796230131384363, 0.07567851304112237]
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.37797184413736795, 0.3083873415076661, 0.23796230131384363, 0.07567851304112237]
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.37797184413736795, 0.3083873415076661, 0.23796230131384363, 0.07567851304112237]
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.37797184413736795, 0.3083873415076661, 0.23796230131384363, 0.07567851304112237]
siam score:  -0.6959778
using another actor
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.3752763389485456, 0.3085793038515966, 0.24100467618758253, 0.07513968101227522]
actor:  0 policy actor:  1  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.04928032786885246 0.0 0.04928032786885246
probs:  [0.37527632883498696, 0.30857929912248955, 0.24100467691377503, 0.07513969512874849]
maxi score, test score, baseline:  0.04848709677419355 0.0 0.04848709677419355
probs:  [0.37527632992658355, 0.3085792996329209, 0.2410046768353942, 0.07513969360510135]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.37527632992658355, 0.3085792996329209, 0.2410046768353942, 0.07513969360510135]
maxi score, test score, baseline:  0.04848709677419355 0.0 0.04848709677419355
probs:  [0.36295011156031176, 0.3312838030011343, 0.2330904690040357, 0.07267561643451814]
from probs:  [0.36295011156031176, 0.3312838030011343, 0.2330904690040357, 0.07267561643451814]
UNIT TEST: sample policy line 217 mcts : [0.2 0.4 0.2 0.2 0. ]
maxi score, test score, baseline:  0.046975 0.0 0.046975
probs:  [0.36295011336095023, 0.3312838042969519, 0.23309046873446576, 0.07267561360763203]
maxi score, test score, baseline:  0.046975 0.0 0.046975
probs:  [0.36295011336095023, 0.3312838042969519, 0.23309046873446576, 0.07267561360763203]
maxi score, test score, baseline:  0.046975 0.0 0.046975
probs:  [0.36295011336095023, 0.3312838042969519, 0.23309046873446576, 0.07267561360763203]
maxi score, test score, baseline:  0.046975 0.0 0.046975
maxi score, test score, baseline:  0.046975 0.0 0.046975
maxi score, test score, baseline:  0.046975 0.0 0.046975
probs:  [0.36199467549525227, 0.328472459700755, 0.2370482348066715, 0.07248462999732128]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.3744885884584364, 0.3297347540558402, 0.2207944992600466, 0.07498215822567672]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.04555454545454546 0.0 0.04555454545454546
probs:  [0.35226463449230533, 0.31883961777108494, 0.2583562541803051, 0.07053949355630475]
maxi score, test score, baseline:  0.04555454545454546 0.0 0.04555454545454546
probs:  [0.35226463449230533, 0.31883961777108494, 0.2583562541803051, 0.07053949355630475]
maxi score, test score, baseline:  0.04555454545454546 0.0 0.04555454545454546
probs:  [0.35226463449230533, 0.31883961777108494, 0.2583562541803051, 0.07053949355630475]
actor:  1 policy actor:  1  step number:  55 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.04555454545454546 0.0 0.04555454545454546
probs:  [0.32866924942249, 0.3153041697554095, 0.2902038981855266, 0.06582268263657398]
maxi score, test score, baseline:  0.04555454545454546 0.0 0.04555454545454546
probs:  [0.3250019214791265, 0.31765976477729596, 0.292248739509253, 0.06508957423432454]
from probs:  [0.3250019214791265, 0.31765976477729596, 0.292248739509253, 0.06508957423432454]
maxi score, test score, baseline:  0.044876119402985075 0.0 0.044876119402985075
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.044217647058823535 0.0 0.044217647058823535
probs:  [0.34602240046595634, 0.30393892080841883, 0.2807470384319605, 0.06929164029366414]
maxi score, test score, baseline:  0.044217647058823535 0.0 0.044217647058823535
probs:  [0.34269840733464085, 0.30607137503226534, 0.2826030520093319, 0.06862716562376191]
maxi score, test score, baseline:  0.044217647058823535 0.0 0.044217647058823535
actor:  1 policy actor:  1  step number:  65 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.044217647058823535 0.0 0.044217647058823535
probs:  [0.33048217609010144, 0.295161365534308, 0.3081713819615987, 0.06618507641399199]
maxi score, test score, baseline:  0.044217647058823535 0.0 0.044217647058823535
probs:  [0.33048217609010144, 0.295161365534308, 0.3081713819615987, 0.06618507641399199]
maxi score, test score, baseline:  0.044217647058823535 0.0 0.044217647058823535
probs:  [0.33048217609010144, 0.295161365534308, 0.3081713819615987, 0.06618507641399199]
siam score:  -0.6814163
maxi score, test score, baseline:  0.044217647058823535 0.0 0.044217647058823535
probs:  [0.33048217609010144, 0.295161365534308, 0.3081713819615987, 0.06618507641399199]
from probs:  [0.32700060774170614, 0.29794377463162836, 0.3095665078756595, 0.06548910975100607]
actor:  1 policy actor:  1  step number:  59 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.04357826086956522 0.0 0.04357826086956522
probs:  [0.3148455697015755, 0.28820835097492686, 0.33071455107064274, 0.06623152825285486]
maxi score, test score, baseline:  0.04295714285714286 0.0 0.04295714285714286
probs:  [0.3180596589549753, 0.2875670837082829, 0.32857035724076694, 0.06580290009597495]
maxi score, test score, baseline:  0.042353521126760565 0.0 0.042353521126760565
probs:  [0.32543131450267493, 0.29397180892809954, 0.31542147181985547, 0.06517540474937014]
maxi score, test score, baseline:  0.042353521126760565 0.0 0.042353521126760565
probs:  [0.32234257983786285, 0.29571427830185065, 0.3173851832753074, 0.06455795858497908]
maxi score, test score, baseline:  0.042353521126760565 0.0 0.042353521126760565
probs:  [0.32347566521525284, 0.29323903182704597, 0.31850083450821887, 0.06478446844948242]
maxi score, test score, baseline:  0.042353521126760565 0.0 0.042353521126760565
probs:  [0.32347566521525284, 0.29323903182704597, 0.31850083450821887, 0.06478446844948242]
maxi score, test score, baseline:  0.042353521126760565 0.0 0.042353521126760565
probs:  [0.32347566521525284, 0.29323903182704597, 0.31850083450821887, 0.06478446844948242]
maxi score, test score, baseline:  0.042353521126760565 0.0 0.042353521126760565
probs:  [0.32347566521525284, 0.29323903182704597, 0.31850083450821887, 0.06478446844948242]
maxi score, test score, baseline:  0.042353521126760565 0.0 0.042353521126760565
probs:  [0.32347566521525284, 0.29323903182704597, 0.31850083450821887, 0.06478446844948242]
maxi score, test score, baseline:  0.042353521126760565 0.0 0.042353521126760565
probs:  [0.32347566521525284, 0.29323903182704597, 0.31850083450821887, 0.06478446844948242]
maxi score, test score, baseline:  0.042353521126760565 0.0 0.042353521126760565
probs:  [0.32347566521525284, 0.29323903182704597, 0.31850083450821887, 0.06478446844948242]
maxi score, test score, baseline:  0.042353521126760565 0.0 0.042353521126760565
probs:  [0.32347566521525284, 0.29323903182704597, 0.31850083450821887, 0.06478446844948242]
maxi score, test score, baseline:  0.042353521126760565 0.0 0.042353521126760565
maxi score, test score, baseline:  0.041195890410958905 0.0 0.041195890410958905
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]] [[-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]] [[0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.041195890410958905 0.0 0.041195890410958905
probs:  [0.34461105717629265, 0.2886919044872407, 0.2976875194133582, 0.06900951892310851]
maxi score, test score, baseline:  0.041195890410958905 0.0 0.041195890410958905
probs:  [0.34461105717629265, 0.2886919044872407, 0.2976875194133582, 0.06900951892310851]
maxi score, test score, baseline:  0.041195890410958905 0.0 0.041195890410958905
probs:  [0.3417385385863256, 0.294207541059308, 0.29561861754839147, 0.0684353028059749]
maxi score, test score, baseline:  0.041195890410958905 0.0 0.041195890410958905
probs:  [0.3417385385863256, 0.294207541059308, 0.29561861754839147, 0.0684353028059749]
maxi score, test score, baseline:  0.041195890410958905 0.0 0.041195890410958905
probs:  [0.3415713005850007, 0.2980225521873747, 0.29200426840955035, 0.06840187881807432]
maxi score, test score, baseline:  0.041195890410958905 0.0 0.041195890410958905
probs:  [0.3400485427096023, 0.29731337556972576, 0.29454060262846543, 0.06809747909220652]
maxi score, test score, baseline:  0.041195890410958905 0.0 0.041195890410958905
maxi score, test score, baseline:  0.041195890410958905 0.0 0.041195890410958905
probs:  [0.3400485427096023, 0.29731337556972576, 0.29454060262846543, 0.06809747909220652]
maxi score, test score, baseline:  0.041195890410958905 0.0 0.041195890410958905
probs:  [0.33721814145326506, 0.2992341330780339, 0.2960160434795769, 0.06753168198912403]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.041195890410958905 0.0 0.041195890410958905
probs:  [0.32286049392393945, 0.3306637095610312, 0.2802543946554432, 0.066221401859586]
maxi score, test score, baseline:  0.040640540540540546 0.0 0.040640540540540546
probs:  [0.3242658332174171, 0.32857353760252156, 0.2813570590689151, 0.0658035701111462]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.072]
 [-0.484]
 [-0.484]
 [-0.064]
 [-0.484]] [[0.069]
 [0.   ]
 [0.   ]
 [0.07 ]
 [0.   ]]
maxi score, test score, baseline:  0.040640540540540546 0.0 0.040640540540540546
probs:  [0.32251139554885566, 0.3310518090676337, 0.28013780539799554, 0.06629898998551507]
using another actor
siam score:  -0.71042037
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.31704353300227217, 0.33372422063920104, 0.2823990279101889, 0.06683321844833784]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.31840202609002205, 0.3316641638852538, 0.28351240204410477, 0.06642140798061932]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.31840202609002205, 0.3316641638852538, 0.28351240204410477, 0.06642140798061932]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.31974384279843865, 0.3296293956573851, 0.2846121087920291, 0.06601465275214718]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.31974384279843865, 0.3296293956573851, 0.2846121087920291, 0.06601465275214718]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.31974384279843865, 0.3296293956573851, 0.2846121087920291, 0.06601465275214718]
siam score:  -0.6911232
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.31974384279843865, 0.3296293956573851, 0.2846121087920291, 0.06601465275214718]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.31974384279843865, 0.3296293956573851, 0.2846121087920291, 0.06601465275214718]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.31974384279843865, 0.3296293956573851, 0.2846121087920291, 0.06601465275214718]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.31707819688843075, 0.330921345259943, 0.28572753734118234, 0.06627292051044373]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.31707819688843075, 0.330921345259943, 0.28572753734118234, 0.06627292051044373]
siam score:  -0.66993064
maxi score, test score, baseline:  0.03906103896103896 0.0 0.03906103896103896
probs:  [0.311927063081537, 0.3334179290791339, 0.28788300873037936, 0.06677199910894978]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
from probs:  [0.311927063081537, 0.3334179290791339, 0.28788300873037936, 0.06677199910894978]
maxi score, test score, baseline:  0.0376 0.0 0.0376
probs:  [0.31161238558236337, 0.3340699520378038, 0.2874153112774408, 0.06690235110239207]
maxi score, test score, baseline:  0.0376 0.0 0.0376
probs:  [0.31161238558236337, 0.3340699520378038, 0.2874153112774408, 0.06690235110239207]
siam score:  -0.64766085
maxi score, test score, baseline:  0.0376 0.0 0.0376
probs:  [0.31161238558236337, 0.3340699520378038, 0.2874153112774408, 0.06690235110239207]
from probs:  [0.3128673130361335, 0.33211387448653773, 0.2885074860063682, 0.06651132647096057]
maxi score, test score, baseline:  0.0376 0.0 0.0376
probs:  [0.3104406252344232, 0.33328701107761843, 0.2895265207809509, 0.06674584290700745]
maxi score, test score, baseline:  0.0376 0.0 0.0376
probs:  [0.3104406252344232, 0.33328701107761843, 0.2895265207809509, 0.06674584290700745]
maxi score, test score, baseline:  0.0376 0.0 0.0376
from probs:  [0.3104406252344232, 0.33328701107761843, 0.2895265207809509, 0.06674584290700745]
maxi score, test score, baseline:  0.03713703703703704 0.0 0.03713703703703704
probs:  [0.3104406255236679, 0.33328701147619677, 0.28952652097010906, 0.06674584203002627]
siam score:  -0.6468804
maxi score, test score, baseline:  0.035814285714285715 0.0 0.035814285714285715
probs:  [0.30806580570076286, 0.33443507500471614, 0.29052377580133126, 0.06697534349318976]
using another actor
maxi score, test score, baseline:  0.03498372093023256 0.0 0.03498372093023256
probs:  [0.3104860452956618, 0.3305943345430848, 0.2927120516491006, 0.06620756851215284]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
probs:  [0.3023503512035836, 0.343505082496257, 0.28535610892849966, 0.06878845737165969]
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
actor:  1 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.03458275862068966 0.0 0.03458275862068966
probs:  [0.3127827833689839, 0.3383673458881418, 0.28108847457659436, 0.06776139616627994]
siam score:  -0.6416231
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.03419090909090909 0.0 0.03419090909090909
probs:  [0.32024279892184804, 0.32264866848736695, 0.29248937500532585, 0.06461915758545911]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03380786516853933 0.0 0.03380786516853933
probs:  [0.33716640617924143, 0.3095890656427715, 0.2857232294664524, 0.06752129871153463]
maxi score, test score, baseline:  0.03380786516853933 0.0 0.03380786516853933
maxi score, test score, baseline:  0.03380786516853933 0.0 0.03380786516853933
probs:  [0.3361336735329861, 0.30894974809994386, 0.2876017230052798, 0.06731485536179022]
maxi score, test score, baseline:  0.033433333333333336 0.0 0.033433333333333336
probs:  [0.33703870639701744, 0.3070894918597025, 0.2883760261820973, 0.06749577556118277]
maxi score, test score, baseline:  0.033433333333333336 0.0 0.033433333333333336
maxi score, test score, baseline:  0.033433333333333336 0.0 0.033433333333333336
probs:  [0.33703870639701744, 0.3070894918597025, 0.2883760261820973, 0.06749577556118277]
from probs:  [0.33807601206813837, 0.308034581595285, 0.2861862685241189, 0.06770313781245768]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.3504956936902348, 0.29890020344314105, 0.2804182367874657, 0.07018586607915848]
maxi score, test score, baseline:  0.032708695652173915 0.0 0.032708695652173915
probs:  [0.3496202167526234, 0.3009702367741403, 0.27939868833378917, 0.07001085813944717]
maxi score, test score, baseline:  0.032358064516129034 0.0 0.032358064516129034
probs:  [0.3496202170662203, 0.3009702369345908, 0.27939868842633403, 0.07001085757285491]
maxi score, test score, baseline:  0.032358064516129034 0.0 0.032358064516129034
probs:  [0.3506124687488888, 0.3018243566395811, 0.2773539607224518, 0.07020921388907835]
maxi score, test score, baseline:  0.032358064516129034 0.0 0.032358064516129034
probs:  [0.3506124687488888, 0.3018243566395811, 0.2773539607224518, 0.07020921388907835]
using explorer policy with actor:  1
siam score:  -0.66667414
maxi score, test score, baseline:  0.032358064516129034 0.0 0.032358064516129034
probs:  [0.3497187405513422, 0.3038724175951425, 0.2763782838059162, 0.07003055804759908]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
using another actor
maxi score, test score, baseline:  0.032358064516129034 0.0 0.032358064516129034
maxi score, test score, baseline:  0.032358064516129034 0.0 0.032358064516129034
probs:  [0.3497245820910737, 0.30412248679070303, 0.27612120020275616, 0.0700317309154671]
maxi score, test score, baseline:  0.03201489361702128 0.0 0.03201489361702128
probs:  [0.3497245824052079, 0.30412248696118976, 0.2761212002850384, 0.070031730348564]
siam score:  -0.65704453
maxi score, test score, baseline:  0.03201489361702128 0.0 0.03201489361702128
probs:  [0.3506632896633808, 0.30493873964422635, 0.27417858781315885, 0.07021938287923407]
maxi score, test score, baseline:  0.031678947368421057 0.0 0.031678947368421057
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.031678947368421057 0.0 0.031678947368421057
maxi score, test score, baseline:  0.031678947368421057 0.0 0.031678947368421057
probs:  [0.34884120205386143, 0.30613409691222665, 0.2751695586035105, 0.0698551424304014]
maxi score, test score, baseline:  0.031678947368421057 0.0 0.031678947368421057
probs:  [0.3497298897004626, 0.30436683056238933, 0.2758704839487831, 0.07003279578836488]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.3429159864013183, 0.2984371296847434, 0.2899762240081868, 0.06867065990575148]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.3429159864013183, 0.2984371296847434, 0.2899762240081868, 0.06867065990575148]
siam score:  -0.68599135
maxi score, test score, baseline:  0.03135 0.0 0.03135
first move QE:  0.010416911367135065
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.067]
 [0.031]
 [0.01 ]
 [0.028]] [[0.126]
 [0.129]
 [0.073]
 [0.051]
 [0.189]] [[0.033]
 [0.089]
 [0.043]
 [0.018]
 [0.059]]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.343867389283658, 0.2992650742805039, 0.28800668606665525, 0.06886085036918288]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.35261982164140787, 0.293838718955938, 0.28293097564510755, 0.07061048375754662]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.350862988372152, 0.2949279249246719, 0.2839497983642329, 0.07025928833894317]
maxi score, test score, baseline:  0.03135 0.0 0.03135
probs:  [0.350862988372152, 0.2949279249246719, 0.2839497983642329, 0.07025928833894317]
Sims:  25 1 epoch:  7844 pick best:  False frame count:  7844
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.3530433619641077, 0.29492560467253104, 0.2813358803307195, 0.0706951530326418]
maxi score, test score, baseline:  0.03135 0.0 0.03135
maxi score, test score, baseline:  0.031027835051546392 0.0 0.031027835051546392
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.031027835051546392 0.0 0.031027835051546392
probs:  [0.3590253980333763, 0.29377693202089633, 0.2753066885918762, 0.07189098135385122]
maxi score, test score, baseline:  0.031027835051546392 0.0 0.031027835051546392
probs:  [0.3590253980333763, 0.29377693202089633, 0.2753066885918762, 0.07189098135385122]
maxi score, test score, baseline:  0.031027835051546392 0.0 0.031027835051546392
probs:  [0.3590253980333763, 0.29377693202089633, 0.2753066885918762, 0.07189098135385122]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.030712244897959182 0.0 0.030712244897959182
probs:  [0.3491091874785853, 0.28990284107592346, 0.2910792803807963, 0.06990869106469481]
maxi score, test score, baseline:  0.030712244897959182 0.0 0.030712244897959182
probs:  [0.3491091874785853, 0.28990284107592346, 0.2910792803807963, 0.06990869106469481]
maxi score, test score, baseline:  0.030403030303030303 0.0 0.030403030303030303
probs:  [0.34759066519712967, 0.2908106418097087, 0.29199355896361334, 0.06960513402954822]
siam score:  -0.71882325
maxi score, test score, baseline:  0.030403030303030303 0.0 0.030403030303030303
probs:  [0.34759066519712967, 0.2908106418097087, 0.29199355896361334, 0.06960513402954822]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.161]
 [-0.056]
 [-0.011]
 [-0.067]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.030403030303030303 0.0 0.030403030303030303
maxi score, test score, baseline:  0.030403030303030303 0.0 0.030403030303030303
probs:  [0.34007695123591725, 0.30400463087683555, 0.2878153106285331, 0.06810310725871419]
maxi score, test score, baseline:  0.030403030303030303 0.0 0.030403030303030303
probs:  [0.34007695123591725, 0.30400463087683555, 0.2878153106285331, 0.06810310725871419]
siam score:  -0.7236835
maxi score, test score, baseline:  0.030403030303030303 0.0 0.030403030303030303
probs:  [0.3409148178308832, 0.30229022047237486, 0.2885243605141522, 0.06827060118258979]
maxi score, test score, baseline:  0.030403030303030303 0.0 0.030403030303030303
probs:  [0.3409148178308832, 0.30229022047237486, 0.2885243605141522, 0.06827060118258979]
maxi score, test score, baseline:  0.030403030303030303 0.0 0.030403030303030303
probs:  [0.3417903147147922, 0.3030664826233115, 0.28669758506343807, 0.0684456175984582]
actor:  0 policy actor:  0  step number:  73 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.035100000000000006 0.0 0.035100000000000006
probs:  [0.3426500370478403, 0.3038287593953845, 0.28490371464391684, 0.06861748891285818]
actor:  0 policy actor:  1  step number:  57 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.03970396039603961 0.0 0.03970396039603961
probs:  [0.34265003315580556, 0.3038287571341507, 0.28490371317768465, 0.06861749653235909]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.03970396039603961 0.0 0.03970396039603961
maxi score, test score, baseline:  0.039315686274509806 0.0 0.039315686274509806
maxi score, test score, baseline:  0.039315686274509806 0.0 0.039315686274509806
probs:  [0.33332534048250384, 0.3166419298055486, 0.2832792772199233, 0.06675345249202419]
maxi score, test score, baseline:  0.039315686274509806 0.0 0.039315686274509806
actor:  0 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.04378932038834952 0.0 0.04378932038834952
probs:  [0.33042763078512566, 0.325020464735841, 0.278377711263786, 0.06617419321524727]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.04378932038834952 0.0 0.04378932038834952
probs:  [0.32439384980178576, 0.31908546624261186, 0.2915526758360233, 0.06496800811957909]
maxi score, test score, baseline:  0.04378932038834952 0.0 0.04378932038834952
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.014]
 [0.   ]
 [0.154]
 [0.57 ]] [[0.025]
 [0.028]
 [0.023]
 [0.075]
 [0.213]]
maxi score, test score, baseline:  0.04378932038834952 0.0 0.04378932038834952
probs:  [0.3260026762014313, 0.31570915104947217, 0.2929985516259748, 0.06528962112312164]
maxi score, test score, baseline:  0.04378932038834952 0.0 0.04378932038834952
probs:  [0.3260026762014313, 0.31570915104947217, 0.2929985516259748, 0.06528962112312164]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.04378932038834952 0.0 0.04378932038834952
maxi score, test score, baseline:  0.04378932038834952 0.0 0.04378932038834952
probs:  [0.3218514200003794, 0.3242750724165633, 0.28892924369817163, 0.06494426388488557]
maxi score, test score, baseline:  0.04378932038834952 0.0 0.04378932038834952
maxi score, test score, baseline:  0.04336923076923077 0.0 0.04336923076923077
probs:  [0.3210266716961002, 0.32585246564416837, 0.2878612702112448, 0.06525959244848663]
maxi score, test score, baseline:  0.04336923076923077 0.0 0.04336923076923077
probs:  [0.32097016764480285, 0.32819401163440526, 0.28510814070860707, 0.06572768001218478]
maxi score, test score, baseline:  0.04336923076923077 0.0 0.04336923076923077
probs:  [0.32097016764480285, 0.32819401163440526, 0.28510814070860707, 0.06572768001218478]
from probs:  [0.32097016764480285, 0.32819401163440526, 0.28510814070860707, 0.06572768001218478]
maxi score, test score, baseline:  0.04336923076923077 0.0 0.04336923076923077
probs:  [0.31939098846031694, 0.3289574298589227, 0.28577129028349624, 0.06588029139726417]
maxi score, test score, baseline:  0.04336923076923077 0.0 0.04336923076923077
maxi score, test score, baseline:  0.04295714285714286 0.0 0.04295714285714286
probs:  [0.31878829954050664, 0.32824810524800496, 0.2872250983465632, 0.06573849686492528]
maxi score, test score, baseline:  0.042552830188679246 0.0 0.042552830188679246
probs:  [0.31973506494401377, 0.32679857421787245, 0.28801762975301, 0.06544873108510385]
maxi score, test score, baseline:  0.042552830188679246 0.0 0.042552830188679246
probs:  [0.31973506494401377, 0.32679857421787245, 0.28801762975301, 0.06544873108510385]
maxi score, test score, baseline:  0.042552830188679246 0.0 0.042552830188679246
probs:  [0.31973506494401377, 0.32679857421787245, 0.28801762975301, 0.06544873108510385]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.0635469159343897
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.046828971962616824 0.0 0.046828971962616824
from probs:  [0.3271097977659398, 0.3129693190925297, 0.2944099408336788, 0.06551094230785177]
maxi score, test score, baseline:  0.046828971962616824 0.0 0.046828971962616824
probs:  [0.3257242357378815, 0.3138437673017947, 0.2951980321173809, 0.06523396484294292]
using another actor
maxi score, test score, baseline:  0.0463962962962963 0.0 0.0463962962962963
probs:  [0.3264489665572427, 0.3123173805523784, 0.2958548112889797, 0.06537884160139922]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.33180984077078496, 0.3089532583407871, 0.29278640735371547, 0.06645049353471252]
maxi score, test score, baseline:  0.04597155963302753 0.0 0.04597155963302753
probs:  [0.3387464361311016, 0.3035841674679581, 0.28983225746996727, 0.06783713893097307]
maxi score, test score, baseline:  0.04555454545454546 0.0 0.04555454545454546
probs:  [0.33874643646771335, 0.30358416767120067, 0.28983225762104947, 0.06783713824003659]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.33874643646771335, 0.30358416767120067, 0.28983225762104947, 0.06783713824003659]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.3293231050456587, 0.3211243344983038, 0.2835991923777179, 0.06595336807831954]
maxi score, test score, baseline:  0.045145045045045046 0.0 0.045145045045045046
probs:  [0.32948413036696117, 0.3212263404298567, 0.2833039686474649, 0.06598556055571725]
maxi score, test score, baseline:  0.04434778761061947 0.0 0.04434778761061947
probs:  [0.3281915288305956, 0.3220848081060953, 0.28399649843266556, 0.06572716463064347]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.3353230278260591, 0.31734850314488644, 0.28017569869458236, 0.06715277033447219]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.04357826086956522 0.0 0.04357826086956522
maxi score, test score, baseline:  0.04357826086956522 0.0 0.04357826086956522
probs:  [0.357365655631983, 0.30957416350229755, 0.26146833097072647, 0.07159184989499305]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.10374770148236087
maxi score, test score, baseline:  0.04357826086956522 0.0 0.04357826086956522
probs:  [0.3519971287389916, 0.3199436820026412, 0.257541026568017, 0.07051816269035033]
actor:  1 policy actor:  1  step number:  59 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.6919016
maxi score, test score, baseline:  0.04357826086956522 0.0 0.04357826086956522
probs:  [0.3419794132227299, 0.33929328915701085, 0.2502126441204099, 0.0685146534998492]
using another actor
from probs:  [0.3414035301275571, 0.33860580587070455, 0.25159087184124845, 0.06839979216049]
maxi score, test score, baseline:  0.043203448275862075 0.0 0.043203448275862075
probs:  [0.34234577056202803, 0.33678086332552504, 0.2522851289590314, 0.06858823715341546]
maxi score, test score, baseline:  0.04283504273504274 0.0 0.04283504273504274
probs:  [0.3423457709105498, 0.3367808636530443, 0.2522851289676557, 0.06858823646875026]
maxi score, test score, baseline:  0.042472881355932204 0.0 0.042472881355932204
probs:  [0.3421027653123246, 0.3365831820598364, 0.25277563226695593, 0.06853842036088317]
maxi score, test score, baseline:  0.042472881355932204 0.0 0.042472881355932204
probs:  [0.3421027653123246, 0.3365831820598364, 0.25277563226695593, 0.06853842036088317]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.042472881355932204 0.0 0.042472881355932204
probs:  [0.34793259047649394, 0.3322220051369512, 0.25014221234260614, 0.06970319204394876]
maxi score, test score, baseline:  0.042472881355932204 0.0 0.042472881355932204
probs:  [0.34793259047649394, 0.3322220051369512, 0.25014221234260614, 0.06970319204394876]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.042472881355932204 0.0 0.042472881355932204
probs:  [0.3419348036714711, 0.3264951597754017, 0.2630663457507547, 0.06850369080237248]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.042472881355932204 0.0 0.042472881355932204
probs:  [0.34172239898491824, 0.3173575163654346, 0.27245989771322127, 0.06846018693642598]
maxi score, test score, baseline:  0.042472881355932204 0.0 0.042472881355932204
probs:  [0.34172239898491824, 0.3173575163654346, 0.27245989771322127, 0.06846018693642598]
maxi score, test score, baseline:  0.042472881355932204 0.0 0.042472881355932204
probs:  [0.34172239898491824, 0.3173575163654346, 0.27245989771322127, 0.06846018693642598]
maxi score, test score, baseline:  0.042472881355932204 0.0 0.042472881355932204
probs:  [0.34172239898491824, 0.3173575163654346, 0.27245989771322127, 0.06846018693642598]
maxi score, test score, baseline:  0.04211680672268908 0.0 0.04211680672268908
probs:  [0.34172239929160897, 0.31735751659065703, 0.2724598977883199, 0.06846018632941413]
siam score:  -0.62304264
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.34291684065740635, 0.31827171898253204, 0.27011211307522787, 0.06869932728483376]
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.34377349920975875, 0.3165690369411309, 0.2707868147730344, 0.06887064907607603]
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.34464845007575623, 0.3173747185003285, 0.2689312023056742, 0.06904562911824107]
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.34549991708013017, 0.31568864466931024, 0.26959552559055905, 0.06921591266000046]
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.34549991708013017, 0.31568864466931024, 0.26959552559055905, 0.06921591266000046]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.04142231404958678 0.0 0.04142231404958678
Starting evaluation
maxi score, test score, baseline:  0.03856153846153847 0.0 0.03856153846153847
probs:  [0.3446448701088373, 0.3239188801798468, 0.26239229272074055, 0.06904395699057533]
actor:  0 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.04176666666666667 0.0 0.04176666666666667
probs:  [0.3446448673425698, 0.3239188780193556, 0.2623922923585402, 0.06904396227953437]
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.027]
 [0.121]
 [0.091]
 [0.113]] [[-0.003]
 [-0.003]
 [-0.001]
 [-0.   ]
 [-0.001]] [[0.001]
 [0.027]
 [0.121]
 [0.092]
 [0.113]]
maxi score, test score, baseline:  0.04521278195488722 0.0 0.04521278195488722
probs:  [0.34546937845534, 0.322301819605626, 0.2630199428233526, 0.06920885911568138]
actor:  0 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.05157058823529412 0.0 0.05157058823529412
probs:  [0.3402344113315777, 0.31741806858689214, 0.27418556735325844, 0.06816195272827168]
actor:  0 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.05484452554744526 0.0 0.05484452554744526
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  45 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  47 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.109]
 [0.174]
 [0.164]
 [0.144]] [[-0.]
 [-0.]
 [ 0.]
 [ 0.]
 [ 0.]] [[0.045]
 [0.109]
 [0.174]
 [0.164]
 [0.144]]
maxi score, test score, baseline:  0.059959154929577464 0.0 0.059959154929577464
probs:  [0.33800297945750973, 0.3243822039187417, 0.269899101763669, 0.06771571486007963]
actor:  0 policy actor:  1  step number:  55 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.0626 0.2 0.2
probs:  [0.3331222071652201, 0.31969822893757105, 0.28043942468689936, 0.0667401392103095]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.6083263
maxi score, test score, baseline:  0.0626 0.2 0.2
probs:  [0.33393245288867224, 0.3309323714458541, 0.2681948192611355, 0.06694035640433826]
maxi score, test score, baseline:  0.06216896551724138 0.2 0.2
probs:  [0.33393245288867224, 0.3309323714458541, 0.2681948192611355, 0.06694035640433826]
maxi score, test score, baseline:  0.06216896551724138 0.2 0.2
probs:  [0.33393245288867224, 0.3309323714458541, 0.2681948192611355, 0.06694035640433826]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.06216896551724138 0.2 0.2
probs:  [0.3423560287134049, 0.33835955170228427, 0.2505989085515394, 0.0686855110327714]
actor:  1 policy actor:  1  step number:  59 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.06216896551724138 0.2 0.2
probs:  [0.3325276028431902, 0.350080074063519, 0.24716622198918636, 0.0702261011041044]
siam score:  -0.63037676
maxi score, test score, baseline:  0.06216896551724138 0.2 0.2
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.3325276028431902, 0.350080074063519, 0.24716622198918636, 0.0702261011041044]
maxi score, test score, baseline:  0.06216896551724138 0.2 0.2
probs:  [0.3251359224864176, 0.3422980478455565, 0.2638983388185808, 0.06866769084944506]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.061743835616438356 0.2 0.2
probs:  [0.33652172881156456, 0.33652172881156456, 0.2594456035820486, 0.0675109387948222]
maxi score, test score, baseline:  0.06132448979591837 0.2 0.2
probs:  [0.3349677749530524, 0.3349677749530524, 0.2628694005552487, 0.0671950495386465]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.332]
 [0.506]
 [0.332]
 [0.332]] [[ 0.001]
 [ 0.001]
 [-0.001]
 [ 0.001]
 [ 0.001]] [[0.257]
 [0.257]
 [0.429]
 [0.257]
 [0.257]]
maxi score, test score, baseline:  0.06132448979591837 0.2 0.2
probs:  [0.32922650427586997, 0.32562714403128135, 0.27910095494673454, 0.06604539674611401]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]]
maxi score, test score, baseline:  0.06132448979591837 0.2 0.2
probs:  [0.32922650427586997, 0.32562714403128135, 0.27910095494673454, 0.06604539674611401]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.06132448979591837 0.2 0.2
probs:  [0.3407122695243272, 0.31473584239482727, 0.27621315780508154, 0.06833873027576391]
maxi score, test score, baseline:  0.06132448979591837 0.2 0.2
probs:  [0.33865110889789496, 0.31611174954319976, 0.2773096548474406, 0.06792748671146481]
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.051]
 [0.063]
 [0.072]
 [0.057]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.04 ]
 [0.036]
 [0.047]
 [0.056]
 [0.041]]
maxi score, test score, baseline:  0.06132448979591837 0.2 0.2
probs:  [0.33865110889789496, 0.31611174954319976, 0.2773096548474406, 0.06792748671146481]
maxi score, test score, baseline:  0.06132448979591837 0.2 0.2
actor:  1 policy actor:  1  step number:  71 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  42 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.06091081081081082 0.2 0.2
probs:  [0.3351719810407748, 0.33175139854817964, 0.26585026795689576, 0.06722635245414983]
deleting a thread, now have 2 threads
Frames:  10918 train batches done:  1267 episodes:  529
maxi score, test score, baseline:  0.06091081081081082 0.2 0.2
probs:  [0.3346448509105945, 0.3313038308568147, 0.2669355545913313, 0.06711576364125961]
first move QE:  0.04433259940917061
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.06091081081081082 0.2 0.2
probs:  [0.33699101021874805, 0.3303094887235622, 0.2651140365584145, 0.06758546449927534]
maxi score, test score, baseline:  0.06091081081081082 0.2 0.2
probs:  [0.33699101021874805, 0.3303094887235622, 0.2651140365584145, 0.06758546449927534]
maxi score, test score, baseline:  0.06091081081081082 0.2 0.2
maxi score, test score, baseline:  0.06091081081081082 0.2 0.2
probs:  [0.33699101021874805, 0.3303094887235622, 0.2651140365584145, 0.06758546449927534]
from probs:  [0.33699101021874805, 0.3303094887235622, 0.2651140365584145, 0.06758546449927534]
maxi score, test score, baseline:  0.06091081081081082 0.2 0.2
probs:  [0.33809698842022934, 0.32811211519909333, 0.2659840151564689, 0.06780688122420835]
maxi score, test score, baseline:  0.060502684563758395 0.2 0.2
probs:  [0.33809698842022934, 0.32811211519909333, 0.2659840151564689, 0.06780688122420835]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0601 0.2 0.2
probs:  [0.33809698842022934, 0.32811211519909333, 0.2659840151564689, 0.06780688122420835]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.061]
 [0.16 ]
 [0.061]
 [0.061]] [[-0.002]
 [-0.002]
 [-0.003]
 [-0.002]
 [-0.002]] [[0.043]
 [0.043]
 [0.142]
 [0.043]
 [0.043]]
first move QE:  0.04347436509376526
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.06301390728476822 0.2 0.2
probs:  [0.35491761246248493, 0.34285249463463124, 0.23100559152777086, 0.07122430137511296]
deleting a thread, now have 1 threads
Frames:  11302 train batches done:  1322 episodes:  547
maxi score, test score, baseline:  0.06301390728476822 0.2 0.2
maxi score, test score, baseline:  0.06301390728476822 0.2 0.2
probs:  [0.3564434122717342, 0.34432640427925915, 0.2277002023516865, 0.07152998109732014]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.06301390728476822 0.2 0.2
probs:  [0.36475201899408927, 0.3375391990027504, 0.22452269148770074, 0.07318609051545956]
maxi score, test score, baseline:  0.06301390728476822 0.2 0.2
maxi score, test score, baseline:  0.06301390728476822 0.2 0.2
probs:  [0.36475201899408927, 0.3375391990027504, 0.22452269148770074, 0.07318609051545956]
maxi score, test score, baseline:  0.06301390728476822 0.2 0.2
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.72968376
maxi score, test score, baseline:  0.06545947712418301 0.2 0.2
probs:  [0.360543096755717, 0.34922564884644486, 0.21788828463404894, 0.07234296976378932]
maxi score, test score, baseline:  0.06545947712418301 0.2 0.2
probs:  [0.360543096755717, 0.34922564884644486, 0.21788828463404894, 0.07234296976378932]
maxi score, test score, baseline:  0.06545947712418301 0.2 0.2
probs:  [0.36198163354763196, 0.35061901118498834, 0.2147682214196245, 0.0726311338477553]
using another actor
maxi score, test score, baseline:  0.06503506493506493 0.2 0.2
probs:  [0.3634322431026474, 0.34801739882424443, 0.21562864175574725, 0.07292171631736102]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.06503506493506493 0.2 0.2
probs:  [0.35595951740960335, 0.3408617568218021, 0.2317539271290905, 0.07142479863950407]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using another actor
from probs:  [0.36374144857238766, 0.33461648115180964, 0.22866588896664988, 0.0729761813091529]
maxi score, test score, baseline:  0.06503506493506493 0.2 0.2
probs:  [0.38136078156301234, 0.34221938346474745, 0.19983109345978783, 0.07658874151245244]
maxi score, test score, baseline:  0.06461612903225807 0.2 0.2
probs:  [0.3826600268101114, 0.3378068549721723, 0.20267976200810214, 0.07685335620961417]
maxi score, test score, baseline:  0.06461612903225807 0.2 0.2
probs:  [0.3826600268101114, 0.3378068549721723, 0.20267976200810214, 0.07685335620961417]
maxi score, test score, baseline:  0.06461612903225807 0.2 0.2
probs:  [0.3826600268101114, 0.3378068549721723, 0.20267976200810214, 0.07685335620961417]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.06461612903225807 0.2 0.2
probs:  [0.41770117797801565, 0.35044327053620794, 0.14781845853192993, 0.08403709295384648]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.06461612903225807 0.2 0.2
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.0642025641025641 0.2 0.2
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.0642025641025641 0.2 0.2
probs:  [0.4453594534992524, 0.341821017399704, 0.08963461589143536, 0.12318491320960843]
siam score:  -0.7841603
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.06697898089171975 0.2 0.2
maxi score, test score, baseline:  0.06697898089171975 0.2 0.2
probs:  [0.4453594534992524, 0.341821017399704, 0.08963461589143536, 0.12318491320960843]
actor:  0 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.06972025316455696 0.2 0.2
probs:  [0.4453594534992524, 0.341821017399704, 0.08963461589143536, 0.12318491320960843]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.06972025316455696 0.2 0.2
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.7894645
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.06928238993710692 0.2 0.2
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.06928238993710692 0.2 0.2
probs:  [0.47941269986116253, 0.30350276936128734, 0.09631050019771013, 0.12077403057984006]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.06928238993710692 0.2 0.2
probs:  [0.49343069877079127, 0.30004389712919116, 0.10736457400707074, 0.09916083009294674]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.08 ]
 [0.079]
 [0.08 ]
 [0.056]] [[-0.001]
 [ 0.   ]
 [-0.001]
 [ 0.   ]
 [-0.001]] [[0.058]
 [0.08 ]
 [0.079]
 [0.08 ]
 [0.056]]
maxi score, test score, baseline:  0.06928238993710692 0.2 0.2
probs:  [0.49343069877079127, 0.30004389712919116, 0.10736457400707074, 0.09916083009294674]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.06928238993710692 0.2 0.2
probs:  [0.4991686214013451, 0.29240509134668724, 0.1081359741209902, 0.10029031313097743]
maxi score, test score, baseline:  0.06885000000000001 0.2 0.2
probs:  [0.4991686214013451, 0.29240509134668724, 0.1081359741209902, 0.10029031313097743]
maxi score, test score, baseline:  0.06885000000000001 0.2 0.2
probs:  [0.4860119211176118, 0.2938221443994703, 0.12254125900206063, 0.09762467548085742]
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.06 ]
 [0.06 ]
 [0.071]
 [0.065]] [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[0.043]
 [0.043]
 [0.043]
 [0.054]
 [0.049]]
maxi score, test score, baseline:  0.06885000000000001 0.2 0.2
probs:  [0.4860119211176118, 0.2938221443994703, 0.12254125900206063, 0.09762467548085742]
maxi score, test score, baseline:  0.06885000000000001 0.2 0.2
probs:  [0.4860119211176118, 0.2938221443994703, 0.12254125900206063, 0.09762467548085742]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.06885000000000001 0.2 0.2
probs:  [0.4753243808086337, 0.3093460948546475, 0.11984900802222102, 0.09548051631449772]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.06885000000000001 0.2 0.2
probs:  [0.4809642661548803, 0.3023959912605121, 0.12004466073394449, 0.09659508185066333]
maxi score, test score, baseline:  0.06885000000000001 0.2 0.2
probs:  [0.4809642661548803, 0.3023959912605121, 0.12004466073394449, 0.09659508185066333]
maxi score, test score, baseline:  0.06885000000000001 0.2 0.2
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
actor:  0 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.07417407407407407 0.2 0.2
probs:  [0.48613200924395733, 0.29602771467118955, 0.12022393440473668, 0.09761634168011651]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.07417407407407407 0.2 0.2
probs:  [0.49088451612476774, 0.2901711390272436, 0.12038880315678208, 0.09855554169120655]
maxi score, test score, baseline:  0.07417407407407407 0.2 0.2
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.4935458715218697, 0.28632402909242355, 0.12104088357487784, 0.09908921581082877]
maxi score, test score, baseline:  0.07417407407407407 0.2 0.2
probs:  [0.48412546192899386, 0.2999416584305762, 0.11873271209479465, 0.09720016754563529]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.0737196319018405 0.2 0.2
probs:  [0.47380842795027045, 0.2829425209619025, 0.09515939758274708, 0.1480896535050799]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.07631951219512195 0.2 0.2
probs:  [0.47380842795027045, 0.2829425209619025, 0.09515939758274708, 0.1480896535050799]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.07631951219512195 0.2 0.2
probs:  [0.47885781372145375, 0.277664998178955, 0.09615798708086853, 0.1473192010187227]
maxi score, test score, baseline:  0.07631951219512195 0.2 0.2
probs:  [0.47885781372145375, 0.277664998178955, 0.09615798708086853, 0.1473192010187227]
maxi score, test score, baseline:  0.07631951219512195 0.2 0.2
probs:  [0.47885781372145375, 0.277664998178955, 0.09615798708086853, 0.1473192010187227]
actor:  0 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.0784132530120482 0.2 0.2
probs:  [0.47885781372145375, 0.277664998178955, 0.09615798708086853, 0.1473192010187227]
maxi score, test score, baseline:  0.0784132530120482 0.2 0.2
maxi score, test score, baseline:  0.0784132530120482 0.2 0.2
probs:  [0.47885781372145375, 0.277664998178955, 0.09615798708086853, 0.1473192010187227]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.0784132530120482 0.2 0.2
probs:  [0.4477642985782678, 0.25963956026792656, 0.08992197431810942, 0.20267416683569622]
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.7376602
maxi score, test score, baseline:  0.0784132530120482 0.2 0.2
probs:  [0.43259554985760784, 0.2847136417026119, 0.0868797801441306, 0.1958110282956497]
maxi score, test score, baseline:  0.0784132530120482 0.2 0.2
probs:  [0.43259554985760784, 0.2847136417026119, 0.0868797801441306, 0.1958110282956497]
maxi score, test score, baseline:  0.0784132530120482 0.2 0.2
probs:  [0.43259554985760784, 0.2847136417026119, 0.0868797801441306, 0.1958110282956497]
siam score:  -0.7445396
maxi score, test score, baseline:  0.0784132530120482 0.2 0.2
actor:  1 policy actor:  1  step number:  51 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.0784132530120482 0.2 0.2
probs:  [0.41406412477889587, 0.26468608960884027, 0.08315201941556923, 0.23809776619669454]
first move QE:  0.03911544881647384
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.0784132530120482 0.2 0.2
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.07748095238095239 0.2 0.2
probs:  [0.370607874385616, 0.25530493747682714, 0.07443212064921988, 0.29965506748833703]
maxi score, test score, baseline:  0.07748095238095239 0.2 0.2
probs:  [0.370607874385616, 0.25530493747682714, 0.07443212064921988, 0.29965506748833703]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.07748095238095239 0.2 0.2
probs:  [0.3556172672760615, 0.24498007167181104, 0.07142628586774455, 0.32797637518438294]
maxi score, test score, baseline:  0.07748095238095239 0.2 0.2
probs:  [0.3556172672760615, 0.24498007167181104, 0.07142628586774455, 0.32797637518438294]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.07748095238095239 0.2 0.2
probs:  [0.3413553665417498, 0.23598286568951785, 0.07068767411612903, 0.3519740936526034]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.07748095238095239 0.2 0.2
probs:  [0.3495581106741977, 0.23068048862631846, 0.07020329002528611, 0.3495581106741978]
maxi score, test score, baseline:  0.07748095238095239 0.2 0.2
probs:  [0.3495581106741977, 0.23068048862631846, 0.07020329002528611, 0.3495581106741978]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.07702307692307693 0.2 0.2
probs:  [0.334417349195612, 0.21396113272798212, 0.07548773027728543, 0.37613378779912043]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.07702307692307693 0.2 0.2
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.07702307692307693 0.2 0.2
probs:  [0.3161686517441196, 0.21063075148466623, 0.07906592774783784, 0.3941346690233763]
from probs:  [0.3161686517441196, 0.21063075148466623, 0.07906592774783784, 0.3941346690233763]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.022]
 [0.038]
 [0.022]] [[0.004]
 [0.004]
 [0.004]
 [0.002]
 [0.004]] [[0.016]
 [0.016]
 [0.016]
 [0.031]
 [0.016]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.0761233918128655 0.2 0.2
probs:  [0.3097369273890038, 0.20504514296847368, 0.08107539303486982, 0.4041425366076527]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.0761233918128655 0.2 0.2
probs:  [0.30958086719335115, 0.19899601846144246, 0.08212257886391348, 0.4093005354812929]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.0761233918128655 0.2 0.2
probs:  [0.30665797821204643, 0.20655806468305385, 0.08134826393224746, 0.40543569317265216]
maxi score, test score, baseline:  0.0761233918128655 0.2 0.2
probs:  [0.30665797821204643, 0.20655806468305385, 0.08134826393224746, 0.40543569317265216]
actor:  0 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.07858837209302326 0.2 0.2
probs:  [0.30665797821204643, 0.20655806468305385, 0.08134826393224746, 0.40543569317265216]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.07813468208092486 0.2 0.2
probs:  [0.3038246589634755, 0.21388837769025912, 0.08059767729882497, 0.4016892860474404]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.07768620689655173 0.2 0.2
probs:  [0.2930784020549423, 0.1992957900828034, 0.08481782622915307, 0.4228079816331013]
maxi score, test score, baseline:  0.07768620689655173 0.2 0.2
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.07768620689655173 0.2 0.2
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.0801 0.2 0.2
probs:  [0.2812008918476528, 0.1978105765394153, 0.08702919378961371, 0.4339593378233182]
maxi score, test score, baseline:  0.0801 0.2 0.2
probs:  [0.2812008918476528, 0.1978105765394153, 0.08702919378961371, 0.4339593378233182]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.0801 0.2 0.2
probs:  [0.2761167468305659, 0.19544133429498206, 0.08826661356671285, 0.44017530530773913]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.709]
 [0.51 ]
 [0.514]
 [0.514]] [[-0.]
 [-0.]
 [ 0.]
 [-0.]
 [-0.]] [[0.44 ]
 [0.635]
 [0.436]
 [0.44 ]
 [0.44 ]]
maxi score, test score, baseline:  0.0801 0.2 0.2
probs:  [0.2761167468305659, 0.19544133429498206, 0.08826661356671285, 0.44017530530773913]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.0801 0.2 0.2
probs:  [0.2820914053548256, 0.2017284096682708, 0.08622473581372334, 0.42995544916318024]
maxi score, test score, baseline:  0.0801 0.2 0.2
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.0801 0.2 0.2
probs:  [0.28000170158828575, 0.20764173343938053, 0.08558676343082706, 0.42676980154150673]
maxi score, test score, baseline:  0.0801 0.2 0.2
probs:  [0.28000170158828575, 0.20764173343938053, 0.08558676343082706, 0.42676980154150673]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.7578509
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.745]
 [0.619]
 [0.619]
 [0.619]] [[-0.004]
 [-0.   ]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.617]
 [0.745]
 [0.617]
 [0.617]
 [0.617]]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.08202090395480226 0.2 0.2
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.08202090395480226 0.2 0.2
probs:  [0.2673259775840081, 0.2136147396020604, 0.08669434848033447, 0.432364934333597]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.08202090395480226 0.2 0.2
probs:  [0.26353381949959737, 0.21125405012773094, 0.08771623894078609, 0.43749589143188566]
actor:  0 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.08436966292134832 0.2 0.2
probs:  [0.26261151577204417, 0.21067989877293375, 0.08796477641202181, 0.4387438090430003]
maxi score, test score, baseline:  0.08436966292134832 0.2 0.2
probs:  [0.263234652896581, 0.21285967184963803, 0.08750029278527828, 0.4364053824685027]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.08436966292134832 0.2 0.2
probs:  [0.26229370718516504, 0.21226171524854606, 0.08775587301938612, 0.4376887045469027]
maxi score, test score, baseline:  0.08436966292134832 0.2 0.2
probs:  [0.26229370718516504, 0.21226171524854606, 0.08775587301938612, 0.4376887045469027]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.08436966292134832 0.2 0.2
probs:  [0.2601743084115284, 0.20803066997099107, 0.08882300772392068, 0.4429720138935598]
maxi score, test score, baseline:  0.08436966292134832 0.2 0.2
probs:  [0.2627279982277065, 0.209598997849535, 0.08813866017425193, 0.43953434374850664]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.08436966292134832 0.2 0.2
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.08669217877094973 0.2 0.2
probs:  [0.2609233544729996, 0.21502816613784118, 0.08753393207415255, 0.43651454731500683]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.08669217877094973 0.2 0.2
maxi score, test score, baseline:  0.08669217877094973 0.2 0.2
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.26644245326277693, 0.21023394188417002, 0.08740849963444161, 0.4359151052186115]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.08621111111111111 0.2 0.2
probs:  [0.26373119387081373, 0.21261102842269647, 0.0874601723740352, 0.43619760533245466]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.08621111111111111 0.2 0.2
probs:  [0.26019983374102623, 0.20537875725630564, 0.08926923305180062, 0.4451521759508676]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
main train batch thing paused
add a thread
Adding thread: now have 2 threads
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.26019983374102623, 0.20537875725630564, 0.08926923305180062, 0.4451521759508676]
from probs:  [0.26019983374102623, 0.20537875725630564, 0.08926923305180062, 0.4451521759508676]
maxi score, test score, baseline:  0.09520869565217392 0.2 0.2
probs:  [0.2580454789334004, 0.20409819647537325, 0.08983934385920125, 0.4480169807320251]
actor:  0 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.0973972972972973 0.2 0.2
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.0973972972972973 0.2 0.2
probs:  [0.25520237128945944, 0.2024082370815386, 0.09059172057480847, 0.45179767105419355]
actor:  0 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.09903048128342247 0.2 0.2
probs:  [0.25520237128945944, 0.2024082370815386, 0.09059172057480847, 0.45179767105419355]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.639]
 [0.491]
 [0.353]
 [0.353]] [[-0.   ]
 [-0.001]
 [-0.   ]
 [-0.   ]
 [-0.   ]] [[0.353]
 [0.639]
 [0.491]
 [0.353]
 [0.353]]
maxi score, test score, baseline:  0.09903048128342247 0.2 0.2
probs:  [0.25520237128945944, 0.2024082370815386, 0.09059172057480847, 0.45179767105419355]
line 256 mcts: sample exp_bonus -0.0007219696516094408
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.09903048128342247 0.2 0.2
probs:  [0.2509560186574303, 0.20585095675989806, 0.09072215799231653, 0.45247086659035507]
using another actor
maxi score, test score, baseline:  0.09903048128342247 0.2 0.2
probs:  [0.24898820009957026, 0.20639168003748237, 0.09096024391140967, 0.45365987595153767]
maxi score, test score, baseline:  0.09850425531914894 0.2 0.2
probs:  [0.24898820009957026, 0.20639168003748237, 0.09096024391140967, 0.45365987595153767]
actor:  0 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10062910052910053 0.2 0.2
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.10062910052910053 0.2 0.2
probs:  [0.24430662494720914, 0.20331896059666893, 0.09224732956812268, 0.4601270848879992]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]] [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.10062910052910053 0.2 0.2
probs:  [0.24632957309046774, 0.19554984605179482, 0.09320483851949625, 0.4649157423382412]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -8.867383003234864e-06
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.10010000000000001 0.2 0.2
probs:  [0.2481528205416045, 0.19337687843409748, 0.09326044589125393, 0.465209855133044]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.00950407110256143
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.69]
 [0.75]
 [0.75]
 [0.69]
 [0.69]] [[-0.011]
 [-0.   ]
 [-0.   ]
 [-0.011]
 [-0.011]] [[0.688]
 [0.75 ]
 [0.75 ]
 [0.688]
 [0.688]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.010698995250282313
maxi score, test score, baseline:  0.10219424083769633 0.2 0.2
probs:  [0.2519281787740013, 0.19240601936812912, 0.09279261674394275, 0.4628731851139269]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.10219424083769633 0.2 0.2
probs:  [0.255625797899853, 0.19145515144253047, 0.09233442075398751, 0.460584629903629]
actor:  0 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.10426666666666667 0.2 0.2
probs:  [0.255625797899853, 0.19145515144253047, 0.09233442075398751, 0.460584629903629]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.10426666666666667 0.2 0.2
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.591]
 [0.656]
 [0.591]
 [0.591]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.578]
 [0.578]
 [0.642]
 [0.578]
 [0.578]]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.10426666666666667 0.2 0.2
probs:  [0.25025375921051074, 0.1934040718541419, 0.09290637653604432, 0.463435792399303]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.10426666666666667 0.2 0.2
probs:  [0.24412785399647732, 0.19994320736045534, 0.09283292432648066, 0.4630960143165867]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.10426666666666667 0.2 0.2
probs:  [0.2408349843224531, 0.1994035091225841, 0.09347016454210462, 0.46629134201285816]
maxi score, test score, baseline:  0.10426666666666667 0.2 0.2
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.454]
 [0.551]
 [0.31 ]
 [0.395]] [[0.014]
 [0.014]
 [0.009]
 [0.014]
 [0.013]] [[0.251]
 [0.463]
 [0.557]
 [0.32 ]
 [0.404]]
maxi score, test score, baseline:  0.10631761658031089 0.2 0.2
probs:  [0.23701674576189669, 0.19481144374104373, 0.09487663872461492, 0.47329517177244457]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.10631761658031089 0.2 0.2
probs:  [0.2355844593125917, 0.1939189901481819, 0.09526241532243988, 0.4752341352167866]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.10631761658031089 0.2 0.2
probs:  [0.23899241444914227, 0.19305461071822086, 0.09483813043113443, 0.4731148444015024]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.10834742268041238 0.2 0.2
probs:  [0.23727006893035707, 0.19349145920303695, 0.0950525596568571, 0.4741859122097488]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.2406196171096627, 0.1926418938684458, 0.09463554633706985, 0.47210294268482156]
actor:  0 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.11234489795918368 0.2 0.2
probs:  [0.24274634320253324, 0.1917565361923909, 0.09443060208476348, 0.4710665185203124]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11431319796954315 0.2 0.2
probs:  [0.2431516187397169, 0.1912873484232674, 0.09444342148476201, 0.47111761135225383]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using another actor
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.11373636363636364 0.2 0.2
probs:  [0.24118985406320434, 0.197812494510273, 0.09368218535621903, 0.4673154660703038]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.7734577
maxi score, test score, baseline:  0.11373636363636364 0.2 0.2
probs:  [0.24042960698778904, 0.19910095699094807, 0.09359014335462755, 0.46687929266663536]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11373636363636364 0.2 0.2
probs:  [0.2390741862638262, 0.19823099150913034, 0.09395953066044412, 0.46873529156659927]
maxi score, test score, baseline:  0.11316532663316584 0.2 0.2
probs:  [0.2390741862638262, 0.19823099150913034, 0.09395953066044412, 0.46873529156659927]
actor:  0 policy actor:  0  step number:  54 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.11510000000000001 0.2 0.2
probs:  [0.2422610881402747, 0.19740089969715457, 0.09356640579774686, 0.4667716063648239]
maxi score, test score, baseline:  0.11510000000000001 0.2 0.2
probs:  [0.2422610881402747, 0.19740089969715457, 0.09356640579774686, 0.4667716063648239]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -3.859695258617401e-06
maxi score, test score, baseline:  0.11452786069651742 0.2 0.2
probs:  [0.24160916298865046, 0.1934445751283651, 0.09434053286071242, 0.47060572902227193]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11452786069651742 0.2 0.2
probs:  [0.24024922310659055, 0.19264652941454005, 0.09469864742713793, 0.4724056000517313]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11452786069651742 0.2 0.2
probs:  [0.2389456676540339, 0.19188157145178783, 0.09504191421551751, 0.47413084667866073]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.23769506121885597, 0.19114768525693343, 0.09537123787696078, 0.47578601564724976]
maxi score, test score, baseline:  0.11452786069651742 0.2 0.2
probs:  [0.23707945398654426, 0.1932426910916019, 0.0951244682090021, 0.47455338671285174]
maxi score, test score, baseline:  0.11452786069651742 0.2 0.2
probs:  [0.23707945398654426, 0.1932426910916019, 0.0951244682090021, 0.47455338671285174]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.11643663366336635 0.2 0.2
probs:  [0.240669669979282, 0.1907648845596404, 0.09493891381356563, 0.47362653164751206]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11643663366336635 0.2 0.2
probs:  [0.2439099291451771, 0.19011503490355497, 0.09450708774348726, 0.4714679482077807]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.11586354679802956 0.2 0.2
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11586354679802956 0.2 0.2
probs:  [0.24331949957747975, 0.19169920130415688, 0.09433822349026116, 0.4706430756281022]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11586354679802956 0.2 0.2
probs:  [0.24216547640279418, 0.1910488136364816, 0.09463774210101707, 0.4721479678597072]
actor:  0 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11774705882352941 0.2 0.2
maxi score, test score, baseline:  0.11774705882352941 0.2 0.2
probs:  [0.23998209964444323, 0.1898182999443219, 0.09520442220568325, 0.4749951782055517]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.11774705882352941 0.2 0.2
probs:  [0.2379240320600006, 0.18633676115450015, 0.09613325623960231, 0.47960595054589683]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11660485436893205 0.2 0.2
probs:  [0.2353651132027955, 0.18308455169304835, 0.0970985288552715, 0.48445180624888473]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.23728683992728933, 0.18485581002882004, 0.09648274507797786, 0.4813746049659128]
actor:  0 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.11788846153846154 0.2 0.2
probs:  [0.23493557713134314, 0.1806099627683207, 0.09759285403096493, 0.48686160606937123]
maxi score, test score, baseline:  0.11788846153846154 0.2 0.2
probs:  [0.23493557713134314, 0.1806099627683207, 0.09759285403096493, 0.48686160606937123]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11788846153846154 0.2 0.2
probs:  [0.23686868750472037, 0.17942112917586442, 0.09746728514763696, 0.4862428981717783]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11732488038277512 0.2 0.2
probs:  [0.23494247960603562, 0.17848441206194032, 0.0979421610726621, 0.488630947259362]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.11676666666666667 0.2 0.2
probs:  [0.2367480650678981, 0.18137324854375164, 0.09715911654662879, 0.4847195698417214]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.23858117456111044, 0.1802554576021867, 0.0970385453641591, 0.4841248224725437]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11747089201877935 0.2 0.2
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1192588785046729 0.2 0.2
maxi score, test score, baseline:  0.1192588785046729 0.2 0.2
maxi score, test score, baseline:  0.1192588785046729 0.2 0.2
probs:  [0.24190918507699796, 0.17731683036418106, 0.09697238914782984, 0.48380159541099116]
actor:  0 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.12103023255813954 0.2 0.2
probs:  [0.24000445558293249, 0.18029363952352082, 0.096792406580674, 0.4829094983128727]
maxi score, test score, baseline:  0.12103023255813954 0.2 0.2
probs:  [0.24000445558293249, 0.18029363952352082, 0.096792406580674, 0.4829094983128727]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.12103023255813954 0.2 0.2
maxi score, test score, baseline:  0.12103023255813954 0.2 0.2
probs:  [0.24170223187356416, 0.1792658466883017, 0.09667953719325001, 0.4823523842448841]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.12047037037037037 0.2 0.2
probs:  [0.24411386368150964, 0.1742244877958271, 0.09712150777072269, 0.4845401407519406]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.12047037037037037 0.2 0.2
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.11991566820276497 0.2 0.2
probs:  [0.2469229061499855, 0.17754855072837136, 0.09610012653263353, 0.4794284165890096]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11991566820276497 0.2 0.2
probs:  [0.24503589044459645, 0.18044770881357053, 0.09593017049323684, 0.47858623024859626]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11991566820276497 0.2 0.2
probs:  [0.24416974620444518, 0.1800477597952982, 0.09614026609202773, 0.47964222790822897]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11991566820276497 0.2 0.2
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11991566820276497 0.2 0.2
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11991566820276497 0.2 0.2
probs:  [0.24043244817239473, 0.18188413511561466, 0.09645381486729937, 0.4812296018446912]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.11991566820276497 0.2 0.2
probs:  [0.23975676877284285, 0.18861114116312513, 0.09544592161086338, 0.4761861684531686]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11991566820276497 0.2 0.2
probs:  [0.23850987890742484, 0.19140057805596877, 0.09518816529080758, 0.4749013777457989]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.11991566820276497 0.2 0.2
maxi score, test score, baseline:  0.11991566820276497 0.2 0.2
probs:  [0.23887029755004338, 0.1916113649666344, 0.09509335586068228, 0.47442498162263996]
STARTED EXPV TRAINING ON FRAME NO.  16339
maxi score, test score, baseline:  0.11991566820276497 0.2 0.2
probs:  [0.23994190317896877, 0.1922380818265947, 0.09481146611641343, 0.47300854887802307]
actor:  0 policy actor:  0  step number:  59 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.12110456621004566 0.2 0.2
probs:  [0.23994190317896877, 0.1922380818265947, 0.09481146611641343, 0.47300854887802307]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  16339
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.283]
 [0.636]
 [0.283]
 [0.283]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.189]
 [0.189]
 [0.542]
 [0.189]
 [0.189]]
maxi score, test score, baseline:  0.12110456621004566 0.2 0.2
probs:  [0.23994190317896877, 0.1922380818265947, 0.09481146611641343, 0.47300854887802307]
maxi score, test score, baseline:  0.12110456621004566 0.2 0.2
actor:  1 policy actor:  1  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.242]
 [0.344]
 [0.242]
 [0.242]] [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[0.242]
 [0.242]
 [0.344]
 [0.242]
 [0.242]]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.8064088
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.12055454545454546 0.2 0.2
probs:  [0.2383158651597799, 0.18858515456465766, 0.0956953727717567, 0.4774036075038058]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.026]
 [0.475]
 [0.162]
 [0.118]] [[0.008]
 [0.008]
 [0.007]
 [0.008]
 [0.008]] [[0.052]
 [0.018]
 [0.467]
 [0.154]
 [0.111]]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.12000950226244345 0.2 0.2
probs:  [0.242228944942725, 0.19041601595124505, 0.09473725768529928, 0.47261778142073063]
actor:  0 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.12172162162162163 0.2 0.2
probs:  [0.2406199679399058, 0.19296356836164538, 0.09457975009749668, 0.4718367136009522]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.711]
 [0.679]
 [0.543]
 [0.543]] [[0.004]
 [0.001]
 [0.002]
 [0.003]
 [0.003]] [[0.342]
 [0.508]
 [0.476]
 [0.34 ]
 [0.34 ]]
actor:  0 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.12341838565022421 0.2 0.2
probs:  [0.23906073691290636, 0.19543826329085642, 0.09442612893413134, 0.47107487086210587]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.12286785714285714 0.2 0.2
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.12399380530973451 0.2 0.2
probs:  [0.24266816045047482, 0.20155115309106075, 0.09280389241112874, 0.46297679404733577]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.667]
 [0.643]
 [0.581]
 [0.581]] [[0.006]
 [0.003]
 [0.006]
 [0.006]
 [0.006]] [[0.48 ]
 [0.564]
 [0.542]
 [0.48 ]
 [0.48 ]]
maxi score, test score, baseline:  0.12399380530973451 0.2 0.2
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.208 0.083 0.625]
maxi score, test score, baseline:  0.12399380530973451 0.2 0.2
probs:  [0.24289819604862953, 0.2049220028822153, 0.09220459826790973, 0.45997520280124543]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.12565066079295154 0.2 0.2
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.8238189
maxi score, test score, baseline:  0.12729298245614035 0.2 0.2
probs:  [0.24216607872339313, 0.203071066290597, 0.09263132853894926, 0.4621315264470605]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12892096069868994 0.2 0.2
probs:  [0.24706917109457394, 0.2018413521000049, 0.0920157331294315, 0.45907374367598963]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.24706917109457394, 0.2018413521000049, 0.0920157331294315, 0.45907374367598963]
maxi score, test score, baseline:  0.1283608695652174 0.2 0.2
probs:  [0.24729677460489083, 0.20491067270260027, 0.09146698746975575, 0.4563255652227531]
deleting a thread, now have 1 threads
Frames:  17332 train batches done:  2029 episodes:  913
maxi score, test score, baseline:  0.1283608695652174 0.2 0.2
probs:  [0.24867742309617313, 0.20289606149721084, 0.09157141507996741, 0.45685510032664867]
maxi score, test score, baseline:  0.1278056277056277 0.2 0.2
probs:  [0.24867742309617313, 0.20289606149721084, 0.09157141507996741, 0.45685510032664867]
maxi score, test score, baseline:  0.1278056277056277 0.2 0.2
maxi score, test score, baseline:  0.1278056277056277 0.2 0.2
probs:  [0.24867742309617313, 0.20289606149721084, 0.09157141507996741, 0.45685510032664867]
actor:  0 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.12885536480686693 0.2 0.2
probs:  [0.24867742309617313, 0.20289606149721084, 0.09157141507996741, 0.45685510032664867]
first move QE:  0.02557717155652313
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.12885536480686693 0.2 0.2
probs:  [0.250005518067696, 0.20200405155217446, 0.09149800163275176, 0.4564924287473778]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]]
maxi score, test score, baseline:  0.12885536480686693 0.2 0.2
probs:  [0.2504426681804216, 0.20060865593948762, 0.09165782525587707, 0.45729085062421365]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.12885536480686693 0.2 0.2
probs:  [0.24985073583965078, 0.20025194804200175, 0.09181538256282792, 0.4580819335555194]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.12885536480686693 0.2 0.2
probs:  [0.24927170678866137, 0.1999030158733477, 0.09196950534280886, 0.4588557719951821]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using another actor
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.383]
 [0.332]
 [0.334]
 [0.349]] [[ 0.   ]
 [-0.002]
 [-0.001]
 [-0.002]
 [ 0.   ]] [[0.349]
 [0.383]
 [0.332]
 [0.334]
 [0.349]]
maxi score, test score, baseline:  0.12885536480686693 0.2 0.2
probs:  [0.25115515793378657, 0.1994015684984304, 0.09173900732459438, 0.45770426624318866]
actor:  0 policy actor:  1  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.13044188034188034 0.2 0.2
probs:  [0.25115515793378657, 0.1994015684984304, 0.09173900732459438, 0.45770426624318866]
actor:  0 policy actor:  0  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.13201489361702126 0.2 0.2
probs:  [0.25115515793378657, 0.1994015684984304, 0.09173900732459438, 0.45770426624318866]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.13201489361702126 0.2 0.2
probs:  [0.25058146294944833, 0.19906329472726172, 0.09189047834293712, 0.45846476398035285]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.13201489361702126 0.2 0.2
probs:  [0.2504447748365198, 0.1973708345707493, 0.09219493283469464, 0.4599894577580363]
actor:  0 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.13357457627118643 0.2 0.2
probs:  [0.25089732844721035, 0.19821038499916338, 0.09197820254539771, 0.4589140840082286]
maxi score, test score, baseline:  0.13357457627118643 0.2 0.2
probs:  [0.25089732844721035, 0.19821038499916338, 0.09197820254539771, 0.4589140840082286]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.13357457627118643 0.2 0.2
probs:  [0.2527287498970681, 0.19772587183942106, 0.09175356400425984, 0.4577918142592509]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.13357457627118643 0.2 0.2
probs:  [0.25097986248319476, 0.19517755480392165, 0.09246795841840948, 0.46137462429447407]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]]
maxi score, test score, baseline:  0.13357457627118643 0.2 0.2
probs:  [0.24916685378413994, 0.1924305186093945, 0.0932313901854353, 0.46517123742103017]
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.213]
 [0.291]
 [0.296]
 [0.296]] [[0.004]
 [0.004]
 [0.004]
 [0.003]
 [0.003]] [[0.245]
 [0.2  ]
 [0.278]
 [0.283]
 [0.283]]
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.751]
 [0.75 ]
 [0.717]
 [0.717]] [[0.004]
 [0.   ]
 [0.   ]
 [0.004]
 [0.004]] [[0.717]
 [0.75 ]
 [0.75 ]
 [0.717]
 [0.717]]
maxi score, test score, baseline:  0.13512109704641348 0.2 0.2
probs:  [0.25098434944481585, 0.19196479780026554, 0.0930059385261784, 0.4640449142287402]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.171]
 [0.246]
 [0.171]
 [0.171]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.171]
 [0.171]
 [0.246]
 [0.171]
 [0.171]]
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.13665462184873947 0.2 0.2
probs:  [0.2527775071053225, 0.1915053134369101, 0.09278350587964318, 0.4629336735781243]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.13665462184873947 0.2 0.2
probs:  [0.2522549776394942, 0.19123342215824318, 0.092915443049054, 0.4635961571532086]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13665462184873947 0.2 0.2
maxi score, test score, baseline:  0.13665462184873947 0.2 0.2
probs:  [0.2517428507118467, 0.19096694370237816, 0.09304475360784857, 0.4642454519779266]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.433]
 [0.534]
 [0.433]
 [0.433]] [[-0.002]
 [-0.002]
 [ 0.001]
 [-0.002]
 [-0.002]] [[0.391]
 [0.391]
 [0.494]
 [0.391]
 [0.391]]
maxi score, test score, baseline:  0.13665462184873947 0.2 0.2
probs:  [0.2510247585350066, 0.19327478810891474, 0.09277961476790184, 0.4629208385881769]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.13665462184873947 0.2 0.2
maxi score, test score, baseline:  0.13665462184873947 0.2 0.2
probs:  [0.24922025516010604, 0.19334163960526699, 0.09306888641600763, 0.46436921881861937]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.13665462184873947 0.2 0.2
maxi score, test score, baseline:  0.13665462184873947 0.2 0.2
maxi score, test score, baseline:  0.13665462184873947 0.2 0.2
probs:  [0.2467464026235622, 0.195671503026897, 0.09309237859058048, 0.46448971575896036]
from probs:  [0.2467464026235622, 0.195671503026897, 0.09309237859058048, 0.46448971575896036]
maxi score, test score, baseline:  0.13665462184873947 0.2 0.2
probs:  [0.24721068406999627, 0.19649636926591524, 0.09287620020267043, 0.46341674646141806]
maxi score, test score, baseline:  0.13665462184873947 0.2 0.2
probs:  [0.24721068406999627, 0.19649636926591524, 0.09287620020267043, 0.46341674646141806]
maxi score, test score, baseline:  0.13608326359832634 0.2 0.2
probs:  [0.24721068406999627, 0.19649636926591524, 0.09287620020267043, 0.46341674646141806]
first move QE:  0.02452768217322404
actor:  0 policy actor:  0  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
first move QE:  0.02450530739001997
maxi score, test score, baseline:  0.1376 0.2 0.2
probs:  [0.24547303958412137, 0.19655601592428038, 0.09315554728189511, 0.46481539720970305]
maxi score, test score, baseline:  0.1376 0.2 0.2
probs:  [0.24547303958412137, 0.19655601592428038, 0.09315554728189511, 0.46481539720970305]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1376 0.2 0.2
probs:  [0.2472056901208744, 0.19610473171743173, 0.09294184733075118, 0.4637477308309427]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.13910414937759336 0.2 0.2
probs:  [0.24760380210772695, 0.19481003907991315, 0.09309137462577402, 0.46449478418658585]
actor:  0 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.14059586776859503 0.2 0.2
maxi score, test score, baseline:  0.14059586776859503 0.2 0.2
probs:  [0.24760380210772695, 0.19481003907991315, 0.09309137462577402, 0.46449478418658585]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.14001769547325102 0.2 0.2
probs:  [0.24878775224017097, 0.19257707121051104, 0.09326535049998506, 0.4653698260493328]
maxi score, test score, baseline:  0.14001769547325102 0.2 0.2
probs:  [0.24878775224017097, 0.19257707121051104, 0.09326535049998506, 0.4653698260493328]
maxi score, test score, baseline:  0.14001769547325102 0.2 0.2
probs:  [0.24878775224017097, 0.19257707121051104, 0.09326535049998506, 0.4653698260493328]
maxi score, test score, baseline:  0.14001769547325102 0.2 0.2
probs:  [0.24878775224017097, 0.19257707121051104, 0.09326535049998506, 0.4653698260493328]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.24878775224017097, 0.19257707121051104, 0.09326535049998506, 0.4653698260493328]
maxi score, test score, baseline:  0.14001769547325102 0.2 0.2
probs:  [0.24833794893077255, 0.19233158060023972, 0.09338083480305133, 0.46594963566593633]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1414934426229508 0.2 0.2
probs:  [0.24833794893077255, 0.19233158060023972, 0.09338083480305133, 0.46594963566593633]
maxi score, test score, baseline:  0.1414934426229508 0.2 0.2
probs:  [0.24833794893077255, 0.19233158060023972, 0.09338083480305133, 0.46594963566593633]
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.627]
 [0.413]
 [0.406]] [[-0.005]
 [-0.005]
 [-0.006]
 [-0.005]
 [-0.006]] [[0.275]
 [0.275]
 [0.489]
 [0.275]
 [0.268]]
maxi score, test score, baseline:  0.1414934426229508 0.2 0.2
probs:  [0.24833794893077255, 0.19233158060023972, 0.09338083480305133, 0.46594963566593633]
maxi score, test score, baseline:  0.1414934426229508 0.2 0.2
maxi score, test score, baseline:  0.1414934426229508 0.2 0.2
probs:  [0.24833794893077255, 0.19233158060023972, 0.09338083480305133, 0.46594963566593633]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.24833794893077255, 0.19233158060023972, 0.09338083480305133, 0.46594963566593633]
siam score:  -0.85010904
maxi score, test score, baseline:  0.1414934426229508 0.2 0.2
probs:  [0.24789648874205708, 0.19209064344198895, 0.09349417705990021, 0.46651869075605373]
actor:  0 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.8497638
maxi score, test score, baseline:  0.14295714285714284 0.2 0.2
probs:  [0.24789648874205708, 0.19209064344198895, 0.09349417705990021, 0.46651869075605373]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.14295714285714284 0.2 0.2
probs:  [0.2491294153028627, 0.19142940723456828, 0.09339838254602693, 0.46604279491654216]
maxi score, test score, baseline:  0.14295714285714284 0.2 0.2
probs:  [0.2491294153028627, 0.19142940723456828, 0.09339838254602693, 0.46604279491654216]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.1444089430894309 0.2 0.2
probs:  [0.24701207766008257, 0.18858052131915123, 0.09422893455389975, 0.4701784664668664]
maxi score, test score, baseline:  0.1444089430894309 0.2 0.2
probs:  [0.24701207766008257, 0.18858052131915123, 0.09422893455389975, 0.4701784664668664]
maxi score, test score, baseline:  0.1444089430894309 0.2 0.2
probs:  [0.24701207766008257, 0.18858052131915123, 0.09422893455389975, 0.4701784664668664]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.1444089430894309 0.2 0.2
probs:  [0.2465932931743797, 0.18836235426934533, 0.09433471186900551, 0.4707096406872695]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.1444089430894309 0.2 0.2
probs:  [0.24577794061033303, 0.1879375938746255, 0.09454065504033542, 0.4717438104747061]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1444089430894309 0.2 0.2
probs:  [0.24743435274084233, 0.1875249310779433, 0.09433323169917429, 0.47070748448204014]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.145848987854251 0.2 0.2
probs:  [0.2474146105654305, 0.18606390868368158, 0.09457975759381769, 0.47194172315707017]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.145848987854251 0.2 0.2
probs:  [0.2470182150714454, 0.18586623613250355, 0.09467841416630353, 0.47243713462974746]
maxi score, test score, baseline:  0.145848987854251 0.2 0.2
probs:  [0.2470182150714454, 0.18586623613250355, 0.09467841416630353, 0.47243713462974746]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.2463613589114107, 0.1880312502675646, 0.09442689694451735, 0.4711804938765073]
maxi score, test score, baseline:  0.145848987854251 0.2 0.2
probs:  [0.24597491570921562, 0.18783060128656728, 0.0945243984098456, 0.4716700845943715]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.14526129032258064 0.2 0.2
probs:  [0.24601965780206153, 0.1878538322692479, 0.09451310976615995, 0.47161340016253067]
maxi score, test score, baseline:  0.14526129032258064 0.2 0.2
probs:  [0.24601965780206153, 0.1878538322692479, 0.09451310976615995, 0.47161340016253067]
maxi score, test score, baseline:  0.14526129032258064 0.2 0.2
probs:  [0.24681968551050412, 0.18826922253502762, 0.09431125895488092, 0.47059983299958735]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.14668634538152608 0.2 0.2
probs:  [0.2480185340300486, 0.18858616629228495, 0.0940593158076856, 0.46933598386998093]
actor:  0 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [0.24964108390676526, 0.1881793342055471, 0.09385656629297029, 0.4683230155947173]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.155]
 [0.294]
 [0.482]
 [0.273]] [[0.162]
 [0.098]
 [0.099]
 [0.021]
 [0.162]] [[0.216]
 [0.15 ]
 [0.29 ]
 [0.464]
 [0.279]]
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [0.24964108390676526, 0.1881793342055471, 0.09385656629297029, 0.4683230155947173]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [0.2512433743675628, 0.18777758188897406, 0.09365634834095453, 0.4673226954025086]
actor:  0 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.149502390438247 0.2 0.2
probs:  [0.2512433743675628, 0.18777758188897406, 0.09365634834095453, 0.4673226954025086]
maxi score, test score, baseline:  0.149502390438247 0.2 0.2
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.2512433743675628, 0.18777758188897406, 0.09365634834095453, 0.4673226954025086]
maxi score, test score, baseline:  0.1489095238095238 0.2 0.2
probs:  [0.2508450686830459, 0.18757957420001875, 0.09375538719759785, 0.4678199699193375]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
probs:  [0.24979572863708438, 0.1895188300416396, 0.09360657138749569, 0.4670788699337803]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
probs:  [0.24994391479967767, 0.19204967168185724, 0.09316061199820247, 0.46484580152026267]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.579]
 [0.582]
 [0.579]
 [0.514]] [[ 0.   ]
 [ 0.   ]
 [-0.059]
 [ 0.   ]
 [-0.056]] [[0.434]
 [0.434]
 [0.418]
 [0.434]
 [0.351]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
probs:  [0.2512588393765979, 0.19248852815729656, 0.09286812772660917, 0.4633845047394964]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.007299386916006915
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
probs:  [0.2512588393765979, 0.19248852815729656, 0.09286812772660917, 0.4633845047394964]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.228]
 [0.515]
 [0.412]
 [0.352]] [[-0.384]
 [-0.053]
 [-0.201]
 [-0.434]
 [-0.373]] [[0.241]
 [0.385]
 [0.574]
 [0.315]
 [0.296]]
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
probs:  [0.2508697771922824, 0.1922806184130634, 0.09296728592613172, 0.46388231846852246]
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
probs:  [0.25166689603144954, 0.19270658829822784, 0.09276412850789947, 0.4628623871624232]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
probs:  [0.25322766331343755, 0.19230473932837808, 0.09257085072278967, 0.4618967466353947]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
first move QE:  0.024592132209366062
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
probs:  [0.2528340207198616, 0.19209817704644075, 0.09267054831383294, 0.4623972539198648]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
probs:  [0.253641528709815, 0.19312569379722358, 0.09236380688972644, 0.4608689706032349]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.150297628458498 0.2 0.2
probs:  [0.2529993633282081, 0.1951685934731832, 0.09213020322044148, 0.4597018399781673]
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.8668374
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25101791813551744, 0.19199604769360015, 0.09299289680477701, 0.4639931373661053]
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25101791813551744, 0.19199604769360015, 0.09299289680477701, 0.4639931373661053]
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25101791813551744, 0.19199604769360015, 0.09299289680477701, 0.4639931373661053]
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25101791813551744, 0.19199604769360015, 0.09299289680477701, 0.4639931373661053]
actor:  1 policy actor:  1  step number:  79 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]]
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25141626134220885, 0.19220776698376685, 0.09289157356293283, 0.4634843981110914]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25298522809558704, 0.1918049883520967, 0.09269707821177096, 0.4625127053405454]
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25298522809558704, 0.1918049883520967, 0.09269707821177096, 0.4625127053405454]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25417287835922514, 0.19096734612506516, 0.09263953713274486, 0.4622202383829648]
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25417287835922514, 0.19096734612506516, 0.09263953713274486, 0.4622202383829648]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.2574230714182854, 0.19266173018614613, 0.09181571046495018, 0.4580994879306183]
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25780362326775497, 0.1934896717849429, 0.09161299488137985, 0.4570937100659223]
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.2586116075901747, 0.19390625611141454, 0.09140958105802671, 0.456072555240384]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25819533305154846, 0.19369163134272893, 0.09151438011552941, 0.4565986554901933]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.2573825162675467, 0.1932725554731698, 0.09171901053363483, 0.45762591772564865]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25673326881241476, 0.19530757646792823, 0.09148789284474518, 0.4564712618749119]
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25673326881241476, 0.19530757646792823, 0.09148789284474518, 0.4564712618749119]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.2582424373449324, 0.1949110779916886, 0.09130232303514046, 0.4555441616282385]
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
siam score:  -0.8553029
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25663321038842885, 0.19502031505728312, 0.0915520888438054, 0.45679438571048253]
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25663321038842885, 0.19502031505728312, 0.0915520888438054, 0.45679438571048253]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.25812440729358527, 0.19462916756490659, 0.0913686246947429, 0.4558778004467653]
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
maxi score, test score, baseline:  0.1516748031496063 0.2 0.2
probs:  [0.2585165356823098, 0.19340566517121213, 0.0915072796702399, 0.45657051947623817]
actor:  0 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.15304117647058824 0.2 0.2
probs:  [0.25890510402341793, 0.19360469048272344, 0.0914096705096997, 0.456080534984159]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.192]
 [0.473]
 [0.325]
 [0.344]] [[0.114]
 [0.443]
 [0.195]
 [0.007]
 [0.143]] [[0.202]
 [0.191]
 [0.43 ]
 [0.251]
 [0.292]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.2592884754386703, 0.1938010539206905, 0.09131336682754385, 0.4555971038130954]
maxi score, test score, baseline:  0.15304117647058824 0.2 0.2
probs:  [0.2607704028872806, 0.19341338438577454, 0.0911308670282498, 0.4546853456986951]
siam score:  -0.8536807
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.15244375 0.2 0.2
maxi score, test score, baseline:  0.15244375 0.2 0.2
probs:  [0.2590513211520552, 0.19325756212490153, 0.0914427783392342, 0.45624833838380907]
from probs:  [0.2590513211520552, 0.19325756212490153, 0.0914427783392342, 0.45624833838380907]
maxi score, test score, baseline:  0.15244375 0.2 0.2
maxi score, test score, baseline:  0.15244375 0.2 0.2
probs:  [0.2598204439628311, 0.19364908599705485, 0.09124997392764138, 0.45528049611247257]
actor:  0 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.535]
 [0.62 ]
 [0.545]
 [0.559]] [[ 0.137]
 [ 0.011]
 [-0.196]
 [-0.196]
 [-0.063]] [[0.513]
 [0.478]
 [0.425]
 [0.35 ]
 [0.453]]
maxi score, test score, baseline:  0.1537964980544747 0.2 0.2
probs:  [0.25993791853952936, 0.19606171068028033, 0.09082877256133993, 0.45317159821885045]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.15513875968992247 0.2 0.2
probs:  [0.2574926170597147, 0.1954506894746633, 0.09134089700149858, 0.4557157964641234]
Starting evaluation
actor:  0 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.16924498141263938 0.2 0.2
probs:  [0.2574926170597147, 0.1954506894746633, 0.09134089700149858, 0.4557157964641234]
actor:  0 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.17995611510791365 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.019548822554056895
maxi score, test score, baseline:  0.17995611510791365 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17995611510791365 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17995611510791365 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
line 256 mcts: sample exp_bonus 2.39872428487557
maxi score, test score, baseline:  0.17995611510791365 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.375 0.125 0.333 0.042 0.125]
maxi score, test score, baseline:  0.17995611510791365 0.5 0.5
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.17995611510791365 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.17995611510791365 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.21579751007905223
siam score:  -0.8669742
maxi score, test score, baseline:  0.17995611510791365 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.18110358422939066 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  85 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18110358422939066 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18110358422939066 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.18110358422939066 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18110358422939066 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18110358422939066 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.479]
 [0.357]
 [0.357]] [[ 0.5  ]
 [ 0.5  ]
 [-0.048]
 [ 0.5  ]
 [ 0.5  ]] [[0.34]
 [0.34]
 [0.28]
 [0.34]
 [0.34]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18110358422939066 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  68 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.369]
 [0.522]
 [0.446]
 [0.464]] [[-1.02 ]
 [-0.854]
 [-1.327]
 [-1.111]
 [-1.047]] [[0.296]
 [0.341]
 [0.178]
 [0.245]
 [0.307]]
maxi score, test score, baseline:  0.18110358422939066 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18110358422939066 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.272]
 [0.415]
 [0.34 ]
 [0.353]] [[ 0.847]
 [ 0.218]
 [-0.142]
 [ 0.078]
 [ 0.301]] [[0.347]
 [0.272]
 [0.415]
 [0.34 ]
 [0.353]]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.44 ]
 [0.39 ]
 [0.371]
 [0.39 ]] [[-0.33 ]
 [-0.561]
 [-0.541]
 [-0.352]
 [-0.279]] [[0.378]
 [0.44 ]
 [0.39 ]
 [0.371]
 [0.39 ]]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.18159466192170817 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8581148
maxi score, test score, baseline:  0.18159466192170817 0.5 0.5
maxi score, test score, baseline:  0.18159466192170817 0.5 0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18159466192170817 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18159466192170817 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.017473535770490965
actor:  0 policy actor:  0  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1827241134751773 0.5 0.5
maxi score, test score, baseline:  0.1827241134751773 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1827241134751773 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1827241134751773 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8475687
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.18384558303886925 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  62 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.18495915492957746 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18495915492957746 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.18495915492957746 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 1.9453743931042808
maxi score, test score, baseline:  0.18495915492957746 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18495915492957746 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18495915492957746 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.18606491228070174 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.18606491228070174 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.18606491228070174 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.423]
 [0.459]
 [0.393]
 [0.41 ]] [[-0.435]
 [ 0.085]
 [-0.298]
 [-0.467]
 [-0.118]] [[0.258]
 [0.634]
 [0.415]
 [0.236]
 [0.486]]
maxi score, test score, baseline:  0.18606491228070174 0.5 0.5
maxi score, test score, baseline:  0.18606491228070174 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18606491228070174 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18606491228070174 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18606491228070174 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18606491228070174 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18606491228070174 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18716293706293705 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18716293706293705 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.    0.917 0.042 0.    0.042]
maxi score, test score, baseline:  0.18716293706293705 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18716293706293705 0.5 0.5
siam score:  -0.870363
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.18825331010452961 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18825331010452961 0.5 0.5
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1876 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1876 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86798644
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
first move QE:  0.020876496449566153
maxi score, test score, baseline:  0.1876 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.1876 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1876 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1876 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.18695121107266435 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18695121107266435 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.37 ]
 [0.588]
 [0.37 ]
 [0.365]] [[3.058]
 [3.058]
 [0.467]
 [3.058]
 [2.505]] [[1.066]
 [1.066]
 [0.303]
 [1.066]
 [0.877]]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18845616438356164 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18845616438356164 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18845616438356164 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.18951979522184298 0.5 0.5
maxi score, test score, baseline:  0.18951979522184298 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.00405961329542108
actor:  0 policy actor:  0  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19266756756756756 0.5 0.5
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19266756756756756 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.1937026936026936 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1937026936026936 0.5 0.5
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.1937026936026936 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1937026936026936 0.5 0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.1940799331103679 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1940799331103679 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1940799331103679 0.5 0.5
maxi score, test score, baseline:  0.1940799331103679 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.673]
 [0.491]
 [0.61 ]] [[1.279]
 [1.279]
 [0.206]
 [1.221]
 [1.096]] [[0.905]
 [0.905]
 [0.627]
 [0.953]
 [1.009]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.1940799331103679 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.19343333333333332 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19343333333333332 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19343333333333332 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.19445215946843852 0.5 0.5
maxi score, test score, baseline:  0.19445215946843852 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19445215946843852 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  53 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.19546423841059601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.19546423841059601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19546423841059601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1948194719471947 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1948194719471947 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1948194719471947 0.5 0.5
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1948194719471947 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1948194719471947 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.1948194719471947 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.1948194719471947 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1948194719471947 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.1948194719471947 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.19682131147540982 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.295]
 [0.295]
 [0.295]
 [0.295]] [[1.823]
 [1.944]
 [1.944]
 [1.944]
 [1.944]] [[0.325]
 [0.395]
 [0.395]
 [0.395]
 [0.395]]
maxi score, test score, baseline:  0.19682131147540982 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19682131147540982 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19682131147540982 0.5 0.5
maxi score, test score, baseline:  0.19682131147540982 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.19682131147540982 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.19682131147540982 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.19682131147540982 0.5 0.5
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.196178431372549 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19553973941368077 0.5 0.5
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.19553973941368077 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19553973941368077 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19553973941368077 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.084]
 [0.277]
 [0.238]
 [0.169]] [[1.204]
 [0.48 ]
 [1.168]
 [0.667]
 [1.859]] [[0.277]
 [0.03 ]
 [0.337]
 [0.215]
 [0.345]]
maxi score, test score, baseline:  0.19553973941368077 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  96 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.19553973941368077 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19553973941368077 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19553973941368077 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.434]
 [0.639]
 [0.515]
 [0.532]] [[3.153]
 [2.995]
 [0.839]
 [2.029]
 [2.728]] [[0.785]
 [0.767]
 [0.384]
 [0.587]
 [0.773]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.19553973941368077 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19553973941368077 0.5 0.5
maxi score, test score, baseline:  0.19553973941368077 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.    0.417 0.25  0.083 0.25 ]
first move QE:  0.05025535551976499
actor:  0 policy actor:  0  step number:  64 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.1965285714285714 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.19751100323624593 0.5 0.5
maxi score, test score, baseline:  0.19751100323624593 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19751100323624593 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19751100323624593 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19751100323624593 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19751100323624593 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  86 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.434]
 [0.346]
 [0.346]] [[1.426]
 [1.426]
 [0.999]
 [1.426]
 [1.426]] [[0.556]
 [0.556]
 [0.501]
 [0.556]
 [0.556]]
rdn beta is 0 so we're just using the maxi policy
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9974997761255645
maxi score, test score, baseline:  0.1968741935483871 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1968741935483871 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87371385
maxi score, test score, baseline:  0.1968741935483871 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1968741935483871 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1968741935483871 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1968741935483871 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  72 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.1968741935483871 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  67 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]] [[0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
maxi score, test score, baseline:  0.1972153846153846 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19818306709265174 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  72 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1985126984126984 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1985126984126984 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19788481012658227 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
first move QE:  0.06752141821189923
maxi score, test score, baseline:  0.19883817034700316 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19883817034700316 0.5 0.5
actor:  1 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.19883817034700316 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19883817034700316 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19883817034700316 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.19978553459119497 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.19978553459119497 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.445]
 [0.425]
 [0.404]
 [0.325]] [[ 0.922]
 [-0.019]
 [-0.002]
 [-0.134]
 [ 1.698]] [[0.621]
 [0.216]
 [0.202]
 [0.138]
 [0.668]]
maxi score, test score, baseline:  0.19978553459119497 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  38 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.20072695924764888 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20072695924764888 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.20072695924764888 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.20259221183800621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20259221183800621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.881403
maxi score, test score, baseline:  0.20259221183800621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.463]
 [0.535]
 [0.463]
 [0.463]] [[1.068]
 [1.068]
 [1.181]
 [1.068]
 [1.068]] [[0.423]
 [0.423]
 [0.507]
 [0.423]
 [0.423]]
maxi score, test score, baseline:  0.20196335403726706 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20196335403726706 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[1.324]
 [1.324]
 [1.324]
 [1.324]
 [1.324]] [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]]
maxi score, test score, baseline:  0.20196335403726706 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.881521
maxi score, test score, baseline:  0.20196335403726706 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.34 ]
 [0.336]
 [0.34 ]
 [0.34 ]] [[2.546]
 [2.546]
 [5.456]
 [2.546]
 [2.546]] [[0.332]
 [0.332]
 [0.994]
 [0.332]
 [0.332]]
maxi score, test score, baseline:  0.20196335403726706 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20196335403726706 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.20196335403726706 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.20288637770897833 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.20380370370370368 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20380370370370368 0.5 0.5
maxi score, test score, baseline:  0.20380370370370368 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20380370370370368 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8805613
maxi score, test score, baseline:  0.20380370370370368 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.20317692307692306 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20317692307692306 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20317692307692306 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20317692307692306 0.5 0.5
maxi score, test score, baseline:  0.20317692307692306 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20317692307692306 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.335]
 [0.517]
 [0.335]
 [0.336]] [[1.801]
 [2.086]
 [1.486]
 [2.086]
 [2.331]] [[0.472]
 [0.591]
 [0.573]
 [0.591]
 [0.673]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.622]
 [0.669]
 [0.622]
 [0.622]] [[ 1.637]
 [ 1.637]
 [-0.121]
 [ 1.637]
 [ 1.637]] [[0.652]
 [0.652]
 [0.405]
 [0.652]
 [0.652]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20317692307692306 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.071]
 [0.281]
 [0.301]
 [0.265]] [[1.953]
 [1.376]
 [3.286]
 [2.151]
 [2.887]] [[0.343]
 [0.024]
 [0.805]
 [0.475]
 [0.668]]
maxi score, test score, baseline:  0.20317692307692306 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.20317692307692306 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20317692307692306 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.445]] [[6.644]
 [6.644]
 [6.644]
 [6.644]
 [5.892]] [[0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.708]]
maxi score, test score, baseline:  0.20408773006134967 0.5 0.5
maxi score, test score, baseline:  0.20408773006134967 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.20589268292682925 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.20589268292682925 0.5 0.5
maxi score, test score, baseline:  0.20589268292682925 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8719002
maxi score, test score, baseline:  0.20589268292682925 0.5 0.5
maxi score, test score, baseline:  0.20589268292682925 0.5 0.5
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.2067869300911854 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2067869300911854 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2067869300911854 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.21761662548798358
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.20767575757575757 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20767575757575757 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20767575757575757 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20767575757575757 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.20855921450151058 0.5 0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20855921450151058 0.5 0.5
maxi score, test score, baseline:  0.20855921450151058 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20855921450151058 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20855921450151058 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.2103102102102102 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2103102102102102 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2103102102102102 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8620875
maxi score, test score, baseline:  0.2103102102102102 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.364]
 [0.366]
 [0.394]
 [0.414]] [[1.034]
 [0.298]
 [0.846]
 [0.743]
 [0.797]] [[0.364]
 [0.364]
 [0.366]
 [0.394]
 [0.414]]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.21117784431137723 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.8432701
maxi score, test score, baseline:  0.21117784431137723 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21117784431137723 0.5 0.5
siam score:  -0.840244
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21117784431137723 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.21117784431137723 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21117784431137723 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21117784431137723 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21117784431137723 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21117784431137723 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  46 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.21204029850746267 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2122661721068249 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2122661721068249 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.336]
 [0.417]
 [0.336]
 [0.325]] [[4.006]
 [3.441]
 [4.385]
 [3.441]
 [5.068]] [[0.615]
 [0.516]
 [0.801]
 [0.516]
 [0.898]]
maxi score, test score, baseline:  0.2122661721068249 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2122661721068249 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.359]
 [0.451]
 [0.359]
 [0.336]] [[5.319]
 [5.319]
 [3.302]
 [5.319]
 [5.467]] [[0.806]
 [0.806]
 [0.371]
 [0.806]
 [0.832]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2122661721068249 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2122661721068249 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2122661721068249 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2122661721068249 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2122661721068249 0.5 0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21311775147928994 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21311775147928994 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  28093 train batches done:  3289 episodes:  1343
maxi score, test score, baseline:  0.21311775147928994 0.5 0.5
maxi score, test score, baseline:  0.21311775147928994 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21311775147928994 0.5 0.5
maxi score, test score, baseline:  0.21311775147928994 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21311775147928994 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.21396430678466075 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21396430678466075 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21396430678466075 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
using explorer policy with actor:  1
siam score:  -0.87377244
maxi score, test score, baseline:  0.21396430678466075 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21396430678466075 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21396430678466075 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21396430678466075 0.5 0.5
maxi score, test score, baseline:  0.21396430678466075 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21396430678466075 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.2099459278665483
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.21396430678466075 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.458 0.083 0.083 0.25  0.125]
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21480588235294115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.411]
 [0.391]
 [0.38 ]
 [0.384]] [[-0.789]
 [-0.493]
 [-1.171]
 [-1.099]
 [-0.902]] [[0.381]
 [0.411]
 [0.391]
 [0.38 ]
 [0.384]]
maxi score, test score, baseline:  0.2156425219941349 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.21647426900584793 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.217301166180758 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.217301166180758 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[0.81]
 [0.81]
 [0.81]
 [0.81]
 [0.81]] [[0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[6.128]
 [6.128]
 [6.128]
 [6.128]
 [6.128]] [[1.044]
 [1.044]
 [1.044]
 [1.044]
 [1.044]]
maxi score, test score, baseline:  0.217301166180758 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.217301166180758 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.217301166180758 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.217301166180758 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.293]
 [0.404]
 [0.329]
 [0.308]] [[1.812]
 [0.888]
 [0.394]
 [0.597]
 [2.266]] [[0.206]
 [0.293]
 [0.404]
 [0.329]
 [0.308]]
maxi score, test score, baseline:  0.217301166180758 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21666976744186045 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21666976744186045 0.5 0.5
maxi score, test score, baseline:  0.21666976744186045 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.21666976744186045 0.5 0.5
maxi score, test score, baseline:  0.21666976744186045 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21666976744186045 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21666976744186045 0.5 0.5
maxi score, test score, baseline:  0.21666976744186045 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21666976744186045 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21666976744186045 0.5 0.5
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.21749130434782607 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2168630057803468 0.5 0.5
actor:  0 policy actor:  0  step number:  47 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8605496
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86180514
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21767925072046107 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.8683322
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.2330700272479564 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2330700272479564 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.042 0.083 0.458]
maxi score, test score, baseline:  0.2330700272479564 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2330700272479564 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2330700272479564 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2330700272479564 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.23379565217391304 0.5 0.5
maxi score, test score, baseline:  0.23379565217391304 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.218]
 [0.262]
 [0.229]
 [0.237]] [[0.818]
 [1.194]
 [0.85 ]
 [0.842]
 [0.929]] [[0.238]
 [0.218]
 [0.262]
 [0.229]
 [0.237]]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.23451734417344172 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23451734417344172 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87958115
line 256 mcts: sample exp_bonus 0.09734185061482335
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.23523513513513514 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23523513513513514 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.23523513513513514 0.5 0.5
maxi score, test score, baseline:  0.23523513513513514 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.23523513513513514 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23523513513513514 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23523513513513514 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23523513513513514 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23523513513513514 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23523513513513514 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  0.19028133280099294
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.8332536388068394
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.871996
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.257]
 [0.392]
 [0.341]
 [0.337]] [[ 0.649]
 [ 0.805]
 [ 0.025]
 [-0.214]
 [ 0.649]] [[0.494]
 [0.466]
 [0.34 ]
 [0.21 ]
 [0.494]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  61 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23460134770889488 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8346997
maxi score, test score, baseline:  0.23397096774193546 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23397096774193546 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23397096774193546 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23397096774193546 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.23468445040214475 0.5 0.5
first move QE:  0.19490886193609477
actor:  0 policy actor:  0  step number:  38 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.23476666666666665 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23476666666666665 0.5 0.5
maxi score, test score, baseline:  0.23476666666666665 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23476666666666665 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23476666666666665 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23476666666666665 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23414255319148936 0.5 0.5
maxi score, test score, baseline:  0.23414255319148936 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23414255319148936 0.5 0.5
start point for exploration sampling:  16339
siam score:  -0.851156
actor:  1 policy actor:  1  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.23414255319148936 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23414255319148936 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23414255319148936 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23414255319148936 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23414255319148936 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.23414255319148936 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  50 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.23414255319148936 0.5 0.5
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23554973544973543 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23554973544973543 0.5 0.5
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  44 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.23624775725593666 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23624775725593666 0.5 0.5
first move QE:  0.20720292861290737
maxi score, test score, baseline:  0.23624775725593666 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.2080210986506073
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.23624775725593666 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23562631578947368 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23562631578947368 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23562631578947368 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  59 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
actor:  1 policy actor:  1  step number:  58 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.363]
 [0.377]
 [0.476]
 [0.494]] [[2.336]
 [3.303]
 [2.336]
 [2.615]
 [2.563]] [[0.767]
 [1.166]
 [0.767]
 [0.969]
 [0.962]]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23632047244094487 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.23570209424083768 0.5 0.5
start point for exploration sampling:  16339
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.86308104
maxi score, test score, baseline:  0.23570209424083768 0.5 0.5
maxi score, test score, baseline:  0.23570209424083768 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 6.814641201815676
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
siam score:  -0.8718245
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8762847
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.551]
 [0.636]
 [0.555]
 [0.519]] [[-0.655]
 [-0.214]
 [-0.983]
 [-0.775]
 [-0.709]] [[0.281]
 [0.368]
 [0.325]
 [0.279]
 [0.254]]
siam score:  -0.8758406
actor:  1 policy actor:  1  step number:  69 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
first move QE:  0.21338270044938484
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.301]
 [0.395]
 [0.301]
 [0.301]] [[2.271]
 [2.271]
 [3.466]
 [2.271]
 [2.271]] [[0.237]
 [0.237]
 [0.637]
 [0.237]
 [0.237]]
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.881316
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.249]
 [0.503]
 [0.408]
 [0.39 ]] [[3.705]
 [3.434]
 [2.506]
 [3.489]
 [4.25 ]] [[0.656]
 [0.575]
 [0.492]
 [0.65 ]
 [0.794]]
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2363924281984334 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.23707916666666665 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23707916666666665 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23707916666666665 0.5 0.5
actor:  0 policy actor:  0  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.164]
 [0.218]
 [0.219]
 [0.211]] [[6.724]
 [6.813]
 [6.724]
 [6.612]
 [6.554]] [[1.06 ]
 [1.04 ]
 [1.06 ]
 [1.031]
 [1.008]]
maxi score, test score, baseline:  0.23776233766233765 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23911808785529715 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.24045989717223648 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24045989717223648 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24045989717223648 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86425245
maxi score, test score, baseline:  0.24045989717223648 0.5 0.5
maxi score, test score, baseline:  0.24045989717223648 0.5 0.5
maxi score, test score, baseline:  0.24045989717223648 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24045989717223648 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24045989717223648 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24045989717223648 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24045989717223648 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24045989717223648 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23984358974358974 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23984358974358974 0.5 0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.2392304347826087 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2392304347826087 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2392304347826087 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2392304347826087 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.426]
 [0.39 ]
 [0.355]
 [0.367]] [[ 1.35 ]
 [-0.038]
 [-0.268]
 [ 0.978]
 [ 0.106]] [[0.32 ]
 [0.426]
 [0.39 ]
 [0.355]
 [0.367]]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.173]
 [0.216]
 [0.32 ]
 [0.235]
 [0.261]] [[3.562]
 [2.838]
 [0.384]
 [2.164]
 [1.826]] [[0.173]
 [0.216]
 [0.32 ]
 [0.235]
 [0.261]]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.24121675126903552 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24121675126903552 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24121675126903552 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24121675126903552 0.5 0.5
maxi score, test score, baseline:  0.24121675126903552 0.5 0.5
maxi score, test score, baseline:  0.24121675126903552 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24121675126903552 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24121675126903552 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.314]
 [0.492]
 [0.431]
 [0.329]] [[2.49 ]
 [2.979]
 [1.43 ]
 [2.449]
 [3.203]] [[0.619]
 [0.779]
 [0.484]
 [0.727]
 [0.858]]
maxi score, test score, baseline:  0.24121675126903552 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24121675126903552 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.24187215189873418 0.5 0.5
maxi score, test score, baseline:  0.24187215189873418 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.248]
 [0.324]
 [0.324]
 [0.351]] [[2.779]
 [4.129]
 [2.779]
 [2.779]
 [3.257]] [[0.534]
 [0.938]
 [0.534]
 [0.534]
 [0.714]]
maxi score, test score, baseline:  0.24187215189873418 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24187215189873418 0.5 0.5
maxi score, test score, baseline:  0.24187215189873418 0.5 0.5
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24252424242424242 0.5 0.5
maxi score, test score, baseline:  0.24252424242424242 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24252424242424242 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85714376
siam score:  -0.8577683
actor:  1 policy actor:  1  step number:  66 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.24317304785894206 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24317304785894206 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24317304785894206 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24317304785894206 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.24317304785894206 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24317304785894206 0.5 0.5
maxi score, test score, baseline:  0.24317304785894206 0.5 0.5
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
first move QE:  0.262616991899737
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.24317304785894206 0.5 0.5
maxi score, test score, baseline:  0.24317304785894206 0.5 0.5
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.492]] [[2.886]
 [2.886]
 [2.886]
 [2.886]
 [3.288]] [[0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.465]]
maxi score, test score, baseline:  0.2438185929648241 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2438185929648241 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2438185929648241 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2438185929648241 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2438185929648241 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2438185929648241 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[1.993]
 [1.993]
 [1.993]
 [1.993]
 [1.993]] [[0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]]
maxi score, test score, baseline:  0.2438185929648241 0.5 0.5
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.292 0.083 0.292 0.208 0.125]
maxi score, test score, baseline:  0.2438185929648241 0.5 0.5
maxi score, test score, baseline:  0.2438185929648241 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.24446090225563907 0.5 0.5
maxi score, test score, baseline:  0.24446090225563907 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.502]
 [0.647]
 [0.502]
 [0.482]] [[1.443]
 [1.443]
 [0.807]
 [1.443]
 [2.444]] [[0.35 ]
 [0.35 ]
 [0.189]
 [0.35 ]
 [0.722]]
maxi score, test score, baseline:  0.24446090225563907 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24446090225563907 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24446090225563907 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24446090225563907 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.27160781788709937
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24446090225563907 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85346025
maxi score, test score, baseline:  0.24446090225563907 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24446090225563907 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24384999999999998 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24384999999999998 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24384999999999998 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.85200167
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.24448902743142142 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24448902743142142 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.24512487562189053 0.5 0.5
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.24512487562189053 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.24512487562189053 0.5 0.5
maxi score, test score, baseline:  0.24512487562189053 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24512487562189053 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.141]
 [0.106]
 [0.283]
 [0.193]] [[5.976]
 [5.555]
 [5.976]
 [5.878]
 [8.583]] [[0.544]
 [0.481]
 [0.544]
 [0.573]
 [1.017]]
maxi score, test score, baseline:  0.24512487562189053 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24512487562189053 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24512487562189053 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24512487562189053 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24512487562189053 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24512487562189053 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
siam score:  -0.85908264
maxi score, test score, baseline:  0.24575756823821338 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24575756823821338 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24575756823821338 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[2.462]
 [2.462]
 [2.462]
 [2.462]
 [2.462]] [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]]
maxi score, test score, baseline:  0.24575756823821338 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24575756823821338 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24575756823821338 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24575756823821338 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24575756823821338 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24575756823821338 0.5 0.5
maxi score, test score, baseline:  0.24575756823821338 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24575756823821338 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[2.586]
 [1.999]
 [1.999]
 [1.999]
 [1.999]] [[0.182]
 [0.185]
 [0.185]
 [0.185]
 [0.185]]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.24638712871287127 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24638712871287127 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8604753
maxi score, test score, baseline:  0.24638712871287127 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24638712871287127 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.24701358024691356 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
start point for exploration sampling:  16339
line 256 mcts: sample exp_bonus 0.9025487247421017
maxi score, test score, baseline:  0.2470287469287469 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2470287469287469 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2470287469287469 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  53 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.24764901960784313 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24764901960784313 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.86713433
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.24826625916870415 0.5 0.5
siam score:  -0.86840576
maxi score, test score, baseline:  0.24826625916870415 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: False
Self play flag: True
add more workers flag:  True
expV_train_flag:  True
expV_train_start_flag:  16339
main train batch thing paused
add a thread
Adding thread: now have 2 threads
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 5.5325053464890415
maxi score, test score, baseline:  0.24888048780487804 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24888048780487804 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24888048780487804 0.5 0.5
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.24949172749391726 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.005]
 [0.018]
 [0.005]
 [0.003]] [[1.087]
 [1.442]
 [1.096]
 [0.442]
 [0.9  ]] [[0.023]
 [0.005]
 [0.018]
 [0.005]
 [0.003]]
maxi score, test score, baseline:  0.24888640776699028 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24888640776699028 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.24949467312348667 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24949467312348667 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24949467312348667 0.5 0.5
maxi score, test score, baseline:  0.24949467312348667 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.091]] [[3.272]
 [3.272]
 [3.272]
 [3.272]
 [6.82 ]] [[0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.918]]
maxi score, test score, baseline:  0.24949467312348667 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.24949759036144578 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24949759036144578 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24949759036144578 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24949759036144578 0.5 0.5
Printing some Q and Qe and total Qs values:  [[1.5  ]
 [0.35 ]
 [0.349]
 [0.344]
 [1.5  ]] [[0.   ]
 [1.77 ]
 [1.621]
 [1.646]
 [0.   ]] [[0.595]
 [0.615]
 [0.55 ]
 [0.557]
 [0.595]]
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.191]
 [0.272]
 [0.191]
 [0.191]] [[4.036]
 [2.226]
 [1.37 ]
 [2.226]
 [2.226]] [[0.164]
 [0.191]
 [0.272]
 [0.191]
 [0.191]]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.327]
 [0.426]
 [0.327]
 [0.327]] [[3.348]
 [3.348]
 [4.125]
 [3.348]
 [3.348]] [[0.725]
 [0.725]
 [1.114]
 [0.725]
 [0.725]]
maxi score, test score, baseline:  0.25069952038369303 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.2512961722488038 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2512961722488038 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.25248095238095236 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0000],
        [0.2140],
        [0.3817],
        [0.3451],
        [0.1921],
        [0.1684],
        [0.2062],
        [0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.21404808997023458
0.0 0.3816917583108634
0.0 0.34512234059934527
0.0 0.19208674327440617
0.0 0.16835070546487693
0.0 0.20620830060516598
0.480298005 0.480298005
maxi score, test score, baseline:  0.25248095238095236 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  49 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.2530691211401425 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2530691211401425 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.695841902930912
maxi score, test score, baseline:  0.2524696682464455 0.5 0.5
maxi score, test score, baseline:  0.2524696682464455 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2524696682464455 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2524696682464455 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.25305508274231675 0.5 0.5
actor:  0 policy actor:  0  step number:  57 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2536377358490566 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2536377358490566 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2536377358490566 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2536377358490566 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2536377358490566 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.2542176470588235 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2542176470588235 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2542176470588235 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.198]
 [0.549]
 [0.362]
 [0.363]] [[2.636]
 [4.555]
 [2.267]
 [2.837]
 [3.684]] [[0.494]
 [0.71 ]
 [0.679]
 [0.587]
 [0.729]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2542176470588235 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2547948356807512 0.5 0.5
siam score:  -0.8404666
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.8430273
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.364]
 [0.575]
 [0.438]
 [0.418]] [[1.125]
 [2.083]
 [0.439]
 [0.707]
 [2.095]] [[0.313]
 [0.493]
 [0.43 ]
 [0.338]
 [0.549]]
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]] [[3.55]
 [3.55]
 [3.55]
 [3.55]
 [3.55]] [[0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]]
maxi score, test score, baseline:  0.25536932084309133 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.625 0.083 0.208]
maxi score, test score, baseline:  0.25536932084309133 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.2559411214953271 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2559411214953271 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2559411214953271 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.2565102564102564 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2565102564102564 0.5 0.5
maxi score, test score, baseline:  0.2565102564102564 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2565102564102564 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2565102564102564 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2570767441860465 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.2576406032482598 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2576406032482598 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2576406032482598 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2576406032482598 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.25820185185185185 0.5 0.5
maxi score, test score, baseline:  0.25820185185185185 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25820185185185185 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25820185185185185 0.5 0.5
maxi score, test score, baseline:  0.25820185185185185 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.2587605080831409 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2587605080831409 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2587605080831409 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[5.137]
 [5.137]
 [5.137]
 [5.137]
 [5.137]] [[1.09]
 [1.09]
 [1.09]
 [1.09]
 [1.09]]
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.151]
 [0.26 ]
 [0.278]
 [0.204]] [[2.223]
 [3.094]
 [2.311]
 [3.352]
 [3.112]] [[0.317]
 [0.538]
 [0.386]
 [0.751]
 [0.598]]
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8567838
maxi score, test score, baseline:  0.25816451612903224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2587206896551724 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.2587206896551724 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.    0.417 0.    0.417 0.167]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2592743119266055 0.5 0.5
first move QE:  0.4149161505889634
maxi score, test score, baseline:  0.2592743119266055 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2592743119266055 0.5 0.5
maxi score, test score, baseline:  0.2592743119266055 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.147]
 [0.609]
 [0.519]
 [0.511]] [[1.059]
 [0.115]
 [0.091]
 [0.278]
 [0.717]] [[0.647]
 [0.17 ]
 [0.628]
 [0.569]
 [0.634]]
maxi score, test score, baseline:  0.2592743119266055 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7960999444491146
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.26676666666666665 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2677991150442478 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2693307692307692 0.5 0.5
maxi score, test score, baseline:  0.2693307692307692 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.2703407002188184 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2703407002188184 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8491069
maxi score, test score, baseline:  0.2703407002188184 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  46 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.425]
 [0.603]
 [0.425]
 [0.524]] [[2.449]
 [1.684]
 [0.782]
 [1.684]
 [2.164]] [[0.678]
 [0.44 ]
 [0.318]
 [0.44 ]
 [0.699]]
line 256 mcts: sample exp_bonus 3.4893541499276366
siam score:  -0.84868026
maxi score, test score, baseline:  0.27084235807860263 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27084235807860263 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27084235807860263 0.5 0.5
maxi score, test score, baseline:  0.27084235807860263 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27084235807860263 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.354]
 [0.596]
 [0.354]
 [0.446]] [[3.054]
 [3.054]
 [1.866]
 [3.054]
 [3.132]] [[0.841]
 [0.841]
 [0.734]
 [0.841]
 [0.923]]
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.179]
 [0.449]
 [0.34 ]
 [0.347]] [[2.456]
 [1.808]
 [4.428]
 [1.651]
 [4.23 ]] [[0.212]
 [0.024]
 [0.753]
 [0.044]
 [0.668]]
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]] [[3.611]
 [3.611]
 [3.611]
 [3.611]
 [3.611]] [[0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2702525054466231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.27075217391304346 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.27075217391304346 0.5 0.5
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.842128
maxi score, test score, baseline:  0.27124967462039046 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27124967462039046 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.27223822894168465 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27223822894168465 0.5 0.5
maxi score, test score, baseline:  0.27223822894168465 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.85232955
maxi score, test score, baseline:  0.27272931034482756 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27272931034482756 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2732182795698925 0.5 0.5
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.27370515021459224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.439]
 [0.439]
 [0.439]
 [0.463]] [[3.272]
 [4.674]
 [4.674]
 [4.674]
 [3.822]] [[0.713]
 [1.075]
 [1.075]
 [1.075]
 [0.888]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.27370515021459224 0.5 0.5
maxi score, test score, baseline:  0.27370515021459224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27370515021459224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27370515021459224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27370515021459224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27370515021459224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[1.216]
 [1.216]
 [1.216]
 [1.216]
 [1.216]] [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]]
maxi score, test score, baseline:  0.27370515021459224 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85669
actor:  0 policy actor:  0  step number:  42 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.2741899357601713 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8551781
actor:  0 policy actor:  0  step number:  63 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2746726495726496 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.005]
 [0.134]
 [0.084]
 [0.108]] [[0.203]
 [0.138]
 [0.195]
 [0.701]
 [1.697]] [[0.053]
 [0.002]
 [0.153]
 [0.412]
 [1.033]]
maxi score, test score, baseline:  0.2746726495726496 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.27515330490405115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27515330490405115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.27515330490405115 0.5 0.5
maxi score, test score, baseline:  0.27515330490405115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27515330490405115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27515330490405115 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  48 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.275631914893617 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.275631914893617 0.5 0.5
siam score:  -0.8545778
maxi score, test score, baseline:  0.275631914893617 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.275631914893617 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]] [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
first move QE:  0.4648627280980851
maxi score, test score, baseline:  0.275631914893617 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  54 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.275631914893617 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2750469214437367 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2750469214437367 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.2755237288135593 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2759985200845666 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2759985200845666 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2759985200845666 0.5 0.5
maxi score, test score, baseline:  0.2759985200845666 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2759985200845666 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.86364233
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2758894736842105 0.5 0.5
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2758894736842105 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.235]
 [0.291]
 [0.235]
 [0.235]] [[2.873]
 [2.873]
 [2.582]
 [2.873]
 [2.873]] [[0.235]
 [0.235]
 [0.291]
 [0.235]
 [0.235]]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
siam score:  -0.8538957
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2763605042016807 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27682955974842766 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27682955974842766 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.27729665271966525 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.27729665271966525 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.85478157
maxi score, test score, baseline:  0.27729665271966525 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.532]
 [0.647]
 [0.546]
 [0.515]] [[2.681]
 [1.093]
 [0.971]
 [2.201]
 [2.796]] [[0.822]
 [0.367]
 [0.383]
 [0.717]
 [0.888]]
maxi score, test score, baseline:  0.27729665271966525 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27729665271966525 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27729665271966525 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.27729665271966525 0.5 0.5
maxi score, test score, baseline:  0.27729665271966525 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8656183
maxi score, test score, baseline:  0.27729665271966525 0.5 0.5
maxi score, test score, baseline:  0.27729665271966525 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.27729665271966525 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2777617954070981 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2786862785862786 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2786862785862786 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.27914564315352697 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.27914564315352697 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.27914564315352697 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.27914564315352697 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.27914564315352697 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.27914564315352697 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27914564315352697 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27914564315352697 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8617213
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.43 ]] [[2.14 ]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [2.472]] [[0.922]
 [0.922]
 [0.922]
 [0.922]
 [1.134]]
maxi score, test score, baseline:  0.27914564315352697 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]] [[2.95]
 [2.95]
 [2.95]
 [2.95]
 [2.95]] [[1.066]
 [1.066]
 [1.066]
 [1.066]
 [1.066]]
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.373]] [[2.735]
 [2.735]
 [2.735]
 [2.735]
 [2.734]] [[1.119]
 [1.119]
 [1.119]
 [1.119]
 [1.136]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.214]
 [0.576]
 [0.434]
 [0.395]] [[2.289]
 [2.377]
 [1.767]
 [1.777]
 [2.118]] [[0.589]
 [0.508]
 [0.566]
 [0.428]
 [0.559]]
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.375]
 [0.568]
 [0.375]
 [0.39 ]] [[1.02 ]
 [1.02 ]
 [0.567]
 [1.02 ]
 [1.267]] [[0.385]
 [0.385]
 [0.503]
 [0.385]
 [0.442]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.5622582325505303
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  65 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2796031055900621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.2800586776859504 0.5 0.5
using explorer policy with actor:  1
siam score:  -0.8473488
maxi score, test score, baseline:  0.2800586776859504 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
deleting a thread, now have 1 threads
Frames:  44959 train batches done:  5265 episodes:  1876
maxi score, test score, baseline:  0.2805123711340206 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2805123711340206 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2805123711340206 0.5 0.5
maxi score, test score, baseline:  0.2805123711340206 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.2805123711340206 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2805123711340206 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2805123711340206 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.471]] [[1.888]
 [1.888]
 [1.888]
 [1.888]
 [2.628]] [[0.868]
 [0.868]
 [0.868]
 [0.868]
 [1.107]]
maxi score, test score, baseline:  0.2805123711340206 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2805123711340206 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2793607802874743 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2793607802874743 0.5 0.5
siam score:  -0.8673054
maxi score, test score, baseline:  0.2793607802874743 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.2793607802874743 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2793607802874743 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2793607802874743 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2793607802874743 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2793607802874743 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2793607802874743 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  53 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.27981311475409837 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.28026359918200405 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28026359918200405 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28026359918200405 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28026359918200405 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.28071224489795915 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.28071224489795915 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28071224489795915 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.568]
 [0.417]
 [0.417]
 [0.417]] [[1.48 ]
 [1.241]
 [1.48 ]
 [1.48 ]
 [1.48 ]] [[0.706]
 [0.698]
 [0.706]
 [0.706]
 [0.706]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.28071224489795915 0.5 0.5
first move QE:  0.49964664733301156
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.28071224489795915 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.28071224489795915 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.479]] [[1.18 ]
 [1.18 ]
 [1.18 ]
 [1.18 ]
 [1.336]] [[0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.713]]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.216]
 [0.496]
 [0.367]
 [0.385]] [[-0.244]
 [ 0.888]
 [ 0.366]
 [ 0.342]
 [ 0.933]] [[0.109]
 [0.391]
 [0.497]
 [0.36 ]
 [0.576]]
maxi score, test score, baseline:  0.28071224489795915 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28071224489795915 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.28071224489795915 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.2811590631364562 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2811590631364562 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2811590631364562 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2811590631364562 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.2816040650406504 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2816040650406504 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2816040650406504 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2816040650406504 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.321]
 [0.414]
 [0.321]
 [0.321]] [[0.785]
 [0.785]
 [1.825]
 [0.785]
 [0.537]] [[0.259]
 [0.259]
 [0.619]
 [0.259]
 [0.186]]
siam score:  -0.8672995
maxi score, test score, baseline:  0.2816040650406504 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2816040650406504 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2816040650406504 0.5 0.5
maxi score, test score, baseline:  0.2816040650406504 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  96 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2816040650406504 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28103306288032454 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28103306288032454 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28103306288032454 0.5 0.5
maxi score, test score, baseline:  0.28103306288032454 0.5 0.5
maxi score, test score, baseline:  0.28103306288032454 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.28147651821862346 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28147651821862346 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  42 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.2819181818181818 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.49949540869693954
maxi score, test score, baseline:  0.2819181818181818 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2819181818181818 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2819181818181818 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2819181818181818 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2819181818181818 0.5 0.5
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.2819181818181818 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.2819181818181818 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.356]
 [0.451]
 [0.371]
 [0.357]] [[1.433]
 [1.433]
 [1.823]
 [0.807]
 [1.592]] [[0.456]
 [0.456]
 [0.704]
 [0.201]
 [0.524]]
maxi score, test score, baseline:  0.2819181818181818 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.28235806451612905 0.5 0.5
maxi score, test score, baseline:  0.28235806451612905 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.095]
 [0.379]
 [0.408]
 [0.31 ]] [[2.243]
 [2.152]
 [0.053]
 [2.057]
 [2.395]] [[0.782]
 [0.696]
 [0.137]
 [0.821]
 [0.885]]
maxi score, test score, baseline:  0.28235806451612905 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28235806451612905 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.429]
 [0.567]
 [0.458]
 [0.452]] [[-0.111]
 [ 0.763]
 [ 0.03 ]
 [ 0.044]
 [ 0.257]] [[0.436]
 [0.571]
 [0.586]
 [0.48 ]
 [0.509]]
maxi score, test score, baseline:  0.28235806451612905 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  59 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.28235806451612905 0.5 0.5
siam score:  -0.8620188
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.28235806451612905 0.5 0.5
maxi score, test score, baseline:  0.28235806451612905 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28235806451612905 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.860332
maxi score, test score, baseline:  0.28235806451612905 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28235806451612905 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.28279617706237425 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28279617706237425 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8583232
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.28279617706237425 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28279617706237425 0.5 0.5
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.28279617706237425 0.5 0.5
siam score:  -0.8630538
maxi score, test score, baseline:  0.28279617706237425 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28279617706237425 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.554]
 [0.615]
 [0.48 ]
 [0.486]] [[0.227]
 [0.52 ]
 [0.233]
 [0.01 ]
 [0.614]] [[0.19 ]
 [0.408]
 [0.421]
 [0.248]
 [0.355]]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86362123
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28323253012048194 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.8600586
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.86162895
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8634651
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28366713426853707 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 1.3999747756719887
maxi score, test score, baseline:  0.28409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 0.5 0.5
maxi score, test score, baseline:  0.28409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8653459
maxi score, test score, baseline:  0.28409999999999996 0.5 0.5
maxi score, test score, baseline:  0.28409999999999996 0.5 0.5
maxi score, test score, baseline:  0.28409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85721993
actor:  0 policy actor:  0  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.28709999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.85681397
maxi score, test score, baseline:  0.28709999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.716]
 [0.62 ]
 [0.62 ]
 [0.595]] [[1.735]
 [0.113]
 [1.735]
 [1.735]
 [2.686]] [[0.615]
 [0.171]
 [0.615]
 [0.615]
 [0.895]]
maxi score, test score, baseline:  0.28709999999999997 0.5 0.5
maxi score, test score, baseline:  0.28709999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28709999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28709999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8532764
maxi score, test score, baseline:  0.28709999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.1376904983471077
maxi score, test score, baseline:  0.28709999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28709999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.510993976817907
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.397]] [[2.222]
 [2.222]
 [2.222]
 [2.222]
 [2.633]] [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.546]]
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[0.962]
 [0.962]
 [0.962]
 [0.962]
 [0.962]] [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]]
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.859881
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.28809999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.28909999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28909999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28909999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28909999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.28909999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28909999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]] [[2.717]
 [2.717]
 [2.717]
 [2.717]
 [2.717]] [[0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]]
maxi score, test score, baseline:  0.28909999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28909999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.28909999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28909999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.29009999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.29109999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29109999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.29109999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[3.37]
 [3.37]
 [3.37]
 [3.37]
 [3.37]] [[1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
maxi score, test score, baseline:  0.29209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.29309999999999997 0.5 0.5
maxi score, test score, baseline:  0.29309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.29309999999999997 0.5 0.5
maxi score, test score, baseline:  0.29309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.29309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
first move QE:  0.5286437140925552
maxi score, test score, baseline:  0.29309999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.2941 0.5 0.5
siam score:  -0.85268366
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
start point for exploration sampling:  16339
Starting evaluation
maxi score, test score, baseline:  0.2941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.308]
 [0.251]
 [0.232]
 [0.242]] [[ 0.115]
 [-0.409]
 [-0.579]
 [-0.774]
 [-0.259]] [[0.224]
 [0.308]
 [0.251]
 [0.232]
 [0.242]]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3111 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3131 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3131 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3131 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3131 0.5 0.5
maxi score, test score, baseline:  0.3131 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3131 0.5 0.5
maxi score, test score, baseline:  0.3131 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3131 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3131 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3141 0.5 0.5
maxi score, test score, baseline:  0.3141 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 0.5 0.5
maxi score, test score, baseline:  0.3141 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3141 0.5 0.5
maxi score, test score, baseline:  0.3141 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.5335619068245053
maxi score, test score, baseline:  0.3141 0.5 0.5
siam score:  -0.8528557
maxi score, test score, baseline:  0.3141 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 0.5 0.5
maxi score, test score, baseline:  0.3141 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3141 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3141 0.5 0.5
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3141 0.5 0.5
maxi score, test score, baseline:  0.3141 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.82844156
siam score:  -0.828229
maxi score, test score, baseline:  0.3141 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.333 0.417 0.083 0.125]
maxi score, test score, baseline:  0.3141 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
siam score:  -0.83577216
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3151 0.5 0.5
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8505242
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.86268497
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8492471935765584
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.369]
 [0.561]
 [0.472]
 [0.42 ]] [[-0.712]
 [ 0.408]
 [-0.568]
 [-0.651]
 [-0.195]] [[0.353]
 [0.487]
 [0.516]
 [0.413]
 [0.437]]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.13607433098219554
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.033]
 [0.352]
 [0.312]
 [0.287]] [[-0.122]
 [ 0.001]
 [ 0.02 ]
 [-0.451]
 [ 0.409]] [[0.273]
 [0.033]
 [0.352]
 [0.312]
 [0.287]]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3151 0.5 0.5
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.386]
 [0.533]
 [0.498]
 [0.497]] [[1.864]
 [1.943]
 [1.94 ]
 [0.991]
 [1.498]] [[0.73 ]
 [0.741]
 [0.852]
 [0.462]
 [0.656]]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.227]
 [0.322]
 [0.322]
 [0.508]] [[2.169]
 [2.465]
 [2.169]
 [2.169]
 [2.775]] [[0.645]
 [0.691]
 [0.645]
 [0.645]
 [1.066]]
maxi score, test score, baseline:  0.3151 0.5 0.5
maxi score, test score, baseline:  0.3151 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3161 0.5 0.5
maxi score, test score, baseline:  0.3161 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 0.5 0.5
maxi score, test score, baseline:  0.3161 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3161 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.311]] [[1.99 ]
 [1.99 ]
 [1.99 ]
 [1.99 ]
 [2.502]] [[0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.565]]
maxi score, test score, baseline:  0.3161 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 0.5 0.5
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3161 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 0.5 0.5
maxi score, test score, baseline:  0.3161 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  51 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3171 0.5 0.5
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.074]
 [0.489]
 [0.415]
 [0.439]] [[1.175]
 [1.317]
 [1.66 ]
 [1.359]
 [1.786]] [[0.428]
 [0.282]
 [0.774]
 [0.586]
 [0.785]]
maxi score, test score, baseline:  0.3181 0.5 0.5
actor:  1 policy actor:  1  step number:  45 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3181 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 0.5 0.5
maxi score, test score, baseline:  0.3181 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.167]
 [0.114]
 [0.425]
 [0.268]] [[-0.142]
 [ 2.882]
 [ 0.205]
 [ 2.488]
 [ 2.628]] [[0.023]
 [0.921]
 [0.122]
 [0.919]
 [0.892]]
maxi score, test score, baseline:  0.3191 0.5 0.5
maxi score, test score, baseline:  0.3191 0.5 0.5
maxi score, test score, baseline:  0.3191 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.38 ]
 [0.463]
 [0.367]
 [0.4  ]] [[3.051]
 [3.051]
 [0.795]
 [1.692]
 [2.905]] [[0.38 ]
 [0.38 ]
 [0.463]
 [0.367]
 [0.4  ]]
actor:  0 policy actor:  0  step number:  80 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3201 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.8546337
maxi score, test score, baseline:  0.3201 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3211 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3211 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3211 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3211 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3231 0.5 0.5
first move QE:  0.5456151686398246
actor:  1 policy actor:  1  step number:  49 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3231 0.5 0.5
actor:  0 policy actor:  0  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85393745
maxi score, test score, baseline:  0.3231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8522442
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3231 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3231 0.5 0.5
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3231 0.5 0.5
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.284]
 [0.495]
 [0.495]
 [0.496]] [[4.571]
 [3.24 ]
 [4.571]
 [4.571]
 [3.459]] [[1.131]
 [0.477]
 [1.131]
 [1.131]
 [0.761]]
maxi score, test score, baseline:  0.3251 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3251 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3251 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3251 0.5 0.5
maxi score, test score, baseline:  0.3251 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  87 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3251 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
first move QE:  0.5531432083708158
maxi score, test score, baseline:  0.3261 0.5 0.5
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8643932
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.863896
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3261 0.5 0.5
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
UNIT TEST: sample policy line 217 mcts : [0.333 0.042 0.125 0.125 0.375]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  58 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3261 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3271 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3271 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.84216094
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3281 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3281 0.5 0.5
siam score:  -0.826807
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]] [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]]
maxi score, test score, baseline:  0.3281 0.5 0.5
actor:  1 policy actor:  1  step number:  55 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3301 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 0.5 0.5
maxi score, test score, baseline:  0.3301 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  51 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3321 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 0.5 0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3331 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3331 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3331 0.5 0.5
maxi score, test score, baseline:  0.3331 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3331 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3331 0.5 0.5
maxi score, test score, baseline:  0.3331 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.3331 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 0.5 0.5
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3341 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84801835
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3341 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
first move QE:  0.5569153813733104
actor:  1 policy actor:  1  step number:  77 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3351 0.5 0.5
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8402188
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84038466
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.046]
 [0.503]
 [0.455]
 [0.416]] [[ 0.377]
 [-0.061]
 [ 0.241]
 [ 0.153]
 [ 0.384]] [[0.717]
 [0.047]
 [0.705]
 [0.598]
 [0.713]]
maxi score, test score, baseline:  0.3351 0.5 0.5
siam score:  -0.834859
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]] [[2.398]
 [2.398]
 [2.398]
 [2.398]
 [2.398]] [[0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86400354
line 256 mcts: sample exp_bonus 1.6824439265584965
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.003]
 [0.416]
 [0.423]
 [0.382]] [[ 0.352]
 [-0.269]
 [ 0.812]
 [ 0.222]
 [ 0.968]] [[0.611]
 [0.002]
 [0.844]
 [0.557]
 [0.896]]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3351 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3361 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.7792716750496813
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3371 0.5 0.5
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4754718708992005
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3371 0.5 0.5
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3371 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3381 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]] [[1.073]
 [1.073]
 [1.073]
 [1.073]
 [1.073]]
maxi score, test score, baseline:  0.3381 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3381 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8464966
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3391 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3391 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3391 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3391 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3401 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.592]
 [0.592]
 [0.592]
 [0.517]] [[2.575]
 [3.136]
 [3.136]
 [3.136]
 [3.568]] [[0.735]
 [0.898]
 [0.898]
 [0.898]
 [0.94 ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3411 0.5 0.5
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3421 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3421 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.371]] [[2.677]
 [2.677]
 [2.677]
 [2.677]
 [3.837]] [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.714]]
maxi score, test score, baseline:  0.3421 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.303]
 [0.329]
 [0.303]
 [0.303]] [[2.461]
 [2.461]
 [0.735]
 [2.461]
 [2.461]] [[0.303]
 [0.303]
 [0.329]
 [0.303]
 [0.303]]
maxi score, test score, baseline:  0.3431 0.5 0.5
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.8434511
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
siam score:  -0.85796845
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.4012],
        [0.3374],
        [0.4170],
        [0.3364],
        [0.3656],
        [0.0000],
        [0.4008],
        [0.4170],
        [0.0000],
        [0.4008]], dtype=torch.float64)
0.0 0.4012056978640879
0.0 0.33738080166004664
0.0 0.41701615932834407
0.0 0.3363869812921152
0.0 0.3655596787402876
0.480298005 0.480298005
0.0 0.4008122821018115
0.0 0.41701615932834407
0.49005 0.49005
0.0 0.4008122821018115
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
maxi score, test score, baseline:  0.34409999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  45 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.34509999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.34609999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 0.5 0.5
maxi score, test score, baseline:  0.34609999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.458]
 [0.568]
 [0.458]
 [0.467]] [[2.839]
 [2.839]
 [2.168]
 [2.839]
 [3.835]] [[0.731]
 [0.731]
 [0.592]
 [0.731]
 [1.045]]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.34609999999999996 0.5 0.5
maxi score, test score, baseline:  0.34609999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 0.5 0.5
maxi score, test score, baseline:  0.34609999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.482]
 [0.637]
 [0.542]
 [0.546]] [[1.586]
 [2.348]
 [1.33 ]
 [1.602]
 [2.198]] [[0.517]
 [0.761]
 [0.578]
 [0.573]
 [0.776]]
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.467]
 [0.467]
 [0.502]
 [0.467]] [[1.529]
 [1.529]
 [1.529]
 [1.12 ]
 [1.529]] [[0.64 ]
 [0.64 ]
 [0.64 ]
 [0.539]
 [0.64 ]]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.499]
 [0.43 ]
 [0.433]
 [0.48 ]] [[3.599]
 [4.142]
 [3.599]
 [3.613]
 [5.255]] [[0.543]
 [0.743]
 [0.543]
 [0.549]
 [1.024]]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[3.799]
 [3.799]
 [3.799]
 [3.799]
 [3.799]] [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0]
 [0]
 [0]
 [0]
 [0]] [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.5592488672698148
first move QE:  0.5594944559631471
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  70 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34709999999999996 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.34909999999999997 0.5 0.5
maxi score, test score, baseline:  0.34909999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.35009999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.83835495
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.35209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.35209999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.35309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35309999999999997 0.5 0.5
maxi score, test score, baseline:  0.35309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35309999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  93 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.35309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.35309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
first move QE:  0.558088558423012
maxi score, test score, baseline:  0.35309999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.35409999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35509999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.35609999999999997 0.5 0.5
maxi score, test score, baseline:  0.35609999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 0.5 0.5
maxi score, test score, baseline:  0.35609999999999997 0.5 0.5
maxi score, test score, baseline:  0.35609999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3571 0.5 0.5
maxi score, test score, baseline:  0.3571 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3571 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3571 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3571 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3571 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3571 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3571 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3571 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.598]] [[1.948]
 [1.948]
 [1.948]
 [1.948]
 [1.64 ]] [[0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.555]]
maxi score, test score, baseline:  0.3571 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3571 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3571 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3571 0.5 0.5
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3581 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  66 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3581 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3581 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3581 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8152855
maxi score, test score, baseline:  0.3601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
siam score:  -0.81982195
maxi score, test score, baseline:  0.3601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8232042
maxi score, test score, baseline:  0.3601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 0.5 0.5
siam score:  -0.82201993
maxi score, test score, baseline:  0.3601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8249823
maxi score, test score, baseline:  0.3601 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3611 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3611 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3621 0.5 0.5
maxi score, test score, baseline:  0.3621 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.737]
 [0.643]
 [0.608]
 [0.597]] [[1.403]
 [0.179]
 [0.915]
 [0.531]
 [1.822]] [[0.65 ]
 [0.304]
 [0.52 ]
 [0.362]
 [0.825]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.727]
 [0.639]
 [0.596]
 [0.573]] [[1.63 ]
 [0.302]
 [1.102]
 [0.828]
 [2.14 ]] [[0.474]
 [0.139]
 [0.355]
 [0.246]
 [0.658]]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.403]] [[7.011]
 [7.011]
 [7.011]
 [7.011]
 [8.67 ]] [[0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.899]]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.417]
 [0.614]
 [0.417]
 [0.473]] [[1.691]
 [1.429]
 [1.225]
 [1.429]
 [1.805]] [[0.648]
 [0.555]
 [0.58 ]
 [0.555]
 [0.678]]
maxi score, test score, baseline:  0.3621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.02203667685389519
maxi score, test score, baseline:  0.3621 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3641 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.003]
 [0.498]
 [0.431]
 [0.431]] [[0.973]
 [0.023]
 [1.229]
 [0.973]
 [0.973]] [[0.62 ]
 [0.001]
 [0.754]
 [0.62 ]
 [0.62 ]]
maxi score, test score, baseline:  0.3641 0.5 0.5
maxi score, test score, baseline:  0.3641 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
UNIT TEST: sample policy line 217 mcts : [0.    0.542 0.    0.25  0.208]
maxi score, test score, baseline:  0.3641 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 0.5 0.5
siam score:  -0.842647
maxi score, test score, baseline:  0.3641 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84168
Starting evaluation
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.04386753871807596
maxi score, test score, baseline:  0.3641 0.5 0.5
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3801 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.697196652627146
maxi score, test score, baseline:  0.3801 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3811 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3811 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8332979
siam score:  -0.8338219
maxi score, test score, baseline:  0.3811 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3811 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3821 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3841 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  50 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3841 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8411374
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3841 0.5 0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.401]
 [0.331]
 [0.331]
 [0.405]] [[3.261]
 [3.672]
 [3.261]
 [3.261]
 [3.003]] [[0.844]
 [1.016]
 [0.844]
 [0.844]
 [0.795]]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3851 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3851 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.637]
 [0.614]
 [0.611]
 [0.593]] [[ 0.2  ]
 [ 0.278]
 [-0.988]
 [-0.944]
 [ 0.472]] [[0.466]
 [0.554]
 [0.319]
 [0.324]
 [0.542]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3871 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3881 0.5 0.5
actor:  0 policy actor:  0  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3911 0.5 0.5
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.492]] [[5.372]
 [5.372]
 [5.372]
 [5.372]
 [5.09 ]] [[0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.492]]
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8357689
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.44 ]
 [0.587]
 [0.528]
 [0.477]] [[-0.211]
 [-0.211]
 [-0.156]
 [-1.05 ]
 [-0.493]] [[0.336]
 [0.336]
 [0.492]
 [0.284]
 [0.327]]
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
first move QE:  0.5574832290501669
maxi score, test score, baseline:  0.3911 0.5 0.5
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]] [[-0.871]
 [-0.871]
 [-0.871]
 [-0.871]
 [-0.871]] [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]]
maxi score, test score, baseline:  0.3911 0.5 0.5
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.579]
 [0.641]
 [0.579]
 [0.581]] [[ 0.   ]
 [ 0.   ]
 [-0.061]
 [ 0.   ]
 [ 0.295]] [[0.348]
 [0.348]
 [0.4  ]
 [0.348]
 [0.401]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3911 0.5 0.5
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8391821
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8404396
actor:  1 policy actor:  1  step number:  47 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3901 0.5 0.5
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.5538159683467733
UNIT TEST: sample policy line 217 mcts : [0.042 0.375 0.208 0.083 0.292]
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3901 0.5 0.5
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 0.5 0.5
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 0.5 0.5
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8678820216033608
maxi score, test score, baseline:  0.3901 0.5 0.5
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.687]
 [0.622]
 [0.638]
 [0.638]] [[1.156]
 [1.054]
 [1.624]
 [1.089]
 [1.214]] [[0.791]
 [0.802]
 [1.022]
 [0.771]
 [0.833]]
maxi score, test score, baseline:  0.3901 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3901 0.5 0.5
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
siam score:  -0.84266716
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.681]
 [0.624]
 [0.625]
 [0.625]] [[ 0.732]
 [ 0.186]
 [-0.672]
 [ 0.732]
 [ 0.732]] [[0.599]
 [0.564]
 [0.364]
 [0.599]
 [0.599]]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.8471753
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.597]] [[1.942]
 [1.942]
 [1.942]
 [1.942]
 [2.2  ]] [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.992]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3911 0.5 0.5
actor:  1 policy actor:  1  step number:  86 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3911 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3921 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 0.5 0.5
maxi score, test score, baseline:  0.3921 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 0.5 0.5
maxi score, test score, baseline:  0.3921 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 0.5 0.5
actor:  1 policy actor:  1  step number:  67 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3921 0.5 0.5
maxi score, test score, baseline:  0.3921 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 0.5 0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.36 ]
 [0.36 ]
 [0.503]
 [0.382]] [[4.429]
 [4.429]
 [4.429]
 [1.934]
 [2.828]] [[0.79 ]
 [0.79 ]
 [0.79 ]
 [0.209]
 [0.39 ]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.531]
 [0.621]
 [0.543]
 [0.555]] [[1.661]
 [1.875]
 [1.267]
 [1.583]
 [1.647]] [[0.735]
 [0.945]
 [0.739]
 [0.816]
 [0.858]]
maxi score, test score, baseline:  0.3921 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3931 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.8285009
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.3931 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3931 0.5 0.5
maxi score, test score, baseline:  0.3931 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3931 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.695]
 [0.668]
 [0.521]
 [0.633]] [[1.502]
 [0.522]
 [0.902]
 [2.039]
 [2.005]] [[0.785]
 [0.558]
 [0.658]
 [0.889]
 [0.99 ]]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.567]
 [0.522]
 [0.516]] [[1.583]
 [1.583]
 [1.835]
 [1.583]
 [1.42 ]] [[0.769]
 [0.769]
 [0.898]
 [0.769]
 [0.71 ]]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.576]
 [0.576]
 [0.576]
 [0.657]] [[1.782]
 [1.719]
 [1.719]
 [1.719]
 [2.154]] [[0.72 ]
 [0.668]
 [0.668]
 [0.668]
 [0.894]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3931 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3931 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3931 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3931 0.5 0.5
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  47 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.511]
 [0.511]
 [0.511]
 [0.547]] [[1.547]
 [1.703]
 [1.703]
 [1.703]
 [2.114]] [[0.792]
 [0.809]
 [0.809]
 [0.809]
 [1.05 ]]
actor:  1 policy actor:  1  step number:  72 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 0.5 0.5
maxi score, test score, baseline:  0.3941 0.5 0.5
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  82 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3941 0.5 0.5
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 0.5 0.5
actor:  1 policy actor:  1  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 0.5 0.5
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.613]
 [0.613]
 [0.613]
 [0.572]] [[0.591]
 [0.902]
 [0.902]
 [0.902]
 [1.097]] [[0.444]
 [0.648]
 [0.648]
 [0.648]
 [0.705]]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.584]] [[2.858]
 [2.858]
 [2.858]
 [2.858]
 [3.052]] [[1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.188]]
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]] [[0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3951 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.73 ]
 [0.713]
 [0.602]
 [0.511]] [[ 0.498]
 [-0.042]
 [ 0.118]
 [ 0.544]
 [ 1.897]] [[0.471]
 [0.462]
 [0.499]
 [0.53 ]
 [0.89 ]]
maxi score, test score, baseline:  0.3951 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3951 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3951 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[3.201]
 [3.201]
 [3.201]
 [3.201]
 [3.201]] [[1.125]
 [1.125]
 [1.125]
 [1.125]
 [1.125]]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3951 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3951 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.001]] [[-0.007]
 [-0.007]
 [-0.   ]
 [-0.007]
 [ 0.001]] [[0.019]
 [0.019]
 [0.021]
 [0.019]
 [0.02 ]]
maxi score, test score, baseline:  0.3951 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3961 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3961 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3971 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3971 0.5 0.5
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.3981 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3981 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.454]
 [0.406]
 [0.412]
 [0.404]] [[0.436]
 [0.668]
 [0.214]
 [0.436]
 [1.229]] [[0.412]
 [0.454]
 [0.406]
 [0.412]
 [0.404]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3981 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3981 0.5 0.5
maxi score, test score, baseline:  0.3981 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3971 0.5 0.5
siam score:  -0.813204
maxi score, test score, baseline:  0.3971 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3981 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81931365
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3981 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.3981 0.5 0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.292 0.208 0.042 0.167 0.292]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3981 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 0.5 0.5
maxi score, test score, baseline:  0.3981 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  82 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  47 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.3991 0.5 0.5
maxi score, test score, baseline:  0.3991 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3991 0.5 0.5
maxi score, test score, baseline:  0.3991 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3991 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3991 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3991 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4001 0.5 0.5
actor:  1 policy actor:  1  step number:  64 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4001 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4011 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4011 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.8222548
maxi score, test score, baseline:  0.4021 0.5 0.5
maxi score, test score, baseline:  0.4021 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4021 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  0.5426304047414359
maxi score, test score, baseline:  0.4021 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4021 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4021 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.46 ]
 [0.428]
 [0.42 ]
 [0.424]] [[0.197]
 [0.324]
 [0.12 ]
 [0.106]
 [0.341]] [[0.41 ]
 [0.46 ]
 [0.428]
 [0.42 ]
 [0.424]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4021 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4021 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4021 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4021 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4021 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4021 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.348]
 [0.59 ]
 [0.568]
 [0.558]] [[-0.09 ]
 [ 0.083]
 [-0.299]
 [-0.676]
 [-0.194]] [[0.478]
 [0.3  ]
 [0.479]
 [0.394]
 [0.465]]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]] [[0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]] [[0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4041 0.5 0.5
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 0.5 0.5
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.345]
 [0.59 ]
 [0.546]
 [0.563]] [[0.505]
 [0.911]
 [0.648]
 [0.512]
 [0.7  ]] [[0.358]
 [0.327]
 [0.485]
 [0.396]
 [0.475]]
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.174]] [[2.986]
 [2.986]
 [2.986]
 [2.986]
 [2.678]] [[1.052]
 [1.052]
 [1.052]
 [1.052]
 [0.969]]
line 256 mcts: sample exp_bonus 5.757798816950394
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.528]
 [0.21 ]
 [0.21 ]
 [0.21 ]] [[3.217]
 [3.758]
 [3.217]
 [3.217]
 [3.217]] [[0.832]
 [1.109]
 [0.832]
 [0.832]
 [0.832]]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.529]
 [0.555]
 [0.555]
 [0.494]] [[0.407]
 [1.142]
 [0.352]
 [0.627]
 [0.893]] [[0.495]
 [0.852]
 [0.472]
 [0.612]
 [0.7  ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.1152342630058527
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 0.5 0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 0.5 0.5
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.407]
 [0.488]
 [0.553]
 [0.544]] [[0.106]
 [0.958]
 [0.89 ]
 [0.225]
 [0.923]] [[0.285]
 [0.432]
 [0.491]
 [0.334]
 [0.558]]
siam score:  -0.82129025
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  0 policy actor:  0  step number:  44 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7848428249135613
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4051 0.5 0.5
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4051 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  80 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4051 0.5 0.5
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.119]] [[5.604]
 [5.604]
 [5.604]
 [5.604]
 [6.642]] [[0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.841]]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.342]
 [0.389]
 [0.342]
 [0.345]] [[0.133]
 [0.133]
 [0.389]
 [0.133]
 [0.272]] [[0.342]
 [0.342]
 [0.389]
 [0.342]
 [0.345]]
siam score:  -0.8229691
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4031 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  55 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4041 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.356]
 [0.557]
 [0.534]
 [0.525]] [[0.233]
 [0.784]
 [0.438]
 [0.297]
 [0.347]] [[0.31 ]
 [0.472]
 [0.501]
 [0.407]
 [0.422]]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4061 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4061 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4061 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4061 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.4061 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.262]
 [0.233]
 [0.233]
 [0.229]] [[-0.9  ]
 [-0.247]
 [-1.202]
 [-1.64 ]
 [-1.11 ]] [[0.22 ]
 [0.262]
 [0.233]
 [0.233]
 [0.229]]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.251]
 [0.208]
 [0.222]
 [0.218]] [[ 0.   ]
 [-0.247]
 [-1.24 ]
 [-1.64 ]
 [-1.11 ]] [[0.186]
 [0.251]
 [0.208]
 [0.222]
 [0.218]]
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.2755235718447331
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.83190084
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.34267583029748455
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.001]
 [0.464]
 [0.487]
 [0.442]] [[ 0.16 ]
 [ 0.137]
 [ 0.229]
 [-0.566]
 [ 0.234]] [[0.474]
 [0.001]
 [0.495]
 [0.254]
 [0.474]]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.46 ]
 [0.533]
 [0.46 ]
 [0.469]] [[-0.571]
 [-0.571]
 [-0.611]
 [-0.571]
 [-0.607]] [[0.383]
 [0.383]
 [0.449]
 [0.383]
 [0.387]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[-0.26]
 [-0.26]
 [-0.26]
 [-0.26]
 [-0.26]] [[0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]] [[0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  54 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4221 0.5 0.5
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4221 0.5 0.5
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.5   0.292 0.042]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.508]
 [0.602]
 [0.586]
 [0.531]] [[-0.254]
 [ 0.   ]
 [-0.395]
 [-0.65 ]
 [ 0.213]] [[0.317]
 [0.377]
 [0.405]
 [0.347]
 [0.435]]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]] [[1.25]
 [1.25]
 [1.25]
 [1.25]
 [1.25]] [[0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]]
actor:  0 policy actor:  0  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4231 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.81695104
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using another actor
from probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
deleting a thread, now have 1 threads
Frames:  71520 train batches done:  8374 episodes:  2738
actor:  1 policy actor:  1  step number:  38 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
siam score:  -0.8189092
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
Printing some Q and Qe and total Qs values:  [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]] [[1.678]
 [1.678]
 [1.678]
 [1.678]
 [1.678]] [[1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.169]]
maxi score, test score, baseline:  0.4211 0.5 0.5
siam score:  -0.8235286
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
siam score:  -0.8230139
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.155]
 [0.155]
 [0.378]
 [0.252]] [[5.156]
 [5.156]
 [5.156]
 [3.844]
 [5.493]] [[0.463]
 [0.463]
 [0.463]
 [0.316]
 [0.536]]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.074]] [[7.987]
 [7.987]
 [7.987]
 [7.987]
 [7.596]] [[1.018]
 [1.018]
 [1.018]
 [1.018]
 [0.925]]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
using explorer policy with actor:  1
siam score:  -0.8152577
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
siam score:  -0.81916
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
siam score:  -0.8189342
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
from probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
start point for exploration sampling:  16339
actor:  0 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
actor:  0 policy actor:  1  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
actor:  0 policy actor:  0  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[0.45]
 [0.45]
 [0.45]
 [0.45]
 [0.45]] [[0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.534]] [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [1.11 ]] [[0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.849]]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.491]
 [0.527]
 [0.493]
 [0.471]] [[-1.313]
 [-0.166]
 [-0.918]
 [-1.374]
 [-1.247]] [[0.298]
 [0.505]
 [0.415]
 [0.305]
 [0.305]]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
actor:  0 policy actor:  0  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.47872378604571497
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12506746356967238, 0.12506746356967238, 0.12506746356967238, 0.6247976092909828]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
line 256 mcts: sample exp_bonus 1.391615959192777
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.034]
 [0.377]
 [0.377]
 [0.398]] [[3.58 ]
 [4.755]
 [3.58 ]
 [3.58 ]
 [5.04 ]] [[0.327]
 [0.523]
 [0.327]
 [0.327]
 [0.77 ]]
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
using another actor
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
actor:  0 policy actor:  1  step number:  48 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
from probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
from probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
using another actor
first move QE:  0.511848796434601
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
actor:  0 policy actor:  1  step number:  45 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[1.662]
 [1.662]
 [1.662]
 [1.662]
 [1.662]] [[1.08]
 [1.08]
 [1.08]
 [1.08]
 [1.08]]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3554260552868247
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
siam score:  -0.8028587
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using another actor
from probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
using another actor
line 256 mcts: sample exp_bonus -0.5151389615333102
siam score:  -0.80405885
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
using explorer policy with actor:  1
from probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
line 256 mcts: sample exp_bonus 1.1086577886163294
siam score:  -0.8038774
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.382]
 [0.611]
 [0.44 ]
 [0.515]] [[0.661]
 [2.427]
 [0.974]
 [1.532]
 [1.632]] [[0.12 ]
 [0.796]
 [0.324]
 [0.452]
 [0.543]]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0000],
        [0.3727],
        [0.2558],
        [0.1324],
        [0.3054],
        [0.3007],
        [0.2329],
        [0.3722]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.37267308926593173
0.0 0.255789047432155
0.0 0.13244255862950113
0.0 0.3054432848798004
0.0 0.3007475035456134
0.0 0.23287509321891367
0.0 0.3722300840607923
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
first move QE:  0.5117202272945551
siam score:  -0.7942979
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.4881075744330883
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
actor:  0 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
siam score:  -0.7874033
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.498]
 [0.158]
 [0.158]
 [0.336]] [[3.215]
 [4.18 ]
 [3.215]
 [3.215]
 [3.737]] [[0.477]
 [0.889]
 [0.477]
 [0.477]
 [0.697]]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
siam score:  -0.7976039
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
siam score:  -0.79275316
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
using another actor
from probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
siam score:  -0.7936252
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.419]
 [0.149]
 [0.269]
 [0.343]] [[0.494]
 [2.173]
 [0.535]
 [1.713]
 [2.005]] [[0.166]
 [0.833]
 [0.037]
 [0.556]
 [0.717]]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
line 256 mcts: sample exp_bonus 1.6346417661951178
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
siam score:  -0.79558426
line 256 mcts: sample exp_bonus 2.6930546818726513
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.353]
 [0.565]
 [0.539]
 [0.527]] [[-0.066]
 [ 0.73 ]
 [ 0.556]
 [ 0.235]
 [ 0.673]] [[0.196]
 [0.389]
 [0.542]
 [0.409]
 [0.543]]
siam score:  -0.79394346
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.247]
 [0.247]
 [0.324]
 [0.386]] [[4.593]
 [4.593]
 [4.593]
 [4.641]
 [4.711]] [[0.72 ]
 [0.72 ]
 [0.72 ]
 [0.755]
 [0.791]]
first move QE:  0.5129700819962212
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.544]
 [0.544]
 [0.544]
 [0.566]] [[0.995]
 [0.659]
 [0.659]
 [0.659]
 [1.053]] [[0.687]
 [0.572]
 [0.572]
 [0.572]
 [0.709]]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12505247795925714, 0.12505247795925714, 0.12505247795925714, 0.6248425661222287]
actor:  0 policy actor:  1  step number:  86 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
siam score:  -0.7931944
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
using explorer policy with actor:  1
first move QE:  0.5096991556695878
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4312741838097571
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504748195685642, 0.12504748195685642, 0.12504748195685642, 0.6248575541294309]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.359]
 [0.435]
 [0.495]
 [0.446]] [[1.233]
 [1.793]
 [1.233]
 [1.529]
 [1.707]] [[0.566]
 [0.796]
 [0.566]
 [0.766]
 [0.819]]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
actor:  0 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]] [[0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.12504498380582987, 0.12504498380582987, 0.12504498380582987, 0.6248650485825104]
actor:  0 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
actor:  0 policy actor:  1  step number:  55 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
siam score:  -0.7681911
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  86 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
actor:  0 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
siam score:  -0.7705362
actor:  1 policy actor:  1  step number:  53 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
siam score:  -0.78736556
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
siam score:  -0.7900141
siam score:  -0.7890634
line 256 mcts: sample exp_bonus 2.0167087486695463
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
using explorer policy with actor:  1
from probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
actor:  0 policy actor:  0  step number:  44 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.1250434848672662, 0.1250434848672662, 0.1250434848672662, 0.6248695453982014]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.794841730420012
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.415852535919845
from probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
actor:  0 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
siam score:  -0.79570407
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
actor:  1 policy actor:  1  step number:  61 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504248555491135, 0.12504248555491135, 0.12504248555491135, 0.624872543335266]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.396]
 [0.54 ]
 [0.396]
 [0.398]] [[0.755]
 [0.909]
 [0.837]
 [0.909]
 [0.83 ]] [[0.592]
 [0.673]
 [0.779]
 [0.673]
 [0.636]]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504177175058623, 0.12504177175058623, 0.12504177175058623, 0.6248746847482413]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504177175058623, 0.12504177175058623, 0.12504177175058623, 0.6248746847482413]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504177175058623, 0.12504177175058623, 0.12504177175058623, 0.6248746847482413]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504177175058623, 0.12504177175058623, 0.12504177175058623, 0.6248746847482413]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504177175058623, 0.12504177175058623, 0.12504177175058623, 0.6248746847482413]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504123639199063, 0.12504123639199063, 0.12504123639199063, 0.624876290824028]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504123639199063, 0.12504123639199063, 0.12504123639199063, 0.624876290824028]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504123639199063, 0.12504123639199063, 0.12504123639199063, 0.624876290824028]
first move QE:  0.5099117777968984
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504123639199063, 0.12504123639199063, 0.12504123639199063, 0.624876290824028]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.476]] [[1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.819]] [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.717]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504123639199063, 0.12504123639199063, 0.12504123639199063, 0.624876290824028]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504048688225017, 0.12504048688225017, 0.12504048688225017, 0.6248785393532496]
from probs:  [0.12504048688225017, 0.12504048688225017, 0.12504048688225017, 0.6248785393532496]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504048688225017, 0.12504048688225017, 0.12504048688225017, 0.6248785393532496]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504048688225017, 0.12504048688225017, 0.12504048688225017, 0.6248785393532496]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504048688225017, 0.12504048688225017, 0.12504048688225017, 0.6248785393532496]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504048688225017, 0.12504048688225017, 0.12504048688225017, 0.6248785393532496]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504048688225017, 0.12504048688225017, 0.12504048688225017, 0.6248785393532496]
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.322]
 [0.139]
 [0.382]
 [0.351]] [[1.146]
 [5.318]
 [2.038]
 [3.684]
 [4.246]] [[0.007]
 [0.875]
 [0.197]
 [0.581]
 [0.679]]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504048688225017, 0.12504048688225017, 0.12504048688225017, 0.6248785393532496]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7833115
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
using another actor
actor:  0 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.308]
 [0.56 ]
 [0.533]
 [0.459]] [[0.67 ]
 [1.049]
 [0.794]
 [0.565]
 [1.278]] [[0.322]
 [0.337]
 [0.463]
 [0.321]
 [0.603]]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
using explorer policy with actor:  1
siam score:  -0.7731998
siam score:  -0.77353376
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.364]
 [0.381]
 [0.35 ]
 [0.334]] [[-0.425]
 [ 0.22 ]
 [-0.365]
 [-0.442]
 [ 0.114]] [[0.289]
 [0.364]
 [0.381]
 [0.35 ]
 [0.334]]
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.77898085
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
actor:  0 policy actor:  0  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.79031754
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
actor:  0 policy actor:  1  step number:  77 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
actor:  0 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.472]] [[0.92]
 [0.92]
 [0.92]
 [0.92]
 [1.14]] [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.518]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.344]
 [0.333]
 [0.458]
 [0.314]] [[1.429]
 [2.089]
 [1.429]
 [0.686]
 [1.73 ]] [[0.444]
 [0.72 ]
 [0.444]
 [0.217]
 [0.555]]
siam score:  -0.79226667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.164]
 [0.164]
 [0.164]
 [0.164]] [[0.117]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.156]
 [0.164]
 [0.164]
 [0.164]
 [0.164]]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.46 ]] [[1.018]
 [1.018]
 [1.018]
 [1.018]
 [0.966]] [[0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.814]]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
actor:  0 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
line 256 mcts: sample exp_bonus 1.1023462940940523
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.4191 0.5 0.5
using explorer policy with actor:  0
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
first move QE:  0.5094151076612016
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.12504021433102439, 0.12504021433102439, 0.12504021433102439, 0.6248793570069269]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7898909
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.11951360406717688, 0.11951360406717688, 0.16373878486121965, 0.5972340070044266]
actor:  0 policy actor:  0  step number:  50 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.548]
 [0.263]
 [0.49 ]
 [0.485]] [[5.006]
 [4.694]
 [5.006]
 [3.344]
 [4.339]] [[0.79 ]
 [0.806]
 [0.79 ]
 [0.601]
 [0.742]]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11951360406717688, 0.11951360406717688, 0.16373878486121965, 0.5972340070044266]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11951360406717688, 0.11951360406717688, 0.16373878486121965, 0.5972340070044266]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11951360406717688, 0.11951360406717688, 0.16373878486121965, 0.5972340070044266]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11951360406717688, 0.11951360406717688, 0.16373878486121965, 0.5972340070044266]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11951360406717688, 0.11951360406717688, 0.16373878486121965, 0.5972340070044266]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11951360406717688, 0.11951360406717688, 0.16373878486121965, 0.5972340070044266]
actor:  0 policy actor:  1  step number:  45 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.11951360406717688, 0.11951360406717688, 0.16373878486121965, 0.5972340070044266]
actor:  0 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11952434090335569, 0.11952434090335569, 0.16366360311466976, 0.5972877150786189]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11952434090335569, 0.11952434090335569, 0.16366360311466976, 0.5972877150786189]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11952434090335569, 0.11952434090335569, 0.16366360311466976, 0.5972877150786189]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  0
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11952434090335569, 0.11952434090335569, 0.16366360311466976, 0.5972877150786189]
actor:  0 policy actor:  1  step number:  64 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.11952434090335569, 0.11952434090335569, 0.16366360311466976, 0.5972877150786189]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  0 policy actor:  0  step number:  51 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.11952434090335569, 0.11952434090335569, 0.16366360311466976, 0.5972877150786189]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.11952434090335569, 0.11952434090335569, 0.16366360311466976, 0.5972877150786189]
maxi score, test score, baseline:  0.4211 0.5 0.5
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.11952434090335569, 0.11952434090335569, 0.16366360311466976, 0.5972877150786189]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.11996509223053561, 0.11996509223053561, 0.16057561932905665, 0.5994941962098721]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.479]
 [0.511]
 [0.515]
 [0.509]] [[0.768]
 [0.768]
 [1.332]
 [0.724]
 [1.126]] [[0.344]
 [0.344]
 [0.752]
 [0.35 ]
 [0.613]]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11996509223053561, 0.11996509223053561, 0.16057561932905665, 0.5994941962098721]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11996509223053561, 0.11996509223053561, 0.16057561932905665, 0.5994941962098721]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11996509223053561, 0.11996509223053561, 0.16057561932905665, 0.5994941962098721]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11996509223053561, 0.11996509223053561, 0.16057561932905665, 0.5994941962098721]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11996509223053561, 0.11996509223053561, 0.16057561932905665, 0.5994941962098721]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11996509223053561, 0.11996509223053561, 0.16057561932905665, 0.5994941962098721]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11996509223053561, 0.11996509223053561, 0.16057561932905665, 0.5994941962098721]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  49 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11530476976361223, 0.11530476976361223, 0.193208315302298, 0.5761821451704775]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.629]] [[-0.238]
 [-0.238]
 [-0.238]
 [-0.238]
 [-0.035]] [[0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.647]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.328]
 [0.52 ]
 [0.46 ]
 [0.483]] [[ 0.009]
 [ 0.349]
 [ 0.602]
 [-0.146]
 [ 0.214]] [[0.352]
 [0.463]
 [0.823]
 [0.264]
 [0.528]]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11530476976361223, 0.11530476976361223, 0.193208315302298, 0.5761821451704775]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11530476976361223, 0.11530476976361223, 0.193208315302298, 0.5761821451704775]
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11530476976361223, 0.11530476976361223, 0.193208315302298, 0.5761821451704775]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[4.624]
 [4.624]
 [4.624]
 [4.624]
 [4.624]] [[0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11530476976361223, 0.11530476976361223, 0.193208315302298, 0.5761821451704775]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11530476976361223, 0.11530476976361223, 0.193208315302298, 0.5761821451704775]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4201 0.5 0.5
from probs:  [0.11532294338927994, 0.11532294338927994, 0.19308105921533078, 0.5762730540061093]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11534104928862202, 0.11534104928862202, 0.1929542773643442, 0.5763636240584119]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.079]] [[1.016]
 [1.016]
 [1.016]
 [1.016]
 [0.486]] [[0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.079]]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11534104928862202, 0.11534104928862202, 0.1929542773643442, 0.5763636240584119]
siam score:  -0.77964103
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11534104928862202, 0.11534104928862202, 0.1929542773643442, 0.5763636240584119]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11534104928862202, 0.11534104928862202, 0.1929542773643442, 0.5763636240584119]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11534104928862202, 0.11534104928862202, 0.1929542773643442, 0.5763636240584119]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11534104928862202, 0.11534104928862202, 0.1929542773643442, 0.5763636240584119]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11534104928862202, 0.11534104928862202, 0.1929542773643442, 0.5763636240584119]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11534104928862202, 0.11534104928862202, 0.1929542773643442, 0.5763636240584119]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1153590878395204, 0.1153590878395204, 0.19282796710331765, 0.5764538572176416]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1153590878395204, 0.1153590878395204, 0.19282796710331765, 0.5764538572176416]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1153590878395204, 0.1153590878395204, 0.19282796710331765, 0.5764538572176416]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1153590878395204, 0.1153590878395204, 0.19282796710331765, 0.5764538572176416]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1153590878395204, 0.1153590878395204, 0.19282796710331765, 0.5764538572176416]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.383]
 [0.383]
 [0.383]
 [0.407]] [[4.653]
 [2.973]
 [2.973]
 [2.973]
 [3.31 ]] [[0.927]
 [0.512]
 [0.512]
 [0.512]
 [0.596]]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
first move QE:  0.5086683732159988
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
actor:  1 policy actor:  1  step number:  93 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
actor:  0 policy actor:  1  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
using another actor
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
actor:  0 policy actor:  0  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.25  0.042 0.    0.125 0.583]
from probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
siam score:  -0.78166384
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.11604997745265742, 0.11604997745265742, 0.18798876507206805, 0.5799112800226172]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.285]
 [0.308]
 [0.327]
 [0.314]] [[0.417]
 [0.54 ]
 [0.394]
 [0.416]
 [0.662]] [[0.316]
 [0.285]
 [0.308]
 [0.327]
 [0.314]]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.11606676602697172, 0.11606676602697172, 0.18787120724022907, 0.5799952607058275]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.11606676602697172, 0.11606676602697172, 0.18787120724022907, 0.5799952607058275]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.11606676602697172, 0.11606676602697172, 0.18787120724022907, 0.5799952607058275]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.11606676602697172, 0.11606676602697172, 0.18787120724022907, 0.5799952607058275]
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.11608349201250995, 0.11608349201250995, 0.18775408767078666, 0.5800789283041936]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.11608349201250995, 0.11608349201250995, 0.18775408767078666, 0.5800789283041936]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.11608349201250995, 0.11608349201250995, 0.18775408767078666, 0.5800789283041936]
actor:  0 policy actor:  1  step number:  69 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using explorer policy with actor:  1
siam score:  -0.78260833
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.1161001557586227, 0.1161001557586227, 0.18763740391749933, 0.5801622845652553]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.1161001557586227, 0.1161001557586227, 0.18763740391749933, 0.5801622845652553]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.1161001557586227, 0.1161001557586227, 0.18763740391749933, 0.5801622845652553]
siam score:  -0.78320885
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.1161001557586227, 0.1161001557586227, 0.18763740391749933, 0.5801622845652553]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.1161001557586227, 0.1161001557586227, 0.18763740391749933, 0.5801622845652553]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using another actor
actor:  0 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.1161001557586227, 0.1161001557586227, 0.18763740391749933, 0.5801622845652553]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.1161001557586227, 0.1161001557586227, 0.18763740391749933, 0.5801622845652553]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11611675761206562, 0.11611675761206562, 0.1875211535522973, 0.5802453312235715]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11611675761206562, 0.11611675761206562, 0.1875211535522973, 0.5802453312235715]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 2.375471129144465
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11611675761206562, 0.11611675761206562, 0.1875211535522973, 0.5802453312235715]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11611675761206562, 0.11611675761206562, 0.1875211535522973, 0.5802453312235715]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11611675761206562, 0.11611675761206562, 0.1875211535522973, 0.5802453312235715]
maxi score, test score, baseline:  0.4191 0.5 0.5
from probs:  [0.11611675761206562, 0.11611675761206562, 0.1875211535522973, 0.5802453312235715]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11611675761206562, 0.11611675761206562, 0.1875211535522973, 0.5802453312235715]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11611675761206562, 0.11611675761206562, 0.1875211535522973, 0.5802453312235715]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11611675761206562, 0.11611675761206562, 0.1875211535522973, 0.5802453312235715]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11611675761206562, 0.11611675761206562, 0.1875211535522973, 0.5802453312235715]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.11211689178382135, 0.11211689178382135, 0.2155292229459553, 0.560236993486402]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10838374871878943, 0.10838374871878943, 0.24166963227757585, 0.5415628702848453]
actor:  0 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10838374871878943, 0.10838374871878943, 0.24166963227757585, 0.5415628702848453]
actor:  0 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.77030164
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10838374871878943, 0.10838374871878943, 0.24166963227757585, 0.5415628702848453]
siam score:  -0.77058274
siam score:  -0.77371407
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10838374871878943, 0.10838374871878943, 0.24166963227757585, 0.5415628702848453]
siam score:  -0.77542114
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10838374871878943, 0.10838374871878943, 0.24166963227757585, 0.5415628702848453]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10838374871878943, 0.10838374871878943, 0.24166963227757585, 0.5415628702848453]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10838374871878943, 0.10838374871878943, 0.24166963227757585, 0.5415628702848453]
siam score:  -0.77655965
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10838374871878943, 0.10838374871878943, 0.24166963227757585, 0.5415628702848453]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1048915104439417, 0.1048915104439417, 0.2661231655062287, 0.5240938136058879]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1048915104439417, 0.1048915104439417, 0.2661231655062287, 0.5240938136058879]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.208 0.25  0.25  0.125 0.167]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1048915104439417, 0.1048915104439417, 0.2661231655062287, 0.5240938136058879]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1048915104439417, 0.1048915104439417, 0.2661231655062287, 0.5240938136058879]
using explorer policy with actor:  1
siam score:  -0.77501357
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.644]
 [0.434]
 [0.434]
 [0.478]] [[3.355]
 [4.346]
 [3.355]
 [3.355]
 [5.942]] [[0.358]
 [0.706]
 [0.358]
 [0.358]
 [1.065]]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.197]
 [0.218]
 [0.206]
 [0.197]] [[-0.94 ]
 [ 0.   ]
 [-0.829]
 [-1.483]
 [ 0.   ]] [[0.189]
 [0.197]
 [0.218]
 [0.206]
 [0.197]]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1048915104439417, 0.1048915104439417, 0.2661231655062287, 0.5240938136058879]
actor:  0 policy actor:  0  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1048915104439417, 0.1048915104439417, 0.2661231655062287, 0.5240938136058879]
siam score:  -0.76992875
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.1048915104439417, 0.1048915104439417, 0.2661231655062287, 0.5240938136058879]
maxi score, test score, baseline:  0.4201 0.5 0.5
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.466]
 [0.537]
 [0.537]
 [0.523]] [[1.044]
 [1.068]
 [1.044]
 [1.044]
 [1.048]] [[1.036]
 [0.98 ]
 [1.036]
 [1.036]
 [1.024]]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
siam score:  -0.7704471
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
siam score:  -0.76468205
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.456]] [[7.022]
 [7.022]
 [7.022]
 [7.022]
 [5.745]] [[1.034]
 [1.034]
 [1.034]
 [1.034]
 [0.818]]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4221 0.5 0.5
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
from probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
line 256 mcts: sample exp_bonus 1.3358832006819912
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]] [[1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.384]] [[0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]] [[0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]] [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
siam score:  -0.777314
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
siam score:  -0.7791925
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
actor:  0 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
first move QE:  0.509268668893347
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
from probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
actor:  0 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
siam score:  -0.7801587
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
first move QE:  0.5080654925689997
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.389]
 [0.563]
 [0.505]
 [0.452]] [[0.451]
 [0.692]
 [0.591]
 [0.451]
 [0.812]] [[0.601]
 [0.565]
 [0.706]
 [0.601]
 [0.668]]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10611240559459201, 0.10611240559459201, 0.25757303128449516, 0.5302021575263208]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
start point for exploration sampling:  16339
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
using explorer policy with actor:  1
siam score:  -0.7484278
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
first move QE:  0.5069753292183202
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
first move QE:  0.5058967044443802
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
siam score:  -0.76117676
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.10299513590305674, 0.1323961087224454, 0.25, 0.5146087553744978]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7614117
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.1000560362013282, 0.15717754621986982, 0.24285981124768227, 0.4999066063311197]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 0.5 0.5
probs:  [0.1000560362013282, 0.15717754621986982, 0.24285981124768227, 0.4999066063311197]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.1000560362013282, 0.15717754621986982, 0.24285981124768227, 0.4999066063311197]
maxi score, test score, baseline:  0.4211 0.5 0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1000560362013282, 0.15717754621986982, 0.24285981124768227, 0.4999066063311197]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1000560362013282, 0.15717754621986982, 0.24285981124768227, 0.4999066063311197]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1000560362013282, 0.15717754621986982, 0.24285981124768227, 0.4999066063311197]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1000560362013282, 0.15717754621986982, 0.24285981124768227, 0.4999066063311197]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1000560362013282, 0.15717754621986982, 0.24285981124768227, 0.4999066063311197]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1000560362013282, 0.15717754621986982, 0.24285981124768227, 0.4999066063311197]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1000560362013282, 0.15717754621986982, 0.24285981124768227, 0.4999066063311197]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.1000560362013282, 0.15717754621986982, 0.24285981124768227, 0.4999066063311197]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]] [[0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.494]] [[1.145]
 [1.145]
 [1.145]
 [1.145]
 [1.338]] [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [1.035]]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.69 ]
 [0.491]
 [0.491]
 [0.554]] [[0.563]
 [0.344]
 [0.563]
 [0.563]
 [0.662]] [[0.392]
 [0.517]
 [0.392]
 [0.392]
 [0.487]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5392440597224981
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
using another actor
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
actor:  0 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.501]
 [0.501]
 [0.55 ]
 [0.484]] [[0.818]
 [0.647]
 [0.647]
 [0.694]
 [0.918]] [[0.606]
 [0.519]
 [0.519]
 [0.592]
 [0.638]]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
siam score:  -0.7558998
actor:  1 policy actor:  1  step number:  99 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
actor:  0 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.711]
 [0.627]
 [0.542]
 [0.485]] [[ 1.098]
 [-0.109]
 [ 0.106]
 [ 0.387]
 [ 0.749]] [[0.749]
 [0.242]
 [0.301]
 [0.404]
 [0.588]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[1.13]
 [1.13]
 [1.13]
 [1.13]
 [1.13]] [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
actor:  0 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
start point for exploration sampling:  16339
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10140629086327, 0.15544036691299004, 0.23649148098757, 0.50666186123617]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10261806702571469, 0.15388134806024872, 0.23077626961204975, 0.5127243153019868]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10261806702571469, 0.15388134806024872, 0.23077626961204975, 0.5127243153019868]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10261806702571469, 0.15388134806024872, 0.23077626961204975, 0.5127243153019868]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10261806702571469, 0.15388134806024872, 0.23077626961204975, 0.5127243153019868]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10261806702571469, 0.15388134806024872, 0.23077626961204975, 0.5127243153019868]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10261806702571469, 0.15388134806024872, 0.23077626961204975, 0.5127243153019868]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10261806702571469, 0.15388134806024872, 0.23077626961204975, 0.5127243153019868]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10261806702571469, 0.15388134806024872, 0.23077626961204975, 0.5127243153019868]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10261806702571469, 0.15388134806024872, 0.23077626961204975, 0.5127243153019868]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10261806702571469, 0.15388134806024872, 0.23077626961204975, 0.5127243153019868]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10261806702571469, 0.15388134806024872, 0.23077626961204975, 0.5127243153019868]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10261806702571469, 0.15388134806024872, 0.23077626961204975, 0.5127243153019868]
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.485]
 [0.306]
 [0.306]
 [0.306]] [[2.126]
 [2.345]
 [2.126]
 [2.126]
 [2.126]] [[0.894]
 [1.17 ]
 [0.894]
 [0.894]
 [0.894]]
actor:  0 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.218]
 [0.221]
 [0.213]
 [0.218]] [[-0.587]
 [-0.587]
 [-0.758]
 [-0.418]
 [-0.439]] [[0.218]
 [0.218]
 [0.221]
 [0.213]
 [0.218]]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4211 0.5 0.5
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.47 ]
 [0.525]
 [0.47 ]
 [0.476]] [[0.219]
 [0.219]
 [0.561]
 [0.219]
 [0.185]] [[0.546]
 [0.546]
 [0.83 ]
 [0.546]
 [0.53 ]]
maxi score, test score, baseline:  0.4211 0.5 0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[2.517]
 [2.256]
 [2.256]
 [2.256]
 [2.256]] [[1.08 ]
 [1.072]
 [1.072]
 [1.072]
 [1.072]]
siam score:  -0.7722645
siam score:  -0.77209777
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.369]] [[3.945]
 [3.945]
 [3.945]
 [3.945]
 [4.053]] [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.647]]
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.413]] [[1.384]
 [1.384]
 [1.384]
 [1.384]
 [5.032]] [[0.408]
 [0.408]
 [0.408]
 [0.408]
 [1.093]]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
siam score:  -0.76557547
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
UNIT TEST: sample policy line 217 mcts : [0.708 0.042 0.042 0.042 0.167]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10371162856017135, 0.15247441904011422, 0.2256186047600286, 0.518195347639686]
maxi score, test score, baseline:  0.4211 0.5 0.5
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
actor:  0 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
actor:  0 policy actor:  0  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.407]
 [0.533]
 [0.533]
 [0.508]] [[0.831]
 [1.864]
 [0.831]
 [0.831]
 [1.169]] [[0.577]
 [1.064]
 [0.577]
 [0.577]
 [0.751]]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
from probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
actor:  0 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.75  0.042 0.125]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.001]
 [0.015]
 [0.008]
 [0.002]] [[ 0.013]
 [ 0.003]
 [-0.004]
 [ 0.008]
 [ 0.025]] [[0.014]
 [0.001]
 [0.011]
 [0.011]
 [0.017]]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.369]
 [0.33 ]
 [0.33 ]
 [0.33 ]] [[0.716]
 [1.488]
 [0.716]
 [0.716]
 [0.716]] [[0.463]
 [1.002]
 [0.463]
 [0.463]
 [0.463]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
siam score:  -0.75933486
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
line 256 mcts: sample exp_bonus 0.6969126931509608
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
siam score:  -0.7629335
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
first move QE:  0.49357206158846206
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
first move QE:  0.4934387758944771
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.105948020403087
maxi score, test score, baseline:  0.4211 0.5 0.5
maxi score, test score, baseline:  0.4211 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
using another actor
using another actor
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
actor:  0 policy actor:  1  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4201 0.5 0.5
using another actor
from probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
from probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4191 0.5 0.5
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.7497204
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10124530292168506, 0.17264755751927627, 0.22024906058433702, 0.5058580789747017]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
from probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.407]
 [0.379]
 [0.379]
 [0.379]] [[1.539]
 [2.475]
 [1.539]
 [1.539]
 [1.539]] [[0.412]
 [0.681]
 [0.412]
 [0.412]
 [0.412]]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.128481817796653
using another actor
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
actor:  0 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
actor:  1 policy actor:  1  step number:  79 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10232669515322582, 0.1704836050825062, 0.21592154503535982, 0.5112681547289081]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10005557939856959, 0.16669754411031645, 0.2333395088220633, 0.4999073676690506]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1311412408530712
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10005557939856959, 0.16669754411031645, 0.2333395088220633, 0.4999073676690506]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10005557939856959, 0.16669754411031645, 0.2333395088220633, 0.4999073676690506]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.10005557939856959, 0.16669754411031645, 0.2333395088220633, 0.4999073676690506]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.10005557939856959, 0.16669754411031645, 0.2333395088220633, 0.4999073676690506]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10005557939856959, 0.16669754411031645, 0.2333395088220633, 0.4999073676690506]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10005557939856959, 0.16669754411031645, 0.2333395088220633, 0.4999073676690506]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10005557939856959, 0.16669754411031645, 0.2333395088220633, 0.4999073676690506]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10005557939856959, 0.16669754411031645, 0.2333395088220633, 0.4999073676690506]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10005557939856959, 0.16669754411031645, 0.2333395088220633, 0.4999073676690506]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.10005557939856959, 0.16669754411031645, 0.2333395088220633, 0.4999073676690506]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [0.547]
 [0.557]
 [0.516]] [[2.557]
 [2.557]
 [2.557]
 [1.951]
 [2.428]] [[1.261]
 [1.261]
 [1.261]
 [0.932]
 [1.164]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
using another actor
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
actor:  0 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.293797982850112
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.503]] [[2.255]
 [2.255]
 [2.255]
 [2.255]
 [1.304]] [[0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.151]]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
actor:  1 policy actor:  1  step number:  67 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
siam score:  -0.75017744
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
actor:  1 policy actor:  1  step number:  98 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
siam score:  -0.7553349
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.4191 0.5 0.5
actor:  1 policy actor:  1  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.123]
 [0.151]
 [0.149]
 [0.141]] [[-0.562]
 [-0.327]
 [-0.195]
 [-0.693]
 [-0.452]] [[0.135]
 [0.123]
 [0.151]
 [0.149]
 [0.141]]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
siam score:  -0.7463442
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09788322995187894, 0.1630761314010737, 0.25, 0.48904063864704733]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3505609842576085
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
UNIT TEST: sample policy line 217 mcts : [0.042 0.167 0.208 0.083 0.5  ]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
first move QE:  0.48801005124659186
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[3.004]
 [3.004]
 [3.004]
 [3.004]
 [3.004]] [[2.436]
 [2.436]
 [2.436]
 [2.436]
 [2.436]]
siam score:  -0.75546014
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.522]
 [0.522]
 [0.527]
 [0.473]] [[1.273]
 [2.968]
 [2.968]
 [2.443]
 [2.446]] [[0.198]
 [1.144]
 [1.144]
 [0.864]
 [0.821]]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.459]
 [0.459]
 [0.527]
 [0.459]] [[0.729]
 [0.729]
 [0.729]
 [1.192]
 [0.729]] [[0.463]
 [0.463]
 [0.463]
 [0.686]
 [0.463]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.3443],
        [0.2524],
        [0.3109],
        [0.2448],
        [0.2002],
        [0.0000],
        [0.4125],
        [0.3147],
        [0.3147],
        [0.0000]], dtype=torch.float64)
0.0 0.3443474824841301
0.0 0.25238497095019846
0.0 0.31091360482303554
0.0 0.24478565434372287
0.0 0.2002349488775412
0.0 0.0
0.0 0.41253593937414007
0.0 0.31467233632513963
0.0 0.31467233632513963
0.0 0.0
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]] [[1.873]
 [1.873]
 [1.873]
 [1.873]
 [1.873]] [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[-0.273]
 [-0.273]
 [-0.273]
 [-0.273]
 [-0.273]] [[0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4191 0.5 0.5
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
start point for exploration sampling:  16339
siam score:  -0.76142365
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.479]
 [0.479]
 [0.51 ]
 [0.492]] [[1.044]
 [1.044]
 [1.044]
 [1.451]
 [1.384]] [[0.501]
 [0.501]
 [0.501]
 [0.668]
 [0.628]]
using another actor
actor:  0 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.76304823
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
from probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
using another actor
from probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
siam score:  -0.76454276
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  0 policy actor:  1  step number:  44 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
UNIT TEST: sample policy line 217 mcts : [0.167 0.333 0.167 0.083 0.25 ]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  0 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  0 policy actor:  0  step number:  57 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
from probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]] [[0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  0 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  1 policy actor:  1  step number:  62 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.77553755
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
line 256 mcts: sample exp_bonus 2.9456050598103616
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09901457540399534, 0.16149130282303176, 0.24479360604841363, 0.4947005157245592]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10005541951694652, 0.1600332517101679, 0.24000369463446308, 0.49990763413842243]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10005541951694652, 0.1600332517101679, 0.24000369463446308, 0.49990763413842243]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10005541951694652, 0.1600332517101679, 0.24000369463446308, 0.49990763413842243]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4201 0.5 0.5
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10005541951694652, 0.1600332517101679, 0.24000369463446308, 0.49990763413842243]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10005541951694652, 0.1600332517101679, 0.24000369463446308, 0.49990763413842243]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10005541951694652, 0.1600332517101679, 0.24000369463446308, 0.49990763413842243]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10005541951694652, 0.1600332517101679, 0.24000369463446308, 0.49990763413842243]
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.625 0.125 0.208]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.10005541951694652, 0.1600332517101679, 0.24000369463446308, 0.49990763413842243]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
maxi score, test score, baseline:  0.4201 0.5 0.5
maxi score, test score, baseline:  0.4201 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
maxi score, test score, baseline:  0.4191 0.5 0.5
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.685]
 [0.618]
 [0.602]
 [0.431]] [[0.995]
 [0.783]
 [0.894]
 [1.253]
 [2.733]] [[0.444]
 [0.477]
 [0.447]
 [0.55 ]
 [0.872]]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
actor:  1 policy actor:  1  step number:  63 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
maxi score, test score, baseline:  0.4191 0.5 0.5
from probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.118]
 [0.398]
 [0.332]
 [0.317]] [[0.407]
 [0.066]
 [0.266]
 [0.625]
 [0.594]] [[0.31 ]
 [0.118]
 [0.398]
 [0.332]
 [0.317]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5944204619430006
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09809604563626001, 0.15689757635770774, 0.25490012756012065, 0.49010625044591166]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09621204897838663, 0.15388253061149165, 0.2692234938777016, 0.48068192653242003]
using another actor
from probs:  [0.09621204897838663, 0.15388253061149165, 0.2692234938777016, 0.48068192653242003]
siam score:  -0.7776577
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09621204897838663, 0.15388253061149165, 0.2692234938777016, 0.48068192653242003]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09621204897838663, 0.15388253061149165, 0.2692234938777016, 0.48068192653242003]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09621204897838663, 0.15388253061149165, 0.2692234938777016, 0.48068192653242003]
Printing some Q and Qe and total Qs values:  [[0.55]
 [0.55]
 [0.55]
 [0.55]
 [0.55]] [[0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]] [[1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.093]]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09621204897838663, 0.15388253061149165, 0.2692234938777016, 0.48068192653242003]
siam score:  -0.7689301
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09621204897838663, 0.15388253061149165, 0.2692234938777016, 0.48068192653242003]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09621204897838663, 0.15388253061149165, 0.2692234938777016, 0.48068192653242003]
siam score:  -0.76746076
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09621204897838663, 0.15388253061149165, 0.2692234938777016, 0.48068192653242003]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09621204897838663, 0.15388253061149165, 0.2692234938777016, 0.48068192653242003]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09621204897838663, 0.15388253061149165, 0.2692234938777016, 0.48068192653242003]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.0943991619810382, 0.16984199253568635, 0.2641455307289965, 0.4716133147542789]
using another actor
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0943991619810382, 0.16984199253568635, 0.2641455307289965, 0.4716133147542789]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0943991619810382, 0.16984199253568635, 0.2641455307289965, 0.4716133147542789]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0943991619810382, 0.16984199253568635, 0.2641455307289965, 0.4716133147542789]
from probs:  [0.0943991619810382, 0.16984199253568635, 0.2641455307289965, 0.4716133147542789]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0943991619810382, 0.16984199253568635, 0.2641455307289965, 0.4716133147542789]
siam score:  -0.75264925
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0943991619810382, 0.16984199253568635, 0.2641455307289965, 0.4716133147542789]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0943991619810382, 0.16984199253568635, 0.2641455307289965, 0.4716133147542789]
actor:  0 policy actor:  1  step number:  61 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.49 ]
 [0.463]
 [0.413]
 [0.436]] [[0.291]
 [0.048]
 [0.112]
 [0.212]
 [0.345]] [[0.431]
 [0.49 ]
 [0.463]
 [0.413]
 [0.436]]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 2.016080170800663
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09551319426370497, 0.1682128675513732, 0.25908745916095854, 0.4771864790239632]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  38 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4191 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.428]
 [0.424]
 [0.389]
 [0.4  ]] [[ 0.502]
 [-0.027]
 [ 0.009]
 [ 0.038]
 [ 0.186]] [[0.379]
 [0.428]
 [0.424]
 [0.389]
 [0.4  ]]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.353]
 [0.38 ]
 [0.347]
 [0.35 ]] [[0.417]
 [0.228]
 [0.103]
 [0.516]
 [0.8  ]] [[0.325]
 [0.353]
 [0.38 ]
 [0.347]
 [0.35 ]]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
siam score:  -0.77759063
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
siam score:  -0.7768782
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09380991005592858, 0.16521109403036122, 0.27231286999201026, 0.4686661259217001]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09488663625985143, 0.1638259090332508, 0.2672348181933499, 0.4740526365135479]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09488663625985143, 0.1638259090332508, 0.2672348181933499, 0.4740526365135479]
first move QE:  0.4746724047573241
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09488663625985143, 0.1638259090332508, 0.2672348181933499, 0.4740526365135479]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.77651155
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09328058298945423, 0.16105114169671728, 0.27964961943442757, 0.46601865587940094]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09328058298945423, 0.16105114169671728, 0.27964961943442757, 0.46601865587940094]
actor:  0 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09328058298945423, 0.16105114169671728, 0.27964961943442757, 0.46601865587940094]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.77334344
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09328058298945423, 0.16105114169671728, 0.27964961943442757, 0.46601865587940094]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.]
 [-0.]
 [-0.]
 [ 0.]
 [-0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09328058298945423, 0.16105114169671728, 0.27964961943442757, 0.46601865587940094]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09328058298945423, 0.16105114169671728, 0.27964961943442757, 0.46601865587940094]
siam score:  -0.7743362
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09328058298945423, 0.16105114169671728, 0.27964961943442757, 0.46601865587940094]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09328058298945423, 0.16105114169671728, 0.27964961943442757, 0.46601865587940094]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09328058298945423, 0.16105114169671728, 0.27964961943442757, 0.46601865587940094]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09432170804322547, 0.15987046255134107, 0.27458078294054333, 0.47122704646489016]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.0928024818827845, 0.1734165937377668, 0.2701535279637456, 0.46362739641570305]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.0928024818827845, 0.1734165937377668, 0.2701535279637456, 0.46362739641570305]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.0928024818827845, 0.1734165937377668, 0.2701535279637456, 0.46362739641570305]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.0928024818827845, 0.1734165937377668, 0.2701535279637456, 0.46362739641570305]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
Starting evaluation
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.506]
 [0.585]
 [0.501]
 [0.501]] [[-0.197]
 [ 0.   ]
 [-0.029]
 [ 0.084]
 [-0.086]] [[0.395]
 [0.434]
 [0.504]
 [0.458]
 [0.401]]
siam score:  -0.7766217
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0928024818827845, 0.1734165937377668, 0.2701535279637456, 0.46362739641570305]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0928024818827845, 0.1734165937377668, 0.2701535279637456, 0.46362739641570305]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0928024818827845, 0.1734165937377668, 0.2701535279637456, 0.46362739641570305]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0928024818827845, 0.1734165937377668, 0.2701535279637456, 0.46362739641570305]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.584]
 [0.457]
 [0.383]
 [0.345]] [[-0.007]
 [ 1.334]
 [ 0.7  ]
 [ 0.306]
 [ 0.375]] [[0.024]
 [0.554]
 [0.355]
 [0.233]
 [0.236]]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.535]
 [0.535]
 [0.484]
 [0.512]] [[2.728]
 [2.284]
 [2.284]
 [1.609]
 [1.848]] [[0.928]
 [0.757]
 [0.757]
 [0.534]
 [0.617]]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0928024818827845, 0.1734165937377668, 0.2701535279637456, 0.46362739641570305]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0928024818827845, 0.1734165937377668, 0.2701535279637456, 0.46362739641570305]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.0928024818827845, 0.1734165937377668, 0.2701535279637456, 0.46362739641570305]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.7769611
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09133149531737508, 0.17066574765868753, 0.281733700936525, 0.45626905608741236]
from probs:  [0.09133149531737508, 0.17066574765868753, 0.281733700936525, 0.45626905608741236]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09133149531737508, 0.17066574765868753, 0.281733700936525, 0.45626905608741236]
maxi score, test score, baseline:  0.41809999999999997 0.5 0.5
probs:  [0.09133149531737508, 0.17066574765868753, 0.281733700936525, 0.45626905608741236]
from probs:  [0.09133149531737508, 0.17066574765868753, 0.281733700936525, 0.45626905608741236]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09133149531737508, 0.17066574765868753, 0.281733700936525, 0.45626905608741236]
actor:  0 policy actor:  0  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 0.44392165387148674
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09133149531737508, 0.17066574765868753, 0.281733700936525, 0.45626905608741236]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09133149531737508, 0.17066574765868753, 0.281733700936525, 0.45626905608741236]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.488]
 [0.368]
 [0.368]
 [0.368]] [[1.205]
 [1.302]
 [1.205]
 [1.205]
 [1.205]] [[0.74 ]
 [0.908]
 [0.74 ]
 [0.74 ]
 [0.74 ]]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09133149531737508, 0.17066574765868753, 0.281733700936525, 0.45626905608741236]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09133149531737508, 0.17066574765868753, 0.281733700936525, 0.45626905608741236]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08990648664554576, 0.16800088340381614, 0.29295191821704875, 0.4491407117335893]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  73 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
siam score:  -0.75920546
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
siam score:  -0.76218116
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
actor:  0 policy actor:  1  step number:  71 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using another actor
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  0 policy actor:  1  step number:  48 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
siam score:  -0.7805933
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09097096765985598, 0.1666990782980198, 0.28786405531908194, 0.45446589872304227]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08961514796882657, 0.1791322746839001, 0.2835689225181526, 0.44768365482912076]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08961514796882657, 0.1791322746839001, 0.2835689225181526, 0.44768365482912076]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.08961514796882657, 0.1791322746839001, 0.2835689225181526, 0.44768365482912076]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08961514796882657, 0.1791322746839001, 0.2835689225181526, 0.44768365482912076]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
first move QE:  0.45423255304057825
siam score:  -0.77849793
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08961514796882657, 0.1791322746839001, 0.2835689225181526, 0.44768365482912076]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08961514796882657, 0.1791322746839001, 0.2835689225181526, 0.44768365482912076]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08961514796882657, 0.1791322746839001, 0.2835689225181526, 0.44768365482912076]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09064179056043095, 0.17756445025474135, 0.2789742198981035, 0.45281953928672425]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09064179056043095, 0.17756445025474135, 0.2789742198981035, 0.45281953928672425]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09064179056043095, 0.17756445025474135, 0.2789742198981035, 0.45281953928672425]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09064179056043095, 0.17756445025474135, 0.2789742198981035, 0.45281953928672425]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09064179056043095, 0.17756445025474135, 0.2789742198981035, 0.45281953928672425]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09064179056043095, 0.17756445025474135, 0.2789742198981035, 0.45281953928672425]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.0916105991652808, 0.17608494627713106, 0.2746383512409563, 0.45766610331663177]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7819554
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09339321976609478, 0.17336263946000385, 0.26666029576956435, 0.46658384500433703]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09339321976609478, 0.17336263946000385, 0.26666029576956435, 0.46658384500433703]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09339321976609478, 0.17336263946000385, 0.26666029576956435, 0.46658384500433703]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09339321976609478, 0.17336263946000385, 0.26666029576956435, 0.46658384500433703]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.285]
 [1.298]
 [1.285]
 [1.285]
 [1.285]] [[2.133]
 [2.069]
 [2.133]
 [2.133]
 [2.133]] [[1.373]
 [1.347]
 [1.373]
 [1.373]
 [1.373]]
siam score:  -0.7879515
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09421508288796156, 0.17210754144398077, 0.2629820764260032, 0.47069529924205444]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09499533594129207, 0.170915987725149, 0.25949008147298214, 0.4745985948605768]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09499533594129207, 0.170915987725149, 0.25949008147298214, 0.4745985948605768]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.2966095780491829
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09499533594129207, 0.170915987725149, 0.25949008147298214, 0.4745985948605768]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09499533594129207, 0.170915987725149, 0.25949008147298214, 0.4745985948605768]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]] [[0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]]
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  51 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09499533594129207, 0.170915987725149, 0.25949008147298214, 0.4745985948605768]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09380950857723207, 0.1687809444601607, 0.2687428589707322, 0.46866668799187505]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09380950857723207, 0.1687809444601607, 0.2687428589707322, 0.46866668799187505]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.78728986
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09380950857723207, 0.1687809444601607, 0.2687428589707322, 0.46866668799187505]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09380950857723207, 0.1687809444601607, 0.2687428589707322, 0.46866668799187505]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09380950857723207, 0.1687809444601607, 0.2687428589707322, 0.46866668799187505]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09380950857723207, 0.1687809444601607, 0.2687428589707322, 0.46866668799187505]
line 256 mcts: sample exp_bonus -1.1791637702856161
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09380950857723207, 0.1687809444601607, 0.2687428589707322, 0.46866668799187505]
siam score:  -0.7904924
actor:  1 policy actor:  1  step number:  52 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09380950857723207, 0.1687809444601607, 0.2687428589707322, 0.46866668799187505]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09380950857723207, 0.1687809444601607, 0.2687428589707322, 0.46866668799187505]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]] [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.189]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  16339
from probs:  [0.09343325342808914, 0.16569482876897107, 0.27408719178029395, 0.46678472602264576]
from probs:  [0.09343325342808914, 0.16569482876897107, 0.27408719178029395, 0.46678472602264576]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09343325342808914, 0.16569482876897107, 0.27408719178029395, 0.46678472602264576]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.437]
 [0.346]
 [0.346]] [[ 0.   ]
 [ 0.   ]
 [-0.396]
 [ 0.   ]
 [ 0.   ]] [[0.346]
 [0.346]
 [0.437]
 [0.346]
 [0.346]]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09343325342808914, 0.16569482876897107, 0.27408719178029395, 0.46678472602264576]
actor:  0 policy actor:  0  step number:  47 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09343325342808914, 0.16569482876897107, 0.27408719178029395, 0.46678472602264576]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09343325342808914, 0.16569482876897107, 0.27408719178029395, 0.46678472602264576]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09232249797339785, 0.1637236309665762, 0.2827255192885401, 0.4612283517714859]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09232249797339785, 0.1637236309665762, 0.2827255192885401, 0.4612283517714859]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09232249797339785, 0.1637236309665762, 0.2827255192885401, 0.4612283517714859]
actor:  0 policy actor:  0  step number:  50 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.781178
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7825249
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.09201482004396468, 0.16095380766114373, 0.287341951625972, 0.45968942066891955]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.09201482004396468, 0.16095380766114373, 0.287341951625972, 0.45968942066891955]
actor:  0 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09201482004396468, 0.16095380766114373, 0.287341951625972, 0.45968942066891955]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09201482004396468, 0.16095380766114373, 0.287341951625972, 0.45968942066891955]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09346624824737122, 0.15937519635374123, 0.2802082678820862, 0.4669502875168013]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09346624824737122, 0.15937519635374123, 0.2802082678820862, 0.4669502875168013]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09346624824737122, 0.15937519635374123, 0.2802082678820862, 0.4669502875168013]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09346624824737122, 0.15937519635374123, 0.2802082678820862, 0.4669502875168013]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09346624824737122, 0.15937519635374123, 0.2802082678820862, 0.4669502875168013]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09346624824737122, 0.15937519635374123, 0.2802082678820862, 0.4669502875168013]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09346624824737122, 0.15937519635374123, 0.2802082678820862, 0.4669502875168013]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09346624824737122, 0.15937519635374123, 0.2802082678820862, 0.4669502875168013]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7719037
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09245172068797967, 0.15764411212743637, 0.2880288950063497, 0.46187527217823426]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]] [[2.498]
 [2.498]
 [2.498]
 [2.498]
 [2.498]] [[0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.345]
 [0.348]
 [0.351]
 [0.338]] [[-0.877]
 [-0.987]
 [-0.338]
 [-0.888]
 [-0.51 ]] [[0.319]
 [0.345]
 [0.348]
 [0.351]
 [0.338]]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.09145901559699333, 0.1666988048051999, 0.28493275927523876, 0.45690942032256804]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09145901559699333, 0.1666988048051999, 0.28493275927523876, 0.45690942032256804]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09145901559699333, 0.1666988048051999, 0.28493275927523876, 0.45690942032256804]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.467]
 [0.338]
 [0.338]
 [0.338]] [[0.421]
 [0.614]
 [0.421]
 [0.421]
 [0.421]] [[0.94 ]
 [1.196]
 [0.94 ]
 [0.94 ]
 [0.94 ]]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09145901559699333, 0.1666988048051999, 0.28493275927523876, 0.45690942032256804]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09145901559699333, 0.1666988048051999, 0.28493275927523876, 0.45690942032256804]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.09145901559699333, 0.1666988048051999, 0.28493275927523876, 0.45690942032256804]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09145901559699333, 0.1666988048051999, 0.28493275927523876, 0.45690942032256804]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09145901559699333, 0.1666988048051999, 0.28493275927523876, 0.45690942032256804]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09216587146325285, 0.16582179811373485, 0.2815668257073494, 0.46044550471566287]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09216587146325285, 0.16582179811373485, 0.2815668257073494, 0.46044550471566287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09216587146325285, 0.16582179811373485, 0.2815668257073494, 0.46044550471566287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09216587146325285, 0.16582179811373485, 0.2815668257073494, 0.46044550471566287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09216587146325285, 0.16582179811373485, 0.2815668257073494, 0.46044550471566287]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09216587146325285, 0.16582179811373485, 0.2815668257073494, 0.46044550471566287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09216587146325285, 0.16582179811373485, 0.2815668257073494, 0.46044550471566287]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09216587146325285, 0.16582179811373485, 0.2815668257073494, 0.46044550471566287]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09284358103108627, 0.16498095367255486, 0.27833968210914833, 0.4638357831872105]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09284358103108627, 0.16498095367255486, 0.27833968210914833, 0.4638357831872105]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using another actor
from probs:  [0.09284358103108627, 0.16498095367255486, 0.27833968210914833, 0.4638357831872105]
siam score:  -0.7686914
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09284358103108627, 0.16498095367255486, 0.27833968210914833, 0.4638357831872105]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09284358103108627, 0.16498095367255486, 0.27833968210914833, 0.4638357831872105]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09284358103108627, 0.16498095367255486, 0.27833968210914833, 0.4638357831872105]
actor:  0 policy actor:  1  step number:  38 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
from probs:  [0.09284358103108627, 0.16498095367255486, 0.27833968210914833, 0.4638357831872105]
siam score:  -0.76975405
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09284358103108627, 0.16498095367255486, 0.27833968210914833, 0.4638357831872105]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
siam score:  -0.7716419
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09189752348280783, 0.1632986419099269, 0.2857005592135595, 0.4591032753937057]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09189752348280783, 0.1632986419099269, 0.2857005592135595, 0.4591032753937057]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09189752348280783, 0.1632986419099269, 0.2857005592135595, 0.4591032753937057]
actor:  0 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09189752348280783, 0.1632986419099269, 0.2857005592135595, 0.4591032753937057]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09189752348280783, 0.1632986419099269, 0.2857005592135595, 0.4591032753937057]
using explorer policy with actor:  1
siam score:  -0.78110015
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09189752348280783, 0.1632986419099269, 0.2857005592135595, 0.4591032753937057]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.544]
 [0.401]
 [0.401]
 [0.354]] [[0.672]
 [1.44 ]
 [0.672]
 [0.672]
 [0.747]] [[0.394]
 [0.723]
 [0.394]
 [0.394]
 [0.396]]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09314898145733844, 0.15695278561028553, 0.2845603939161797, 0.46533783901619635]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09314898145733844, 0.15695278561028553, 0.2845603939161797, 0.46533783901619635]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09216985722053962, 0.16582392385095446, 0.2815660285558921, 0.4604401903726138]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09216985722053962, 0.16582392385095446, 0.2815660285558921, 0.4604401903726138]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09216985722053962, 0.16582392385095446, 0.2815660285558921, 0.4604401903726138]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
first move QE:  0.4118734000758067
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
start point for exploration sampling:  16339
siam score:  -0.7811865
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09121113525396014, 0.16409782726853583, 0.28904644215066555, 0.4556445953268385]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09121113525396014, 0.16409782726853583, 0.28904644215066555, 0.4556445953268385]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.09121113525396014, 0.16409782726853583, 0.28904644215066555, 0.4556445953268385]
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.379]
 [0.379]
 [0.37 ]
 [0.367]] [[-0.921]
 [-0.921]
 [-0.921]
 [-0.637]
 [-0.549]] [[0.379]
 [0.379]
 [0.379]
 [0.37 ]
 [0.367]]
actor:  0 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09121113525396014, 0.16409782726853583, 0.28904644215066555, 0.4556445953268385]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09121113525396014, 0.16409782726853583, 0.28904644215066555, 0.4556445953268385]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.368]
 [0.409]
 [0.345]
 [0.364]] [[-0.949]
 [-0.405]
 [-0.79 ]
 [-0.649]
 [-0.845]] [[0.351]
 [0.368]
 [0.409]
 [0.345]
 [0.364]]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09121113525396014, 0.16409782726853583, 0.28904644215066555, 0.4556445953268385]
actor:  0 policy actor:  1  step number:  49 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09121113525396014, 0.16409782726853583, 0.28904644215066555, 0.4556445953268385]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09027218444172758, 0.16240732695191515, 0.29637259161369206, 0.4509478969926653]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09027218444172758, 0.16240732695191515, 0.29637259161369206, 0.4509478969926653]
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097443595904228, 0.16165246442169015, 0.2929116601380362, 0.4544614394812313]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097443595904228, 0.16165246442169015, 0.2929116601380362, 0.4544614394812313]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.696]
 [0.501]
 [0.501]
 [0.501]] [[0.86]
 [1.09]
 [0.86]
 [0.86]
 [0.86]] [[0.816]
 [1.087]
 [0.816]
 [0.816]
 [0.816]]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09006602074663579, 0.16003713666998265, 0.29997936851667634, 0.4499174740667053]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08917559728698633, 0.16835068785339305, 0.297010210023804, 0.44546350483581665]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
siam score:  -0.796476
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08917559728698633, 0.16835068785339305, 0.297010210023804, 0.44546350483581665]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08987186643799722, 0.16750974937715007, 0.29367130915327344, 0.4489470750315792]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.08987186643799722, 0.16750974937715007, 0.29367130915327344, 0.4489470750315792]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08987186643799722, 0.16750974937715007, 0.29367130915327344, 0.4489470750315792]
siam score:  -0.79427415
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08987186643799722, 0.16750974937715007, 0.29367130915327344, 0.4489470750315792]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08987186643799722, 0.16750974937715007, 0.29367130915327344, 0.4489470750315792]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.090541613974, 0.16670084312074626, 0.29045959048420894, 0.45229795242104487]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.09118632692747485, 0.16592217307925142, 0.28736792307588827, 0.4555235769173855]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09034323705058704, 0.16438695320103944, 0.2939634564643311, 0.45130635328404245]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097397352789154, 0.16367158562942682, 0.2908924068071136, 0.4544620340355681]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09097397352789154, 0.16367158562942682, 0.2908924068071136, 0.4544620340355681]
Printing some Q and Qe and total Qs values:  [[1.338]
 [1.385]
 [1.338]
 [1.035]
 [1.035]] [[0.415]
 [1.21 ]
 [0.415]
 [0.684]
 [0.937]] [[0.933]
 [1.113]
 [0.933]
 [0.674]
 [0.717]]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09204890203712596, 0.15805831611116286, 0.2900771442592367, 0.45981563759247457]
siam score:  -0.80823976
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09204890203712596, 0.15805831611116286, 0.2900771442592367, 0.45981563759247457]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09204890203712596, 0.15805831611116286, 0.2900771442592367, 0.45981563759247457]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09204890203712596, 0.15805831611116286, 0.2900771442592367, 0.45981563759247457]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09320808795399096, 0.15200505497124434, 0.2891979780115022, 0.4655888790632624]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.09230447793174898, 0.15053051684925706, 0.29609561414302726, 0.46106939107596673]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.09230447793174898, 0.15053051684925706, 0.29609561414302726, 0.46106939107596673]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.001]
 [0.024]
 [0.024]
 [0.024]] [[-0.244]
 [-0.   ]
 [-0.244]
 [-0.244]
 [-0.244]] [[0.211]
 [0.271]
 [0.211]
 [0.211]
 [0.211]]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.55 ]
 [0.511]
 [0.511]
 [0.511]] [[0.184]
 [1.024]
 [0.184]
 [0.184]
 [0.184]] [[0.321]
 [0.716]
 [0.321]
 [0.321]
 [0.321]]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09230447793174898, 0.15053051684925706, 0.29609561414302726, 0.46106939107596673]
actor:  0 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
actor:  0 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09141824755053662, 0.15869535465030898, 0.29324956884985365, 0.4566368289493007]
actor:  0 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09141824755053662, 0.15869535465030898, 0.29324956884985365, 0.4566368289493007]
siam score:  -0.8008024
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09141824755053662, 0.15869535465030898, 0.29324956884985365, 0.4566368289493007]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09141824755053662, 0.15869535465030898, 0.29324956884985365, 0.4566368289493007]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09141824755053662, 0.15869535465030898, 0.29324956884985365, 0.4566368289493007]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.09075915061616899, 0.1593552088122808, 0.29654732520450444, 0.4533383153670457]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.08987932529611649, 0.16751359181921155, 0.29366927491924094, 0.4489378079654309]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08987932529611649, 0.16751359181921155, 0.29366927491924094, 0.4489378079654309]
siam score:  -0.7958453
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08987932529611649, 0.16751359181921155, 0.29366927491924094, 0.4489378079654309]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08987932529611649, 0.16751359181921155, 0.29366927491924094, 0.4489378079654309]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08987932529611649, 0.16751359181921155, 0.29366927491924094, 0.4489378079654309]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08816995733779298, 0.18336410008026768, 0.2880776570969899, 0.4403882854849494]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08816995733779298, 0.18336410008026768, 0.2880776570969899, 0.4403882854849494]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08816995733779298, 0.18336410008026768, 0.2880776570969899, 0.4403882854849494]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08816995733779298, 0.18336410008026768, 0.2880776570969899, 0.4403882854849494]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08816995733779298, 0.18336410008026768, 0.2880776570969899, 0.4403882854849494]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08885896076827564, 0.1822740559750724, 0.2850306607025488, 0.44383632255410327]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08885896076827564, 0.1822740559750724, 0.2850306607025488, 0.44383632255410327]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08885896076827564, 0.1822740559750724, 0.2850306607025488, 0.44383632255410327]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using another actor
from probs:  [0.08803746572872774, 0.1805874853123119, 0.29164750881261287, 0.43972754014634746]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08803746572872774, 0.1805874853123119, 0.29164750881261287, 0.43972754014634746]
actor:  0 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08803746572872774, 0.1805874853123119, 0.29164750881261287, 0.43972754014634746]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08803746572872774, 0.1805874853123119, 0.29164750881261287, 0.43972754014634746]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08803746572872774, 0.1805874853123119, 0.29164750881261287, 0.43972754014634746]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08803746572872774, 0.1805874853123119, 0.29164750881261287, 0.43972754014634746]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08803746572872774, 0.1805874853123119, 0.29164750881261287, 0.43972754014634746]
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08803746572872774, 0.1805874853123119, 0.29164750881261287, 0.43972754014634746]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.80500185
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08871008778896339, 0.17957764396419526, 0.2886187113744736, 0.44309355687236784]
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41709999999999997 0.5 0.5
probs:  [0.08871008778896339, 0.17957764396419526, 0.2886187113744736, 0.44309355687236784]
first move QE:  0.3720285306933588
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08871008778896339, 0.17957764396419526, 0.2886187113744736, 0.44309355687236784]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08791213541036325, 0.1869658304373635, 0.2860195254643637, 0.43910250868790957]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08856911913050586, 0.18586992403814617, 0.2831707289457865, 0.4423902278855615]
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08856911913050586, 0.18586992403814617, 0.2831707289457865, 0.4423902278855615]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.5 0.5
probs:  [0.08856911913050586, 0.18586992403814617, 0.2831707289457865, 0.4423902278855615]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  44 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.08856911913050586, 0.18586992403814617, 0.2831707289457865, 0.4423902278855615]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.08856911913050586, 0.18586992403814617, 0.2831707289457865, 0.4423902278855615]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.08856911913050586, 0.18586992403814617, 0.2831707289457865, 0.4423902278855615]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.08856911913050586, 0.18586992403814617, 0.2831707289457865, 0.4423902278855615]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.08703117563189312, 0.1913312232274815, 0.2869396001901042, 0.4346980009505211]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08703117563189312, 0.1913312232274815, 0.2869396001901042, 0.4346980009505211]
maxi score, test score, baseline:  0.41509999999999997 0.5 0.5
probs:  [0.08768075529365772, 0.19019817300292655, 0.28417247256975625, 0.43794859913365947]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08768075529365772, 0.19019817300292655, 0.28417247256975625, 0.43794859913365947]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08768075529365772, 0.19019817300292655, 0.28417247256975625, 0.43794859913365947]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08768075529365772, 0.19019817300292655, 0.28417247256975625, 0.43794859913365947]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.288]
 [0.324]
 [0.288]
 [0.301]] [[-1.048]
 [-1.048]
 [-0.602]
 [-1.048]
 [-0.855]] [[0.288]
 [0.288]
 [0.324]
 [0.288]
 [0.301]]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.0875737415213493, 0.18752836212359592, 0.28748298272584244, 0.4374149136292123]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.0875737415213493, 0.18752836212359592, 0.28748298272584244, 0.4374149136292123]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.0875737415213493, 0.18752836212359592, 0.28748298272584244, 0.4374149136292123]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08878206947074088, 0.18551282778829636, 0.2822435861058518, 0.4434615166351109]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08878206947074088, 0.18551282778829636, 0.2822435861058518, 0.4434615166351109]
first move QE:  0.36452230873319164
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.0
using explorer policy with actor:  1
start point for exploration sampling:  16339
using explorer policy with actor:  1
siam score:  -0.80185944
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.0880729060547779, 0.18402970246676137, 0.2879828985797435, 0.4399144928987173]
Printing some Q and Qe and total Qs values:  [[1.056]
 [1.291]
 [1.056]
 [1.171]
 [1.213]] [[2.11 ]
 [1.989]
 [2.11 ]
 [2.283]
 [2.309]] [[0.832]
 [0.922]
 [0.832]
 [0.965]
 [1.   ]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.0880729060547779, 0.18402970246676137, 0.2879828985797435, 0.4399144928987173]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.0880729060547779, 0.18402970246676137, 0.2879828985797435, 0.4399144928987173]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08865493942555491, 0.18310082854230328, 0.28541720841878065, 0.4428270236133612]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08865493942555491, 0.18310082854230328, 0.28541720841878065, 0.4428270236133612]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08865493942555491, 0.18310082854230328, 0.28541720841878065, 0.4428270236133612]
first move QE:  0.36062015392150615
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08865493942555491, 0.18310082854230328, 0.28541720841878065, 0.4428270236133612]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08853367664661818, 0.18848901967490214, 0.2807554901625489, 0.44222181351593076]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08853367664661818, 0.18848901967490214, 0.2807554901625489, 0.44222181351593076]
siam score:  -0.79678255
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.0890866847980004, 0.18752777174510601, 0.27839646738858814, 0.44498907606830546]
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.08841788411901823, 0.18611869837263512, 0.283819512626252, 0.44164390488209465]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.08841788411901823, 0.18611869837263512, 0.283819512626252, 0.44164390488209465]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.08775906706764171, 0.18473065916514322, 0.28916160450091405, 0.438348669266301]
first move QE:  0.3561774903402594
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  0.35584779236186176
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[-1.112]
 [-1.112]
 [-1.112]
 [-1.112]
 [-1.112]] [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
siam score:  -0.80708617
siam score:  -0.80619127
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.704]
 [0.514]
 [0.397]
 [0.384]] [[-0.283]
 [-0.146]
 [ 0.153]
 [ 0.278]
 [ 0.465]] [[0.336]
 [0.54 ]
 [0.549]
 [0.516]
 [0.627]]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.442]
 [0.441]
 [0.294]
 [0.499]] [[0.786]
 [0.595]
 [0.786]
 [0.457]
 [1.276]] [[0.617]
 [0.532]
 [0.617]
 [0.337]
 [0.889]]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
actor:  0 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.409]
 [0.411]
 [0.366]
 [0.42 ]] [[-0.504]
 [-0.069]
 [-0.506]
 [-0.024]
 [-0.367]] [[0.39 ]
 [0.409]
 [0.411]
 [0.366]
 [0.42 ]]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
from probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08766361683418808, 0.18251184070634785, 0.29195209902037833, 0.43787244343908566]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08702936252840035, 0.18843331473295122, 0.2898372669375021, 0.4347000558011463]
from probs:  [0.08702936252840035, 0.18843331473295122, 0.2898372669375021, 0.4347000558011463]
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08702936252840035, 0.18843331473295122, 0.2898372669375021, 0.4347000558011463]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
probs:  [0.08702936252840035, 0.18843331473295122, 0.2898372669375021, 0.4347000558011463]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
maxi score, test score, baseline:  0.41409999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.876]
 [0.747]
 [0.747]
 [0.747]] [[0.891]
 [1.323]
 [0.891]
 [0.891]
 [0.891]] [[0.768]
 [0.972]
 [0.768]
 [0.768]
 [0.768]]
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08640423556984685, 0.1942695747545632, 0.2877528687146507, 0.43157332096093926]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08578804034302308, 0.19288279664105148, 0.2928379025192114, 0.42849126049671404]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08578804034302308, 0.19288279664105148, 0.2928379025192114, 0.42849126049671404]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.0863407682255848, 0.1919273693703688, 0.2904748637721672, 0.4312569986318792]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08573794344464382, 0.19058606465019035, 0.29543418585573683, 0.4282418060494291]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08573794344464382, 0.19058606465019035, 0.29543418585573683, 0.4282418060494291]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08627992699532373, 0.1896820783666982, 0.2930842297380727, 0.4309537648999053]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08568990522593734, 0.18838371445972651, 0.297923777642435, 0.4280026026719012]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08568990522593734, 0.18838371445972651, 0.297923777642435, 0.4280026026719012]
Printing some Q and Qe and total Qs values:  [[0.924]
 [0.978]
 [0.924]
 [0.924]
 [0.924]] [[1.994]
 [2.395]
 [1.994]
 [1.994]
 [1.994]] [[0.924]
 [1.079]
 [0.924]
 [0.924]
 [0.924]]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08568990522593734, 0.18838371445972651, 0.297923777642435, 0.4280026026719012]
siam score:  -0.78533447
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08568990522593734, 0.18838371445972651, 0.297923777642435, 0.4280026026719012]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
using another actor
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08568990522593734, 0.18838371445972651, 0.297923777642435, 0.4280026026719012]
from probs:  [0.08568990522593734, 0.18838371445972651, 0.297923777642435, 0.4280026026719012]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.08568990522593734, 0.18838371445972651, 0.297923777642435, 0.4280026026719012]
actor:  0 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.08568990522593734, 0.18838371445972651, 0.297923777642435, 0.4280026026719012]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08568990522593734, 0.18838371445972651, 0.297923777642435, 0.4280026026719012]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08568990522593734, 0.18838371445972651, 0.297923777642435, 0.4280026026719012]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08568990522593734, 0.18838371445972651, 0.297923777642435, 0.4280026026719012]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
siam score:  -0.7751715
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
using another actor
from probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.564]
 [0.512]
 [0.512]
 [0.529]] [[-0.354]
 [ 0.346]
 [ 0.   ]
 [ 0.   ]
 [-0.22 ]] [[0.24 ]
 [0.603]
 [0.402]
 [0.402]
 [0.31 ]]
actor:  0 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  51 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]] [[1.964]
 [1.964]
 [1.964]
 [1.964]
 [1.964]] [[1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.269]]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
first move QE:  0.34170474751446506
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.0862215521901467, 0.1875278085673755, 0.295587815369753, 0.43066282387272475]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08568990522593734, 0.18838371445972651, 0.297923777642435, 0.4280026026719012]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.77620804
Printing some Q and Qe and total Qs values:  [[1.047]
 [1.33 ]
 [1.047]
 [0.891]
 [0.996]] [[2.656]
 [1.706]
 [2.656]
 [1.675]
 [2.529]] [[1.259]
 [1.099]
 [1.259]
 [0.719]
 [1.164]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08510791218194186, 0.1871030180487819, 0.30269747136453407, 0.4250915984047421]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08510791218194186, 0.1871030180487819, 0.30269747136453407, 0.4250915984047421]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[-0.179]
 [-0.179]
 [-0.179]
 [-0.179]
 [-0.179]] [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]]
siam score:  -0.7768709
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08510791218194186, 0.1871030180487819, 0.30269747136453407, 0.4250915984047421]
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.072]
 [1.124]
 [1.072]
 [1.072]
 [1.072]] [[2.206]
 [2.226]
 [2.206]
 [2.206]
 [2.206]] [[1.149]
 [1.199]
 [1.149]
 [1.149]
 [1.149]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08510791218194186, 0.1871030180487819, 0.30269747136453407, 0.4250915984047421]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08564380134286392, 0.1862700454186615, 0.3003131220378988, 0.42777303120057575]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08564380134286392, 0.1862700454186615, 0.3003131220378988, 0.42777303120057575]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.392]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.533]] [[0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.392]]
actor:  0 policy actor:  0  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  46 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  52 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  52 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]] [[0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]]
siam score:  -0.7777035
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08507375501675651, 0.18502905500660105, 0.30497541499441444, 0.42492177498222794]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08507375501675651, 0.18502905500660105, 0.30497541499441444, 0.42492177498222794]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08507375501675651, 0.18502905500660105, 0.30497541499441444, 0.42492177498222794]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.0845112601186435, 0.1838045040474574, 0.30957594635728836, 0.4221082894766108]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.0845112601186435, 0.1838045040474574, 0.30957594635728836, 0.4221082894766108]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.0845112601186435, 0.1838045040474574, 0.30957594635728836, 0.4221082894766108]
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.08396736556454987, 0.18459320461633782, 0.3120526007486025, 0.4193868290705097]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.08396736556454987, 0.18459320461633782, 0.3120526007486025, 0.4193868290705097]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
actor:  0 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.08396736556454987, 0.18459320461633782, 0.3120526007486025, 0.4193868290705097]
from probs:  [0.08396736556454987, 0.18459320461633782, 0.3120526007486025, 0.4193868290705097]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.514]
 [0.371]
 [0.371]
 [0.371]] [[0.827]
 [1.288]
 [0.827]
 [0.827]
 [0.827]] [[0.371]
 [0.514]
 [0.371]
 [0.371]
 [0.371]]
actor:  0 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.0845112601186435, 0.1838045040474574, 0.30957594635728836, 0.4221082894766108]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.0845112601186435, 0.1838045040474574, 0.30957594635728836, 0.4221082894766108]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.0845112601186435, 0.1838045040474574, 0.30957594635728836, 0.4221082894766108]
UNIT TEST: sample policy line 217 mcts : [0.042 0.25  0.    0.5   0.208]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.0845112601186435, 0.1838045040474574, 0.30957594635728836, 0.4221082894766108]
Printing some Q and Qe and total Qs values:  [[1.147]
 [1.485]
 [1.147]
 [1.031]
 [1.177]] [[1.939]
 [1.794]
 [1.939]
 [3.49 ]
 [2.285]] [[0.866]
 [0.975]
 [0.866]
 [1.136]
 [0.949]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.08395616758471917, 0.18259606802944042, 0.31411593528906884, 0.4193318290967715]
using another actor
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08395616758471917, 0.18259606802944042, 0.31411593528906884, 0.4193318290967715]
Printing some Q and Qe and total Qs values:  [[0.857]
 [0.876]
 [0.876]
 [0.876]
 [0.846]] [[1.728]
 [1.534]
 [1.534]
 [1.534]
 [1.293]] [[0.838]
 [0.789]
 [0.789]
 [0.789]
 [0.706]]
first move QE:  0.3344886089779375
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08395616758471917, 0.18259606802944042, 0.31411593528906884, 0.4193318290967715]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.084489612697905, 0.1818486640520785, 0.3116607325243099, 0.42200099072570657]
actor:  0 policy actor:  0  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.084489612697905, 0.1818486640520785, 0.3116607325243099, 0.42200099072570657]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
siam score:  -0.7822174
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.78273875
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08394540305416642, 0.18712495843798532, 0.3096506804562703, 0.41927895805157794]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08394540305416642, 0.18712495843798532, 0.3096506804562703, 0.41927895805157794]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08394540305416642, 0.18712495843798532, 0.3096506804562703, 0.41927895805157794]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08394540305416642, 0.18712495843798532, 0.3096506804562703, 0.41927895805157794]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08394540305416642, 0.18712495843798532, 0.3096506804562703, 0.41927895805157794]
siam score:  -0.78783983
Printing some Q and Qe and total Qs values:  [[1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]] [[1.016]
 [1.016]
 [1.016]
 [1.016]
 [1.016]] [[1.062]
 [1.062]
 [1.062]
 [1.062]
 [1.062]]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08394540305416642, 0.18712495843798532, 0.3096506804562703, 0.41927895805157794]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.08340833224885424, 0.1879364375044751, 0.31206356249552486, 0.4165916677511457]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08286761271602916, 0.19320744121418468, 0.31003784785929056, 0.4138870982104957]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08286761271602916, 0.19320744121418468, 0.31003784785929056, 0.4138870982104957]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08233387124066745, 0.19841042192020536, 0.30803827533976896, 0.41121743149935824]
Printing some Q and Qe and total Qs values:  [[1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.191]] [[1.752]
 [1.752]
 [1.752]
 [1.752]
 [1.961]] [[1.251]
 [1.251]
 [1.251]
 [1.251]
 [1.361]]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08233387124066745, 0.19841042192020536, 0.30803827533976896, 0.41121743149935824]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08233387124066745, 0.19841042192020536, 0.30803827533976896, 0.41121743149935824]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08233387124066745, 0.19841042192020536, 0.30803827533976896, 0.41121743149935824]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08233387124066745, 0.19841042192020536, 0.30803827533976896, 0.41121743149935824]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08233387124066745, 0.19841042192020536, 0.30803827533976896, 0.41121743149935824]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08233387124066745, 0.19841042192020536, 0.30803827533976896, 0.41121743149935824]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.08233387124066745, 0.19841042192020536, 0.30803827533976896, 0.41121743149935824]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08287778470176611, 0.19747587519198365, 0.3057074050994113, 0.413938935006839]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08287778470176611, 0.19747587519198365, 0.3057074050994113, 0.413938935006839]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08287778470176611, 0.19747587519198365, 0.3057074050994113, 0.413938935006839]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
from probs:  [0.08287778470176611, 0.19747587519198365, 0.3057074050994113, 0.413938935006839]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
actor:  0 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08287778470176611, 0.19747587519198365, 0.3057074050994113, 0.413938935006839]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08235411265095088, 0.19622679085030498, 0.3100994690496591, 0.411319627449085]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08235411265095088, 0.19622679085030498, 0.3100994690496591, 0.411319627449085]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08235411265095088, 0.19622679085030498, 0.3100994690496591, 0.411319627449085]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08235411265095088, 0.19622679085030498, 0.3100994690496591, 0.411319627449085]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.08235411265095088, 0.19622679085030498, 0.3100994690496591, 0.411319627449085]
actor:  0 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08235411265095088, 0.19622679085030498, 0.3100994690496591, 0.411319627449085]
actor:  0 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08235411265095088, 0.19622679085030498, 0.3100994690496591, 0.411319627449085]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08180697360964916, 0.1971393345630326, 0.312471695516416, 0.40858199631090225]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.893]
 [0.507]
 [0.717]
 [0.705]] [[2.513]
 [2.222]
 [2.422]
 [2.513]
 [2.499]] [[0.982]
 [0.974]
 [0.871]
 [0.982]
 [0.973]]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.0812867890297959, 0.19588444176427416, 0.31684863076177905, 0.405980138444151]
using another actor
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.163]
 [0.201]
 [0.201]
 [0.181]] [[-0.901]
 [-0.427]
 [-0.901]
 [-0.901]
 [-0.629]] [[0.201]
 [0.163]
 [0.201]
 [0.201]
 [0.181]]
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08183702864390613, 0.19499342058445523, 0.31443627874392377, 0.40873327202771476]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08183702864390613, 0.19499342058445523, 0.31443627874392377, 0.40873327202771476]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08132640913666109, 0.19377546971222034, 0.31871887035173063, 0.4061792507993879]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
first move QE:  0.3254329888659963
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08132640913666109, 0.19377546971222034, 0.31871887035173063, 0.4061792507993879]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.08132640913666109, 0.19377546971222034, 0.31871887035173063, 0.4061792507993879]
siam score:  -0.76739794
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08132640913666109, 0.19377546971222034, 0.31871887035173063, 0.4061792507993879]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.08132640913666109, 0.19377546971222034, 0.31871887035173063, 0.4061792507993879]
siam score:  -0.7793856
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08132640913666109, 0.19377546971222034, 0.31871887035173063, 0.4061792507993879]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08082213365510858, 0.19257265087375242, 0.3229482542955036, 0.40365696117563526]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08082213365510858, 0.19257265087375242, 0.3229482542955036, 0.40365696117563526]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08082213365510858, 0.19257265087375242, 0.3229482542955036, 0.40365696117563526]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08082213365510858, 0.19257265087375242, 0.3229482542955036, 0.40365696117563526]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.7986783
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08082213365510858, 0.19257265087375242, 0.3229482542955036, 0.40365696117563526]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08082213365510858, 0.19257265087375242, 0.3229482542955036, 0.40365696117563526]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08136457091052338, 0.1917441244963626, 0.3205202703465084, 0.40637103424660553]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.08136457091052338, 0.1917441244963626, 0.3205202703465084, 0.40637103424660553]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.042 0.542 0.    0.292 0.125]
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07983214765688373, 0.19634346998190028, 0.3251191420253396, 0.3987052403358764]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07983214765688373, 0.19634346998190028, 0.3251191420253396, 0.3987052403358764]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07934621070574607, 0.201241774487356, 0.32313733826896596, 0.39627467653793197]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
siam score:  -0.7982884
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07934621070574607, 0.201241774487356, 0.32313733826896596, 0.39627467653793197]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.343]
 [0.394]
 [0.332]
 [0.331]] [[-0.707]
 [-0.707]
 [-0.816]
 [-0.565]
 [-0.477]] [[0.343]
 [0.343]
 [0.394]
 [0.332]
 [0.331]]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07989662290905315, 0.20032379253096244, 0.3207509621528717, 0.39902862240711273]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.07989662290905315, 0.20032379253096244, 0.3207509621528717, 0.39902862240711273]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07989662290905315, 0.20032379253096244, 0.3207509621528717, 0.39902862240711273]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.125 0.75  0.    0.042 0.083]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07989662290905315, 0.20032379253096244, 0.3207509621528717, 0.39902862240711273]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07989662290905315, 0.20032379253096244, 0.3207509621528717, 0.39902862240711273]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07941902874427026, 0.19912497348513322, 0.3248162154630393, 0.39663978230755714]
siam score:  -0.8026262
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07941902874427026, 0.19912497348513322, 0.3248162154630393, 0.39663978230755714]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07941902874427026, 0.19912497348513322, 0.3248162154630393, 0.39663978230755714]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.491]
 [0.534]
 [0.534]
 [0.507]] [[-0.612]
 [ 0.032]
 [-0.612]
 [-0.612]
 [-0.624]] [[0.517]
 [0.813]
 [0.517]
 [0.517]
 [0.49 ]]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.797]
 [0.555]
 [0.555]
 [0.555]] [[0.754]
 [1.146]
 [0.754]
 [0.754]
 [0.754]] [[0.438]
 [0.785]
 [0.438]
 [0.438]
 [0.438]]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07886616473870015, 0.20002286226882396, 0.3272373946754539, 0.39387357831702197]
UNIT TEST: sample policy line 217 mcts : [0.042 0.292 0.    0.042 0.625]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07886616473870015, 0.20002286226882396, 0.3272373946754539, 0.39387357831702197]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07941902874427026, 0.19912497348513322, 0.3248162154630393, 0.39663978230755714]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07995880922194593, 0.19824833324146177, 0.3224523334619535, 0.39934052407463877]
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07948927687460666, 0.19708287903005034, 0.32643584140103843, 0.3969920026943046]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.324]
 [0.372]
 [0.336]
 [0.327]] [[-0.694]
 [-0.419]
 [-0.417]
 [-0.538]
 [-0.261]] [[0.336]
 [0.324]
 [0.372]
 [0.336]
 [0.327]]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07948927687460666, 0.19708287903005034, 0.32643584140103843, 0.3969920026943046]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07948927687460666, 0.19708287903005034, 0.32643584140103843, 0.3969920026943046]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07948927687460666, 0.19708287903005034, 0.32643584140103843, 0.3969920026943046]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07948927687460666, 0.19708287903005034, 0.32643584140103843, 0.3969920026943046]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07894712104022997, 0.19794042814267865, 0.3288330659553722, 0.39427938486171904]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07894712104022997, 0.19794042814267865, 0.3288330659553722, 0.39427938486171904]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07894712104022997, 0.19794042814267865, 0.3288330659553722, 0.39427938486171904]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07839190327710047, 0.1988186378194861, 0.3312880458161103, 0.39150141308730313]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07792332239304732, 0.19762883725005784, 0.3352901793356201, 0.3891576610212748]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07792332239304732, 0.19762883725005784, 0.3352901793356201, 0.3891576610212748]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07792332239304732, 0.19762883725005784, 0.3352901793356201, 0.3891576610212748]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07792332239304732, 0.19762883725005784, 0.3352901793356201, 0.3891576610212748]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07792332239304732, 0.19762883725005784, 0.3352901793356201, 0.3891576610212748]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07792332239304732, 0.19762883725005784, 0.3352901793356201, 0.3891576610212748]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07792332239304732, 0.19762883725005784, 0.3352901793356201, 0.3891576610212748]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07792332239304732, 0.19762883725005784, 0.3352901793356201, 0.3891576610212748]
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07848079883944194, 0.19676990308810266, 0.33280237297406257, 0.3919469250983929]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
actor:  0 policy actor:  1  step number:  44 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.636]
 [0.636]
 [0.636]
 [0.589]] [[1.9  ]
 [1.464]
 [1.464]
 [1.464]
 [0.842]] [[0.884]
 [0.737]
 [0.737]
 [0.737]
 [0.474]]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
actor:  0 policy actor:  1  step number:  65 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07801996356022312, 0.2014928102349347, 0.3308453162751088, 0.3896419099297334]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.559]
 [0.441]
 [0.441]
 [0.441]] [[0.075]
 [0.149]
 [0.075]
 [0.075]
 [0.075]] [[0.668]
 [0.757]
 [0.668]
 [0.668]
 [0.668]]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07856659356823702, 0.2006039337400005, 0.3284525758247051, 0.3923768968670574]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07856659356823702, 0.2006039337400005, 0.3284525758247051, 0.3923768968670574]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07856659356823702, 0.2006039337400005, 0.3284525758247051, 0.3923768968670574]
actor:  0 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07856659356823702, 0.2006039337400005, 0.3284525758247051, 0.3923768968670574]
from probs:  [0.07856659356823702, 0.2006039337400005, 0.3284525758247051, 0.3923768968670574]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07856659356823702, 0.2006039337400005, 0.3284525758247051, 0.3923768968670574]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.355]
 [0.547]
 [0.469]
 [0.443]] [[-1.137]
 [-0.191]
 [-0.9  ]
 [-1.134]
 [-0.885]] [[0.12 ]
 [0.635]
 [0.387]
 [0.182]
 [0.304]]
line 256 mcts: sample exp_bonus -0.7527756272733714
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07856659356823702, 0.2006039337400005, 0.3284525758247051, 0.3923768968670574]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07811325320194674, 0.20522278024588528, 0.3265546015150993, 0.3901093650370686]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07811325320194674, 0.20522278024588528, 0.3265546015150993, 0.3901093650370686]
actor:  0 policy actor:  0  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07811325320194674, 0.20522278024588528, 0.3265546015150993, 0.3901093650370686]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07811325320194674, 0.20522278024588528, 0.3265546015150993, 0.3901093650370686]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07766512438441983, 0.20404403316917863, 0.33042294195393734, 0.38786790049246417]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07766512438441983, 0.20404403316917863, 0.33042294195393734, 0.38786790049246417]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.077222117762206, 0.20287875938969252, 0.33424706654570124, 0.38565205630240024]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.077222117762206, 0.20287875938969252, 0.33424706654570124, 0.38565205630240024]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07776231968126654, 0.20199933499313982, 0.33188348736464374, 0.3883548579609498]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.79677236
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07829045192166159, 0.20113955948990372, 0.3295727174021569, 0.3909972711862779]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07829045192166159, 0.20113955948990372, 0.3295727174021569, 0.3909972711862779]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07829045192166159, 0.20113955948990372, 0.3295727174021569, 0.3909972711862779]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07829045192166159, 0.20113955948990372, 0.3295727174021569, 0.3909972711862779]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07829045192166159, 0.20113955948990372, 0.3295727174021569, 0.3909972711862779]
from probs:  [0.07829045192166159, 0.20113955948990372, 0.3295727174021569, 0.3909972711862779]
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07829045192166159, 0.20113955948990372, 0.3295727174021569, 0.3909972711862779]
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07829045192166159, 0.20113955948990372, 0.3295727174021569, 0.3909972711862779]
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07829045192166159, 0.20113955948990372, 0.3295727174021569, 0.3909972711862779]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07829045192166159, 0.20113955948990372, 0.3295727174021569, 0.3909972711862779]
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07829045192166159, 0.20113955948990372, 0.3295727174021569, 0.3909972711862779]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.752]
 [0.603]
 [0.603]
 [0.505]] [[1.742]
 [1.456]
 [1.742]
 [1.742]
 [1.653]] [[0.955]
 [0.927]
 [0.955]
 [0.955]
 [0.849]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07742689692907129, 0.20444070078927482, 0.3314545046494784, 0.3866778976321755]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.472]
 [0.544]
 [0.496]
 [0.487]] [[-0.963]
 [-0.285]
 [-0.455]
 [-0.67 ]
 [-0.706]] [[0.158]
 [0.39 ]
 [0.406]
 [0.286]
 [0.266]]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.813]
 [1.22 ]
 [0.813]
 [0.583]
 [0.813]] [[2.176]
 [1.527]
 [2.176]
 [1.831]
 [2.176]] [[0.985]
 [1.176]
 [0.985]
 [0.64 ]
 [0.985]]
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07742689692907129, 0.20444070078927482, 0.3314545046494784, 0.3866778976321755]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07700223765740599, 0.20881005658509666, 0.3296338906021465, 0.38455381515535086]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.569]
 [0.569]
 [0.59 ]
 [0.659]] [[-0.202]
 [ 0.   ]
 [ 0.   ]
 [-0.228]
 [-0.049]] [[0.448]
 [0.555]
 [0.555]
 [0.5  ]
 [0.628]]
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07700223765740599, 0.20881005658509666, 0.3296338906021465, 0.38455381515535086]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07700223765740599, 0.20881005658509666, 0.3296338906021465, 0.38455381515535086]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07700223765740599, 0.20881005658509666, 0.3296338906021465, 0.38455381515535086]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7756539
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07616676864882906, 0.20654169216220725, 0.33691661567558545, 0.3803749235133782]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07669217757812717, 0.20566544077579996, 0.3346387039734728, 0.3830036776726001]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.2848348094951985
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0762828026174959, 0.20991141598865293, 0.3328497402901173, 0.3809560411037339]
siam score:  -0.7706879
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0762828026174959, 0.20991141598865293, 0.3328497402901173, 0.3809560411037339]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0762828026174959, 0.20991141598865293, 0.3328497402901173, 0.3809560411037339]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0762828026174959, 0.20991141598865293, 0.3328497402901173, 0.3809560411037339]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0762828026174959, 0.20991141598865293, 0.3328497402901173, 0.3809560411037339]
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using another actor
from probs:  [0.0762828026174959, 0.20991141598865293, 0.3328497402901173, 0.3809560411037339]
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0762828026174959, 0.20991141598865293, 0.3328497402901173, 0.3809560411037339]
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0762828026174959, 0.20991141598865293, 0.3328497402901173, 0.3809560411037339]
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
start point for exploration sampling:  16339
actor:  0 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.0762828026174959, 0.20991141598865293, 0.3328497402901173, 0.3809560411037339]
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.744]
 [0.45 ]
 [0.45 ]
 [0.45 ]] [[0.203]
 [0.723]
 [0.203]
 [0.203]
 [0.203]] [[0.337]
 [0.805]
 [0.337]
 [0.337]
 [0.337]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07587778332369591, 0.21411221488350987, 0.331079810818737, 0.3789301909740573]
siam score:  -0.7829126
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
siam score:  -0.7829243
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.0750805366034628, 0.2223811373584415, 0.32759585218342635, 0.37494247385466944]
siam score:  -0.7852655
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0750805366034628, 0.2223811373584415, 0.32759585218342635, 0.37494247385466944]
line 256 mcts: sample exp_bonus 6.341495472222215
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.512]
 [0.512]
 [0.512]
 [0.555]] [[-0.036]
 [ 1.395]
 [ 1.395]
 [ 1.395]
 [ 0.379]] [[0.338]
 [0.587]
 [0.587]
 [0.587]
 [0.46 ]]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0750805366034628, 0.2223811373584415, 0.32759585218342635, 0.37494247385466944]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.5693666199333965
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  42 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0750805366034628, 0.2223811373584415, 0.32759585218342635, 0.37494247385466944]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.0750805366034628, 0.2223811373584415, 0.32759585218342635, 0.37494247385466944]
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.4706],
        [0.0000],
        [0.2691],
        [0.0000],
        [0.2865],
        [0.6265],
        [0.6381],
        [0.4323],
        [0.3271],
        [0.6026]], dtype=torch.float64)
0.0 0.4706421945616459
0.0 0.0
0.0 0.2690824477732688
0.0 0.0
0.0 0.28649307010558067
0.0 0.626477438176796
0.0 0.6380717680292133
0.0 0.4322736710256059
0.0 0.32711588949424003
0.0 0.6025722573823358
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0750805366034628, 0.2223811373584415, 0.32759585218342635, 0.37494247385466944]
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0750805366034628, 0.2223811373584415, 0.32759585218342635, 0.37494247385466944]
actor:  0 policy actor:  1  step number:  48 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0750805366034628, 0.2223811373584415, 0.32759585218342635, 0.37494247385466944]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0750805366034628, 0.2223811373584415, 0.32759585218342635, 0.37494247385466944]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07468817521218488, 0.2212174616020005, 0.3311144263943623, 0.3729799367914524]
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07468817521218488, 0.2212174616020005, 0.3311144263943623, 0.3729799367914524]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07468817521218488, 0.2212174616020005, 0.3311144263943623, 0.3729799367914524]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.785]
 [1.045]
 [0.785]
 [0.785]
 [0.773]] [[0.918]
 [1.167]
 [0.918]
 [0.918]
 [1.467]] [[0.828]
 [1.113]
 [0.828]
 [0.828]
 [1.022]]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.564]
 [0.564]
 [0.535]
 [0.538]] [[-0.956]
 [-0.956]
 [-0.956]
 [-1.038]
 [-0.813]] [[0.55 ]
 [0.55 ]
 [0.55 ]
 [0.466]
 [0.62 ]]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.8127],
        [0.8127],
        [0.2865],
        [0.0000],
        [0.3841],
        [0.0000],
        [0.1928],
        [0.3501],
        [0.3497],
        [0.4764]], dtype=torch.float64)
0.0 0.8126576820368098
0.0 0.8126576820368098
0.0 0.28650947779995645
0.0 0.0
0.0 0.3840600504185316
0.0 0.0
0.0 0.1928426050683026
0.0 0.3501428803963428
0.0 0.3497109814696692
0.0 0.47636601110012183
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07468817521218488, 0.2212174616020005, 0.3311144263943623, 0.3729799367914524]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07429990148300254, 0.22527183798649672, 0.32939041488545806, 0.3710378456450427]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07429990148300254, 0.22527183798649672, 0.32939041488545806, 0.3710378456450427]
from probs:  [0.07429990148300254, 0.22527183798649672, 0.32939041488545806, 0.3710378456450427]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07429990148300254, 0.22527183798649672, 0.32939041488545806, 0.3710378456450427]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.002]
 [0.407]
 [0.363]
 [0.366]] [[-0.402]
 [-0.178]
 [-0.468]
 [-0.446]
 [-0.458]] [[0.325]
 [0.002]
 [0.407]
 [0.363]
 [0.366]]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.659]
 [0.657]
 [0.657]
 [0.542]] [[1.353]
 [2.804]
 [1.353]
 [1.353]
 [1.433]] [[0.657]
 [0.659]
 [0.657]
 [0.657]
 [0.542]]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07429990148300254, 0.22527183798649672, 0.32939041488545806, 0.3710378456450427]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07376590894073946, 0.22632676388756204, 0.3315411466095087, 0.3683661805621899]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07429990148300254, 0.22527183798649672, 0.32939041488545806, 0.3710378456450427]
actor:  0 policy actor:  0  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07391565186839995, 0.22928419433745884, 0.3276842712345294, 0.3691158825596118]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.9516290332578122
start point for exploration sampling:  16339
actor:  0 policy actor:  1  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07415769685823481, 0.2275239161397744, 0.32800523222078304, 0.37031315478120774]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.07415769685823481, 0.2275239161397744, 0.32800523222078304, 0.37031315478120774]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07415769685823481, 0.2275239161397744, 0.32800523222078304, 0.37031315478120774]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07415769685823481, 0.2275239161397744, 0.32800523222078304, 0.37031315478120774]
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0746903770402377, 0.2264509461695842, 0.3258802845646732, 0.37297839222550494]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0746903770402377, 0.2264509461695842, 0.3258802845646732, 0.37297839222550494]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0746903770402377, 0.2264509461695842, 0.3258802845646732, 0.37297839222550494]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[ 0.112]
 [-0.001]
 [ 0.112]
 [ 0.112]
 [ 0.112]] [[0.076]
 [0.001]
 [0.076]
 [0.076]
 [0.076]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.781]
 [0.546]
 [0.546]
 [0.546]] [[0.131]
 [0.73 ]
 [0.131]
 [0.131]
 [0.131]] [[0.481]
 [0.837]
 [0.481]
 [0.481]
 [0.481]]
maxi score, test score, baseline:  0.40709999999999996 0.5 0.5
probs:  [0.0746903770402377, 0.2264509461695842, 0.3258802845646732, 0.37297839222550494]
maxi score, test score, baseline:  0.40709999999999996 0.5 0.5
actor:  0 policy actor:  0  step number:  51 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.40709999999999996 0.5 0.5
probs:  [0.0746903770402377, 0.2264509461695842, 0.3258802845646732, 0.37297839222550494]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.40709999999999996 0.5 0.5
probs:  [0.07430209669288132, 0.23047801074365354, 0.32418355917411673, 0.37103633338934844]
maxi score, test score, baseline:  0.40709999999999996 0.5 0.5
probs:  [0.07430209669288132, 0.23047801074365354, 0.32418355917411673, 0.37103633338934844]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.40709999999999996 0.5 0.5
siam score:  -0.7773467
actor:  1 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0748250517487406, 0.22939118255867538, 0.3221308610446362, 0.3736529046479478]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.0748250517487406, 0.22939118255867538, 0.3221308610446362, 0.3736529046479478]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
actor:  0 policy actor:  0  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.0748250517487406, 0.22939118255867538, 0.3221308610446362, 0.3736529046479478]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.487]
 [0.563]
 [0.518]
 [0.494]] [[ 0.   ]
 [-0.815]
 [-0.837]
 [-1.025]
 [-0.997]] [[0.647]
 [0.384]
 [0.452]
 [0.345]
 [0.33 ]]
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.0744420537809082, 0.23334121678212996, 0.3204794674602193, 0.3717372619767425]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.0744420537809082, 0.23334121678212996, 0.3204794674602193, 0.3717372619767425]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07457777005078613, 0.2310695435306604, 0.3219357345834905, 0.3724169518350629]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.284]
 [0.368]
 [0.327]
 [0.323]] [[-0.789]
 [-0.093]
 [-0.487]
 [-0.789]
 [-0.499]] [[0.327]
 [0.284]
 [0.368]
 [0.327]
 [0.323]]
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07457777005078613, 0.2310695435306604, 0.3219357345834905, 0.3724169518350629]
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07457777005078613, 0.2310695435306604, 0.3219357345834905, 0.3724169518350629]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07508228129487889, 0.2300094035765576, 0.31996708748204844, 0.37494122764651516]
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07508228129487889, 0.2300094035765576, 0.31996708748204844, 0.37494122764651516]
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07508228129487889, 0.2300094035765576, 0.31996708748204844, 0.37494122764651516]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.104]
 [1.117]
 [1.104]
 [1.104]
 [1.104]] [[1.548]
 [1.605]
 [1.548]
 [1.548]
 [1.548]] [[1.252]
 [1.293]
 [1.252]
 [1.252]
 [1.252]]
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07508228129487889, 0.2300094035765576, 0.31996708748204844, 0.37494122764651516]
using another actor
from probs:  [0.07508228129487889, 0.2300094035765576, 0.31996708748204844, 0.37494122764651516]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07508228129487889, 0.2300094035765576, 0.31996708748204844, 0.37494122764651516]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.07508228129487889, 0.2300094035765576, 0.31996708748204844, 0.37494122764651516]
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07508228129487889, 0.2300094035765576, 0.31996708748204844, 0.37494122764651516]
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07508228129487889, 0.2300094035765576, 0.31996708748204844, 0.37494122764651516]
actor:  0 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07508228129487889, 0.2300094035765576, 0.31996708748204844, 0.37494122764651516]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07508228129487889, 0.2300094035765576, 0.31996708748204844, 0.37494122764651516]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07508228129487889, 0.2300094035765576, 0.31996708748204844, 0.37494122764651516]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07557680390201266, 0.22897025295272494, 0.31803741691765464, 0.3774155262276078]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7816322
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07569128156060473, 0.22684010034721322, 0.31947969895836037, 0.3779889191338217]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07532452761929753, 0.22573951772490244, 0.3227814468252927, 0.37615450783050736]
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.7818359
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07532452761929753, 0.22573951772490244, 0.3227814468252927, 0.37615450783050736]
using another actor
actor:  0 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.40809999999999996 0.5 0.5
probs:  [0.07532452761929753, 0.22573951772490244, 0.3227814468252927, 0.37615450783050736]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07532452761929753, 0.22573951772490244, 0.3227814468252927, 0.37615450783050736]
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07532452761929753, 0.22573951772490244, 0.3227814468252927, 0.37615450783050736]
using another actor
from probs:  [0.07532452761929753, 0.22573951772490244, 0.3227814468252927, 0.37615450783050736]
actor:  0 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
maxi score, test score, baseline:  0.40909999999999996 0.5 0.5
probs:  [0.07532452761929753, 0.22573951772490244, 0.3227814468252927, 0.37615450783050736]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07532452761929753, 0.22573951772490244, 0.3227814468252927, 0.37615450783050736]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07532452761929753, 0.22573951772490244, 0.3227814468252927, 0.37615450783050736]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07580245719930333, 0.2247713903530026, 0.3208803794844214, 0.3785457729632727]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07580245719930333, 0.2247713903530026, 0.3208803794844214, 0.3785457729632727]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07580245719930333, 0.2247713903530026, 0.3208803794844214, 0.3785457729632727]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07580245719930333, 0.2247713903530026, 0.3208803794844214, 0.3785457729632727]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
siam score:  -0.77555954
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07580245719930333, 0.2247713903530026, 0.3208803794844214, 0.3785457729632727]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.08733636834863573
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07580245719930333, 0.2247713903530026, 0.3208803794844214, 0.3785457729632727]
using another actor
siam score:  -0.774808
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07580245719930333, 0.2247713903530026, 0.3208803794844214, 0.3785457729632727]
from probs:  [0.07580245719930333, 0.2247713903530026, 0.3208803794844214, 0.3785457729632727]
actor:  0 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07580245719930333, 0.2247713903530026, 0.3208803794844214, 0.3785457729632727]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07580245719930333, 0.2247713903530026, 0.3208803794844214, 0.3785457729632727]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
UNIT TEST: sample policy line 217 mcts : [0.042 0.833 0.042 0.042 0.042]
from probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
actor:  0 policy actor:  0  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.401]
 [0.444]
 [0.428]
 [0.43 ]] [[-1.029]
 [-0.487]
 [-0.86 ]
 [-1.029]
 [-1.176]] [[0.388]
 [0.722]
 [0.517]
 [0.388]
 [0.292]]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
line 256 mcts: sample exp_bonus 1.8004954221267273
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.7787781
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07544043530277333, 0.2284789577770542, 0.31934558049615847, 0.37673502642401396]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  63 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]] [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]] [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07601545786099059, 0.2254786886918174, 0.3188932079610842, 0.3796126454861077]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07601545786099059, 0.2254786886918174, 0.3188932079610842, 0.3796126454861077]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07601545786099059, 0.2254786886918174, 0.3188932079610842, 0.3796126454861077]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07566254740584012, 0.22443050695285652, 0.322059480405586, 0.3778474652357173]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07611754140981963, 0.22351459240017119, 0.32024390711258943, 0.38012395907741975]
actor:  0 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07611754140981963, 0.22351459240017119, 0.32024390711258943, 0.38012395907741975]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7974749
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.54 ]
 [0.475]
 [0.475]
 [0.473]] [[-0.07 ]
 [ 0.492]
 [-0.07 ]
 [-0.07 ]
 [-0.004]] [[0.668]
 [0.913]
 [0.668]
 [0.668]
 [0.691]]
maxi score, test score, baseline:  0.41209999999999997 0.5 0.5
probs:  [0.07576901887049906, 0.22248984508481562, 0.32336041310715824, 0.37838072293752706]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.541]
 [0.618]
 [0.498]
 [0.504]] [[-0.558]
 [-0.333]
 [-0.499]
 [-0.494]
 [-0.467]] [[0.313]
 [0.448]
 [0.47 ]
 [0.351]
 [0.367]]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41309999999999997 0.5 0.5
probs:  [0.07576901887049906, 0.22248984508481562, 0.32336041310715824, 0.37838072293752706]
using another actor
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07542367961425067, 0.2260385442607795, 0.32188436721766145, 0.3766534089073083]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07542367961425067, 0.2260385442607795, 0.32188436721766145, 0.3766534089073083]
siam score:  -0.793985
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07542367961425067, 0.2260385442607795, 0.32188436721766145, 0.3766534089073083]
actor:  0 policy actor:  1  step number:  51 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.07542367961425067, 0.2260385442607795, 0.32188436721766145, 0.3766534089073083]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07542367961425067, 0.2260385442607795, 0.32188436721766145, 0.3766534089073083]
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07542367961425067, 0.2260385442607795, 0.32188436721766145, 0.3766534089073083]
actor:  0 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07542367961425067, 0.2260385442607795, 0.32188436721766145, 0.3766534089073083]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.0759734065946619, 0.22322667793764026, 0.3213955254996259, 0.37940438996807196]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41109999999999997 0.5 0.5
probs:  [0.07563636728719685, 0.2266774758791792, 0.3199675723624624, 0.3777185844711616]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07563636728719685, 0.2266774758791792, 0.3199675723624624, 0.3777185844711616]
Printing some Q and Qe and total Qs values:  [[0.934]
 [0.231]
 [0.037]
 [0.868]
 [0.604]] [[ 1.172]
 [-0.037]
 [ 0.552]
 [ 1.485]
 [ 0.827]] [[0.912]
 [0.052]
 [0.118]
 [0.971]
 [0.581]]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07563636728719685, 0.2266774758791792, 0.3199675723624624, 0.3777185844711616]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07563636728719685, 0.2266774758791792, 0.3199675723624624, 0.3777185844711616]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07563636728719685, 0.2266774758791792, 0.3199675723624624, 0.3777185844711616]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.575]
 [0.455]
 [0.455]
 [0.455]] [[-0.597]
 [ 0.429]
 [-0.597]
 [-0.597]
 [-0.597]] [[0.488]
 [0.856]
 [0.488]
 [0.488]
 [0.488]]
maxi score, test score, baseline:  0.41009999999999996 0.5 0.5
probs:  [0.07530231103371143, 0.2300977316367519, 0.31855225769563217, 0.3760476996339044]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.536]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[-0.145]
 [ 0.511]
 [-0.145]
 [-0.145]
 [-0.145]] [[0.521]
 [0.943]
 [0.521]
 [0.521]
 [0.521]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]]
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.282]
 [0.306]
 [0.284]
 [0.277]] [[ 0.   ]
 [-1.026]
 [-1.103]
 [-1.239]
 [-1.347]] [[0.26 ]
 [0.282]
 [0.306]
 [0.284]
 [0.277]]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41409999999999997 0.525 0.525
probs:  [0.07573855321886111, 0.22917630510162493, 0.31685502046320424, 0.3782301212163097]
maxi score, test score, baseline:  0.41409999999999997 0.525 0.525
probs:  [0.07573855321886112, 0.22917630510162493, 0.3168550204632042, 0.3782301212163098]
maxi score, test score, baseline:  0.41409999999999997 0.525 0.525
probs:  [0.07573855321886112, 0.22917630510162493, 0.3168550204632042, 0.3782301212163098]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.23778205177862388
from probs:  [0.0754084267585255, 0.23254084267585248, 0.3154718399655529, 0.37657889060006905]
maxi score, test score, baseline:  0.41409999999999997 0.525 0.525
probs:  [0.0754084267585255, 0.2325408426758525, 0.31547183996555295, 0.37657889060006905]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.0754084267585255, 0.2325408426758525, 0.31547183996555295, 0.37657889060006905]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.0754084267585255, 0.2325408426758525, 0.31547183996555295, 0.37657889060006905]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
from probs:  [0.0754084267585255, 0.23254084267585248, 0.3154718399655529, 0.37657889060006905]
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
probs:  [0.0754084267585255, 0.2325408426758525, 0.31547183996555295, 0.37657889060006905]
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.0754084267585255, 0.2325408426758525, 0.31547183996555295, 0.37657889060006905]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.0754084267585255, 0.2325408426758525, 0.31547183996555295, 0.37657889060006905]
using another actor
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
probs:  [0.0754084267585255, 0.2325408426758525, 0.31547183996555295, 0.37657889060006905]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
probs:  [0.0750811713531438, 0.23587611942603026, 0.31410068875878583, 0.3749420204620401]
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
probs:  [0.0750811713531438, 0.23587611942603026, 0.31410068875878583, 0.3749420204620401]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
probs:  [0.0750811713531438, 0.23587611942603026, 0.31410068875878583, 0.3749420204620401]
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
probs:  [0.0750811713531438, 0.23587611942603026, 0.31410068875878583, 0.3749420204620401]
siam score:  -0.7827216
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.0750811713531438, 0.23587611942603026, 0.31410068875878583, 0.3749420204620401]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41509999999999997 0.525 0.525
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41509999999999997 0.525 0.525
probs:  [0.07475674971124328, 0.23918251541427427, 0.31274141059720917, 0.3733193242772732]
maxi score, test score, baseline:  0.41509999999999997 0.525 0.525
probs:  [0.07475674971124328, 0.23918251541427427, 0.31274141059720917, 0.3733193242772732]
maxi score, test score, baseline:  0.41509999999999997 0.525 0.525
probs:  [0.07475674971124328, 0.23918251541427427, 0.31274141059720917, 0.3733193242772732]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41509999999999997 0.525 0.525
probs:  [0.0744351251843943, 0.23815206366275055, 0.3157021924156561, 0.37171061873719907]
maxi score, test score, baseline:  0.41509999999999997 0.525 0.525
probs:  [0.0744351251843943, 0.23815206366275055, 0.3157021924156561, 0.37171061873719907]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.421]
 [0.444]
 [0.444]] [[-0.898]
 [-0.898]
 [-0.443]
 [-0.898]
 [-0.898]] [[0.474]
 [0.474]
 [0.679]
 [0.474]
 [0.474]]
Printing some Q and Qe and total Qs values:  [[1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]] [[1.284]
 [1.284]
 [1.284]
 [1.284]
 [1.284]] [[1.435]
 [1.435]
 [1.435]
 [1.435]
 [1.435]]
siam score:  -0.78654325
Printing some Q and Qe and total Qs values:  [[0.98 ]
 [1.142]
 [0.98 ]
 [0.98 ]
 [0.98 ]] [[1.041]
 [1.908]
 [1.041]
 [1.041]
 [1.041]] [[0.756]
 [1.225]
 [0.756]
 [0.756]
 [0.756]]
siam score:  -0.78585863
maxi score, test score, baseline:  0.41509999999999997 0.525 0.525
probs:  [0.0744351251843943, 0.23815206366275055, 0.3157021924156561, 0.37171061873719907]
maxi score, test score, baseline:  0.41509999999999997 0.525 0.525
probs:  [0.0744351251843943, 0.23815206366275055, 0.3157021924156561, 0.37171061873719907]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.0744351251843943, 0.23815206366275055, 0.3157021924156561, 0.37171061873719907]
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.0744351251843943, 0.23815206366275055, 0.3157021924156561, 0.37171061873719907]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[-0.9]
 [-0.9]
 [-0.9]
 [-0.9]
 [-0.9]] [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using another actor
actor:  0 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.41509999999999997 0.525 0.525
probs:  [0.0744351251843943, 0.23815206366275055, 0.3157021924156561, 0.37171061873719907]
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.29 ]
 [0.302]
 [0.29 ]
 [0.298]] [[-1.368]
 [-1.368]
 [-0.334]
 [-1.368]
 [-1.254]] [[0.29 ]
 [0.29 ]
 [0.302]
 [0.29 ]
 [0.298]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.499]
 [0.499]
 [0.499]
 [0.   ]] [[0.   ]
 [0.005]
 [0.005]
 [0.005]
 [0.   ]] [[0.   ]
 [0.499]
 [0.499]
 [0.499]
 [0.   ]]
actor:  0 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using another actor
actor:  0 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.07411626175340913, 0.24142030545138582, 0.31434770911460636, 0.37011572368059864]
maxi score, test score, baseline:  0.41509999999999997 0.525 0.525
probs:  [0.07411626175340913, 0.24142030545138582, 0.31434770911460636, 0.37011572368059864]
maxi score, test score, baseline:  0.41509999999999997 0.525 0.525
probs:  [0.07411626175340913, 0.24142030545138582, 0.31434770911460636, 0.37011572368059864]
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.816]
 [0.63 ]
 [0.63 ]
 [0.63 ]] [[0.803]
 [1.083]
 [0.803]
 [0.803]
 [0.803]] [[0.867]
 [1.071]
 [0.867]
 [0.867]
 [0.867]]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[-0.885]
 [-0.885]
 [-0.885]
 [-0.885]
 [-0.885]] [[0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.07411626175340913, 0.24142030545138582, 0.31434770911460636, 0.37011572368059864]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.07411626175340913, 0.24142030545138582, 0.31434770911460636, 0.37011572368059864]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.07411626175340913, 0.24142030545138582, 0.31434770911460636, 0.37011572368059864]
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.07411626175340913, 0.24142030545138582, 0.31434770911460636, 0.37011572368059864]
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.  ]
 [0.37]
 [0.  ]
 [0.37]
 [0.  ]] [[ 0.036]
 [-0.99 ]
 [-0.01 ]
 [-0.99 ]
 [-0.001]] [[1.   ]
 [0.733]
 [0.974]
 [0.733]
 [0.978]]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]] [[-0.697]
 [-0.697]
 [-0.697]
 [-0.697]
 [-0.697]] [[0.82]
 [0.82]
 [0.82]
 [0.82]
 [0.82]]
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
line 256 mcts: sample exp_bonus 1.9854922729144537
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.07411626175340913, 0.24142030545138582, 0.31434770911460636, 0.37011572368059864]
Printing some Q and Qe and total Qs values:  [[0.991]
 [0.114]
 [0.802]
 [0.657]
 [0.648]] [[4.034]
 [1.247]
 [3.474]
 [1.776]
 [1.787]] [[1.14 ]
 [0.024]
 [0.907]
 [0.469]
 [0.465]]
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.07411626175340913, 0.24142030545138582, 0.31434770911460636, 0.37011572368059864]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07411626175340913, 0.24142030545138582, 0.31434770911460636, 0.37011572368059864]
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.07380012401489494, 0.24466060981863316, 0.3130048041401285, 0.36853446202634343]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.07380012401489494, 0.24466060981863316, 0.3130048041401285, 0.36853446202634343]
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.07380012401489494, 0.24466060981863316, 0.3130048041401285, 0.36853446202634343]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.07304425540270353, 0.24892754094183456, 0.31327508443176055, 0.36475311922370135]
maxi score, test score, baseline:  0.41609999999999997 0.525 0.525
probs:  [0.07304425540270353, 0.24892754094183456, 0.31327508443176055, 0.36475311922370135]
actor:  0 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
probs:  [0.07228187374549619, 0.2489229204469424, 0.3178560118426288, 0.3609391939649326]
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
probs:  [0.07228187374549619, 0.2489229204469424, 0.3178560118426288, 0.3609391939649326]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.07228187374549619, 0.2489229204469424, 0.3178560118426288, 0.3609391939649326]
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.279]
 [0.273]
 [0.273]
 [0.273]] [[ 0.   ]
 [-0.179]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.273]
 [0.279]
 [0.273]
 [0.273]
 [0.273]]
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
probs:  [0.07228187374549619, 0.2489229204469424, 0.3178560118426288, 0.3609391939649326]
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
probs:  [0.07228187374549619, 0.2489229204469424, 0.3178560118426288, 0.3609391939649326]
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
probs:  [0.07228187374549619, 0.2489229204469424, 0.3178560118426288, 0.3609391939649326]
actor:  0 policy actor:  0  step number:  51 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.7953094
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
using explorer policy with actor:  0
maxi score, test score, baseline:  0.41709999999999997 0.525 0.525
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07228187374549619, 0.2489229204469424, 0.3178560118426288, 0.3609391939649326]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07228187374549619, 0.2489229204469424, 0.3178560118426288, 0.3609391939649326]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07197225457046176, 0.2521449125955366, 0.3164922904616348, 0.35939054237236684]
siam score:  -0.79058486
from probs:  [0.07151289449821457, 0.2532452201000324, 0.3181496221006817, 0.35709226330107124]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07151289449821457, 0.2532452201000324, 0.3181496221006817, 0.35709226330107124]
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07151289449821457, 0.2532452201000324, 0.3181496221006817, 0.35709226330107124]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
from probs:  [0.07151289449821457, 0.2532452201000324, 0.3181496221006817, 0.35709226330107124]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07151289449821457, 0.2532452201000324, 0.3181496221006817, 0.35709226330107124]
actor:  0 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07151289449821457, 0.2532452201000324, 0.3181496221006817, 0.35709226330107124]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07120525637522873, 0.2564624606129435, 0.31677875966708324, 0.3555535233447445]
siam score:  -0.7883786
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
siam score:  -0.7873931
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.459]
 [0.411]
 [0.411]
 [0.411]] [[-1.002]
 [-0.23 ]
 [-1.002]
 [-1.002]
 [-1.002]] [[0.411]
 [0.459]
 [0.411]
 [0.411]
 [0.411]]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
Printing some Q and Qe and total Qs values:  [[1.185]
 [1.185]
 [1.185]
 [1.185]
 [1.185]] [[1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]] [[1.137]
 [1.137]
 [1.137]
 [1.137]
 [1.137]]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
from probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
siam score:  -0.7773517
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
maxi score, test score, baseline:  0.41809999999999997 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4191 0.525 0.525
line 256 mcts: sample exp_bonus 1.9497796764037656
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4191 0.525 0.525
from probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
maxi score, test score, baseline:  0.4191 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
maxi score, test score, baseline:  0.4191 0.525 0.525
probs:  [0.07166528205304451, 0.2553393628127831, 0.3151402263159538, 0.35785512881821857]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
first move QE:  0.21098796083723687
maxi score, test score, baseline:  0.4191 0.525 0.525
probs:  [0.07136092240180501, 0.2585066227427712, 0.31379967057078395, 0.3563327842846398]
maxi score, test score, baseline:  0.4191 0.525 0.525
first move QE:  0.21076545822705786
maxi score, test score, baseline:  0.4191 0.525 0.525
probs:  [0.07136092240180501, 0.2585066227427712, 0.31379967057078395, 0.3563327842846398]
maxi score, test score, baseline:  0.4191 0.525 0.525
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4201 0.525 0.525
from probs:  [0.071512664353826, 0.26049925503801025, 0.31089567922045935, 0.3570924013877044]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.4201 0.525 0.525
probs:  [0.071512664353826, 0.26049925503801025, 0.31089567922045935, 0.3570924013877044]
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4211 0.525 0.525
probs:  [0.071512664353826, 0.26049925503801025, 0.31089567922045935, 0.3570924013877044]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.696]
 [0.551]
 [0.552]
 [0.541]] [[-0.89 ]
 [ 0.013]
 [-0.836]
 [-0.816]
 [-0.97 ]] [[0.141]
 [0.54 ]
 [0.172]
 [0.179]
 [0.122]]
maxi score, test score, baseline:  0.4211 0.525 0.525
maxi score, test score, baseline:  0.4201 0.525 0.525
probs:  [0.071512664353826, 0.2604992550380102, 0.31089567922045935, 0.3570924013877044]
maxi score, test score, baseline:  0.4201 0.525 0.525
probs:  [0.071512664353826, 0.2604992550380102, 0.31089567922045935, 0.3570924013877044]
maxi score, test score, baseline:  0.4201 0.525 0.525
probs:  [0.071512664353826, 0.2604992550380102, 0.31089567922045935, 0.3570924013877044]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4221 0.525 0.525
probs:  [0.071512664353826, 0.2604992550380102, 0.31089567922045935, 0.3570924013877044]
maxi score, test score, baseline:  0.4221 0.525 0.525
probs:  [0.071512664353826, 0.2604992550380102, 0.31089567922045935, 0.3570924013877044]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4221 0.525 0.525
probs:  [0.07195862889250766, 0.25937059847934174, 0.30934712370249745, 0.35932364892565316]
maxi score, test score, baseline:  0.4221 0.525 0.525
probs:  [0.07195862889250766, 0.25937059847934174, 0.30934712370249745, 0.35932364892565316]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.583]
 [0.482]
 [0.482]
 [0.482]] [[0.271]
 [0.61 ]
 [0.271]
 [0.271]
 [0.271]] [[0.824]
 [1.038]
 [0.824]
 [0.824]
 [0.824]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]]
maxi score, test score, baseline:  0.4221 0.525 0.525
probs:  [0.07166062876320431, 0.26244228171419504, 0.3080639813329103, 0.35783310818969044]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4221 0.525 0.525
probs:  [0.07166062876320431, 0.26244228171419504, 0.3080639813329103, 0.35783310818969044]
using another actor
maxi score, test score, baseline:  0.4221 0.525 0.525
probs:  [0.07166062876320431, 0.26244228171419504, 0.3080639813329102, 0.35783310818969044]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
siam score:  -0.7730256
maxi score, test score, baseline:  0.4221 0.525 0.525
probs:  [0.07209987033723142, 0.2613115689380951, 0.30655784469047553, 0.36003071603419795]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.7764072
actor:  0 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4241 0.525 0.525
probs:  [0.07209987033723142, 0.2613115689380951, 0.30655784469047553, 0.36003071603419795]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4241 0.525 0.525
probs:  [0.07209987033723142, 0.2613115689380951, 0.30655784469047553, 0.36003071603419795]
maxi score, test score, baseline:  0.4231 0.525 0.525
probs:  [0.07209987033723142, 0.2613115689380951, 0.30655784469047553, 0.36003071603419795]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4231 0.525 0.525
probs:  [0.0718049549558648, 0.26433753235987295, 0.3053019105309385, 0.3585556021533237]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4231 0.525 0.525
probs:  [0.0718049549558648, 0.26433753235987295, 0.3053019105309385, 0.3585556021533237]
maxi score, test score, baseline:  0.4231 0.525 0.525
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4221 0.525 0.525
probs:  [0.07151244736000176, 0.26733879082788553, 0.30405623022811373, 0.35709253158399895]
maxi score, test score, baseline:  0.4221 0.525 0.525
probs:  [0.07151244736000176, 0.26733879082788553, 0.30405623022811373, 0.35709253158399895]
actor:  0 policy actor:  0  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4221 0.525 0.525
probs:  [0.07151244736000176, 0.26733879082788553, 0.30405623022811373, 0.35709253158399895]
maxi score, test score, baseline:  0.4221 0.525 0.525
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.008]] [[-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.002]] [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.008]]
maxi score, test score, baseline:  0.4211 0.525 0.525
probs:  [0.07151244736000176, 0.26733879082788553, 0.30405623022811373, 0.35709253158399895]
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4231 0.525 0.525
probs:  [0.07151244736000176, 0.26733879082788553, 0.30405623022811373, 0.35709253158399895]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07151244736000176, 0.26733879082788553, 0.30405623022811373, 0.35709253158399895]
siam score:  -0.79886526
actor:  0 policy actor:  0  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07151244736000176, 0.26733879082788553, 0.30405623022811373, 0.35709253158399895]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07151244736000176, 0.26733879082788553, 0.3040562302281138, 0.35709253158399895]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4251 0.525 0.525
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07122231818260237, 0.26625251652885434, 0.3068838078509902, 0.35564135743755315]
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07122231818260237, 0.26625251652885434, 0.3068838078509902, 0.35564135743755315]
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07122231818260237, 0.26625251652885434, 0.3068838078509902, 0.35564135743755315]
maxi score, test score, baseline:  0.4251 0.525 0.525
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.496]
 [0.558]
 [0.558]
 [0.558]] [[-1.099]
 [-0.739]
 [-1.099]
 [-1.099]
 [-1.099]] [[0.792]
 [0.919]
 [0.792]
 [0.792]
 [0.792]]
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07122231818260237, 0.26625251652885434, 0.3068838078509902, 0.35564135743755315]
line 256 mcts: sample exp_bonus 1.4735868272135884
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.656]
 [0.61 ]
 [0.58 ]
 [0.584]] [[-0.647]
 [-0.686]
 [-0.967]
 [-0.598]
 [-0.575]] [[0.296]
 [0.364]
 [0.224]
 [0.317]
 [0.329]]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4251 0.525 0.525
maxi score, test score, baseline:  0.4251 0.525 0.525
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07136902671179822, 0.2640496271125552, 0.3082055980377286, 0.35637574813791795]
siam score:  -0.7961714
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07136902671179822, 0.2640496271125552, 0.3082055980377286, 0.35637574813791795]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07108411326491171, 0.26699201158936586, 0.3069732153290504, 0.3549506598166719]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07108411326491171, 0.26699201158936586, 0.3069732153290504, 0.3549506598166719]
maxi score, test score, baseline:  0.4251 0.525 0.525
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07108411326491171, 0.26699201158936586, 0.3069732153290504, 0.3549506598166719]
maxi score, test score, baseline:  0.4251 0.525 0.525
siam score:  -0.79865164
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07108411326491171, 0.26699201158936586, 0.3069732153290504, 0.3549506598166719]
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.07108411326491171, 0.26699201158936586, 0.3069732153290504, 0.3549506598166719]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.07108411326491171, 0.26699201158936586, 0.3069732153290504, 0.3549506598166719]
siam score:  -0.80667233
maxi score, test score, baseline:  0.4241 0.525 0.525
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4241 0.525 0.525
probs:  [0.0705210708982428, 0.26487394440069806, 0.3124705664829321, 0.352134418218127]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4251 0.525 0.525
maxi score, test score, baseline:  0.4251 0.525 0.525
probs:  [0.0705210708982428, 0.26487394440069806, 0.31247056648293203, 0.352134418218127]
actor:  0 policy actor:  1  step number:  45 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4261 0.525 0.525
probs:  [0.0705210708982428, 0.26487394440069806, 0.31247056648293203, 0.352134418218127]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4261 0.525 0.525
probs:  [0.07008501476448042, 0.26599244313204623, 0.3139697725281847, 0.3499527695752886]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4261 0.525 0.525
probs:  [0.07008501476448042, 0.26599244313204623, 0.3139697725281847, 0.3499527695752886]
Printing some Q and Qe and total Qs values:  [[1.039]
 [1.078]
 [1.039]
 [1.113]
 [1.118]] [[1.813]
 [1.907]
 [1.813]
 [2.077]
 [1.96 ]] [[0.965]
 [1.037]
 [0.965]
 [1.14 ]
 [1.089]]
line 256 mcts: sample exp_bonus -1.1381509249691628
maxi score, test score, baseline:  0.4261 0.525 0.525
Printing some Q and Qe and total Qs values:  [[1.144]
 [1.2  ]
 [1.144]
 [1.111]
 [1.124]] [[1.368]
 [1.395]
 [1.368]
 [1.496]
 [1.707]] [[1.142]
 [1.191]
 [1.142]
 [1.164]
 [1.245]]
Printing some Q and Qe and total Qs values:  [[1.383]
 [1.383]
 [1.383]
 [1.383]
 [1.383]] [[0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]] [[1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]]
maxi score, test score, baseline:  0.4261 0.525 0.525
probs:  [0.07008501476448042, 0.2659924431320462, 0.3139697725281847, 0.3499527695752886]
maxi score, test score, baseline:  0.4261 0.525 0.525
maxi score, test score, baseline:  0.4261 0.525 0.525
probs:  [0.07008501476448042, 0.2659924431320462, 0.3139697725281847, 0.3499527695752886]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4261 0.525 0.525
probs:  [0.06980635349036458, 0.26493317512510794, 0.3167015155588153, 0.34855895582571217]
maxi score, test score, baseline:  0.4261 0.525 0.525
probs:  [0.06980635349036458, 0.26493317512510794, 0.3167015155588153, 0.34855895582571217]
UNIT TEST: sample policy line 217 mcts : [0.042 0.333 0.333 0.125 0.167]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4281 0.525 0.525
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4281 0.525 0.525
probs:  [0.06980635349036458, 0.26493317512510794, 0.3167015155588153, 0.34855895582571217]
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.06980635349036458, 0.26493317512510794, 0.3167015155588153, 0.34855895582571217]
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.168]
 [0.174]
 [0.168]
 [0.168]] [[-0.929]
 [-0.929]
 [-0.355]
 [-0.929]
 [-0.929]] [[0.168]
 [0.168]
 [0.174]
 [0.168]
 [0.168]]
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06980635349036458, 0.26493317512510794, 0.3167015155588153, 0.34855895582571217]
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06980635349036458, 0.26493317512510794, 0.3167015155588153, 0.34855895582571217]
actor:  0 policy actor:  0  step number:  53 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06980635349036458, 0.26493317512510794, 0.3167015155588153, 0.34855895582571217]
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06980635349036458, 0.26493317512510794, 0.3167015155588153, 0.34855895582571217]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
siam score:  -0.80186766
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06980635349036458, 0.26493317512510794, 0.3167015155588153, 0.34855895582571217]
maxi score, test score, baseline:  0.4321 0.525 0.525
using explorer policy with actor:  1
siam score:  -0.7986685
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06980635349036458, 0.26493317512510794, 0.3167015155588153, 0.34855895582571217]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.725]
 [0.663]
 [0.663]
 [0.407]] [[-0.037]
 [-0.111]
 [-0.299]
 [ 0.351]
 [ 8.002]] [[0.126]
 [0.139]
 [0.1  ]
 [0.18 ]
 [1.057]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4331 0.525 0.525
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
maxi score, test score, baseline:  0.4331 0.525 0.525
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
first move QE:  0.1900010137265761
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
siam score:  -0.79205954
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
using another actor
actor:  0 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.7949595
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4341 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
maxi score, test score, baseline:  0.4341 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
maxi score, test score, baseline:  0.4341 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
actor:  0 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
Printing some Q and Qe and total Qs values:  [[0.773]
 [0.812]
 [0.773]
 [0.773]
 [0.773]] [[0.376]
 [0.879]
 [0.376]
 [0.376]
 [0.376]] [[0.924]
 [1.214]
 [0.924]
 [0.924]
 [0.924]]
using explorer policy with actor:  1
siam score:  -0.7965887
siam score:  -0.7988181
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.06992505537390019, 0.26416319789194037, 0.31676936149057633, 0.349142385243583]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.06947524097830447, 0.26529870839166914, 0.31833423081612205, 0.3468918198139044]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.4351 0.525 0.525
probs:  [0.06947524097830447, 0.26529870839166914, 0.31833423081612205, 0.3468918198139044]
maxi score, test score, baseline:  0.4351 0.525 0.525
probs:  [0.06947524097830447, 0.26529870839166914, 0.31833423081612205, 0.3468918198139044]
maxi score, test score, baseline:  0.4351 0.525 0.525
probs:  [0.06947524097830447, 0.26529870839166914, 0.31833423081612205, 0.3468918198139044]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4351 0.525 0.525
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4351 0.525 0.525
probs:  [0.06930442626248477, 0.2675532843059301, 0.3171154988167913, 0.3460267906147938]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4361 0.525 0.525
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06930442626248477, 0.2675532843059301, 0.3171154988167913, 0.3460267906147938]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.167]
 [0.375]
 [0.343]
 [0.357]] [[-1.154]
 [-0.484]
 [-0.814]
 [-1.174]
 [-1.326]] [[0.301]
 [0.167]
 [0.375]
 [0.343]
 [0.357]]
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.0690198101982223, 0.2705659306592929, 0.3158109781097373, 0.34460328103274745]
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.0690198101982223, 0.2705659306592929, 0.3158109781097373, 0.34460328103274745]
actor:  0 policy actor:  0  step number:  77 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.0690198101982223, 0.2705659306592929, 0.3158109781097373, 0.34460328103274745]
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.0690198101982223, 0.2705659306592929, 0.3158109781097373, 0.34460328103274745]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.0690198101982223, 0.2705659306592929, 0.3158109781097373, 0.34460328103274745]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.0690198101982223, 0.2705659306592929, 0.3158109781097373, 0.34460328103274745]
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.0690198101982223, 0.2705659306592929, 0.3158109781097373, 0.34460328103274745]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.06873752729399178, 0.26945755356731166, 0.31861347836894094, 0.3431914407697556]
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.0687375272939918, 0.26945755356731166, 0.31861347836894094, 0.3431914407697557]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.06845754897754167, 0.26835822538429355, 0.3213930987166971, 0.3417911269214677]
maxi score, test score, baseline:  0.4351 0.525 0.525
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4351 0.525 0.525
probs:  [0.06891559672525191, 0.2671979600875459, 0.3198034850612158, 0.3440829581259864]
siam score:  -0.79530907
from probs:  [0.06891559672525191, 0.2671979600875459, 0.3198034850612158, 0.3440829581259864]
maxi score, test score, baseline:  0.4351 0.525 0.525
Printing some Q and Qe and total Qs values:  [[1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]] [[0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[1.226]
 [1.226]
 [1.226]
 [1.226]
 [1.226]]
maxi score, test score, baseline:  0.4341 0.525 0.525
probs:  [0.06891559672525191, 0.2671979600875459, 0.3198034850612158, 0.3440829581259864]
maxi score, test score, baseline:  0.4341 0.525 0.525
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4341 0.525 0.525
probs:  [0.06891559672525191, 0.2671979600875459, 0.3198034850612158, 0.3440829581259864]
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06891559672525191, 0.2671979600875459, 0.3198034850612158, 0.3440829581259864]
siam score:  -0.79693395
maxi score, test score, baseline:  0.4331 0.525 0.525
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06891559672525191, 0.2671979600875459, 0.3198034850612158, 0.3440829581259864]
maxi score, test score, baseline:  0.4331 0.525 0.525
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using explorer policy with actor:  0
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06891559672525192, 0.267197960087546, 0.3198034850612157, 0.34408295812598644]
maxi score, test score, baseline:  0.4331 0.525 0.525
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06891559672525192, 0.267197960087546, 0.3198034850612157, 0.34408295812598644]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06891559672525192, 0.267197960087546, 0.3198034850612157, 0.34408295812598644]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06863828682258147, 0.27015130146415767, 0.3185144249781359, 0.3426959867351251]
first move QE:  0.18051525419275402
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06863828682258147, 0.27015130146415767, 0.3185144249781359, 0.3426959867351251]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.    0.875 0.042]
maxi score, test score, baseline:  0.4321 0.525 0.525
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06863828682258147, 0.27015130146415767, 0.3185144249781359, 0.3426959867351251]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4331 0.525 0.525
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4331 0.525 0.525
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06863828682258147, 0.27015130146415767, 0.3185144249781359, 0.3426959867351251]
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06863828682258147, 0.27015130146415767, 0.3185144249781359, 0.3426959867351251]
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06863828682258147, 0.27015130146415767, 0.3185144249781359, 0.3426959867351251]
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06863828682258147, 0.27015130146415767, 0.3185144249781359, 0.3426959867351251]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7881932
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06809032314321813, 0.27198908181785275, 0.31996526032953154, 0.33995533470939765]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06755105819243606, 0.2777639694054989, 0.3174267828419258, 0.33725818956013925]
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06755105819243606, 0.2777639694054989, 0.3174267828419258, 0.33725818956013925]
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06755105819243606, 0.2777639694054989, 0.3174267828419258, 0.33725818956013925]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
first move QE:  0.17842465951054037
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.06728462326681284, 0.28061717123637187, 0.3161725958979651, 0.3359256095988502]
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06728462326681284, 0.28061717123637187, 0.3161725958979651, 0.3359256095988502]
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06728462326681284, 0.28061717123637187, 0.3161725958979651, 0.3359256095988502]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4301 0.525 0.525
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06728462326681284, 0.28061717123637187, 0.3161725958979651, 0.3359256095988502]
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06728462326681284, 0.28061717123637187, 0.3161725958979651, 0.3359256095988502]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.1766360963806364
maxi score, test score, baseline:  0.4291 0.525 0.525
probs:  [0.06702028646754134, 0.2834479046242129, 0.3149282854470014, 0.33460352346124433]
Printing some Q and Qe and total Qs values:  [[1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.068]] [[0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.585]] [[0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.825]]
maxi score, test score, baseline:  0.4291 0.525 0.525
probs:  [0.06702028646754134, 0.2834479046242129, 0.3149282854470014, 0.33460352346124433]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4281 0.525 0.525
probs:  [0.06702028646754134, 0.28344790462421293, 0.3149282854470014, 0.3346035234612444]
maxi score, test score, baseline:  0.4281 0.525 0.525
probs:  [0.06702028646754134, 0.28344790462421293, 0.3149282854470014, 0.3346035234612444]
maxi score, test score, baseline:  0.4281 0.525 0.525
maxi score, test score, baseline:  0.4281 0.525 0.525
probs:  [0.06702028646754134, 0.28344790462421293, 0.3149282854470014, 0.3346035234612444]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
first move QE:  0.17497046224728519
line 256 mcts: sample exp_bonus 1.4492744564344437
maxi score, test score, baseline:  0.4271 0.525 0.525
probs:  [0.06721147049852387, 0.28500205884070823, 0.3122258823834812, 0.33556058827728674]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4271 0.525 0.525
probs:  [0.06695151531693506, 0.2838978675339009, 0.31489020356489594, 0.33426041358426795]
maxi score, test score, baseline:  0.4271 0.525 0.525
probs:  [0.06695151531693506, 0.2838978675339009, 0.31489020356489594, 0.33426041358426795]
maxi score, test score, baseline:  0.4271 0.525 0.525
probs:  [0.06695151531693506, 0.2838978675339009, 0.31489020356489594, 0.33426041358426795]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4281 0.525 0.525
probs:  [0.06695151531693506, 0.2838978675339009, 0.31489020356489594, 0.33426041358426795]
Printing some Q and Qe and total Qs values:  [[1.041]
 [1.175]
 [1.041]
 [0.137]
 [1.005]] [[ 0.47 ]
 [ 0.773]
 [ 0.47 ]
 [-0.198]
 [ 1.075]] [[0.999]
 [1.199]
 [0.999]
 [0.04 ]
 [1.139]]
maxi score, test score, baseline:  0.4281 0.525 0.525
probs:  [0.06695151531693506, 0.2838978675339009, 0.31489020356489594, 0.33426041358426795]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06695151531693506, 0.2838978675339009, 0.31489020356489594, 0.33426041358426795]
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4291 0.525 0.525
probs:  [0.06739824301318875, 0.28267610388185044, 0.31343008400594496, 0.33649556909901585]
maxi score, test score, baseline:  0.4291 0.525 0.525
probs:  [0.06739824301318875, 0.28267610388185044, 0.31343008400594496, 0.33649556909901585]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4301 0.525 0.525
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4301 0.525 0.525
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06739824301318875, 0.28267610388185044, 0.31343008400594496, 0.33649556909901585]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.0678381517644514, 0.281472989485723, 0.3119922520173333, 0.33869660673249236]
from probs:  [0.0678381517644514, 0.281472989485723, 0.3119922520173333, 0.33869660673249236]
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.376]
 [0.303]
 [0.115]
 [0.276]] [[-0.699]
 [-0.206]
 [-0.625]
 [-0.57 ]
 [-0.453]] [[0.188]
 [0.376]
 [0.303]
 [0.115]
 [0.276]]
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.0678381517644514, 0.281472989485723, 0.3119922520173333, 0.33869660673249236]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.0678381517644514, 0.281472989485723, 0.3119922520173333, 0.33869660673249236]
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.0678381517644514, 0.281472989485723, 0.3119922520173333, 0.33869660673249236]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06758075496166872, 0.2842036084446872, 0.31080641501277706, 0.33740922158086706]
maxi score, test score, baseline:  0.4301 0.525 0.525
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06758075496166872, 0.2842036084446872, 0.31080641501277706, 0.33740922158086706]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4311 0.525 0.525
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4311 0.525 0.525
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06758075496166872, 0.28420360844468706, 0.31080641501277706, 0.3374092215808671]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06732530833892797, 0.28691353872943937, 0.3096295625629406, 0.33613159036869206]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06707178981641532, 0.2858312989019392, 0.3122333086191576, 0.33486360266248777]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.9166231155395508e-06
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4311 0.525 0.525
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06707178981641532, 0.2858312989019392, 0.3122333086191576, 0.33486360266248777]
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06707178981641532, 0.2858312989019392, 0.3122333086191576, 0.33486360266248777]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06682017764637874, 0.28475719706196917, 0.31481747560205053, 0.33360514968960153]
line 256 mcts: sample exp_bonus 0.7475101767059498
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06682017764637874, 0.28475719706196917, 0.31481747560205053, 0.33360514968960153]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06657045040690894, 0.2874346019577737, 0.3136388233282152, 0.3323561243071022]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4311 0.525 0.525
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06632258699585962, 0.2863625335388907, 0.316198458493878, 0.33111642097137167]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.465]
 [0.384]
 [0.384]
 [0.384]] [[-0.269]
 [-0.334]
 [-0.269]
 [-0.269]
 [-0.269]] [[0.384]
 [0.465]
 [0.384]
 [0.384]
 [0.384]]
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06632258699585962, 0.2863625335388907, 0.316198458493878, 0.33111642097137167]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06632258699585962, 0.2863625335388907, 0.316198458493878, 0.33111642097137167]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.984]
 [0.622]
 [0.846]
 [0.926]] [[1.642]
 [1.211]
 [0.438]
 [1.083]
 [1.341]] [[0.523]
 [0.926]
 [0.404]
 [0.785]
 [0.93 ]]
Printing some Q and Qe and total Qs values:  [[1.5  ]
 [0.398]
 [0.47 ]
 [1.5  ]
 [0.437]] [[ 0.   ]
 [-1.355]
 [-1.495]
 [ 0.   ]
 [-1.548]] [[2.399]
 [0.393]
 [0.372]
 [2.399]
 [0.304]]
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06588120904166174, 0.28757526346088536, 0.31763547422959365, 0.32890805326785927]
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4311 0.525 0.525
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06632258699585962, 0.2863625335388907, 0.31619845849387795, 0.33111642097137167]
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.06632258699585962, 0.2863625335388907, 0.31619845849387795, 0.33111642097137167]
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06632258699585962, 0.2863625335388907, 0.31619845849387795, 0.33111642097137167]
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06632258699585962, 0.2863625335388907, 0.31619845849387795, 0.33111642097137167]
maxi score, test score, baseline:  0.4301 0.525 0.525
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06632258699585962, 0.2863625335388907, 0.31619845849387795, 0.33111642097137167]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.7799786
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06632258699585962, 0.2863625335388907, 0.31619845849387795, 0.33111642097137167]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
first move QE:  0.1598618082075285
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.575]
 [0.395]
 [0.395]
 [0.395]] [[-0.236]
 [ 0.338]
 [-0.236]
 [-0.236]
 [-0.236]] [[0.458]
 [0.689]
 [0.458]
 [0.458]
 [0.458]]
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06675742729384478, 0.28516776647895903, 0.31478272772439836, 0.33329207850279785]
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.804]
 [0.4  ]
 [0.4  ]
 [0.4  ]] [[-0.053]
 [ 2.9  ]
 [-0.053]
 [-0.053]
 [-0.053]] [[0.274]
 [0.905]
 [0.274]
 [0.274]
 [0.274]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06651161747221868, 0.287804139113764, 0.3136215999719443, 0.33206264344207304]
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06651161747221868, 0.287804139113764, 0.3136215999719443, 0.33206264344207304]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.4120569229125975e-06
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
siam score:  -0.77948153
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.3161436585051405, 0.3308422492840606]
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.3161436585051405, 0.3308422492840606]
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.3161436585051405, 0.3308422492840606]
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.3161436585051405, 0.3308422492840606]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.5  ]
 [0.5  ]
 [0.531]
 [0.633]] [[-0.509]
 [ 0.   ]
 [ 0.   ]
 [ 0.754]
 [-0.171]] [[0.255]
 [0.406]
 [0.406]
 [0.688]
 [0.482]]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.269]
 [0.363]
 [0.269]
 [0.272]] [[-0.997]
 [-0.997]
 [-0.838]
 [-0.997]
 [-0.973]] [[0.269]
 [0.269]
 [0.363]
 [0.269]
 [0.272]]
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.667 0.25  0.    0.042 0.042]
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
actor:  0 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
UNIT TEST: sample policy line 217 mcts : [0.375 0.042 0.208 0.208 0.167]
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
actor:  0 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
maxi score, test score, baseline:  0.4331 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
siam score:  -0.7718293
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.762]
 [0.   ]
 [0.762]
 [0.762]
 [0.   ]] [[0.509]
 [0.   ]
 [0.509]
 [0.509]
 [0.001]]
maxi score, test score, baseline:  0.4321 0.525 0.525
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4321 0.525 0.525
probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4321 0.525 0.525
from probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using another actor
from probs:  [0.06626761526349874, 0.2867464769473002, 0.31614365850514037, 0.3308422492840606]
maxi score, test score, baseline:  0.4311 0.525 0.525
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06626761526349875, 0.2867464769473002, 0.3161436585051404, 0.33084224928406064]
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.06626761526349875, 0.2867464769473002, 0.3161436585051404, 0.33084224928406064]
maxi score, test score, baseline:  0.4301 0.525 0.525
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.343]] [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06621423684237238, 0.2871192920663189, 0.31609044684978727, 0.3305760242415215]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[-0.003]
 [ 0.005]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.   ]
 [0.005]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.4301 0.525 0.525
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06621423684237238, 0.2871192920663189, 0.3160904468497872, 0.3305760242415215]
siam score:  -0.78130877
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06621423684237238, 0.2871192920663189, 0.3160904468497872, 0.3305760242415215]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06597571262428241, 0.2860831936030819, 0.31855806784585555, 0.32938302592678015]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4301 0.525 0.525
probs:  [0.06573890458586654, 0.2850545498592741, 0.32100793433032465, 0.3281986112245347]
Printing some Q and Qe and total Qs values:  [[1.248]
 [1.401]
 [1.248]
 [0.932]
 [1.261]] [[1.737]
 [1.857]
 [1.737]
 [0.728]
 [1.345]] [[1.033]
 [1.175]
 [1.033]
 [0.484]
 [0.911]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Starting evaluation
UNIT TEST: sample policy line 217 mcts : [0.    0.583 0.    0.    0.417]
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.136]
 [0.178]
 [0.152]
 [0.15 ]] [[-1.721]
 [-1.332]
 [-1.358]
 [-1.872]
 [-1.701]] [[0.16 ]
 [0.136]
 [0.178]
 [0.152]
 [0.15 ]]
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4311 0.525 0.525
probs:  [0.0655037942716714, 0.287615731264999, 0.3198577866349982, 0.32702268782833127]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.46720489109043656
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4361 0.525 0.525
probs:  [0.0655037942716714, 0.287615731264999, 0.3198577866349982, 0.32702268782833127]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4391 0.55 0.55
probs:  [0.06550383441545432, 0.2876157230803443, 0.3198577714349251, 0.3270226710692763]
maxi score, test score, baseline:  0.4391 0.55 0.55
probs:  [0.06550383441545432, 0.2876157230803443, 0.3198577714349251, 0.3270226710692763]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7806272
maxi score, test score, baseline:  0.4391 0.55 0.55
maxi score, test score, baseline:  0.4391 0.55 0.55
probs:  [0.06527040354092757, 0.28658895388802885, 0.3222854942665936, 0.32585514830445]
maxi score, test score, baseline:  0.4391 0.55 0.55
probs:  [0.06527040354092757, 0.28658895388802885, 0.3222854942665935, 0.32585514830445]
actor:  0 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4391 0.55 0.55
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.182]
 [1.153]
 [1.153]
 [1.153]
 [1.153]] [[1.349]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]] [[1.073]
 [0.842]
 [0.842]
 [0.842]
 [0.842]]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06503863426635365, 0.28556949341031657, 0.3246959361616649, 0.32469593616166487]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06503863426635365, 0.28556949341031657, 0.3246959361616649, 0.32469593616166487]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06503863426635365, 0.28556949341031657, 0.3246959361616649, 0.32469593616166487]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06503863426635365, 0.28556949341031657, 0.3246959361616649, 0.32469593616166487]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06503863426635365, 0.28556949341031657, 0.3246959361616649, 0.32469593616166487]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.652]
 [1.017]
 [0.652]
 [0.652]
 [0.597]] [[1.85 ]
 [1.241]
 [1.85 ]
 [1.85 ]
 [1.115]] [[0.974]
 [1.119]
 [0.974]
 [0.974]
 [0.704]]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06546257133583228, 0.2844352139612562, 0.3268170157597253, 0.3232851989431862]
Printing some Q and Qe and total Qs values:  [[1.127]
 [1.046]
 [1.127]
 [0.955]
 [0.908]] [[1.336]
 [0.65 ]
 [1.336]
 [0.67 ]
 [0.339]] [[0.992]
 [0.773]
 [0.992]
 [0.713]
 [0.601]]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.944]
 [0.163]
 [0.719]
 [0.585]] [[0.566]
 [1.528]
 [0.622]
 [1.15 ]
 [1.543]] [[0.052]
 [0.927]
 [0.049]
 [0.632]
 [0.677]]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06546257133583228, 0.2844352139612562, 0.3268170157597253, 0.3232851989431862]
from probs:  [0.06546257133583228, 0.2844352139612562, 0.3268170157597253, 0.3232851989431862]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4361 0.55 0.55
maxi score, test score, baseline:  0.4361 0.55 0.55
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.06611202996077604, 0.28431402311736714, 0.33006605394052335, 0.3195078929813335]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.06611202996077604, 0.28431402311736714, 0.33006605394052335, 0.3195078929813335]
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.06611202996077604, 0.28431402311736714, 0.33006605394052335, 0.3195078929813335]
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.06611202996077604, 0.28431402311736714, 0.33006605394052335, 0.3195078929813335]
start point for exploration sampling:  16339
first move QE:  0.14642051941453527
maxi score, test score, baseline:  0.4351 0.55 0.55
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.218]
 [0.196]
 [0.196]
 [0.196]] [[-0.94 ]
 [-0.417]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.184]
 [0.218]
 [0.196]
 [0.196]
 [0.196]]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06611202996077604, 0.28431402311736714, 0.33006605394052335, 0.3195078929813335]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06611202996077604, 0.28431402311736714, 0.33006605394052335, 0.3195078929813335]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.507]
 [0.751]
 [0.507]
 [0.507]] [[1.133]
 [0.332]
 [0.662]
 [0.332]
 [0.332]] [[1.24 ]
 [0.634]
 [1.019]
 [0.634]
 [0.634]]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4341 0.55 0.55
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06611202996077604, 0.28431402311736714, 0.33006605394052335, 0.3195078929813335]
siam score:  -0.78120923
maxi score, test score, baseline:  0.4341 0.55 0.55
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06634513635876554, 0.28531824300792963, 0.33123195891823837, 0.31710466171506646]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4341 0.55 0.55
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.477]] [[1.714]
 [1.714]
 [1.714]
 [1.714]
 [2.685]] [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.82 ]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06634513635876554, 0.28531824300792963, 0.33123195891823837, 0.31710466171506646]
siam score:  -0.78254104
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06634513635876554, 0.28531824300792963, 0.33123195891823837, 0.31710466171506646]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06611202996077604, 0.28783341010376373, 0.33006605394052335, 0.31598850599493683]
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06611202996077604, 0.28783341010376373, 0.33006605394052335, 0.31598850599493683]
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06611202996077604, 0.28783341010376373, 0.33006605394052335, 0.31598850599493683]
from probs:  [0.06611202996077604, 0.28783341010376373, 0.33006605394052335, 0.31598850599493683]
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06611202996077604, 0.28783341010376373, 0.33006605394052335, 0.31598850599493683]
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06611202996077604, 0.28783341010376373, 0.33006605394052335, 0.31598850599493683]
actor:  0 policy actor:  0  step number:  41 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06611202996077604, 0.28783341010376373, 0.33006605394052335, 0.31598850599493683]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4341 0.55 0.55
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06652401662438674, 0.2866951966751227, 0.33212734493956025, 0.31465344176093035]
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06652401662438674, 0.2866951966751227, 0.33212734493956025, 0.31465344176093035]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06652401662438674, 0.2866951966751227, 0.33212734493956025, 0.31465344176093035]
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06652401662438674, 0.2866951966751227, 0.33212734493956025, 0.31465344176093035]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06693028234704054, 0.28557278873825276, 0.33416001238074444, 0.31333691653396223]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4341 0.55 0.55
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06693028234704054, 0.28557278873825276, 0.33416001238074444, 0.31333691653396223]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06693028234704054, 0.28557278873825276, 0.33416001238074444, 0.31333691653396223]
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.881]
 [0.878]
 [0.878]
 [0.7  ]] [[0.646]
 [0.381]
 [0.646]
 [0.646]
 [0.831]] [[1.232]
 [1.147]
 [1.232]
 [1.232]
 [1.115]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06669918345442545, 0.2880435656981381, 0.3330041433413922, 0.3122531075060442]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4341 0.55 0.55
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06669918345442545, 0.2880435656981381, 0.3330041433413922, 0.31225310750604424]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06646967851870228, 0.29049730098413606, 0.3318562466700623, 0.3111767738270992]
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06646967851870228, 0.29049730098413606, 0.3318562466700623, 0.3111767738270992]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.007]
 [0.442]
 [0.327]
 [0.374]] [[ 0.689]
 [ 0.664]
 [ 0.248]
 [-0.067]
 [ 0.328]] [[0.263]
 [0.007]
 [0.442]
 [0.327]
 [0.374]]
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06646967851870228, 0.29049730098413606, 0.3318562466700623, 0.3111767738270992]
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06646967851870228, 0.29049730098413606, 0.3318562466700623, 0.3111767738270992]
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06646967851870228, 0.29049730098413606, 0.3318562466700623, 0.3111767738270992]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.4341 0.55 0.55
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.41 ]
 [0.372]
 [0.372]
 [0.372]] [[0.042]
 [0.095]
 [0.042]
 [0.042]
 [0.042]] [[0.491]
 [0.565]
 [0.491]
 [0.491]
 [0.491]]
siam score:  -0.77188575
maxi score, test score, baseline:  0.4321 0.55 0.55
probs:  [0.06646967851870228, 0.29049730098413606, 0.3318562466700623, 0.3111767738270992]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06624175110556442, 0.2929341703024382, 0.3307162401685838, 0.3101078384234135]
maxi score, test score, baseline:  0.4321 0.55 0.55
probs:  [0.06583493912286373, 0.2940958596466382, 0.32868084760478594, 0.31138835362571204]
maxi score, test score, baseline:  0.4321 0.55 0.55
probs:  [0.06583493912286373, 0.2940958596466382, 0.32868084760478594, 0.31138835362571204]
actor:  0 policy actor:  1  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4321 0.55 0.55
maxi score, test score, baseline:  0.4321 0.55 0.55
probs:  [0.06583493912286373, 0.2940958596466382, 0.32868084760478594, 0.31138835362571204]
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4321 0.55 0.55
maxi score, test score, baseline:  0.4321 0.55 0.55
probs:  [0.06542245839433894, 0.29527373662025636, 0.32661709274197254, 0.31268671224343203]
using another actor
from probs:  [0.06542245839433894, 0.29527373662025636, 0.32661709274197254, 0.31268671224343203]
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.06542245839433894, 0.29527373662025636, 0.32661709274197254, 0.31268671224343203]
siam score:  -0.77252215
maxi score, test score, baseline:  0.4321 0.55 0.55
probs:  [0.06542245839433894, 0.29527373662025636, 0.32661709274197254, 0.31268671224343203]
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06542245839433894, 0.29527373662025636, 0.32661709274197254, 0.31268671224343203]
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06542245839433894, 0.29527373662025636, 0.32661709274197254, 0.31268671224343203]
from probs:  [0.06542245839433894, 0.29527373662025636, 0.32661709274197254, 0.31268671224343203]
maxi score, test score, baseline:  0.4331 0.55 0.55
maxi score, test score, baseline:  0.4331 0.55 0.55
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
first move QE:  0.15018411970637607
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06565070802165672, 0.2963057463263137, 0.3277587060951306, 0.310284839556899]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06606303636712699, 0.2951166137212707, 0.32982170119917137, 0.30899864871243093]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4321 0.55 0.55
probs:  [0.06606303636712699, 0.2951166137212707, 0.32982170119917137, 0.30899864871243093]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.06606303636712699, 0.2951166137212707, 0.32982170119917137, 0.30899864871243093]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06606303636712699, 0.2951166137212707, 0.32982170119917137, 0.30899864871243093]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06583493912286373, 0.297554358442453, 0.32868084760478594, 0.3079298548298973]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.06583493912286373, 0.297554358442453, 0.32868084760478594, 0.3079298548298973]
siam score:  -0.7881154
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  0 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06560841512898935, 0.2999752893575636, 0.32754786279621945, 0.3068684327172276]
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06560841512898935, 0.2999752893575636, 0.32754786279621945, 0.3068684327172276]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
siam score:  -0.7906544
maxi score, test score, baseline:  0.4331 0.55 0.55
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.827]
 [0.627]
 [0.627]
 [0.628]] [[0.275]
 [0.015]
 [0.275]
 [0.275]
 [0.473]] [[0.632]
 [0.688]
 [0.632]
 [0.632]
 [0.713]]
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06477740308133775, 0.30242148969396104, 0.3233900855715454, 0.3094110216531558]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06477740308133775, 0.30242148969396104, 0.3233900855715454, 0.3094110216531558]
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06477740308133775, 0.30242148969396104, 0.3233900855715454, 0.3094110216531558]
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06477740308133777, 0.302421489693961, 0.3233900855715454, 0.3094110216531558]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06477740308133777, 0.302421489693961, 0.3233900855715454, 0.3094110216531558]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4351 0.55 0.55
maxi score, test score, baseline:  0.4351 0.55 0.55
probs:  [0.06477740308133777, 0.302421489693961, 0.3233900855715454, 0.3094110216531558]
maxi score, test score, baseline:  0.4351 0.55 0.55
maxi score, test score, baseline:  0.4351 0.55 0.55
probs:  [0.06477740308133777, 0.302421489693961, 0.3233900855715454, 0.3094110216531558]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.06477740308133777, 0.302421489693961, 0.3233900855715454, 0.3094110216531558]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.106]
 [0.158]
 [0.106]
 [0.106]] [[-1.25 ]
 [-1.25 ]
 [-0.987]
 [-1.25 ]
 [-1.25 ]] [[0.106]
 [0.106]
 [0.158]
 [0.106]
 [0.106]]
maxi score, test score, baseline:  0.4331 0.55 0.55
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06477740308133777, 0.302421489693961, 0.3233900855715454, 0.3094110216531558]
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06477740308133777, 0.302421489693961, 0.3233900855715454, 0.3094110216531558]
maxi score, test score, baseline:  0.4331 0.55 0.55
probs:  [0.06477740308133777, 0.302421489693961, 0.3233900855715454, 0.3094110216531558]
actor:  0 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.14306535624122046
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [0.06474715530135625, 0.3060066739786597, 0.32323949674132424, 0.3060066739786597]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4351 0.55 0.55
probs:  [0.06474715530135625, 0.3060066739786597, 0.32323949674132424, 0.3060066739786597]
maxi score, test score, baseline:  0.4351 0.55 0.55
maxi score, test score, baseline:  0.4351 0.55 0.55
probs:  [0.06474715530135626, 0.3060066739786597, 0.3232394967413243, 0.30600667397865977]
maxi score, test score, baseline:  0.4351 0.55 0.55
probs:  [0.06474715530135626, 0.3060066739786597, 0.3232394967413243, 0.30600667397865977]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.06474715530135625, 0.3060066739786597, 0.32323949674132424, 0.3060066739786597]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.06474715530135625, 0.3060066739786597, 0.32323949674132424, 0.3060066739786597]
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.06474715530135625, 0.3060066739786597, 0.32323949674132424, 0.3060066739786597]
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.06474715530135625, 0.3060066739786597, 0.32323949674132424, 0.3060066739786597]
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.06474715530135625, 0.3060066739786597, 0.32323949674132424, 0.3060066739786597]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.06474715530135625, 0.3060066739786597, 0.32323949674132424, 0.3060066739786597]
maxi score, test score, baseline:  0.4361 0.55 0.55
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.0651600222315079, 0.3047674008202939, 0.3253051761279042, 0.3047674008202939]
maxi score, test score, baseline:  0.4361 0.55 0.55
siam score:  -0.7763988
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.0651600222315079, 0.3047674008202939, 0.3253051761279042, 0.3047674008202939]
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.0651600222315079, 0.3047674008202939, 0.3253051761279042, 0.3047674008202939]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.0651600222315079, 0.3047674008202939, 0.3253051761279042, 0.3047674008202939]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4361 0.55 0.55
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.06556727301289178, 0.3035449852543218, 0.3273427564784648, 0.3035449852543218]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.06556727301289177, 0.3035449852543218, 0.32734275647846467, 0.3035449852543217]
maxi score, test score, baseline:  0.4361 0.55 0.55
maxi score, test score, baseline:  0.4351 0.55 0.55
probs:  [0.06556727301289177, 0.3035449852543218, 0.32734275647846467, 0.3035449852543218]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.003]
 [0.011]
 [0.011]
 [0.006]] [[-0.033]
 [-0.003]
 [-0.033]
 [-0.033]
 [-0.003]] [[0.031]
 [0.032]
 [0.031]
 [0.031]
 [0.035]]
maxi score, test score, baseline:  0.4351 0.55 0.55
probs:  [0.06596902146445285, 0.3023390856385501, 0.32935280725844684, 0.3023390856385501]
actor:  0 policy actor:  0  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4351 0.55 0.55
maxi score, test score, baseline:  0.4351 0.55 0.55
probs:  [0.06596902146445285, 0.3023390856385501, 0.32935280725844684, 0.3023390856385501]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4351 0.55 0.55
maxi score, test score, baseline:  0.4351 0.55 0.55
probs:  [0.06636537835017811, 0.3011493695006353, 0.3313358826485513, 0.3011493695006353]
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [0.06636537835017811, 0.3011493695006353, 0.3313358826485513, 0.3011493695006353]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06636537835017811, 0.3011493695006353, 0.3313358826485513, 0.3011493695006353]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06636537835017811, 0.3011493695006353, 0.3313358826485513, 0.3011493695006353]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06636537835017811, 0.3011493695006353, 0.3313358826485513, 0.3011493695006353]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06658835037007149, 0.30216294622502554, 0.332451108549234, 0.29879759485566904]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06658835037007149, 0.30216294622502554, 0.332451108549234, 0.29879759485566904]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06681282912253177, 0.3031833721902327, 0.3335738705846515, 0.2964299281025841]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06681282912253177, 0.3031833721902327, 0.3335738705846515, 0.2964299281025841]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06681282912253177, 0.3031833721902327, 0.3335738705846515, 0.2964299281025841]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06681282912253175, 0.3031833721902328, 0.3335738705846514, 0.2964299281025841]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06681282912253175, 0.3031833721902328, 0.3335738705846514, 0.2964299281025841]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06681282912253175, 0.3031833721902328, 0.3335738705846514, 0.2964299281025841]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06681282912253177, 0.3031833721902327, 0.3335738705846515, 0.2964299281025841]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4381 0.55 0.55
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06658835037007148, 0.305528297594382, 0.3324511085492339, 0.29543224348631253]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06658835037007148, 0.305528297594382, 0.3324511085492339, 0.29543224348631253]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06636537835017811, 0.3078574835335055, 0.3313358826485513, 0.29444125546776506]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06636537835017811, 0.3078574835335055, 0.3313358826485513, 0.2944412554677651]
line 256 mcts: sample exp_bonus 2.0641826908195275
using another actor
from probs:  [0.06636537835017811, 0.3078574835335055, 0.3313358826485513, 0.2944412554677651]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06636537835017811, 0.3078574835335055, 0.3313358826485513, 0.2944412554677651]
maxi score, test score, baseline:  0.4371 0.55 0.55
probs:  [0.06658835037007149, 0.30889364896373844, 0.33245110854923393, 0.2920668921169561]
maxi score, test score, baseline:  0.4371 0.55 0.55
probs:  [0.06658835037007149, 0.30889364896373844, 0.33245110854923393, 0.2920668921169561]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.0669792404130241, 0.3076640749383623, 0.3344068343300665, 0.2909498503185471]
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06697924041302408, 0.30766407493836223, 0.33440683433006646, 0.2909498503185471]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.65 ]
 [0.595]
 [0.564]
 [0.559]] [[-0.95 ]
 [ 0.828]
 [-0.498]
 [ 0.057]
 [-0.614]] [[0.158]
 [0.743]
 [0.323]
 [0.475]
 [0.273]]
siam score:  -0.7863077
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06675645148090896, 0.3099706158789752, 0.33329252205413235, 0.2899804105859835]
from probs:  [0.06675645148090896, 0.3099706158789752, 0.33329252205413235, 0.2899804105859835]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.0669792404130241, 0.3110069198623253, 0.3344068343300665, 0.28760700539458406]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.0669792404130241, 0.3110069198623253, 0.3344068343300665, 0.28760700539458406]
actor:  0 policy actor:  0  step number:  49 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06720352473840664, 0.31205017967595367, 0.3355286260398282, 0.2852176695458115]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06720352473840664, 0.31205017967595367, 0.3355286260398282, 0.2852176695458115]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.473]
 [0.514]
 [0.514]
 [0.514]] [[-0.828]
 [ 0.118]
 [-0.828]
 [-0.828]
 [-0.828]] [[0.651]
 [0.991]
 [0.651]
 [0.651]
 [0.651]]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06720352473840664, 0.31205017967595367, 0.3355286260398282, 0.2852176695458115]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06720352473840664, 0.31205017967595367, 0.3355286260398282, 0.2852176695458115]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06720352473840663, 0.31205017967595367, 0.33552862603982814, 0.2852176695458116]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06720352473840663, 0.31205017967595367, 0.33552862603982814, 0.2852176695458116]
maxi score, test score, baseline:  0.4381 0.55 0.55
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.645]
 [0.506]
 [0.506]
 [0.506]] [[1.022]
 [1.187]
 [1.022]
 [1.022]
 [1.022]] [[0.808]
 [0.923]
 [0.808]
 [0.808]
 [0.808]]
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.782]
 [0.782]
 [0.782]
 [0.782]] [[1.686]
 [1.628]
 [1.628]
 [1.628]
 [1.628]] [[0.99 ]
 [0.932]
 [0.932]
 [0.932]
 [0.932]]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06742931956371698, 0.31310046558857707, 0.3366579727416459, 0.2828122421060601]
using another actor
from probs:  [0.06742931956371698, 0.31310046558857707, 0.3366579727416459, 0.2828122421060601]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06742931956371698, 0.31310046558857707, 0.3366579727416459, 0.2828122421060601]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [0.06742931956371698, 0.31310046558857707, 0.3366579727416459, 0.2828122421060601]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.14220477332004694
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4391 0.55 0.55
probs:  [0.06697924041302408, 0.3143497647862884, 0.33440683433006646, 0.2842641604706211]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.719]
 [0.532]
 [0.502]
 [0.498]] [[-1.327]
 [-0.189]
 [-0.704]
 [-1.173]
 [-1.057]] [[0.288]
 [0.912]
 [0.559]
 [0.376]
 [0.41 ]]
maxi score, test score, baseline:  0.4391 0.55 0.55
probs:  [0.06675645148090895, 0.3133023167611406, 0.3332925220541323, 0.2866487097038182]
maxi score, test score, baseline:  0.4391 0.55 0.55
from probs:  [0.06675645148090895, 0.3133023167611406, 0.3332925220541323, 0.2866487097038182]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4401 0.55 0.55
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4401 0.55 0.55
probs:  [0.0665351430361758, 0.3155824601816386, 0.33218561465800267, 0.285696782124183]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06609690896930774, 0.3200975907516092, 0.32999372121065984, 0.28381177906842325]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4391 0.55 0.55
maxi score, test score, baseline:  0.4391 0.55 0.55
probs:  [0.06609690896930775, 0.3200975907516091, 0.3299937212106599, 0.2838117790684232]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4391 0.55 0.55
maxi score, test score, baseline:  0.4391 0.55 0.55
probs:  [0.06626653156626276, 0.3210436077943784, 0.3308427261108443, 0.2818471345285144]
maxi score, test score, baseline:  0.4391 0.55 0.55
probs:  [0.06626653156626276, 0.3210436077943784, 0.3308427261108443, 0.2818471345285144]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4401 0.55 0.55
probs:  [0.06626653156626276, 0.3210436077943784, 0.3308427261108443, 0.2818471345285144]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.805]
 [0.789]
 [0.811]
 [0.774]] [[0.553]
 [0.847]
 [0.553]
 [0.952]
 [0.779]] [[0.789]
 [0.805]
 [0.789]
 [0.811]
 [0.774]]
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.823]
 [0.708]
 [0.708]
 [0.673]] [[0.065]
 [0.411]
 [0.065]
 [0.065]
 [0.221]] [[0.463]
 [0.809]
 [0.463]
 [0.463]
 [0.532]]
maxi score, test score, baseline:  0.4401 0.55 0.55
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4401 0.55 0.55
probs:  [0.06664808908967788, 0.31977108114286595, 0.33275174740200375, 0.2808290823654524]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4411 0.55 0.55
probs:  [0.06664808908967788, 0.31977108114286595, 0.33275174740200375, 0.2808290823654524]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06664808908967788, 0.31977108114286595, 0.33275174740200375, 0.2808290823654524]
maxi score, test score, baseline:  0.4421 0.55 0.55
probs:  [0.06664808908967788, 0.31977108114286595, 0.33275174740200375, 0.2808290823654524]
maxi score, test score, baseline:  0.4421 0.55 0.55
probs:  [0.06664808908967788, 0.31977108114286595, 0.33275174740200375, 0.2808290823654524]
maxi score, test score, baseline:  0.4421 0.55 0.55
probs:  [0.06664808908967788, 0.31977108114286595, 0.33275174740200375, 0.2808290823654524]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.821]
 [0.641]
 [0.641]
 [0.641]] [[-0.184]
 [ 1.237]
 [-0.184]
 [-0.184]
 [-0.184]] [[0.404]
 [0.799]
 [0.404]
 [0.404]
 [0.404]]
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06664808908967788, 0.31977108114286595, 0.33275174740200375, 0.2808290823654524]
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.0666480890896779, 0.319771081142866, 0.3327517474020038, 0.28082908236545234]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4441 0.55 0.55
Printing some Q and Qe and total Qs values:  [[1.061]
 [0.95 ]
 [1.061]
 [0.918]
 [0.893]] [[2.132]
 [2.417]
 [2.132]
 [1.8  ]
 [2.482]] [[0.757]
 [0.794]
 [0.757]
 [0.589]
 [0.787]]
maxi score, test score, baseline:  0.4431 0.55 0.55
siam score:  -0.78271055
siam score:  -0.7825554
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06724117192795648, 0.31954539475307847, 0.3357187423700735, 0.2774946909488915]
siam score:  -0.7768663
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06686471965958772, 0.3208123083982927, 0.33383526166694427, 0.2784877102751752]
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06686471965958772, 0.3208123083982927, 0.33383526166694427, 0.2784877102751752]
UNIT TEST: sample policy line 217 mcts : [0.167 0.042 0.042 0.042 0.708]
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06686471965958772, 0.3208123083982927, 0.33383526166694427, 0.2784877102751752]
using explorer policy with actor:  1
siam score:  -0.7753861
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06686471965958772, 0.3208123083982927, 0.33383526166694427, 0.2784877102751752]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06686471965958772, 0.3208123083982927, 0.33383526166694427, 0.2784877102751752]
siam score:  -0.7784421
maxi score, test score, baseline:  0.4431 0.55 0.55
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06686471965958772, 0.3208123083982927, 0.33383526166694427, 0.2784877102751752]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06670155740293489, 0.3231549838167659, 0.3330185771403748, 0.2771248816399244]
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06670155740293489, 0.3231549838167659, 0.3330185771403748, 0.2771248816399244]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[1.288]
 [1.288]
 [1.288]
 [1.288]
 [1.288]] [[0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]]
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.0667015574029349, 0.32315498381676583, 0.33301857714037486, 0.27712488163992444]
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.0667015574029349, 0.32315498381676583, 0.33301857714037486, 0.27712488163992444]
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06670155740293489, 0.32315498381676594, 0.3330185771403748, 0.2771248816399244]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06648333124894647, 0.3253729175227541, 0.3319270842638632, 0.27621666696443614]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.]
 [0.]
 [0.]
 [1.]
 [1.]] [[0.231]
 [0.   ]
 [0.   ]
 [0.231]
 [0.231]] [[1.]
 [0.]
 [0.]
 [1.]
 [1.]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4441 0.55 0.55
probs:  [0.06653660515029683, 0.32554375082046605, 0.33218495968380374, 0.27573468434543347]
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06653660515029683, 0.32554375082046605, 0.33218495968380374, 0.27573468434543347]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06631675938880612, 0.32777578656510015, 0.3310853945040405, 0.2748220595420532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4431 0.55 0.55
probs:  [0.06631675938880612, 0.32777578656510015, 0.3310853945040405, 0.2748220595420532]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4441 0.55 0.55
probs:  [0.06631675938880612, 0.32777578656510015, 0.3310853945040405, 0.2748220595420532]
maxi score, test score, baseline:  0.4441 0.55 0.55
probs:  [0.06631675938880612, 0.32777578656510015, 0.3310853945040405, 0.2748220595420532]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4441 0.55 0.55
probs:  [0.06670300377655743, 0.3264422450617945, 0.33301792205635733, 0.2738368291052907]
maxi score, test score, baseline:  0.4441 0.55 0.55
probs:  [0.06670300377655743, 0.3264422450617944, 0.3330179220563574, 0.2738368291052908]
maxi score, test score, baseline:  0.4441 0.55 0.55
probs:  [0.06670300377655743, 0.3264422450617944, 0.3330179220563574, 0.2738368291052908]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4451 0.55 0.55
probs:  [0.06670300377655743, 0.3264422450617944, 0.3330179220563574, 0.2738368291052908]
Printing some Q and Qe and total Qs values:  [[1.004]
 [1.116]
 [0.736]
 [1.004]
 [1.127]] [[1.451]
 [1.402]
 [0.796]
 [1.451]
 [1.683]] [[1.023]
 [1.077]
 [0.543]
 [1.023]
 [1.212]]
maxi score, test score, baseline:  0.4451 0.55 0.55
probs:  [0.06670300377655743, 0.3264422450617944, 0.3330179220563574, 0.2738368291052908]
Printing some Q and Qe and total Qs values:  [[1.265]
 [1.338]
 [1.265]
 [1.265]
 [1.172]] [[1.921]
 [1.833]
 [1.921]
 [1.921]
 [1.933]] [[1.141]
 [1.152]
 [1.141]
 [1.141]
 [1.078]]
maxi score, test score, baseline:  0.4451 0.55 0.55
probs:  [0.06670300377655743, 0.3264422450617944, 0.3330179220563574, 0.2738368291052908]
first move QE:  0.1398548126393837
maxi score, test score, baseline:  0.4451 0.55 0.55
probs:  [0.0669226735618259, 0.3275192283116593, 0.3341166094445665, 0.2714414886819483]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.68 ]
 [0.749]
 [0.749]
 [0.749]] [[0.3  ]
 [0.183]
 [0.766]
 [0.102]
 [0.862]] [[0.849]
 [0.741]
 [1.004]
 [0.783]
 [1.036]]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.0669226735618259, 0.3275192283116593, 0.3341166094445665, 0.2714414886819483]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.349]
 [0.362]
 [0.321]
 [0.319]] [[-0.741]
 [-0.006]
 [-0.499]
 [-0.96 ]
 [-0.578]] [[0.316]
 [0.349]
 [0.362]
 [0.321]
 [0.319]]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.0669226735618259, 0.3275192283116593, 0.3341166094445665, 0.2714414886819483]
maxi score, test score, baseline:  0.4441 0.55 0.55
maxi score, test score, baseline:  0.4441 0.55 0.55
probs:  [0.0669226735618259, 0.3275192283116593, 0.3341166094445665, 0.2714414886819483]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4461 0.55 0.55
probs:  [0.0669226735618259, 0.3275192283116593, 0.3341166094445665, 0.2714414886819483]
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[0.541]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[1.103]
 [1.014]
 [1.014]
 [1.014]
 [1.014]]
maxi score, test score, baseline:  0.4461 0.55 0.55
probs:  [0.0669226735618259, 0.3275192283116593, 0.3341166094445665, 0.2714414886819483]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4461 0.55 0.55
probs:  [0.06670300377655743, 0.3264422450617944, 0.3330179220563574, 0.2738368291052908]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.804]
 [0.718]
 [0.468]
 [0.47 ]] [[1.957]
 [1.297]
 [1.957]
 [0.971]
 [1.258]] [[0.718]
 [0.804]
 [0.718]
 [0.468]
 [0.47 ]]
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4481 0.55 0.55
probs:  [0.06648477459621067, 0.32864938231590973, 0.331926439912406, 0.27293940317547366]
UNIT TEST: sample policy line 217 mcts : [0.125 0.583 0.125 0.042 0.125]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.06648477459621067, 0.32864938231590973, 0.331926439912406, 0.27293940317547366]
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.06648477459621067, 0.32864938231590973, 0.331926439912406, 0.27293940317547366]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4491 0.55 0.55
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.06686615062572511, 0.3273231808469161, 0.3338346066024459, 0.2719760619249129]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.4491 0.55 0.55
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.06686615062572511, 0.3273231808469161, 0.3338346066024459, 0.2719760619249129]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.06686615062572508, 0.32732318084691614, 0.3338346066024458, 0.2719760619249129]
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.06686615062572508, 0.32732318084691614, 0.3338346066024458, 0.2719760619249129]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.06664951709653409, 0.3295059616130073, 0.33275110290333415, 0.27109341838712436]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4491 0.55 0.55
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.06664951709653409, 0.3295059616130073, 0.33275110290333415, 0.27109341838712436]
siam score:  -0.7636792
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.06664951709653409, 0.3295059616130073, 0.33275110290333415, 0.2710934183871243]
maxi score, test score, baseline:  0.4491 0.55 0.55
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.06664951709653409, 0.3295059616130073, 0.33275110290333415, 0.2710934183871243]
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.0662679718957683, 0.330842092365862, 0.33084209236586193, 0.27204784337250776]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.0666495170965341, 0.33275110290333426, 0.3295059616130073, 0.27109341838712436]
maxi score, test score, baseline:  0.4491 0.55 0.55
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.0666495170965341, 0.3327511029033342, 0.3295059616130073, 0.27109341838712436]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.594]
 [0.518]
 [0.518]
 [0.518]] [[-1.226]
 [ 0.608]
 [-1.226]
 [-1.226]
 [-1.226]] [[0.276]
 [0.785]
 [0.276]
 [0.276]
 [0.276]]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.469]
 [0.531]
 [0.415]
 [0.56 ]] [[1.226]
 [0.545]
 [1.226]
 [0.394]
 [1.009]] [[0.701]
 [0.413]
 [0.701]
 [0.308]
 [0.658]]
maxi score, test score, baseline:  0.4491 0.55 0.55
probs:  [0.0666495170965341, 0.33275110290333426, 0.3295059616130073, 0.27109341838712436]
maxi score, test score, baseline:  0.4491 0.55 0.55
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4491 0.55 0.55
maxi score, test score, baseline:  0.4481 0.55 0.55
maxi score, test score, baseline:  0.4481 0.55 0.55
probs:  [0.06724259071152928, 0.3357180769229111, 0.3260141436863551, 0.2710251886792046]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06724259071152928, 0.3357180769229111, 0.3260141436863551, 0.2710251886792046]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.06761419006238077, 0.3375773272387687, 0.32472193975417885, 0.2700865429446717]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4501 0.55 0.55
maxi score, test score, baseline:  0.4501 0.55 0.55
probs:  [0.06783183993256948, 0.338665918616891, 0.3257690577271613, 0.2677331837233781]
maxi score, test score, baseline:  0.4501 0.55 0.55
maxi score, test score, baseline:  0.4501 0.55 0.55
probs:  [0.06783183993256948, 0.338665918616891, 0.3257690577271613, 0.2677331837233781]
from probs:  [0.06783183993256948, 0.338665918616891, 0.3257690577271613, 0.2677331837233781]
maxi score, test score, baseline:  0.4501 0.55 0.55
probs:  [0.06783183993256948, 0.338665918616891, 0.3257690577271613, 0.2677331837233781]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.081]
 [0.428]
 [0.409]
 [0.406]] [[-1.632]
 [-0.654]
 [-0.605]
 [-1.422]
 [-1.538]] [[0.147]
 [0.393]
 [0.683]
 [0.253]
 [0.191]]
maxi score, test score, baseline:  0.4521 0.55 0.55
probs:  [0.06783183993256948, 0.338665918616891, 0.32576905772716136, 0.2677331837233781]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
siam score:  -0.7489439
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4531 0.55 0.55
probs:  [0.06783183993256948, 0.338665918616891, 0.32576905772716136, 0.2677331837233781]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06783183993256948, 0.338665918616891, 0.32576905772716136, 0.2677331837233781]
actor:  0 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4531 0.55 0.55
probs:  [0.0681984723332424, 0.34050031993983965, 0.32448256419827515, 0.26681864352864276]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.0681984723332424, 0.34050031993983965, 0.32448256419827515, 0.26681864352864276]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.453]
 [0.432]
 [0.432]
 [0.432]] [[-0.432]
 [ 0.271]
 [-0.432]
 [-0.432]
 [-0.432]] [[0.432]
 [0.453]
 [0.432]
 [0.432]
 [0.432]]
maxi score, test score, baseline:  0.4521 0.55 0.55
probs:  [0.0681984723332424, 0.34050031993983965, 0.32448256419827515, 0.26681864352864276]
maxi score, test score, baseline:  0.4521 0.55 0.55
probs:  [0.0681984723332424, 0.34050031993983965, 0.32448256419827515, 0.26681864352864276]
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4541 0.55 0.55
siam score:  -0.7491077
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [0.06798104145405495, 0.3394128217418677, 0.32663956149302953, 0.2659665753110478]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
start point for exploration sampling:  16339
siam score:  -0.7635923
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [0.0692707043830294, 0.3458651046316104, 0.3207201591544668, 0.26414403183089336]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4561 0.55 0.55
probs:  [0.06948877539573678, 0.34695581059530295, 0.32173153466806964, 0.2618238793408906]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4561 0.55 0.55
probs:  [0.06948877539573678, 0.34695581059530295, 0.32173153466806975, 0.2618238793408906]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4551 0.55 0.55
maxi score, test score, baseline:  0.4551 0.55 0.55
probs:  [0.06948877539573678, 0.34695581059530295, 0.32173153466806975, 0.2618238793408906]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4551 0.55 0.55
probs:  [0.06927070438302942, 0.34586510463161046, 0.32386327733910975, 0.2610009136462504]
maxi score, test score, baseline:  0.4551 0.55 0.55
probs:  [0.06927070438302942, 0.34586510463161046, 0.32386327733910975, 0.2610009136462504]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.765793
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.348]
 [0.296]
 [0.296]
 [0.296]] [[0.17 ]
 [1.161]
 [0.17 ]
 [0.17 ]
 [0.17 ]] [[0.296]
 [0.348]
 [0.296]
 [0.296]
 [0.296]]
from probs:  [0.06927070438302942, 0.34586510463161046, 0.32386327733910975, 0.2610009136462504]
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4541 0.55 0.55
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [0.06905400073838269, 0.34478123770846625, 0.3259816533695969, 0.2601831081835542]
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [0.06905400073838269, 0.34478123770846625, 0.3259816533695969, 0.2601831081835542]
actor:  0 policy actor:  1  step number:  31 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.06927070438302942, 0.34586510463161046, 0.32700639552375266, 0.2578577954616074]
maxi score, test score, baseline:  0.4531 0.55 0.55
probs:  [0.06970822675930095, 0.34805342053441535, 0.3259123255750312, 0.2563260271312526]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4531 0.55 0.55
probs:  [0.06970822675930095, 0.34805342053441535, 0.3259123255750312, 0.2563260271312526]
maxi score, test score, baseline:  0.4531 0.55 0.55
probs:  [0.06970822675930095, 0.34805342053441535, 0.3259123255750312, 0.2563260271312526]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06970822675930095, 0.34805342053441535, 0.3259123255750312, 0.2563260271312526]
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [0.06970822675930095, 0.34805342053441535, 0.3259123255750312, 0.2563260271312526]
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [0.06970822675930095, 0.34805342053441535, 0.3259123255750312, 0.2563260271312526]
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [0.06970822675930095, 0.34805342053441535, 0.3259123255750312, 0.2563260271312526]
using another actor
maxi score, test score, baseline:  0.4531 0.55 0.55
probs:  [0.06970822675930095, 0.34805342053441535, 0.3259123255750312, 0.2563260271312526]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.4521 0.55 0.55
probs:  [0.06970822675930095, 0.34805342053441535, 0.3259123255750312, 0.2563260271312526]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4531 0.55 0.55
probs:  [0.06970822675930095, 0.34805342053441535, 0.3259123255750312, 0.2563260271312526]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [0.06970822675930095, 0.34805342053441535, 0.3259123255750312, 0.2563260271312526]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [0.07005614457830765, 0.3497941905613753, 0.32464919766402084, 0.25550046719629627]
using another actor
from probs:  [0.07005614457830765, 0.3497941905613753, 0.32464919766402084, 0.25550046719629627]
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [0.07005614457830765, 0.3497941905613753, 0.32464919766402084, 0.25550046719629627]
actor:  0 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4561 0.55 0.55
probs:  [0.07005614457830765, 0.3497941905613753, 0.32464919766402084, 0.25550046719629627]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4561 0.55 0.55
probs:  [0.07005614457830765, 0.3497941905613753, 0.32464919766402084, 0.25550046719629627]
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.083 0.458 0.375 0.042 0.042]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using explorer policy with actor:  1
first move QE:  0.11147716783309423
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07005614457830765, 0.34979419056137534, 0.32464919766402084, 0.25550046719629627]
maxi score, test score, baseline:  0.4571 0.55 0.55
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07005614457830765, 0.34979419056137534, 0.32464919766402084, 0.25550046719629627]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07005614457830765, 0.34979419056137534, 0.32464919766402084, 0.25550046719629627]
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07005614457830765, 0.34979419056137534, 0.32464919766402084, 0.25550046719629627]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07005614457830765, 0.34979419056137534, 0.32464919766402084, 0.25550046719629627]
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07005614457830765, 0.3497941905613753, 0.32464919766402084, 0.25550046719629627]
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07005614457830765, 0.3497941905613753, 0.32464919766402084, 0.25550046719629627]
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07005614457830765, 0.3497941905613753, 0.32464919766402084, 0.25550046719629627]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4581 0.55 0.55
probs:  [0.07005614457830765, 0.3497941905613753, 0.32464919766402084, 0.25550046719629627]
maxi score, test score, baseline:  0.4581 0.55 0.55
probs:  [0.07005614457830765, 0.3497941905613753, 0.32464919766402084, 0.25550046719629627]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4581 0.55 0.55
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4581 0.55 0.55
probs:  [0.0707390167252538, 0.3532108691581872, 0.32217000625346925, 0.25388010786308973]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.797]
 [0.529]
 [0.529]
 [0.529]] [[-0.332]
 [-0.502]
 [-0.332]
 [-0.332]
 [-0.332]] [[0.427]
 [0.481]
 [0.427]
 [0.427]
 [0.427]]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.335]
 [0.374]
 [0.353]
 [0.365]] [[-1.523]
 [-1.1  ]
 [-1.392]
 [-1.496]
 [-1.416]] [[0.345]
 [0.335]
 [0.374]
 [0.353]
 [0.365]]
maxi score, test score, baseline:  0.4581 0.55 0.55
probs:  [0.0707390167252538, 0.3532108691581872, 0.32217000625346925, 0.25388010786308973]
maxi score, test score, baseline:  0.4581 0.55 0.55
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.027]
 [1.027]
 [1.027]
 [1.027]
 [1.027]] [[0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[1.057]
 [1.057]
 [1.057]
 [1.057]
 [1.057]]
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.0707390167252538, 0.3532108691581872, 0.32217000625346925, 0.25388010786308973]
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.0707390167252538, 0.3532108691581872, 0.32217000625346925, 0.25388010786308973]
Printing some Q and Qe and total Qs values:  [[0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]] [[1.587]
 [1.587]
 [1.587]
 [1.587]
 [1.587]] [[1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.0707390167252538, 0.3532108691581872, 0.32217000625346925, 0.25388010786308973]
first move QE:  0.10366301428935526
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.070520449436966, 0.35211767532034693, 0.3211729252232721, 0.25618895001941494]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.070520449436966, 0.35211767532034693, 0.3211729252232721, 0.25618895001941494]
actor:  0 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using another actor
siam score:  -0.7789838
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4561 0.55 0.55
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.4561 0.55 0.55
probs:  [0.07118720285886738, 0.35545370087810385, 0.31877415274658943, 0.25458494351643923]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07118720285886738, 0.35545370087810385, 0.31877415274658943, 0.25458494351643923]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07151450085395113, 0.35709129948762935, 0.3175966358468015, 0.253797563811618]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.625]
 [0.582]
 [0.582]
 [0.582]] [[-0.713]
 [ 0.167]
 [-0.713]
 [-0.713]
 [-0.713]] [[0.618]
 [1.043]
 [0.618]
 [0.618]
 [0.618]]
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07173210168856889, 0.35817966478727875, 0.31551726177257733, 0.25457097175157517]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07205510652429159, 0.3597957853360753, 0.3143630465763201, 0.25378606156331296]
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07205510652429159, 0.3597957853360753, 0.3143630465763201, 0.25378606156331296]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4561 0.55 0.55
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07205510652429159, 0.3597957853360753, 0.3143630465763201, 0.25378606156331296]
maxi score, test score, baseline:  0.4571 0.55 0.55
probs:  [0.07227368965131256, 0.36088906543123234, 0.3122801600367195, 0.2545570848807356]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4581 0.55 0.55
probs:  [0.07249360576314588, 0.3619890126730367, 0.31018457143652994, 0.2553328101272874]
using another actor
maxi score, test score, baseline:  0.4561 0.55 0.55
probs:  [0.07249360576314588, 0.3619890126730367, 0.31018457143652994, 0.2553328101272874]
Printing some Q and Qe and total Qs values:  [[0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]] [[0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]] [[0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4591 0.55 0.55
probs:  [0.07249360576314588, 0.3619890126730367, 0.31018457143652994, 0.2553328101272874]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [0.07249360576314588, 0.3619890126730367, 0.31018457143652994, 0.2553328101272874]
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [0.0724936057631459, 0.3619890126730368, 0.3101845714365299, 0.2553328101272875]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.7763385
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [0.0724936057631459, 0.3619890126730368, 0.3101845714365299, 0.2553328101272875]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4601 0.55 0.55
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [0.07227368965131256, 0.36088906543123234, 0.3122801600367195, 0.2545570848807356]
Printing some Q and Qe and total Qs values:  [[1.107]
 [1.349]
 [0.194]
 [1.173]
 [1.307]] [[ 0.782]
 [ 1.024]
 [-0.097]
 [ 1.188]
 [ 1.05 ]] [[0.99 ]
 [1.241]
 [0.051]
 [1.146]
 [1.215]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.77787423
maxi score, test score, baseline:  0.4601 0.55 0.55
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [0.07281199683150381, 0.36358205331313853, 0.3120915224778491, 0.2515144273775085]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [0.07281199683150381, 0.36358205331313853, 0.31209152247784916, 0.2515144273775085]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [0.07312655254187855, 0.3651559040897557, 0.3109648904004589, 0.2507526529679069]
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [0.07312655254187855, 0.3651559040897557, 0.3109648904004589, 0.2507526529679069]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [0.07312655254187855, 0.3651559040897557, 0.3109648904004589, 0.25075265296790683]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4601 0.55 0.55
Starting evaluation
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [0.07312655254187855, 0.3651559040897557, 0.3109648904004589, 0.2507526529679069]
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07334699288870486, 0.36625881664589516, 0.3119040452270351, 0.248490145238365]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07334699288870486, 0.36625881664589516, 0.3119040452270351, 0.248490145238365]
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07334699288870486, 0.36625881664589516, 0.3119040452270351, 0.248490145238365]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.4631 0.5 0.5
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.07334699288870486, 0.36625881664589516, 0.3119040452270351, 0.248490145238365]
maxi score, test score, baseline:  0.4641 0.5 0.5
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.07384435377541747, 0.36873045758749395, 0.3128573010757321, 0.2445678875613565]
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.07384435377541747, 0.36873045758749395, 0.3128573010757321, 0.2445678875613565]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.07416030065256889, 0.370311373237716, 0.3116981401219056, 0.24383018598780948]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4651 0.5 0.5
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4641 0.5 0.5
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.07447237161403371, 0.3718728948182037, 0.3105531993121892, 0.2431015342555734]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4631 0.5 0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4631 0.5 0.5
siam score:  -0.771485
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.268]
 [1.357]
 [1.268]
 [1.268]
 [1.268]] [[1.108]
 [1.154]
 [1.108]
 [1.108]
 [1.108]] [[1.376]
 [1.488]
 [1.376]
 [1.376]
 [1.376]]
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07500889168793153, 0.37455698975924523, 0.3103681116011066, 0.24006600695171665]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
first move QE:  0.09021898721427951
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07577272547210089, 0.37837799175739933, 0.3080757581759664, 0.23777352459453338]
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07577272547210089, 0.37837799175739933, 0.3080757581759664, 0.23777352459453338]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.735]
 [0.644]
 [0.644]
 [0.644]] [[0.663]
 [0.606]
 [0.663]
 [0.663]
 [0.663]] [[1.132]
 [1.174]
 [1.132]
 [1.132]
 [1.132]]
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07577272547210089, 0.37837799175739933, 0.3080757581759664, 0.23777352459453338]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07623815005692007, 0.3807058163288654, 0.30997090396265586, 0.2330851296515586]
maxi score, test score, baseline:  0.4631 0.5 0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07623815005692007, 0.3807058163288654, 0.30997090396265586, 0.2330851296515586]
Printing some Q and Qe and total Qs values:  [[0.359]
 [1.189]
 [0.721]
 [0.428]
 [1.112]] [[-0.248]
 [ 0.274]
 [-0.017]
 [-0.186]
 [ 0.462]] [[0.165]
 [1.169]
 [0.603]
 [0.254]
 [1.154]]
siam score:  -0.7716884
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07623815005692007, 0.3807058163288654, 0.30997090396265586, 0.2330851296515586]
Printing some Q and Qe and total Qs values:  [[0.98 ]
 [1.296]
 [0.98 ]
 [0.98 ]
 [1.131]] [[-0.182]
 [-0.39 ]
 [-0.182]
 [-0.182]
 [-0.227]] [[0.949]
 [1.231]
 [0.949]
 [0.949]
 [1.093]]
siam score:  -0.7713158
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07640958221597152, 0.38155506997982425, 0.3099392922393283, 0.232096055564876]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
UNIT TEST: sample policy line 217 mcts : [0.25  0.042 0.625 0.042 0.042]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.07647430209352507, 0.3818795304089209, 0.30475699800604317, 0.23688916949151076]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.7695595
maxi score, test score, baseline:  0.4641 0.5 0.5
siam score:  -0.77071047
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.07700796544514096, 0.38454936020933483, 0.30458859757064444, 0.23385407677487982]
using another actor
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07748391902568612, 0.3869298041814061, 0.30028495633780455, 0.2353013204551033]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07748391902568612, 0.3869298041814061, 0.30028495633780455, 0.2353013204551033]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07748391902568612, 0.3869298041814061, 0.30028495633780455, 0.2353013204551033]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07748391902568612, 0.3869298041814061, 0.30028495633780455, 0.2353013204551033]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07748391902568612, 0.3869298041814061, 0.30028495633780455, 0.2353013204551033]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07777650113692111, 0.3883938830149741, 0.29920671396087967, 0.23462290188722507]
line 256 mcts: sample exp_bonus 1.8312120206826308
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07801611607761494, 0.3895923004033709, 0.30012983163656964, 0.23226175188244455]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.07801611607761494, 0.3895923004033709, 0.30012983163656964, 0.23226175188244455]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07801611607761494, 0.3895923004033709, 0.30012983163656964, 0.23226175188244455]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07801611607761494, 0.3895923004033709, 0.30012983163656964, 0.23226175188244455]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07801611607761494, 0.3895923004033709, 0.30012983163656964, 0.23226175188244455]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07854503966499023, 0.39223842000886455, 0.29997566108419566, 0.22924087924194947]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07854503966499023, 0.39223842000886455, 0.29997566108419566, 0.22924087924194947]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07854503966499023, 0.39223842000886455, 0.29997566108419566, 0.22924087924194947]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.07882934432346754, 0.3936610860142326, 0.2989059016218664, 0.22860366804043342]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.07911019302861945, 0.3950664583623719, 0.2978491459519866, 0.227974202657022]
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.07911019302861945, 0.3950664583623719, 0.2978491459519866, 0.227974202657022]
maxi score, test score, baseline:  0.4601 0.5 0.5
probs:  [0.07911019302861945, 0.3950664583623719, 0.2978491459519866, 0.227974202657022]
maxi score, test score, baseline:  0.4601 0.5 0.5
probs:  [0.07911019302861945, 0.3950664583623719, 0.2978491459519866, 0.227974202657022]
maxi score, test score, baseline:  0.4601 0.5 0.5
probs:  [0.07911019302861945, 0.3950664583623719, 0.2978491459519866, 0.227974202657022]
using explorer policy with actor:  1
siam score:  -0.7681027
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.5701123633913696
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.76414925
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.07935095341622327, 0.39627061135752295, 0.29875687045250765, 0.22562156477374617]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.07935095341622327, 0.39627061135752295, 0.29875687045250765, 0.22562156477374617]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.07935095341622327, 0.39627061135752295, 0.29875687045250765, 0.22562156477374617]
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [1.304]
 [0.78 ]
 [0.78 ]
 [0.78 ]] [[0.634]
 [1.779]
 [0.634]
 [0.634]
 [0.634]] [[0.702]
 [1.356]
 [0.702]
 [0.702]
 [0.702]]
first move QE:  0.07919163852343848
line 256 mcts: sample exp_bonus 0.27950820336855575
maxi score, test score, baseline:  0.4601 0.5 0.5
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4601 0.5 0.5
probs:  [0.07911019302861945, 0.3950664583623719, 0.2978491459519866, 0.227974202657022]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4601 0.5 0.5
probs:  [0.07911019302861945, 0.3950664583623719, 0.2978491459519866, 0.227974202657022]
from probs:  [0.07911019302861945, 0.3950664583623719, 0.2978491459519866, 0.227974202657022]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1717195171296524
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07911019302861945, 0.3950664583623719, 0.2978491459519866, 0.227974202657022]
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.07935095341622327, 0.39627061135752295, 0.29875687045250765, 0.22562156477374617]
Printing some Q and Qe and total Qs values:  [[0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]] [[1.631]
 [1.631]
 [1.631]
 [1.631]
 [1.631]] [[0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]]
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07935095341622327, 0.39627061135752295, 0.29875687045250765, 0.22562156477374617]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07935095341622327, 0.39627061135752295, 0.29875687045250765, 0.22562156477374617]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.07935095341622327, 0.39627061135752295, 0.29875687045250765, 0.22562156477374617]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7486624
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07959318651510211, 0.39748213005643634, 0.29661352297120525, 0.22631116045725638]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07959318651510211, 0.39748213005643634, 0.29661352297120525, 0.22631116045725638]
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07959318651510211, 0.39748213005643634, 0.29661352297120525, 0.22631116045725638]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07959318651510211, 0.39748213005643634, 0.29661352297120525, 0.22631116045725638]
maxi score, test score, baseline:  0.4631 0.5 0.5
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.0798693931635689, 0.39886428098187715, 0.29557069825975835, 0.22569562759479556]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.07962779078765206, 0.3976559146507015, 0.2977042185794574, 0.22501207598218892]
siam score:  -0.7681285
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07962779078765206, 0.3976559146507015, 0.2977042185794574, 0.22501207598218892]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07938764841462774, 0.396454850475939, 0.2998248460382061, 0.22433265507122715]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07887089196246864, 0.39386960410234945, 0.29997575721450037, 0.22728374672068166]
from probs:  [0.07887089196246864, 0.39386960410234945, 0.29997575721450037, 0.22728374672068166]
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07887089196246864, 0.39386960410234945, 0.29997575721450037, 0.22728374672068166]
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.07887089196246864, 0.39386960410234945, 0.29997575721450037, 0.22728374672068166]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.07887089196246864, 0.39386960410234945, 0.29997575721450037, 0.22728374672068166]
maxi score, test score, baseline:  0.4641 0.5 0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[-0.826]
 [-0.826]
 [-0.826]
 [-0.826]
 [-0.826]] [[0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.07863303698977789, 0.3926799824182025, 0.3020895173907724, 0.22659746320124716]
siam score:  -0.75623304
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.07863303698977789, 0.3926799824182025, 0.3020895173907724, 0.22659746320124716]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.0783966150419459, 0.3914975279478692, 0.3041905426183329, 0.22591531439185203]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]] [[0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]] [[0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]]
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.0783966150419459, 0.3914975279478692, 0.3041905426183329, 0.22591531439185203]
using another actor
actor:  0 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4641 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.473]
 [0.494]
 [0.494]] [[-1.489]
 [-1.489]
 [-1.242]
 [-1.489]
 [-1.489]] [[0.392]
 [0.392]
 [0.454]
 [0.392]
 [0.392]]
Printing some Q and Qe and total Qs values:  [[1.397]
 [1.451]
 [1.397]
 [1.327]
 [1.33 ]] [[ 0.393]
 [-0.117]
 [ 0.393]
 [ 0.118]
 [ 0.357]] [[1.388]
 [1.286]
 [1.388]
 [1.246]
 [1.319]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.07914895285016878, 0.3952610224665966, 0.30193269715127025, 0.22365732753196432]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.7968189682261072
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4661 0.5 0.5
probs:  [0.07914895285016878, 0.3952610224665966, 0.30193269715127025, 0.22365732753196432]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4661 0.5 0.5
probs:  [0.07887089196246863, 0.3938696041023494, 0.30300459098507615, 0.22425491295010588]
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4651 0.5 0.5
maxi score, test score, baseline:  0.4651 0.5 0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4651 0.5 0.5
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4651 0.5 0.5
probs:  [0.07959318651510211, 0.39748213005643634, 0.29661352297120525, 0.22631116045725638]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.0798693931635689, 0.39886428098187715, 0.29557069825975835, 0.22569562759479556]
maxi score, test score, baseline:  0.4651 0.5 0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.0800833793927829, 0.3999264299475445, 0.29536235572771863, 0.224627834931954]
line 256 mcts: sample exp_bonus -1.1823622618753074
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.0800833793927829, 0.3999264299475445, 0.29536235572771863, 0.224627834931954]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4641 0.5 0.5
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.342]
 [0.547]
 [0.486]
 [0.491]] [[-1.576]
 [-1.079]
 [-1.566]
 [-1.79 ]
 [-1.436]] [[0.398]
 [0.41 ]
 [0.452]
 [0.316]
 [0.439]]
maxi score, test score, baseline:  0.4661 0.5 0.5
probs:  [0.0800833793927829, 0.3999264299475445, 0.29536235572771863, 0.224627834931954]
maxi score, test score, baseline:  0.4651 0.5 0.5
probs:  [0.0800833793927829, 0.3999264299475445, 0.29536235572771863, 0.224627834931954]
siam score:  -0.755515
maxi score, test score, baseline:  0.4651 0.5 0.5
probs:  [0.0800833793927829, 0.3999264299475445, 0.29536235572771863, 0.224627834931954]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.756619
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08030269515439153, 0.40101503275251393, 0.2951488242249784, 0.22353344786811613]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08005374833452673, 0.3997699843444582, 0.2973366271762277, 0.22283964014478738]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08033137085342208, 0.40115932414876937, 0.2962732624945212, 0.22223604250328724]
actor:  0 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08033137085342208, 0.40115932414876937, 0.2962732624945212, 0.22223604250328724]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08033137085342208, 0.40115932414876937, 0.2962732624945212, 0.22223604250328724]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.0805796361116705, 0.4024009666027439, 0.2971901470191238, 0.21982925026646186]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08108206988949002, 0.40490630457138943, 0.29592699482267326, 0.2180846307164474]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08108206988949002, 0.40490630457138943, 0.29592699482267326, 0.2180846307164474]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08108206988949002, 0.40490630457138943, 0.29592699482267326, 0.2180846307164474]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08108206988949002, 0.40490630457138943, 0.29592699482267326, 0.2180846307164474]
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08158953038952041, 0.407444206473053, 0.2915151390202577, 0.21945112411716877]
maxi score, test score, baseline:  0.4631 0.5 0.5
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.081860179408925, 0.40879871944712637, 0.2904781049571107, 0.21886299618683794]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08237378027642848, 0.41135981898624174, 0.2891650046083111, 0.21710139612901866]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08237378027642848, 0.41135981898624174, 0.2891650046083111, 0.21710139612901866]
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08263236940893108, 0.4126530494476585, 0.2869308856233814, 0.217783695520029]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08289734637237942, 0.413979239541123, 0.2859192619947222, 0.2172041520917754]
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08289734637237942, 0.413979239541123, 0.2859192619947222, 0.2172041520917754]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08289734637237942, 0.413979239541123, 0.2859192619947222, 0.2172041520917754]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.456]
 [0.429]
 [0.429]
 [0.429]] [[1.481]
 [0.872]
 [1.481]
 [1.481]
 [1.481]] [[0.429]
 [0.456]
 [0.429]
 [0.429]
 [0.429]]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08289734637237942, 0.413979239541123, 0.2859192619947222, 0.2172041520917754]
from probs:  [0.08289734637237942, 0.413979239541123, 0.2859192619947222, 0.2172041520917754]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08289734637237942, 0.413979239541123, 0.2859192619947222, 0.2172041520917754]
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08289734637237942, 0.413979239541123, 0.2859192619947222, 0.2172041520917754]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08263954250283065, 0.4126899331018995, 0.2881426158947037, 0.21652790850056614]
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08263954250283065, 0.4126899331018995, 0.2881426158947037, 0.21652790850056614]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08238334004627709, 0.41140863551099244, 0.29035215887774807, 0.21585586556498237]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08290659619760411, 0.414027469787673, 0.28832417518403575, 0.2147417588306871]
maxi score, test score, baseline:  0.4611 0.5 0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08214042148175826, 0.4101947575813357, 0.29368947934036427, 0.21397534159654172]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.533]
 [0.621]
 [0.621]
 [0.621]] [[0.215]
 [0.723]
 [0.215]
 [0.215]
 [0.215]] [[1.045]
 [1.192]
 [1.045]
 [1.045]
 [1.045]]
start point for exploration sampling:  16339
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08290782570219796, 0.4140262628427965, 0.29138980464257486, 0.2116761068124307]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08264782179340578, 0.4127249290395916, 0.29241645069752387, 0.21221079846947874]
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08264782179340578, 0.4127249290395916, 0.29241645069752387, 0.21221079846947874]
line 256 mcts: sample exp_bonus 1.6136298977867858
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08290782570219796, 0.4140262628427965, 0.28832389318756924, 0.21474201826743622]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using another actor
siam score:  -0.7549649
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08290782570219796, 0.4140262628427965, 0.28832389318756924, 0.21474201826743622]
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08290782570219796, 0.4140262628427965, 0.28832389318756924, 0.21474201826743622]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.7546722
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08367149917543595, 0.4178474825672541, 0.28417708921052687, 0.21430392904678303]
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08367149917543595, 0.4178474825672541, 0.28417708921052687, 0.21430392904678303]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
siam score:  -0.7512401
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08367149917543595, 0.4178474825672541, 0.28417708921052687, 0.21430392904678303]
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08367149917543595, 0.4178474825672541, 0.28417708921052687, 0.21430392904678303]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08341838082181557, 0.4165816191781844, 0.28634508054796753, 0.21365491945203252]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.0831667920205594, 0.4153234051922963, 0.2884999710721787, 0.21300983171496568]
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.0831667920205594, 0.4153234051922963, 0.2884999710721787, 0.21300983171496568]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08291671894918283, 0.41407277148233396, 0.2906418791745231, 0.2123686303939601]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08291671894918283, 0.41407277148233396, 0.2906418791745231, 0.2123686303939601]
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.892]
 [0.754]
 [0.754]
 [0.754]] [[1.027]
 [1.235]
 [1.027]
 [1.027]
 [1.027]] [[1.128]
 [1.286]
 [1.128]
 [1.128]
 [1.128]]
maxi score, test score, baseline:  0.4641 0.5 0.5
Printing some Q and Qe and total Qs values:  [[1.096]
 [1.37 ]
 [0.089]
 [0.957]
 [1.218]] [[ 1.024]
 [ 1.292]
 [-0.098]
 [ 0.692]
 [ 1.113]] [[0.851]
 [1.067]
 [0.014]
 [0.679]
 [0.937]]
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08266149574030372, 0.41279537744721584, 0.29164532911440405, 0.21289779769807637]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.0631950297212568
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08291671894918283, 0.41407277148233396, 0.2906418791745231, 0.2123686303939601]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
siam score:  -0.7541197
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08341838082181557, 0.4165816191781844, 0.28634508054796753, 0.21365491945203252]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.08341838082181557, 0.4165816191781844, 0.28634508054796753, 0.21365491945203252]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08341838082181557, 0.4165816191781844, 0.28634508054796753, 0.21365491945203252]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4641 0.5 0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08366663655262037, 0.4178251424917508, 0.2835650284983502, 0.21494319245727875]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08341805215252435, 0.41658194784747565, 0.2827214540414684, 0.21727854595853155]
Printing some Q and Qe and total Qs values:  [[0.983]
 [1.18 ]
 [0.983]
 [0.983]
 [0.983]] [[-0.192]
 [ 0.26 ]
 [-0.192]
 [-0.192]
 [-0.192]] [[0.829]
 [1.115]
 [0.829]
 [0.829]
 [0.829]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08341789218027616, 0.4165821078197238, 0.28390609274206774, 0.2160939072579323]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08341789218027616, 0.4165821078197238, 0.28390609274206774, 0.2160939072579323]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.477]
 [0.527]
 [0.527]
 [0.527]] [[-0.808]
 [-0.338]
 [-0.808]
 [-0.808]
 [-0.808]] [[0.927]
 [1.097]
 [0.927]
 [0.927]
 [0.927]]
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08341789218027616, 0.4165821078197238, 0.28390609274206774, 0.2160939072579323]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.63 ]
 [0.507]
 [0.507]
 [0.507]] [[0.191]
 [0.036]
 [0.191]
 [0.191]
 [0.191]] [[1.184]
 [1.228]
 [1.184]
 [1.184]
 [1.184]]
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08366426997332833, 0.417814269849131, 0.28474568582779364, 0.21377577434974704]
maxi score, test score, baseline:  0.4641 0.5 0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08366426997332833, 0.417814269849131, 0.28474568582779364, 0.21377577434974704]
maxi score, test score, baseline:  0.4641 0.5 0.5
maxi score, test score, baseline:  0.4641 0.5 0.5
probs:  [0.08366426997332833, 0.417814269849131, 0.28474568582779364, 0.21377577434974704]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08390759171064245, 0.4190320969316471, 0.28380641938632944, 0.21325389197138106]
maxi score, test score, baseline:  0.4631 0.5 0.5
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08390759171064245, 0.4190320969316471, 0.28380641938632944, 0.21325389197138106]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08366426997332833, 0.417814269849131, 0.28474568582779364, 0.21377577434974704]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08366194503029963, 0.4178035884936625, 0.2829745042891477, 0.21555996218689022]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08366194503029963, 0.4178035884936625, 0.2829745042891477, 0.21555996218689022]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08366194503029963, 0.4178035884936625, 0.2829745042891477, 0.21555996218689022]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08390315236664113, 0.4190108274164002, 0.28205377761345524, 0.21503224260350337]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08415586129450771, 0.4202666490709721, 0.28169465761927187, 0.21388283201524833]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08415586129450771, 0.4202666490709721, 0.28169465761927187, 0.21388283201524833]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08415586129450771, 0.4202666490709721, 0.28169465761927187, 0.21388283201524833]
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08415586129450771, 0.4202666490709721, 0.28169465761927187, 0.21388283201524833]
maxi score, test score, baseline:  0.4611 0.5 0.5
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08390876352629391, 0.4190309043758956, 0.2838061808751791, 0.21325415122263136]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08391329229884407, 0.419052541767248, 0.28559000879310487, 0.21144415714080309]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.008]
 [0.458]
 [0.526]
 [0.509]] [[-1.33 ]
 [ 0.152]
 [ 0.323]
 [-1.071]
 [-0.361]] [[0.184]
 [0.193]
 [0.7  ]
 [0.303]
 [0.523]]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
first move QE:  0.052079235735082115
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.007]
 [0.455]
 [0.524]
 [0.508]] [[-1.055]
 [ 0.786]
 [ 1.418]
 [-0.67 ]
 [ 0.025]] [[0.131]
 [0.462]
 [1.043]
 [0.302]
 [0.554]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7582488
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08391329229884407, 0.419052541767248, 0.28559000879310487, 0.21144415714080309]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08391329229884407, 0.419052541767248, 0.28559000879310487, 0.21144415714080309]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
using another actor
from probs:  [0.08391329229884407, 0.419052541767248, 0.28559000879310487, 0.21144415714080309]
maxi score, test score, baseline:  0.4611 0.5 0.5
probs:  [0.08391329229884407, 0.419052541767248, 0.28559000879310487, 0.21144415714080309]
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4621 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.846]
 [0.887]
 [0.846]
 [0.846]
 [0.775]] [[1.664]
 [1.629]
 [1.664]
 [1.664]
 [1.667]] [[1.144]
 [1.157]
 [1.144]
 [1.144]
 [1.104]]
maxi score, test score, baseline:  0.4621 0.5 0.5
Printing some Q and Qe and total Qs values:  [[1.105]
 [1.182]
 [0.014]
 [0.926]
 [0.965]] [[2.611]
 [2.622]
 [0.507]
 [2.935]
 [2.347]] [[1.02 ]
 [1.054]
 [0.003]
 [1.035]
 [0.891]]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08416260952555692, 0.4202993830432622, 0.28346494426614327, 0.21207306316503766]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.052015082230322474
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08464338884572795, 0.42270579387223967, 0.28160148568726084, 0.21104933159477143]
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08464338884572795, 0.42270579387223967, 0.28160148568726084, 0.21104933159477143]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4621 0.5 0.5
probs:  [0.08463251469191761, 0.4226523965551345, 0.2827821006117344, 0.2099329881412135]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4631 0.5 0.5
probs:  [0.08487956202928985, 0.4238878948541107, 0.2836085847197021, 0.2076239583968974]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]]
actor:  0 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08511298958657901, 0.4250562533464073, 0.28268685228459883, 0.2071439047824148]
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4661 0.5 0.5
probs:  [0.08511298958657901, 0.4250562533464073, 0.28268685228459883, 0.2071439047824148]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.896]
 [0.684]
 [0.684]
 [0.634]] [[0.886]
 [0.655]
 [0.886]
 [0.886]
 [0.867]] [[0.759]
 [0.784]
 [0.759]
 [0.759]
 [0.73 ]]
maxi score, test score, baseline:  0.4661 0.5 0.5
probs:  [0.08534371911972558, 0.4262111076087147, 0.28177577350321087, 0.20666939976834886]
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.4671 0.5 0.5
probs:  [0.0855906805323258, 0.4274461818043179, 0.27969507532235527, 0.20726806234100098]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4681 0.5 0.5
probs:  [0.0855906805323258, 0.4274461818043179, 0.27969507532235527, 0.20726806234100098]
maxi score, test score, baseline:  0.4681 0.5 0.5
probs:  [0.0855906805323258, 0.4274461818043179, 0.27969507532235527, 0.20726806234100098]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4681 0.5 0.5
probs:  [0.08581799278106116, 0.4285839376767405, 0.27880386091560333, 0.20679420862659506]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4681 0.5 0.5
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.0855906805323258, 0.4274461818043179, 0.27969507532235527, 0.20726806234100098]
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.47009999999999996 0.5 0.5
maxi score, test score, baseline:  0.47009999999999996 0.5 0.5
probs:  [0.08536071796480905, 0.42629516040936377, 0.2805966807322037, 0.20774744089362354]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47009999999999996 0.5 0.5
probs:  [0.08536071796480905, 0.42629516040936377, 0.2805966807322037, 0.20774744089362354]
maxi score, test score, baseline:  0.47009999999999996 0.5 0.5
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.08511298958657902, 0.42505625334640734, 0.2797813543037457, 0.21004940276326803]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.46909999999999996 0.5 0.5
probs:  [0.08487956202928985, 0.4238878948541107, 0.2806860990919019, 0.21054644402469755]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.46909999999999996 0.5 0.5
probs:  [0.08487956202928985, 0.4238878948541107, 0.2806860990919019, 0.21054644402469755]
siam score:  -0.749807
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using another actor
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[1.743]
 [1.743]
 [1.743]
 [1.743]
 [1.743]] [[0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]]
maxi score, test score, baseline:  0.46909999999999996 0.5 0.5
maxi score, test score, baseline:  0.46909999999999996 0.5 0.5
probs:  [0.08511298958657902, 0.42505625334640734, 0.2797813543037457, 0.21004940276326803]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.756653
maxi score, test score, baseline:  0.46909999999999996 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.93 ]
 [0.618]
 [0.618]
 [0.618]] [[0.208]
 [0.581]
 [0.208]
 [0.208]
 [0.208]] [[0.665]
 [1.062]
 [0.665]
 [0.665]
 [0.665]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.46909999999999996 0.5 0.5
probs:  [0.08486669744519659, 0.4238245290050562, 0.2818678303175936, 0.20944094323215354]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.46909999999999996 0.5 0.5
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08486669744519659, 0.4238245290050562, 0.2818678303175936, 0.20944094323215354]
maxi score, test score, baseline:  0.47109999999999996 0.5 0.5
probs:  [0.08486669744519659, 0.4238245290050562, 0.2818678303175936, 0.20944094323215354]
actor:  0 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.47109999999999996 0.5 0.5
probs:  [0.08486669744519659, 0.4238245290050562, 0.2818678303175936, 0.20944094323215354]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.08511298958657901, 0.4250562533464073, 0.28268685228459883, 0.2071439047824148]
maxi score, test score, baseline:  0.47109999999999996 0.5 0.5
probs:  [0.08511298958657901, 0.4250562533464073, 0.28268685228459883, 0.2071439047824148]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.129]
 [1.129]
 [1.129]
 [1.129]
 [1.129]] [[0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]] [[0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.492]
 [0.536]
 [0.516]
 [0.515]] [[-1.237]
 [-0.812]
 [-0.755]
 [-1.403]
 [-1.217]] [[0.444]
 [0.713]
 [0.794]
 [0.343]
 [0.466]]
siam score:  -0.76903
maxi score, test score, baseline:  0.47209999999999996 0.5 0.5
probs:  [0.08534371911972558, 0.4262111076087147, 0.28177577350321087, 0.20666939976834886]
maxi score, test score, baseline:  0.47209999999999996 0.5 0.5
probs:  [0.08534371911972558, 0.4262111076087147, 0.28177577350321087, 0.20666939976834886]
maxi score, test score, baseline:  0.47209999999999996 0.5 0.5
probs:  [0.08534371911972558, 0.4262111076087147, 0.28177577350321087, 0.20666939976834886]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.658]
 [0.592]
 [0.592]
 [0.592]] [[-1.006]
 [-0.282]
 [-1.006]
 [-1.006]
 [-1.006]] [[0.449]
 [0.691]
 [0.449]
 [0.449]
 [0.449]]
maxi score, test score, baseline:  0.47209999999999996 0.5 0.5
probs:  [0.08534371911972558, 0.4262111076087147, 0.28177577350321087, 0.20666939976834886]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.47209999999999996 0.5 0.5
probs:  [0.08511298958657901, 0.4250562533464073, 0.28268685228459883, 0.2071439047824148]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.57 ]
 [0.624]
 [0.57 ]
 [0.57 ]] [[-0.809]
 [-0.809]
 [-0.953]
 [-0.809]
 [-0.809]] [[0.294]
 [0.294]
 [0.323]
 [0.294]
 [0.294]]
Printing some Q and Qe and total Qs values:  [[0.151]
 [1.427]
 [1.187]
 [1.187]
 [1.187]] [[-0.345]
 [-0.146]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.094]
 [1.403]
 [1.187]
 [1.187]
 [1.187]]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.08534371911972558, 0.4262111076087147, 0.28177577350321087, 0.20666939976834886]
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.47409999999999997 0.5 0.5
maxi score, test score, baseline:  0.47409999999999997 0.5 0.5
probs:  [0.0856110482000681, 0.42754006794392646, 0.2814165552328759, 0.2054323286231296]
maxi score, test score, baseline:  0.47409999999999997 0.5 0.5
probs:  [0.0856110482000681, 0.42754006794392646, 0.2814165552328759, 0.2054323286231296]
maxi score, test score, baseline:  0.47409999999999997 0.5 0.5
probs:  [0.08586168861743018, 0.4287935177560136, 0.28224145402157624, 0.2031033396049801]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
siam score:  -0.76162875
Printing some Q and Qe and total Qs values:  [[0.75]
 [0.75]
 [0.75]
 [0.75]
 [0.  ]] [[0.06]
 [0.06]
 [0.06]
 [0.06]
 [0.  ]] [[0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.043]]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.47409999999999997 0.5 0.5
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.823]
 [0.708]
 [0.708]
 [0.708]] [[0.972]
 [1.572]
 [0.972]
 [0.972]
 [0.972]] [[0.749]
 [0.99 ]
 [0.749]
 [0.749]
 [0.749]]
maxi score, test score, baseline:  0.47409999999999997 0.5 0.5
probs:  [0.08611380351553087, 0.43005434142912524, 0.2801315428514046, 0.20370031220393922]
maxi score, test score, baseline:  0.47409999999999997 0.5 0.5
probs:  [0.08611380351553087, 0.43005434142912524, 0.2801315428514046, 0.20370031220393922]
maxi score, test score, baseline:  0.47409999999999997 0.5 0.5
probs:  [0.08611380351553087, 0.43005434142912524, 0.2801315428514046, 0.20370031220393922]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08611380351553087, 0.43005434142912524, 0.2801315428514046, 0.20370031220393922]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.757]
 [0.757]
 [0.757]
 [0.756]] [[-0.158]
 [ 0.827]
 [ 0.501]
 [ 0.182]
 [ 0.316]] [[0.819]
 [1.145]
 [1.037]
 [0.93 ]
 [0.974]]
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08611380351553087, 0.43005434142912524, 0.2801315428514046, 0.20370031220393922]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.488]
 [0.448]
 [0.448]
 [0.453]] [[-1.049]
 [-0.164]
 [-1.049]
 [-1.049]
 [-0.344]] [[0.448]
 [0.488]
 [0.448]
 [0.448]
 [0.453]]
actor:  0 policy actor:  0  step number:  71 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08634137879115197, 0.43119347348122467, 0.27922475378729433, 0.20324039394032917]
maxi score, test score, baseline:  0.47209999999999996 0.5 0.5
probs:  [0.08659416156654817, 0.43245764022389904, 0.2771121794710211, 0.20383601873853155]
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08659416156654817, 0.43245764022389904, 0.2771121794710211, 0.20383601873853155]
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08659416156654817, 0.43245764022389904, 0.2771121794710211, 0.20383601873853155]
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08659416156654817, 0.43245764022389904, 0.2771121794710211, 0.20383601873853155]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.47209999999999996 0.5 0.5
probs:  [0.08659416156654817, 0.43245764022389904, 0.2771121794710211, 0.20383601873853155]
siam score:  -0.7549845
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47109999999999996 0.5 0.5
probs:  [0.08659416156654817, 0.43245764022389904, 0.2771121794710211, 0.20383601873853155]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.558]
 [0.613]
 [0.613]
 [0.613]] [[-0.879]
 [-0.215]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.284]
 [0.637]
 [0.801]
 [0.801]
 [0.801]]
maxi score, test score, baseline:  0.47109999999999996 0.5 0.5
probs:  [0.08659416156654817, 0.43245764022389904, 0.2771121794710211, 0.20383601873853155]
maxi score, test score, baseline:  0.47109999999999996 0.5 0.5
probs:  [0.08659416156654817, 0.43245764022389904, 0.2771121794710211, 0.20383601873853155]
from probs:  [0.08659416156654817, 0.43245764022389904, 0.2771121794710211, 0.20383601873853155]
maxi score, test score, baseline:  0.47109999999999996 0.5 0.5
probs:  [0.08659416156654817, 0.43245764022389904, 0.2771121794710211, 0.20383601873853155]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.87 ]
 [0.833]
 [0.833]
 [0.833]] [[0.281]
 [0.355]
 [0.281]
 [0.281]
 [0.281]] [[0.951]
 [1.013]
 [0.951]
 [0.951]
 [0.951]]
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08659416156654817, 0.43245764022389904, 0.2771121794710211, 0.20383601873853155]
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08659416156654817, 0.43245764022389904, 0.2771121794710211, 0.20383601873853155]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08681827336948819, 0.43357944245932584, 0.2762256346370466, 0.20337664953413948]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08703978732258197, 0.4346882410344072, 0.2753493664164873, 0.20292260522652367]
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08703978732258197, 0.4346882410344072, 0.2753493664164873, 0.20292260522652367]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08729239672018663, 0.43595154660550106, 0.2761494362413986, 0.2006066204329138]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08751053076195746, 0.43704343347845775, 0.2752761396592511, 0.2001698961003336]
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08751053076195746, 0.43704343347845775, 0.2752761396592511, 0.2001698961003336]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7607983
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
siam score:  -0.7629958
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08819089053089656, 0.4404478899061129, 0.274342963371458, 0.19701825619153246]
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08844390548373945, 0.4417132321592959, 0.272258839688907, 0.1975840226680577]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08844390548373945, 0.4417132321592959, 0.272258839688907, 0.1975840226680577]
from probs:  [0.08844390548373945, 0.4417132321592959, 0.272258839688907, 0.1975840226680577]
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
probs:  [0.08823242580183566, 0.4406546410192651, 0.2731096534568806, 0.19800327972201856]
maxi score, test score, baseline:  0.47309999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.47509999999999997 0.5 0.5
probs:  [0.08853351845428911, 0.44215241921503146, 0.2726491444702128, 0.19666491786046653]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47709999999999997 0.5 0.5
probs:  [0.08853351845428911, 0.44215241921503146, 0.2726491444702128, 0.19666491786046653]
maxi score, test score, baseline:  0.47709999999999997 0.5 0.5
probs:  [0.08853351845428911, 0.44215241921503146, 0.2726491444702128, 0.19666491786046653]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.47709999999999997 0.5 0.5
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47709999999999997 0.5 0.5
probs:  [0.08853351845428911, 0.44215241921503146, 0.2755716147244339, 0.19374244760624548]
maxi score, test score, baseline:  0.47709999999999997 0.5 0.5
probs:  [0.08853351845428911, 0.44215241921503146, 0.2755716147244339, 0.19374244760624548]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.47709999999999997 0.5 0.5
probs:  [0.08884173276422683, 0.4436856239255623, 0.27513477562392796, 0.19233786768628303]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47709999999999997 0.5 0.5
probs:  [0.08884173276422683, 0.4436856239255623, 0.27513477562392796, 0.19233786768628303]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.47909999999999997 0.5 0.5
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.47909999999999997 0.5 0.5
probs:  [0.08910493347871327, 0.4450018548161217, 0.27298500950304094, 0.192908202202124]
maxi score, test score, baseline:  0.47909999999999997 0.5 0.5
probs:  [0.08910493347871327, 0.4450018548161217, 0.27298500950304094, 0.192908202202124]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.47909999999999997 0.5 0.5
probs:  [0.0893697009849136, 0.44632592101843893, 0.2708224461686223, 0.19348193182802514]
maxi score, test score, baseline:  0.47909999999999997 0.5 0.5
probs:  [0.0893697009849136, 0.44632592101843893, 0.2708224461686223, 0.19348193182802514]
maxi score, test score, baseline:  0.47909999999999997 0.5 0.5
probs:  [0.08963604931488343, 0.44765789270491113, 0.2716304863714808, 0.19107557160872463]
maxi score, test score, baseline:  0.47909999999999997 0.5 0.5
probs:  [0.08963604931488343, 0.44765789270491113, 0.2716304863714808, 0.19107557160872463]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47909999999999997 0.5 0.5
probs:  [0.08984610040676719, 0.4487094680138259, 0.2707606906880117, 0.19068374089139525]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
siam score:  -0.7742779
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.47909999999999997 0.5 0.5
probs:  [0.08984610040676719, 0.4487094680138259, 0.2707606906880117, 0.19068374089139525]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.47909999999999997 0.5 0.5
probs:  [0.08963604931488343, 0.44765789270491113, 0.2716304863714808, 0.19107557160872463]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.2566748158475084
maxi score, test score, baseline:  0.48009999999999997 0.5 0.5
probs:  [0.0899640939817803, 0.44928999617363213, 0.27113681777599125, 0.18960909206859633]
maxi score, test score, baseline:  0.48009999999999997 0.5 0.5
probs:  [0.0899640939817803, 0.44928999617363213, 0.27113681777599125, 0.18960909206859633]
maxi score, test score, baseline:  0.48009999999999997 0.5 0.5
probs:  [0.09023627706851772, 0.4506511212172646, 0.2689293510582325, 0.1901832506559853]
from probs:  [0.09023627706851772, 0.4506511212172646, 0.2689293510582325, 0.1901832506559853]
maxi score, test score, baseline:  0.48009999999999997 0.5 0.5
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.09023627706851772, 0.4506511212172646, 0.2689293510582325, 0.1901832506559853]
maxi score, test score, baseline:  0.48009999999999997 0.5 0.5
maxi score, test score, baseline:  0.48009999999999997 0.5 0.5
probs:  [0.09023627706851772, 0.4506511212172646, 0.2689293510582325, 0.1901832506559853]
maxi score, test score, baseline:  0.48009999999999997 0.5 0.5
probs:  [0.09023627706851772, 0.4506511212172646, 0.2689293510582325, 0.1901832506559853]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.11459963689525134
Starting evaluation
actor:  0 policy actor:  0  step number:  34 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.48009999999999997 0.5 0.5
probs:  [0.09023627706851772, 0.4506511212172646, 0.2689293510582325, 0.1901832506559853]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4961 0.9 0.9
probs:  [0.0904514081216856, 0.45169350294051064, 0.2680621047409413, 0.1897929841968625]
Printing some Q and Qe and total Qs values:  [[1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]] [[0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]] [[1.328]
 [1.328]
 [1.328]
 [1.328]
 [1.328]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4961 0.9 0.9
probs:  [0.09065845611359839, 0.4527303210949053, 0.2672058944102687, 0.1894053283812276]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4961 0.9 0.9
probs:  [0.0907242322002158, 0.4530577324082556, 0.26585209063410176, 0.1903659447574268]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.4971 0.9 0.9
probs:  [0.0907242322002158, 0.4530577324082556, 0.26585209063410176, 0.1903659447574268]
maxi score, test score, baseline:  0.4971 0.9 0.9
maxi score, test score, baseline:  0.4971 0.9 0.9
probs:  [0.09099870981810254, 0.4544302302338682, 0.26362868201559125, 0.19094237793243807]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.09099870981810254, 0.4544302302338682, 0.26362868201559125, 0.19094237793243807]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.0907242322002158, 0.4530577324082556, 0.26585209063410176, 0.1903659447574268]
Printing some Q and Qe and total Qs values:  [[1.189]
 [1.189]
 [1.189]
 [1.189]
 [1.189]] [[1.714]
 [1.714]
 [1.714]
 [1.714]
 [1.714]] [[1.208]
 [1.208]
 [1.208]
 [1.208]
 [1.208]]
from probs:  [0.0907242322002158, 0.4530577324082556, 0.26585209063410176, 0.1903659447574268]
maxi score, test score, baseline:  0.4981 0.9 0.9
probs:  [0.0907242322002158, 0.4530577324082556, 0.26585209063410176, 0.1903659447574268]
first move QE:  0.04018336526574669
maxi score, test score, baseline:  0.4981 0.9 0.9
probs:  [0.0907242322002158, 0.4530577324082556, 0.26585209063410176, 0.1903659447574268]
maxi score, test score, baseline:  0.4981 0.9 0.9
probs:  [0.0907242322002158, 0.4530577324082556, 0.26585209063410176, 0.1903659447574268]
maxi score, test score, baseline:  0.4981 0.9 0.9
probs:  [0.0907242322002158, 0.4530577324082556, 0.26585209063410176, 0.1903659447574268]
maxi score, test score, baseline:  0.4981 0.9 0.9
probs:  [0.0907242322002158, 0.4530577324082556, 0.26585209063410176, 0.1903659447574268]
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]] [[0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
maxi score, test score, baseline:  0.4981 0.9 0.9
maxi score, test score, baseline:  0.4981 0.9 0.9
probs:  [0.0907242322002158, 0.4530577324082556, 0.26585209063410176, 0.1903659447574268]
Printing some Q and Qe and total Qs values:  [[0.708]
 [1.   ]
 [0.708]
 [0.708]
 [0.708]] [[0.4  ]
 [0.274]
 [0.4  ]
 [0.4  ]
 [0.4  ]] [[0.848]
 [1.092]
 [0.848]
 [0.848]
 [0.848]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4981 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.26528177141404063, 0.1888729143438374]
using another actor
from probs:  [0.09106957729397726, 0.4547757369481446, 0.26528177141404063, 0.1888729143438374]
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.26528177141404063, 0.1888729143438374]
maxi score, test score, baseline:  0.4991 0.9 0.9
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.26528177141404063, 0.1888729143438374]
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.26528177141404063, 0.1888729143438374]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.26528177141404063, 0.1888729143438374]
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.09086060126708788, 0.45372918195276185, 0.26614457668304903, 0.18926564009710117]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5001 0.9 0.9
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -1.3001471375646279
maxi score, test score, baseline:  0.5001 0.9 0.9
probs:  [0.09036971299862487, 0.4512729705669513, 0.26927902016924815, 0.18907829626517564]
maxi score, test score, baseline:  0.5001 0.9 0.9
maxi score, test score, baseline:  0.5001 0.9 0.9
probs:  [0.09036971299862487, 0.4512729705669513, 0.26927902016924815, 0.18907829626517564]
maxi score, test score, baseline:  0.5001 0.9 0.9
probs:  [0.09036971299862487, 0.4512729705669513, 0.26927902016924815, 0.18907829626517564]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5001 0.9 0.9
probs:  [0.09036971299862487, 0.4512729705669513, 0.26927902016924815, 0.18907829626517564]
maxi score, test score, baseline:  0.5001 0.9 0.9
probs:  [0.09036971299862487, 0.4512729705669513, 0.26927902016924815, 0.18907829626517564]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5001 0.9 0.9
probs:  [0.09015444494794932, 0.450194918463248, 0.2701746817055986, 0.1894759548832041]
maxi score, test score, baseline:  0.5001 0.9 0.9
probs:  [0.09015444494794932, 0.450194918463248, 0.2701746817055986, 0.1894759548832041]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.09015444494794932, 0.450194918463248, 0.2701746817055986, 0.1894759548832041]
maxi score, test score, baseline:  0.5001 0.9 0.9
probs:  [0.08993648679796143, 0.4491033944708284, 0.2710815358851465, 0.18987858284606357]
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4991 0.9 0.9
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.09015444494794932, 0.450194918463248, 0.2701746817055986, 0.1894759548832041]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.09015444494794932, 0.450194918463248, 0.2701746817055986, 0.1894759548832041]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.08987578692928481, 0.4488015592230618, 0.27243286076836104, 0.18888979307929224]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.08987578692928481, 0.4488015592230618, 0.27243286076836104, 0.18888979307929224]
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.08987578692928481, 0.4488015592230618, 0.27243286076836104, 0.18888979307929224]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.08987578692928481, 0.4488015592230618, 0.27243286076836104, 0.18888979307929224]
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.08987578692928481, 0.4488015592230618, 0.27243286076836104, 0.18888979307929224]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.08987578692928481, 0.4488015592230618, 0.27243286076836104, 0.18888979307929224]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4991 0.9 0.9
maxi score, test score, baseline:  0.4991 0.9 0.9
probs:  [0.09009210694639888, 0.44988486631700136, 0.2715260625264463, 0.18849696421015344]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.09036971299862487, 0.4512729705669513, 0.2723636633963279, 0.18599365303809595]
maxi score, test score, baseline:  0.4991 0.9 0.9
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5001 0.9 0.9
probs:  [0.09036971299862487, 0.4512729705669513, 0.2723636633963279, 0.18599365303809595]
siam score:  -0.7511676
siam score:  -0.7488434
maxi score, test score, baseline:  0.5001 0.9 0.9
actor:  1 policy actor:  1  step number:  46 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.5001 0.9 0.9
probs:  [0.09036971299862487, 0.4512729705669513, 0.2723636633963279, 0.18599365303809595]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7500404
maxi score, test score, baseline:  0.5001 0.9 0.9
probs:  [0.09086060126708789, 0.4537291819527619, 0.269219734146487, 0.1861904826336633]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.03566252739895423
siam score:  -0.752769
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 1.6670694550255956
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5041 0.9 0.9
maxi score, test score, baseline:  0.5041 0.9 0.9
probs:  [0.0907923755849547, 0.4533896445876416, 0.270567492149312, 0.18525048767809163]
maxi score, test score, baseline:  0.5041 0.9 0.9
probs:  [0.0907923755849547, 0.4533896445876416, 0.270567492149312, 0.18525048767809163]
maxi score, test score, baseline:  0.5041 0.9 0.9
probs:  [0.0907923755849547, 0.4533896445876416, 0.270567492149312, 0.18525048767809163]
maxi score, test score, baseline:  0.5041 0.9 0.9
probs:  [0.0907923755849547, 0.4533896445876416, 0.270567492149312, 0.18525048767809163]
maxi score, test score, baseline:  0.5041 0.9 0.9
maxi score, test score, baseline:  0.5041 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.1827602057782211]
using explorer policy with actor:  1
using explorer policy with actor:  1
using another actor
from probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.1827602057782211]
maxi score, test score, baseline:  0.5041 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.18276020577822116]
actor:  0 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5041 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.1827602057782211]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5041 0.9 0.9
maxi score, test score, baseline:  0.5041 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.18276020577822116]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5061 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.18276020577822116]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.74807304
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.04238750093395356
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09086060126708788, 0.45372918195276185, 0.2722948916099249, 0.18311532517022538]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.7  ]
 [0.636]
 [0.598]
 [0.596]] [[-1.173]
 [-0.613]
 [-0.502]
 [-0.812]
 [-0.57 ]] [[0.385]
 [0.691]
 [0.664]
 [0.523]
 [0.602]]
maxi score, test score, baseline:  0.5071 0.9 0.9
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09086060126708788, 0.45372918195276185, 0.2722948916099249, 0.18311532517022538]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.665]
 [0.538]
 [0.391]
 [0.412]] [[0.058]
 [0.143]
 [0.058]
 [0.415]
 [0.716]] [[0.538]
 [0.665]
 [0.538]
 [0.391]
 [0.412]]
maxi score, test score, baseline:  0.5061 0.9 0.9
Printing some Q and Qe and total Qs values:  [[0.041]
 [1.304]
 [1.246]
 [0.972]
 [1.209]] [[0.341]
 [0.404]
 [0.363]
 [0.626]
 [0.588]] [[0.016]
 [1.27 ]
 [1.2  ]
 [1.017]
 [1.236]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5061 0.9 0.9
probs:  [0.09086060126708788, 0.45372918195276185, 0.2722948916099249, 0.18311532517022538]
maxi score, test score, baseline:  0.5061 0.9 0.9
probs:  [0.09086060126708788, 0.45372918195276185, 0.2722948916099249, 0.18311532517022538]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.75085443
maxi score, test score, baseline:  0.5061 0.9 0.9
probs:  [0.09086060126708788, 0.45372918195276185, 0.2722948916099249, 0.18311532517022538]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5061 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.18276020577822116]
maxi score, test score, baseline:  0.5061 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.18276020577822116]
Printing some Q and Qe and total Qs values:  [[1.184]
 [1.184]
 [1.184]
 [1.184]
 [1.184]] [[1.085]
 [1.085]
 [1.085]
 [1.085]
 [1.085]] [[1.08]
 [1.08]
 [1.08]
 [1.08]
 [1.08]]
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.18276020577822116]
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.18276020577822116]
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.18276020577822116]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5051 0.9 0.9
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.18276020577822116]
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.18276020577822116]
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09106957729397726, 0.4547757369481446, 0.2713944799796569, 0.18276020577822116]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5051 0.9 0.9
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09142346957403712, 0.4565362615791808, 0.27088568937317553, 0.18115457947360636]
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09142346957403712, 0.4565362615791808, 0.27088568937317553, 0.18115457947360636]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5051 0.9 0.9
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09093128293498387, 0.4540735248199963, 0.2740542938000756, 0.18094089844494424]
maxi score, test score, baseline:  0.5051 0.9 0.9
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.458 0.042 0.417]
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5061 0.9 0.9
probs:  [0.09093128293498387, 0.4540735248199963, 0.2740542938000756, 0.18094089844494424]
maxi score, test score, baseline:  0.5061 0.9 0.9
probs:  [0.09093128293498387, 0.4540735248199963, 0.2740542938000756, 0.18094089844494424]
siam score:  -0.7678699
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
first move QE:  0.029183330438355738
maxi score, test score, baseline:  0.5061 0.9 0.9
probs:  [0.09136411630929175, 0.4562266487979208, 0.27538174139051336, 0.1770274935022742]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[-0.007]
 [-0.116]
 [-0.004]
 [-0.034]
 [ 0.031]] [[0.018]
 [0.   ]
 [0.019]
 [0.014]
 [0.025]]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5061 0.9 0.9
from probs:  [0.09136411630929175, 0.4562266487979208, 0.27538174139051336, 0.1770274935022742]
maxi score, test score, baseline:  0.5061 0.9 0.9
probs:  [0.09136411630929175, 0.4562266487979208, 0.27538174139051336, 0.1770274935022742]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09136411630929175, 0.4562266487979208, 0.27538174139051336, 0.1770274935022742]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09173544078527501, 0.4580737098305268, 0.27490457530790097, 0.17528627407629732]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09173544078527501, 0.4580737098305268, 0.27490457530790097, 0.17528627407629732]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.015]
 [1.163]
 [1.015]
 [1.015]
 [1.015]] [[1.053]
 [1.286]
 [1.053]
 [1.053]
 [1.21 ]] [[1.024]
 [1.249]
 [1.024]
 [1.024]
 [1.076]]
maxi score, test score, baseline:  0.5071 0.9 0.9
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09194816921699527, 0.4591392912381173, 0.27394724708833407, 0.17496529245655332]
line 256 mcts: sample exp_bonus 4.107178367212328e-06
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09194816921699527, 0.4591392912381173, 0.27394724708833407, 0.17496529245655332]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.528]
 [0.748]
 [0.548]
 [0.548]] [[-1.405]
 [-1.193]
 [ 0.008]
 [-1.671]
 [-1.459]] [[0.435]
 [0.483]
 [1.07 ]
 [0.351]
 [0.418]]
maxi score, test score, baseline:  0.5071 0.9 0.9
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.008]
 [0.727]
 [0.647]
 [0.691]] [[-0.704]
 [-0.235]
 [-0.488]
 [-0.731]
 [-0.513]] [[0.528]
 [0.003]
 [0.637]
 [0.477]
 [0.593]]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09194816921699527, 0.4591392912381173, 0.27394724708833407, 0.17496529245655332]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09215819714520494, 0.4601913455101542, 0.2730020717728093, 0.1746483855718315]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.331]
 [0.284]
 [0.284]
 [0.236]] [[-0.775]
 [ 0.095]
 [ 0.   ]
 [ 0.   ]
 [-0.558]] [[0.182]
 [0.331]
 [0.284]
 [0.284]
 [0.236]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09215819714520494, 0.4601913455101542, 0.2730020717728093, 0.1746483855718315]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09215819714520494, 0.4601913455101542, 0.2730020717728093, 0.1746483855718315]
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09194816921699527, 0.4591392912381173, 0.27394724708833407, 0.17496529245655332]
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09194816921699527, 0.4591392912381173, 0.27394724708833407, 0.17496529245655332]
Printing some Q and Qe and total Qs values:  [[0.68]
 [0.68]
 [0.68]
 [0.68]
 [0.68]] [[-1.192]
 [-1.192]
 [-1.192]
 [-1.192]
 [-1.192]] [[0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]]
maxi score, test score, baseline:  0.5051 0.9 0.9
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.812]
 [0.751]
 [0.751]
 [0.751]] [[-0.98 ]
 [ 0.472]
 [-0.98 ]
 [-0.98 ]
 [-0.98 ]] [[0.224]
 [0.638]
 [0.224]
 [0.224]
 [0.224]]
maxi score, test score, baseline:  0.5051 0.9 0.9
probs:  [0.09194816921699527, 0.4591392912381173, 0.27394724708833407, 0.17496529245655332]
from probs:  [0.09173544078527501, 0.4580737098305268, 0.27490457530790097, 0.17528627407629732]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5061 0.9 0.9
probs:  [0.09173544078527501, 0.4580737098305268, 0.27490457530790097, 0.17528627407629732]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.76015747
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.09194816921699527, 0.4591392912381173, 0.27394724708833407, 0.17496529245655332]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09194816921699527, 0.4591392912381173, 0.27394724708833407, 0.17496529245655332]
actor:  0 policy actor:  1  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.5081 0.9 0.9
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.09173544078527501, 0.4580737098305268, 0.27490457530790097, 0.17528627407629732]
Printing some Q and Qe and total Qs values:  [[0.882]
 [0.926]
 [0.882]
 [0.882]
 [0.882]] [[1.74 ]
 [1.713]
 [1.74 ]
 [1.74 ]
 [1.74 ]] [[0.988]
 [1.002]
 [0.988]
 [0.988]
 [0.988]]
maxi score, test score, baseline:  0.5081 0.9 0.9
from probs:  [0.09173544078527501, 0.4580737098305268, 0.27490457530790097, 0.17528627407629732]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.841]
 [0.626]
 [0.626]
 [0.624]] [[0.588]
 [0.385]
 [0.588]
 [0.588]
 [0.549]] [[0.509]
 [0.547]
 [0.509]
 [0.509]
 [0.497]]
maxi score, test score, baseline:  0.5071 0.9 0.9
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09203087683824478, 0.45955087766355296, 0.27579087725089885, 0.17262736824730357]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09203087683824478, 0.45955087766355296, 0.27579087725089885, 0.17262736824730357]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09203087683824478, 0.45955087766355296, 0.27579087725089885, 0.17262736824730357]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.091735440785275, 0.45807370983052675, 0.2749045753079009, 0.17528627407629732]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09151995942910962, 0.4569943387048364, 0.27587429233810457, 0.17561140952794943]
Printing some Q and Qe and total Qs values:  [[0.987]
 [1.323]
 [0.987]
 [0.987]
 [0.987]] [[0.408]
 [0.697]
 [0.408]
 [0.408]
 [0.408]] [[0.732]
 [1.165]
 [0.732]
 [0.732]
 [0.732]]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09151995942910962, 0.4569943387048364, 0.27587429233810457, 0.17561140952794943]
Printing some Q and Qe and total Qs values:  [[1.416]
 [1.483]
 [1.416]
 [1.416]
 [1.272]] [[0.616]
 [0.275]
 [0.616]
 [0.616]
 [0.724]] [[1.384]
 [1.337]
 [1.384]
 [1.384]
 [1.276]]
from probs:  [0.09151995942910962, 0.4569943387048364, 0.27587429233810457, 0.17561140952794943]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.969]
 [0.728]
 [0.728]
 [0.728]] [[0.101]
 [0.309]
 [0.101]
 [0.101]
 [0.101]] [[0.661]
 [0.971]
 [0.661]
 [0.661]
 [0.661]]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09151995942910962, 0.4569943387048364, 0.27587429233810457, 0.17561140952794943]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09151995942910962, 0.4569943387048364, 0.27587429233810457, 0.17561140952794943]
maxi score, test score, baseline:  0.5071 0.9 0.9
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09130167136196697, 0.45590090843806347, 0.2768566402310518, 0.17594077996891794]
using another actor
from probs:  [0.09130167136196697, 0.45590090843806347, 0.2768566402310518, 0.17594077996891794]
actor:  0 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09085645285030214, 0.45367075723303296, 0.2788602287577172, 0.17661256115894763]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09055807959603662, 0.4521789299967782, 0.2812308916255186, 0.17603209878166648]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09078336543793881, 0.4533073949023242, 0.2802103357886988, 0.17569890387103812]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09108052138695745, 0.4547931425425805, 0.277851867385791, 0.176274468684671]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09108052138695745, 0.4547931425425805, 0.277851867385791, 0.176274468684671]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.255886218887522
siam score:  -0.76139486
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09108052138695745, 0.4547931425425805, 0.277851867385791, 0.176274468684671]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09108052138695745, 0.4547931425425805, 0.277851867385791, 0.176274468684671]
line 256 mcts: sample exp_bonus 1.2149031656815032
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09108052138695745, 0.4547931425425805, 0.277851867385791, 0.176274468684671]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09085645285030214, 0.45367075723303296, 0.2788602287577172, 0.17661256115894763]
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09085645285030214, 0.45367075723303296, 0.2788602287577172, 0.17661256115894763]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.088]
 [1.171]
 [1.088]
 [0.838]
 [1.062]] [[1.681]
 [1.236]
 [1.681]
 [1.823]
 [1.656]] [[0.871]
 [0.784]
 [0.871]
 [0.76 ]
 [0.847]]
from probs:  [0.09123588670592875, 0.45555774668600796, 0.2784104202736759, 0.1747959463343873]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  0.02148233576161606
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.09123588670592875, 0.45555774668600796, 0.2784104202736759, 0.1747959463343873]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.727]
 [0.69 ]
 [0.69 ]
 [0.69 ]] [[-0.349]
 [-0.244]
 [-0.349]
 [-0.349]
 [ 0.099]] [[0.576]
 [0.683]
 [0.576]
 [0.576]
 [0.874]]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09123588670592875, 0.45555774668600796, 0.2784104202736759, 0.1747959463343873]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09123588670592875, 0.45555774668600796, 0.2784104202736759, 0.1747959463343873]
maxi score, test score, baseline:  0.5071 0.9 0.9
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09154153758031078, 0.4570859270775833, 0.2759905414550813, 0.17538199388702466]
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09154153758031078, 0.4570859270775833, 0.2759905414550813, 0.17538199388702466]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09131528694247647, 0.45595249992572207, 0.27701016392468486, 0.17572204920711668]
maxi score, test score, baseline:  0.5091 0.9 0.9
siam score:  -0.7702411
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.423]
 [1.423]
 [1.423]
 [1.423]
 [1.423]] [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[1.051]
 [1.051]
 [1.051]
 [1.051]
 [1.051]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09123588670592875, 0.45555774668600796, 0.2784104202736759, 0.1747959463343873]
siam score:  -0.7694275
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09123588670592875, 0.45555774668600796, 0.2784104202736759, 0.1747959463343873]
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09123588670592875, 0.45555774668600796, 0.2784104202736759, 0.1747959463343873]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09093227345924168, 0.45403975393971613, 0.2808141669215082, 0.174213805679534]
start point for exploration sampling:  16339
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5091 0.9 0.9
maxi score, test score, baseline:  0.5091 0.9 0.9
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.78253555
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09185052384490312, 0.4586227132258724, 0.28028394224246533, 0.16924282068675903]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.0913976110728343, 0.4563536458084629, 0.2824026385980231, 0.1698461045206797]
maxi score, test score, baseline:  0.5071 0.9 0.9
maxi score, test score, baseline:  0.5071 0.9 0.9
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[1.153]
 [1.153]
 [1.153]
 [1.153]
 [1.153]] [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.764]
 [1.008]
 [0.764]
 [0.764]
 [0.764]] [[0.845]
 [1.044]
 [0.845]
 [0.845]
 [0.845]] [[0.975]
 [1.197]
 [0.975]
 [0.975]
 [0.975]]
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.09171009229071679, 0.45791593282895043, 0.28336922378736246, 0.16700475109297044]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.09171009229071679, 0.45791593282895043, 0.28336922378736246, 0.16700475109297044]
Printing some Q and Qe and total Qs values:  [[1.518]
 [0.826]
 [1.518]
 [1.518]
 [1.518]] [[2.651]
 [2.246]
 [2.651]
 [2.651]
 [2.651]] [[1.333]
 [0.724]
 [1.333]
 [1.333]
 [1.333]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
from probs:  [0.0913976110728343, 0.4563536458084629, 0.28240263859802317, 0.16984610452067972]
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.246]
 [0.226]
 [0.226]
 [0.226]] [[-0.843]
 [-0.541]
 [-0.843]
 [-0.843]
 [-0.843]] [[0.226]
 [0.246]
 [0.226]
 [0.226]
 [0.226]]
actor:  0 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5091 0.9 0.9
Printing some Q and Qe and total Qs values:  [[1.049]
 [1.154]
 [1.049]
 [1.049]
 [1.049]] [[0.61 ]
 [0.837]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[0.897]
 [1.078]
 [0.897]
 [0.897]
 [0.897]]
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.809]
 [0.691]
 [0.691]
 [0.691]] [[-0.335]
 [ 1.061]
 [-0.335]
 [-0.335]
 [-0.335]] [[0.378]
 [0.889]
 [0.378]
 [0.378]
 [0.378]]
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.0916256022461724, 0.4574958687149078, 0.28133611078551674, 0.1695424182534031]
first move QE:  0.015532396254225325
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.0916256022461724, 0.4574958687149078, 0.28133611078551674, 0.1695424182534031]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09202602312189545, 0.45948723020792115, 0.28090795199789004, 0.16757879467229328]
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09202602312189545, 0.45948723020792115, 0.28090795199789004, 0.16757879467229328]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.0922512805785234, 0.4606158578221877, 0.2798443523229821, 0.16728850927630687]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.18838263397123664
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.0922512805785234, 0.4606158578221877, 0.2798443523229821, 0.16728850927630687]
actor:  0 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.0922512805785234, 0.4606158578221877, 0.2798443523229821, 0.16728850927630687]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7601341
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.09202602312189545, 0.45948723020792115, 0.28090795199789004, 0.16757879467229328]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.0917113924674473, 0.45791422502924495, 0.2833689496960517, 0.16700543280725616]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.2984676042028163
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.0917113924674473, 0.45791422502924495, 0.2833689496960517, 0.16700543280725616]
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.0917113924674473, 0.45791422502924495, 0.2833689496960517, 0.16700543280725616]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.09211717057313897, 0.4599321138532988, 0.2829645468034106, 0.1649861687701518]
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09211717057313897, 0.4599321138532988, 0.2829645468034106, 0.1649861687701518]
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09211717057313897, 0.4599321138532988, 0.2829645468034106, 0.1649861687701518]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09179896192455453, 0.4583412577933463, 0.28544394842127463, 0.16441583186082462]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09116910527895514, 0.4551923450720526, 0.29035163271291414, 0.16328691693607822]
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09116910527895514, 0.4551923450720526, 0.29035163271291414, 0.16328691693607822]
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09116910527895514, 0.4551923450720526, 0.29035163271291414, 0.16328691693607822]
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.09116910527895514, 0.4551923450720526, 0.29035163271291414, 0.16328691693607822]
maxi score, test score, baseline:  0.5111 0.9 0.9
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.09116910527895514, 0.4551923450720526, 0.29035163271291414, 0.16328691693607822]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  33 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.954]
 [0.787]
 [0.787]
 [0.787]] [[0.993]
 [1.228]
 [0.993]
 [0.993]
 [0.993]] [[0.794]
 [0.969]
 [0.794]
 [0.794]
 [0.794]]
maxi score, test score, baseline:  0.5121 0.9 0.9
probs:  [0.09085741234836962, 0.4536340637692905, 0.2893578442579301, 0.1661506796244098]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  16339
using explorer policy with actor:  0
maxi score, test score, baseline:  0.5121 0.9 0.9
probs:  [0.09108984830963787, 0.454798644691857, 0.28824041083457913, 0.16587109616392595]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5121 0.9 0.9
maxi score, test score, baseline:  0.5121 0.9 0.9
probs:  [0.09108984830963787, 0.454798644691857, 0.28824041083457913, 0.16587109616392595]
maxi score, test score, baseline:  0.5121 0.9 0.9
maxi score, test score, baseline:  0.5121 0.9 0.9
probs:  [0.09108984830963787, 0.454798644691857, 0.28824041083457913, 0.16587109616392595]
maxi score, test score, baseline:  0.5121 0.9 0.9
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
siam score:  -0.7691851
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.09108984830963787, 0.454798644691857, 0.28824041083457913, 0.16587109616392595]
siam score:  -0.77316153
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.09108984830963787, 0.454798644691857, 0.28824041083457913, 0.16587109616392595]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.09078159299710019, 0.45325754085476566, 0.2906515081709532, 0.16530935797718097]
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.09078159299710019, 0.45325754085476566, 0.2906515081709532, 0.16530935797718097]
siam score:  -0.77702594
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.09078159299710019, 0.45325754085476566, 0.2906515081709532, 0.16530935797718097]
using explorer policy with actor:  1
using another actor
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5111 0.9 0.9
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.09108984830963787, 0.454798644691857, 0.28824041083457913, 0.16587109616392595]
maxi score, test score, baseline:  0.5111 0.9 0.9
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.0914002076463518, 0.45635026747087554, 0.2858128563379206, 0.16643666854485206]
maxi score, test score, baseline:  0.5111 0.9 0.9
probs:  [0.0914002076463518, 0.45635026747087554, 0.2858128563379206, 0.16643666854485206]
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.0914002076463518, 0.45635026747087554, 0.2858128563379206, 0.16643666854485206]
maxi score, test score, baseline:  0.5101 0.9 0.9
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.0914002076463518, 0.45635026747087554, 0.2858128563379206, 0.16643666854485206]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.0914002076463518, 0.45635026747087554, 0.2858128563379206, 0.16643666854485206]
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.0914002076463518, 0.45635026747087554, 0.2858128563379206, 0.16643666854485206]
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.0914002076463518, 0.45635026747087554, 0.2858128563379205, 0.16643666854485203]
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.0914002076463518, 0.45635026747087554, 0.2858128563379205, 0.16643666854485203]
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.0909348054010241, 0.4540184017682517, 0.28803732914323343, 0.1670094636874909]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7680853
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.091333958880775, 0.4560028710663971, 0.2876941423653408, 0.16496902768748717]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09180027483528211, 0.45833952876883616, 0.28544365427187673, 0.16441654212400506]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5101 0.9 0.9
siam score:  -0.76743746
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.3822],
        [0.3997],
        [0.0000],
        [0.0000],
        [0.3820],
        [0.0000],
        [0.5202],
        [0.0000],
        [0.4208],
        [0.0000]], dtype=torch.float64)
0.0 0.38219898416642967
0.0 0.3997390075530642
0.0 0.0
0.0 0.0
0.0 0.38204400646480907
0.9509900498999999 0.9509900498999999
0.0 0.5201976039149845
0.0 0.0
0.0 0.42078690617029674
0.0 0.0
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09180027483528211, 0.45833952876883616, 0.2854436542718767, 0.16441654212400506]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5101 0.9 0.9
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.0914842586976898, 0.45675966256823064, 0.28445994376137174, 0.16729613497270773]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
siam score:  -0.7691583
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.0917139927568323, 0.45791080951399865, 0.283368401526938, 0.16700679620223102]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.0917139927568323, 0.45791080951399865, 0.283368401526938, 0.16700679620223102]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5091 0.9 0.9
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09140150590122825, 0.4563485783435632, 0.2858125631835936, 0.16643735257161488]
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09140150590122825, 0.4563485783435632, 0.2858125631835936, 0.16643735257161488]
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09140150590122825, 0.4563485783435632, 0.2858125631835936, 0.16643735257161488]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5091 0.9 0.9
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3062997457124628
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09132043009590997, 0.45594582476913814, 0.2871377716796807, 0.1655959734552713]
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09132043009590997, 0.45594582476913814, 0.2871377716796807, 0.1655959734552713]
maxi score, test score, baseline:  0.5091 0.9 0.9
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09132043009590997, 0.45594582476913814, 0.2871377716796807, 0.1655959734552713]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.09109114468081923, 0.45479697396750035, 0.2882400988735997, 0.1658717824780808]
actor:  0 policy actor:  0  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.09140150590122827, 0.4563485783435633, 0.289223283486793, 0.16302663226841552]
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.09140150590122827, 0.4563485783435633, 0.289223283486793, 0.16302663226841552]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5071 0.9 0.9
probs:  [0.09140150590122827, 0.4563485783435633, 0.289223283486793, 0.16302663226841552]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5081 0.9 0.9
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
first move QE:  0.016915087601733894
maxi score, test score, baseline:  0.5081 0.9 0.9
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5081 0.9 0.9
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.09125133505024786, 0.45559253329558064, 0.2925064731286222, 0.16064965852554933]
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.0915687410599504, 0.45717933861391113, 0.29004306544638625, 0.16120885487975242]
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.0915687410599504, 0.45717933861391113, 0.29004306544638625, 0.16120885487975242]
maxi score, test score, baseline:  0.5081 0.9 0.9
actor:  0 policy actor:  0  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.774873
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.0915687410599504, 0.45717933861391113, 0.29004306544638625, 0.16120885487975242]
maxi score, test score, baseline:  0.5081 0.9 0.9
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.0915687410599504, 0.45717933861391113, 0.29004306544638625, 0.16120885487975242]
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.0915687410599504, 0.45717933861391113, 0.29004306544638625, 0.16120885487975242]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.0915687410599504, 0.45717933861391113, 0.29004306544638625, 0.16120885487975242]
from probs:  [0.0915687410599504, 0.45717933861391113, 0.29004306544638625, 0.16120885487975242]
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.0915687410599504, 0.45717933861391113, 0.29004306544638625, 0.16120885487975242]
maxi score, test score, baseline:  0.5081 0.9 0.9
probs:  [0.0915687410599504, 0.45717933861391113, 0.29004306544638625, 0.16120885487975242]
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.0915687410599504, 0.45717933861391113, 0.29004306544638625, 0.16120885487975242]
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.0915687410599504, 0.45717933861391113, 0.29004306544638625, 0.16120885487975242]
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.0915687410599504, 0.45717933861391113, 0.29004306544638625, 0.16120885487975242]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09133529411352187, 0.4560011374769191, 0.2912002274953839, 0.1614633409141752]
from probs:  [0.09133529411352187, 0.4560011374769191, 0.2912002274953839, 0.1614633409141752]
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09109720613576652, 0.45480804542501196, 0.29237407836379564, 0.16172067007542587]
using another actor
maxi score, test score, baseline:  0.5091 0.9 0.9
probs:  [0.09109720613576652, 0.45480804542501196, 0.29237407836379564, 0.16172067007542587]
siam score:  -0.77004975
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09109720613576652, 0.45480804542501196, 0.29237407836379564, 0.16172067007542587]
siam score:  -0.76685196
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09109720613576652, 0.45480804542501196, 0.29237407836379564, 0.16172067007542587]
maxi score, test score, baseline:  0.5101 0.9 0.9
probs:  [0.09109720613576652, 0.45480804542501196, 0.29237407836379564, 0.16172067007542587]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5121 0.9 0.9
probs:  [0.09150726057925603, 0.45684645653215733, 0.292085642671045, 0.15956064021754157]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5131 0.9 0.9
probs:  [0.09150726057925603, 0.45684645653215733, 0.292085642671045, 0.15956064021754157]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5131 0.9 0.9
probs:  [0.09150726057925603, 0.45684645653215733, 0.292085642671045, 0.15956064021754154]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
from probs:  [0.0911810136719949, 0.455215544131692, 0.29461207481123747, 0.15899136738507572]
maxi score, test score, baseline:  0.5141 0.9 0.9
probs:  [0.0911810136719949, 0.455215544131692, 0.29461207481123747, 0.15899136738507572]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus -0.5381285537655341
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5141 0.9 0.9
probs:  [0.09165768220626444, 0.4576043722184533, 0.29222461807832956, 0.15851332749695282]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5141 0.9 0.9
probs:  [0.09165768220626444, 0.4576043722184533, 0.29222461807832956, 0.15851332749695282]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5141 0.9 0.9
probs:  [0.0918910183616066, 0.4587737381855029, 0.2910559234088646, 0.15827932004402592]
maxi score, test score, baseline:  0.5141 0.9 0.9
probs:  [0.09221289057569153, 0.4603828125657446, 0.29207656251314895, 0.15532773434541491]
siam score:  -0.76807463
maxi score, test score, baseline:  0.5141 0.9 0.9
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5141 0.9 0.9
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.77208865
maxi score, test score, baseline:  0.5141 0.9 0.9
probs:  [0.09212111496497517, 0.4599268691125055, 0.2899034544594019, 0.15804856146311744]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5141 0.9 0.9
maxi score, test score, baseline:  0.5141 0.9 0.9
probs:  [0.09253835906321056, 0.46200142718305176, 0.28958532872712583, 0.1558748850266119]
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.155]
 [0.221]
 [0.207]
 [0.207]] [[-1.218]
 [-1.044]
 [-1.134]
 [-1.15 ]
 [-1.007]] [[0.203]
 [0.155]
 [0.221]
 [0.207]
 [0.207]]
siam score:  -0.7758389
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5161 0.9 0.9
probs:  [0.09276553756016839, 0.4631400490851051, 0.28843509081862556, 0.15565932253610104]
maxi score, test score, baseline:  0.5161 0.9 0.9
probs:  [0.09276553756016839, 0.4631400490851051, 0.28843509081862556, 0.15565932253610104]
maxi score, test score, baseline:  0.5161 0.9 0.9
probs:  [0.09276553756016839, 0.4631400490851051, 0.28843509081862556, 0.15565932253610104]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5161 0.9 0.9
maxi score, test score, baseline:  0.5161 0.9 0.9
probs:  [0.09298956201547964, 0.4642628628849531, 0.2873008222836153, 0.15544675281595183]
maxi score, test score, baseline:  0.5161 0.9 0.9
probs:  [0.09298956201547964, 0.4642628628849531, 0.2873008222836153, 0.15544675281595183]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.849]
 [0.678]
 [0.612]
 [0.687]] [[-0.614]
 [-0.146]
 [-0.58 ]
 [-0.615]
 [ 0.   ]] [[0.303]
 [0.715]
 [0.406]
 [0.339]
 [0.644]]
maxi score, test score, baseline:  0.5161 0.9 0.9
probs:  [0.09266834487098805, 0.4626570723172359, 0.2897651436040361, 0.15490943920774003]
maxi score, test score, baseline:  0.5161 0.9 0.9
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.77663994
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09212242971845747, 0.4599251209238094, 0.29337295886855563, 0.1545794904891776]
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09212242971845747, 0.4599251209238094, 0.29337295886855563, 0.1545794904891776]
maxi score, test score, baseline:  0.5151 0.9 0.9
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5171 0.9 0.9
probs:  [0.09234934282935893, 0.46106235522845385, 0.29221247104569076, 0.15437583089649642]
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5181 0.9 0.9
maxi score, test score, baseline:  0.5181 0.9 0.9
probs:  [0.09234934282935893, 0.46106235522845385, 0.29221247104569076, 0.15437583089649642]
maxi score, test score, baseline:  0.5181 0.9 0.9
probs:  [0.0923493428293589, 0.4610623552284538, 0.2922124710456908, 0.1543758308964964]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5181 0.9 0.9
probs:  [0.0923493428293589, 0.4610623552284538, 0.2922124710456908, 0.1543758308964964]
siam score:  -0.7702346
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5181 0.9 0.9
probs:  [0.09234934282935893, 0.46106235522845385, 0.29221247104569076, 0.15437583089649642]
using another actor
maxi score, test score, baseline:  0.5181 0.9 0.9
probs:  [0.09234934282935893, 0.46106235522845385, 0.29221247104569076, 0.15437583089649642]
maxi score, test score, baseline:  0.5171 0.9 0.9
probs:  [0.09234934282935893, 0.46106235522845385, 0.29221247104569076, 0.15437583089649642]
Printing some Q and Qe and total Qs values:  [[1.28 ]
 [1.385]
 [1.28 ]
 [1.148]
 [1.297]] [[ 0.563]
 [-0.023]
 [ 0.563]
 [ 0.742]
 [ 0.267]] [[0.956]
 [0.866]
 [0.956]
 [0.883]
 [0.875]]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.854]
 [0.854]
 [0.705]
 [0.624]] [[0.513]
 [1.012]
 [1.012]
 [0.729]
 [0.895]] [[0.969]
 [1.215]
 [1.215]
 [0.979]
 [1.   ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5161 0.9 0.9
probs:  [0.09234934282935893, 0.46106235522845385, 0.29221247104569076, 0.15437583089649642]
Printing some Q and Qe and total Qs values:  [[1.078]
 [1.245]
 [0.052]
 [0.816]
 [1.14 ]] [[ 0.464]
 [ 0.529]
 [-0.277]
 [ 0.692]
 [ 0.731]] [[0.945]
 [1.079]
 [0.012]
 [0.847]
 [1.076]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5171 0.9 0.9
probs:  [0.09234934282935893, 0.46106235522845385, 0.29221247104569076, 0.1543758308964964]
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.744]
 [0.683]
 [0.676]
 [0.699]] [[-0.759]
 [-0.431]
 [-0.661]
 [-0.514]
 [-0.42 ]] [[0.301]
 [0.528]
 [0.353]
 [0.419]
 [0.489]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5171 0.9 0.9
probs:  [0.09234934282935893, 0.46106235522845385, 0.29221247104569076, 0.1543758308964964]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5171 0.9 0.9
probs:  [0.09234934282935893, 0.46106235522845385, 0.29221247104569076, 0.1543758308964964]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.533]
 [1.009]
 [0.533]
 [0.533]
 [0.533]] [[-0.542]
 [-0.372]
 [-0.542]
 [-0.542]
 [-0.542]] [[0.264]
 [0.694]
 [0.264]
 [0.264]
 [0.264]]
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09299086952870891, 0.4642610785989442, 0.29077032669696506, 0.15197772517538183]
maxi score, test score, baseline:  0.5151 0.9 0.9
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09331432678085454, 0.4658780386574892, 0.2917828461917721, 0.14902478836988406]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09385799821789928, 0.468598802494941, 0.2916378671418935, 0.14590533214526621]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09407294560506244, 0.46967623685474624, 0.290489345616365, 0.14576147192382627]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
using explorer policy with actor:  1
first move QE:  0.003973933536202181
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09428494964414834, 0.4707389175374161, 0.2893565511888416, 0.14561958162959396]
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09428494964414834, 0.4707389175374161, 0.2893565511888416, 0.14561958162959396]
using another actor
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09407294560506244, 0.46967623685474624, 0.290489345616365, 0.14576147192382627]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]] [[-0.202]
 [-0.202]
 [-0.202]
 [-0.202]
 [-0.202]] [[0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.65 ]
 [0.454]
 [0.454]
 [0.423]] [[-0.135]
 [-0.257]
 [ 0.   ]
 [ 0.   ]
 [ 0.07 ]] [[0.014]
 [0.65 ]
 [0.454]
 [0.454]
 [0.423]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09407294560506244, 0.46967623685474624, 0.2904893456163651, 0.14576147192382627]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09451454295661361, 0.47187250611809056, 0.29018163496626836, 0.1434313159590273]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5151 0.9 0.9
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09496865709193093, 0.4741310271756656, 0.289865202462075, 0.14103511327032858]
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09496865709193093, 0.4741310271756656, 0.289865202462075, 0.14103511327032858]
maxi score, test score, baseline:  0.5151 0.9 0.9
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09530605219622733, 0.4758176019664268, 0.2908961011435261, 0.13798024469381978]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.14955655888862598
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.0955160822172238, 0.4768706678295627, 0.2897244360012853, 0.13788881395192812]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5151 0.9 0.9
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5141 0.9 0.9
probs:  [0.09518032270050555, 0.4751922578901737, 0.2922235483544076, 0.1374038710549131]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5141 0.9 0.9
probs:  [0.09538902915491217, 0.4762386522535466, 0.29105489056338496, 0.1373174280281563]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.09559483807929693, 0.4772705192315966, 0.2899024575750132, 0.13723218511409324]
maxi score, test score, baseline:  0.5141 0.9 0.9
probs:  [0.09526461307163056, 0.4756197541246057, 0.2923577316172632, 0.13675790118650052]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5141 0.9 0.9
probs:  [0.09572446389251203, 0.4779070419769708, 0.29207514621113306, 0.13429334791938402]
maxi score, test score, baseline:  0.5141 0.9 0.9
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.794]
 [0.665]
 [0.665]
 [0.665]] [[-0.446]
 [-0.104]
 [-0.446]
 [-0.446]
 [-0.446]] [[0.677]
 [0.944]
 [0.677]
 [0.677]
 [0.677]]
siam score:  -0.76277274
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5141 0.9 0.9
probs:  [0.096060954935796, 0.47958909006718425, 0.2931029326179772, 0.13124702237904262]
using another actor
using explorer policy with actor:  0
from probs:  [0.096060954935796, 0.47958909006718425, 0.2931029326179772, 0.13124702237904262]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.09585553368003435, 0.4785590362675352, 0.29429438687355347, 0.131291043178877]
using explorer policy with actor:  0
siam score:  -0.76367414
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09585553368003435, 0.4785590362675352, 0.29429438687355347, 0.131291043178877]
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09585553368003435, 0.4785590362675352, 0.29429438687355347, 0.131291043178877]
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09585553368003435, 0.4785590362675352, 0.29429438687355347, 0.131291043178877]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Starting evaluation
maxi score, test score, baseline:  0.5151 0.9 0.9
maxi score, test score, baseline:  0.5151 0.9 0.9
probs:  [0.09585553368003435, 0.4785590362675352, 0.29429438687355347, 0.131291043178877]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5191 0.9 0.9
probs:  [0.09585553368003435, 0.4785590362675352, 0.29429438687355347, 0.131291043178877]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.513]
 [0.447]
 [0.454]
 [0.468]] [[-0.694]
 [ 0.349]
 [-0.597]
 [-0.695]
 [-0.424]] [[0.42 ]
 [0.513]
 [0.447]
 [0.454]
 [0.468]]
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5371 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7806735
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.607]
 [0.648]
 [0.607]
 [0.607]] [[-1.152]
 [-1.152]
 [-0.595]
 [-1.152]
 [-1.152]] [[0.568]
 [0.568]
 [0.91 ]
 [0.568]
 [0.568]]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.092]
 [1.247]
 [1.092]
 [1.006]
 [1.202]] [[1.505]
 [0.911]
 [1.505]
 [1.626]
 [1.706]] [[0.997]
 [0.776]
 [0.997]
 [0.993]
 [1.22 ]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.78529906
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0027987948831346486
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5341 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.318]
 [1.318]
 [1.318]
 [1.247]
 [1.32 ]] [[1.653]
 [1.653]
 [1.653]
 [1.749]
 [1.891]] [[0.982]
 [0.982]
 [0.982]
 [0.978]
 [1.056]]
siam score:  -0.7840882
maxi score, test score, baseline:  0.5341 1.0 1.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7802217
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5361 1.0 1.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.979]
 [1.127]
 [0.979]
 [0.979]
 [1.038]] [[1.555]
 [1.421]
 [1.555]
 [1.555]
 [1.971]] [[1.105]
 [1.132]
 [1.105]
 [1.105]
 [1.259]]
maxi score, test score, baseline:  0.5391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.479]] [[0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.093]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  16339
Printing some Q and Qe and total Qs values:  [[1.235]
 [1.498]
 [1.499]
 [1.235]
 [1.235]] [[0.167]
 [1.432]
 [0.674]
 [0.167]
 [0.167]] [[0.88 ]
 [1.348]
 [1.175]
 [0.88 ]
 [0.88 ]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5401 1.0 1.0
maxi score, test score, baseline:  0.5401 1.0 1.0
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7931981
siam score:  -0.79378873
maxi score, test score, baseline:  0.5391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5391 1.0 1.0
maxi score, test score, baseline:  0.5391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5361 1.0 1.0
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5361 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5351 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5351 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5351 1.0 1.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5371 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5371 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.901]
 [0.97 ]
 [0.901]
 [0.901]
 [0.887]] [[0.521]
 [0.724]
 [0.521]
 [0.521]
 [0.99 ]] [[0.885]
 [0.994]
 [0.885]
 [0.885]
 [1.044]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.708 0.042 0.    0.083 0.167]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.78695345
maxi score, test score, baseline:  0.5321 1.0 1.0
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5331 1.0 1.0
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.901]
 [0.718]
 [0.645]
 [0.77 ]] [[1.389]
 [0.835]
 [1.389]
 [1.391]
 [1.545]] [[0.718]
 [0.901]
 [0.718]
 [0.645]
 [0.77 ]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.643]
 [1.034]
 [0.643]
 [0.643]
 [0.643]] [[-0.078]
 [ 2.569]
 [-0.078]
 [-0.078]
 [-0.078]] [[0.482]
 [1.069]
 [0.482]
 [0.482]
 [0.482]]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.991]
 [0.912]
 [0.912]
 [0.912]
 [0.912]] [[3.178]
 [1.645]
 [1.645]
 [1.645]
 [1.645]] [[1.003]
 [0.705]
 [0.705]
 [0.705]
 [0.705]]
line 256 mcts: sample exp_bonus 0.7196919140620134
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5331 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5331 1.0 1.0
maxi score, test score, baseline:  0.5331 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5331 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5351 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5351 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78418756
maxi score, test score, baseline:  0.5351 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5351 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.833 0.042 0.042]
maxi score, test score, baseline:  0.5351 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5351 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.625 0.    0.292 0.042]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5351 1.0 1.0
maxi score, test score, baseline:  0.5351 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5351 1.0 1.0
maxi score, test score, baseline:  0.5351 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5351 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  61 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.017]
 [1.081]
 [1.017]
 [1.017]
 [0.915]] [[0.076]
 [0.225]
 [0.076]
 [0.076]
 [0.266]] [[1.32 ]
 [1.432]
 [1.32 ]
 [1.32 ]
 [1.282]]
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5361 1.0 1.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5371 1.0 1.0
siam score:  -0.7679677
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.636]
 [0.505]
 [0.505]
 [0.505]] [[0.204]
 [2.103]
 [0.204]
 [0.204]
 [0.204]] [[0.505]
 [0.636]
 [0.505]
 [0.505]
 [0.505]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5381 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.397]
 [0.366]
 [0.366]
 [0.366]] [[-0.399]
 [-0.098]
 [-0.399]
 [-0.399]
 [-0.399]] [[0.366]
 [0.397]
 [0.366]
 [0.366]
 [0.366]]
maxi score, test score, baseline:  0.5371 1.0 1.0
maxi score, test score, baseline:  0.5371 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5371 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.226]
 [0.308]
 [0.245]
 [0.245]] [[-0.13 ]
 [ 0.   ]
 [ 0.011]
 [-0.746]
 [-0.553]] [[0.225]
 [0.226]
 [0.308]
 [0.245]
 [0.245]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus -0.8584761956493704
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.722]
 [0.614]
 [0.614]
 [0.614]] [[ 0.115]
 [-0.673]
 [ 0.115]
 [ 0.115]
 [ 0.115]] [[0.614]
 [0.722]
 [0.614]
 [0.614]
 [0.614]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.896]
 [0.797]
 [0.797]
 [0.797]] [[-0.508]
 [ 0.074]
 [-0.508]
 [-0.508]
 [-0.508]] [[0.534]
 [0.73 ]
 [0.534]
 [0.534]
 [0.534]]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.708]
 [0.729]
 [0.708]
 [0.708]] [[-1.16 ]
 [-1.16 ]
 [-0.755]
 [-1.16 ]
 [-1.16 ]] [[0.669]
 [0.669]
 [0.96 ]
 [0.669]
 [0.669]]
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.779]
 [1.017]
 [0.779]
 [0.779]
 [0.868]] [[0.999]
 [1.568]
 [0.999]
 [0.999]
 [1.297]] [[0.577]
 [1.062]
 [0.577]
 [0.577]
 [0.803]]
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.2142422732701994
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  27 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.27 ]
 [1.057]
 [1.057]
 [0.903]
 [1.154]] [[-0.637]
 [ 0.   ]
 [ 0.   ]
 [ 0.223]
 [-0.237]] [[0.852]
 [0.745]
 [0.745]
 [0.627]
 [0.802]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]] [[0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.236]]
maxi score, test score, baseline:  0.5391 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.801]
 [0.747]
 [0.747]
 [0.702]] [[1.133]
 [1.119]
 [1.133]
 [1.133]
 [1.225]] [[1.172]
 [1.198]
 [1.172]
 [1.172]
 [1.182]]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.6076],
        [0.0000],
        [0.7467],
        [0.7467],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.7556],
        [0.4786]], dtype=torch.float64)
0.0 0.6075747712544325
0.0 0.0
0.0 0.7466705079951355
0.0 0.7466705079951355
0.0 0.0
0.0 0.0
0.49005 0.49005
0.0 0.0
0.0 0.7555746629588544
0.0 0.4785656740303117
siam score:  -0.7914122
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.805]
 [0.664]
 [0.65 ]
 [0.65 ]] [[-0.766]
 [-0.689]
 [-0.661]
 [-0.766]
 [-0.766]] [[0.622]
 [0.789]
 [0.667]
 [0.622]
 [0.622]]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5401 1.0 1.0
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.492]
 [0.479]
 [0.479]
 [0.479]] [[-0.706]
 [-0.032]
 [-0.706]
 [-0.706]
 [-0.706]] [[0.479]
 [0.492]
 [0.479]
 [0.479]
 [0.479]]
actor:  0 policy actor:  0  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.36881102235328383
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5411 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5411 1.0 1.0
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.2162994591423566
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.77361625
first move QE:  -0.03425736573100951
actor:  1 policy actor:  1  step number:  37 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.302]
 [1.302]
 [1.302]
 [1.302]
 [1.302]] [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]] [[1.204]
 [1.204]
 [1.204]
 [1.204]
 [1.204]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.77108526
Printing some Q and Qe and total Qs values:  [[0.993]
 [1.198]
 [0.993]
 [0.993]
 [0.993]] [[2.157]
 [2.015]
 [2.157]
 [2.157]
 [2.157]] [[1.216]
 [1.304]
 [1.216]
 [1.216]
 [1.216]]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5401 1.0 1.0
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.228]] [[1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.155]] [[1.31]
 [1.31]
 [1.31]
 [1.31]
 [1.31]]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.769306
maxi score, test score, baseline:  0.5401 1.0 1.0
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5401 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.41 ]
 [0.363]
 [0.363]
 [0.372]] [[-0.735]
 [-0.154]
 [-0.362]
 [-0.735]
 [-0.497]] [[0.363]
 [0.41 ]
 [0.363]
 [0.363]
 [0.372]]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.354]
 [0.385]
 [0.37 ]
 [0.369]] [[-0.211]
 [-0.419]
 [-0.291]
 [-0.623]
 [-0.501]] [[0.341]
 [0.354]
 [0.385]
 [0.37 ]
 [0.369]]
maxi score, test score, baseline:  0.5401 1.0 1.0
maxi score, test score, baseline:  0.5401 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]] [[-0.399]
 [-0.399]
 [-0.399]
 [-0.399]
 [-0.399]] [[0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
siam score:  -0.7702495
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.857]
 [0.998]
 [0.857]
 [0.857]
 [0.857]] [[-0.058]
 [-0.149]
 [-0.058]
 [-0.058]
 [-0.058]] [[0.671]
 [0.797]
 [0.671]
 [0.671]
 [0.671]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7712484
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5411 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5411 1.0 1.0
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.77241564
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5411 1.0 1.0
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5411 1.0 1.0
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7673917
first move QE:  -0.036695086953224126
maxi score, test score, baseline:  0.5411 1.0 1.0
first move QE:  -0.036852620889836554
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5411 1.0 1.0
maxi score, test score, baseline:  0.5411 1.0 1.0
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5411 1.0 1.0
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5411 1.0 1.0
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5411 1.0 1.0
siam score:  -0.76937836
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5431 1.0 1.0
maxi score, test score, baseline:  0.5431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5451 1.0 1.0
maxi score, test score, baseline:  0.5451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5451 1.0 1.0
maxi score, test score, baseline:  0.5451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.208]
 [0.4  ]
 [0.185]
 [0.187]] [[-1.144]
 [ 0.   ]
 [-0.347]
 [-1.459]
 [-1.125]] [[0.175]
 [0.208]
 [0.4  ]
 [0.185]
 [0.187]]
maxi score, test score, baseline:  0.5471 1.0 1.0
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.6734],
        [0.0000],
        [0.8476],
        [0.8476],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.6153],
        [0.6830]], dtype=torch.float64)
0.0 0.0
0.0 0.6734338283809723
0.0 0.0
0.0 0.8475754994852608
0.0 0.8475754994852608
0.0 0.0
0.96059601 0.96059601
0.0 0.0
0.0 0.6153193332952454
0.0 0.6830453681847968
maxi score, test score, baseline:  0.5471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5491 1.0 1.0
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5491 1.0 1.0
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.055]
 [0.953]
 [0.953]
 [0.953]
 [0.953]] [[1.316]
 [1.052]
 [1.052]
 [1.052]
 [1.052]] [[1.291]
 [1.153]
 [1.153]
 [1.153]
 [1.153]]
Printing some Q and Qe and total Qs values:  [[0.869]
 [1.048]
 [0.869]
 [0.869]
 [0.866]] [[0.847]
 [0.983]
 [0.847]
 [0.847]
 [1.113]] [[0.97 ]
 [1.087]
 [0.97 ]
 [0.97 ]
 [1.045]]
siam score:  -0.7811155
maxi score, test score, baseline:  0.5491 1.0 1.0
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.868]
 [1.038]
 [0.868]
 [0.866]
 [0.864]] [[ 0.064]
 [-0.129]
 [ 0.064]
 [-0.013]
 [ 0.325]] [[0.878]
 [0.977]
 [0.878]
 [0.853]
 [0.956]]
Printing some Q and Qe and total Qs values:  [[1.105]
 [0.369]
 [1.02 ]
 [0.886]
 [0.904]] [[0.481]
 [0.341]
 [0.679]
 [0.55 ]
 [0.747]] [[0.911]
 [0.128]
 [0.892]
 [0.716]
 [0.799]]
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.147]
 [1.086]
 [1.086]
 [1.086]
 [1.282]] [[0.632]
 [1.456]
 [1.456]
 [1.456]
 [1.874]] [[0.156]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [1.111]]
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7962529
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.104]
 [1.107]
 [1.107]
 [1.107]
 [1.107]] [[0.944]
 [0.877]
 [0.877]
 [0.877]
 [0.877]] [[1.076]
 [1.057]
 [1.057]
 [1.057]
 [1.057]]
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.056]
 [1.126]
 [1.056]
 [1.056]
 [0.911]] [[1.949]
 [1.77 ]
 [1.949]
 [1.949]
 [2.174]] [[1.189]
 [1.159]
 [1.189]
 [1.189]
 [1.192]]
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7949604
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5551 1.0 1.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
first move QE:  -0.04042895002877569
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.275886309883607
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5531 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5531 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5531 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.04237120828399861
first move QE:  -0.042481865414810194
maxi score, test score, baseline:  0.5531 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.673]] [[0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.618]] [[0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.955]]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.833]
 [0.666]
 [0.666]
 [0.666]] [[0.448]
 [1.116]
 [0.448]
 [0.448]
 [0.448]] [[0.874]
 [1.198]
 [0.874]
 [0.874]
 [0.874]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5551 1.0 1.0
maxi score, test score, baseline:  0.5551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.131]
 [1.082]
 [1.131]
 [1.131]
 [1.095]] [[0.354]
 [0.394]
 [0.354]
 [0.354]
 [0.644]] [[1.269]
 [1.234]
 [1.269]
 [1.269]
 [1.328]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5561 1.0 1.0
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5561 1.0 1.0
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.12 ]
 [1.099]
 [1.12 ]
 [0.933]
 [1.08 ]] [[2.11 ]
 [1.629]
 [2.11 ]
 [1.799]
 [1.756]] [[1.175]
 [0.999]
 [1.175]
 [0.972]
 [1.033]]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.805]
 [0.833]
 [0.805]
 [0.805]
 [0.805]] [[-0.556]
 [ 0.744]
 [-0.556]
 [-0.556]
 [-0.556]] [[0.59 ]
 [1.165]
 [0.59 ]
 [0.59 ]
 [0.59 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.002]] [[0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]] [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.78042865
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.869]
 [0.765]
 [0.765]
 [0.765]] [[2.08 ]
 [1.629]
 [2.08 ]
 [2.08 ]
 [2.08 ]] [[0.765]
 [0.869]
 [0.765]
 [0.765]
 [0.765]]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.335]
 [1.335]
 [1.335]
 [1.335]
 [1.238]] [[2.58 ]
 [2.58 ]
 [2.58 ]
 [2.58 ]
 [2.599]] [[0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.75 ]]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5561 1.0 1.0
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5571 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5581 1.0 1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.330699717865991
first move QE:  -0.0444396448532015
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.891]
 [0.668]
 [0.668]
 [0.668]] [[0.314]
 [0.528]
 [0.314]
 [0.314]
 [0.314]] [[0.626]
 [0.891]
 [0.626]
 [0.626]
 [0.626]]
maxi score, test score, baseline:  0.5591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5591 1.0 1.0
maxi score, test score, baseline:  0.5591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5591 1.0 1.0
using explorer policy with actor:  1
Starting evaluation
maxi score, test score, baseline:  0.5591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.76829165
maxi score, test score, baseline:  0.5811 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.232]
 [0.266]
 [0.219]
 [0.229]] [[-1.162]
 [-0.992]
 [-0.914]
 [-1.306]
 [-1.192]] [[0.214]
 [0.232]
 [0.266]
 [0.219]
 [0.229]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5851 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5851 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5851 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.20637457213732727
actor:  1 policy actor:  1  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.5851 1.0 1.0
maxi score, test score, baseline:  0.5851 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5851 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5851 1.0 1.0
maxi score, test score, baseline:  0.5851 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.064]
 [1.281]
 [1.016]
 [1.033]
 [1.149]] [[0.538]
 [1.359]
 [1.18 ]
 [1.507]
 [1.54 ]] [[0.015]
 [1.213]
 [0.952]
 [1.112]
 [1.206]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5851 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5851 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5851 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5851 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.151]
 [1.415]
 [1.151]
 [0.937]
 [1.151]] [[0.401]
 [0.287]
 [0.401]
 [0.241]
 [0.401]] [[0.937]
 [1.163]
 [0.937]
 [0.67 ]
 [0.937]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5871 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5871 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5871 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.094]
 [1.094]
 [1.094]
 [1.094]
 [1.094]] [[0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]] [[1.1]
 [1.1]
 [1.1]
 [1.1]
 [1.1]]
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5891 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5891 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5911 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.5911 1.0 1.0
maxi score, test score, baseline:  0.5911 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5911 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.069]
 [0.968]
 [0.968]
 [0.968]
 [0.968]] [[1.07 ]
 [1.171]
 [1.171]
 [1.171]
 [1.171]] [[1.232]
 [1.191]
 [1.191]
 [1.191]
 [1.191]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5911 1.0 1.0
maxi score, test score, baseline:  0.5911 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5911 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.5911 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5911 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5921 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.867]
 [0.671]
 [0.662]
 [0.693]] [[-0.818]
 [-0.306]
 [-0.471]
 [-0.591]
 [-0.531]] [[0.358]
 [0.808]
 [0.559]
 [0.486]
 [0.544]]
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5921 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5921 1.0 1.0
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5931 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5931 1.0 1.0
maxi score, test score, baseline:  0.5931 1.0 1.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5931 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5931 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5931 1.0 1.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78583425
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]] [[-0.653]
 [-0.653]
 [-0.653]
 [-0.653]
 [-0.653]] [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]]
Printing some Q and Qe and total Qs values:  [[1.118]
 [1.145]
 [1.145]
 [1.145]
 [1.145]] [[-0.565]
 [-0.474]
 [-0.474]
 [-0.474]
 [-0.474]] [[1.061]
 [1.103]
 [1.103]
 [1.103]
 [1.103]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5971 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.862]
 [0.976]
 [0.862]
 [0.862]
 [0.735]] [[1.267]
 [0.389]
 [1.267]
 [1.267]
 [0.924]] [[0.862]
 [0.976]
 [0.862]
 [0.862]
 [0.735]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.102]
 [1.174]
 [1.102]
 [1.102]
 [1.021]] [[0.463]
 [0.204]
 [0.463]
 [0.463]
 [0.165]] [[1.195]
 [1.18 ]
 [1.195]
 [1.195]
 [1.015]]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7939444
maxi score, test score, baseline:  0.6001 1.0 1.0
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6011 1.0 1.0
actor:  0 policy actor:  0  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6001 1.0 1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.915]
 [0.798]
 [0.798]
 [0.715]] [[1.683]
 [0.397]
 [1.683]
 [1.683]
 [1.725]] [[0.798]
 [0.915]
 [0.798]
 [0.798]
 [0.715]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7881557
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.949]
 [0.942]
 [0.949]
 [0.949]
 [0.949]] [[-0.066]
 [ 0.018]
 [-0.066]
 [-0.066]
 [-0.066]] [[0.958]
 [0.979]
 [0.958]
 [0.958]
 [0.958]]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.948]
 [1.039]
 [0.948]
 [0.948]
 [0.914]] [[-0.488]
 [-0.79 ]
 [-0.488]
 [-0.488]
 [-0.336]] [[0.877]
 [0.867]
 [0.877]
 [0.877]
 [0.894]]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6021 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.979]
 [1.084]
 [0.979]
 [0.979]
 [0.979]] [[0.093]
 [0.37 ]
 [0.093]
 [0.093]
 [0.093]] [[1.204]
 [1.413]
 [1.204]
 [1.204]
 [1.204]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[-0.02]
 [-0.02]
 [-0.02]
 [-0.02]
 [-0.02]] [[0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7938878
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5991 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6001 1.0 1.0
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7873613
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.002]
 [0.002]
 [0.002]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.003]
 [0.001]
 [0.003]
 [0.003]
 [0.003]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78361785
maxi score, test score, baseline:  0.6011 1.0 1.0
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.91 ]
 [0.808]
 [0.808]
 [0.808]] [[0.738]
 [1.108]
 [0.738]
 [0.738]
 [0.738]] [[0.929]
 [1.109]
 [0.929]
 [0.929]
 [0.929]]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus -0.021009875955649363
maxi score, test score, baseline:  0.6011 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
siam score:  -0.7789924
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]] [[-0.183]
 [-0.183]
 [-0.183]
 [-0.183]
 [-0.183]] [[0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6021 1.0 1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6021 1.0 1.0
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.823]
 [0.737]
 [0.845]
 [0.701]
 [0.702]] [[ 0.   ]
 [-1.012]
 [-0.604]
 [-1.606]
 [-1.594]] [[0.912]
 [0.656]
 [0.833]
 [0.521]
 [0.524]]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6011 1.0 1.0
maxi score, test score, baseline:  0.6011 1.0 1.0
maxi score, test score, baseline:  0.6011 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.2320528094249228
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.823]
 [0.964]
 [0.823]
 [0.868]
 [0.889]] [[0.881]
 [1.104]
 [0.881]
 [0.646]
 [0.805]] [[0.745]
 [0.997]
 [0.745]
 [0.672]
 [0.773]]
Printing some Q and Qe and total Qs values:  [[1.1  ]
 [1.188]
 [1.1  ]
 [1.1  ]
 [1.087]] [[1.57 ]
 [1.217]
 [1.57 ]
 [1.57 ]
 [1.591]] [[1.245]
 [1.179]
 [1.245]
 [1.245]
 [1.243]]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.12]
 [1.12]
 [1.12]
 [1.12]
 [1.12]] [[0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.52]] [[0.842]
 [0.842]
 [0.842]
 [0.842]
 [0.842]]
maxi score, test score, baseline:  0.5991 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5991 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5991 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.237]
 [1.429]
 [1.237]
 [1.237]
 [1.237]] [[1.155]
 [1.028]
 [1.155]
 [1.155]
 [1.155]] [[1.088]
 [1.217]
 [1.088]
 [1.088]
 [1.088]]
maxi score, test score, baseline:  0.5991 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5991 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.117]
 [0.014]
 [0.014]
 [0.001]] [[0.519]
 [0.599]
 [0.87 ]
 [1.135]
 [1.35 ]] [[0.407]
 [0.513]
 [0.59 ]
 [0.768]
 [0.898]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5991 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.985]
 [1.033]
 [0.985]
 [0.985]
 [0.985]] [[0.276]
 [0.418]
 [0.276]
 [0.276]
 [0.276]] [[1.302]
 [1.398]
 [1.302]
 [1.302]
 [1.302]]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.959]
 [0.878]
 [0.878]
 [0.878]] [[0.112]
 [0.493]
 [0.112]
 [0.112]
 [0.112]] [[0.979]
 [1.186]
 [0.979]
 [0.979]
 [0.979]]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.74434295917064
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6011 1.0 1.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.042 0.292 0.042 0.083 0.542]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.98 ]
 [0.985]
 [0.98 ]
 [0.98 ]
 [0.98 ]] [[0.473]
 [0.731]
 [0.473]
 [0.473]
 [0.473]] [[0.82 ]
 [0.926]
 [0.82 ]
 [0.82 ]
 [0.82 ]]
first move QE:  -0.06427647662140332
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.6021 1.0 1.0
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.94 ]
 [0.789]
 [0.789]
 [0.722]] [[0.362]
 [0.022]
 [0.362]
 [0.362]
 [0.433]] [[0.789]
 [0.94 ]
 [0.789]
 [0.789]
 [0.722]]
maxi score, test score, baseline:  0.6031 1.0 1.0
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06560826048893387
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6071 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6071 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6071 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.   ]
 [0.548]
 [0.46 ]
 [0.495]] [[ 0.   ]
 [ 0.214]
 [ 0.023]
 [ 0.   ]
 [-0.379]] [[0.46 ]
 [0.   ]
 [0.548]
 [0.46 ]
 [0.495]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7916166
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.6971873037989877
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6081 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.931]
 [0.852]
 [0.852]
 [0.867]] [[0.498]
 [0.442]
 [0.498]
 [0.498]
 [0.263]] [[0.935]
 [0.995]
 [0.935]
 [0.935]
 [0.871]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7853957
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.236]
 [0.327]
 [0.248]
 [0.236]] [[ 0.   ]
 [ 0.   ]
 [-0.539]
 [-0.733]
 [ 0.   ]] [[0.236]
 [0.236]
 [0.327]
 [0.248]
 [0.236]]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6071 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.235]
 [0.265]
 [0.239]
 [0.244]] [[-1.925]
 [-1.762]
 [-1.993]
 [-1.902]
 [-1.839]] [[0.251]
 [0.235]
 [0.265]
 [0.239]
 [0.244]]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6041 1.0 1.0
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6041 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.288]
 [1.37 ]
 [1.288]
 [1.288]
 [1.11 ]] [[ 0.208]
 [ 0.117]
 [ 0.208]
 [ 0.208]
 [-0.001]] [[1.212]
 [1.274]
 [1.212]
 [1.212]
 [1.017]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6001 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.005]
 [1.003]
 [0.829]
 [0.829]
 [0.864]] [[0.241]
 [0.854]
 [0.578]
 [0.578]
 [0.931]] [[0.093]
 [0.992]
 [0.776]
 [0.776]
 [0.922]]
maxi score, test score, baseline:  0.6001 1.0 1.0
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.14729644284971435
Printing some Q and Qe and total Qs values:  [[1.336]
 [1.401]
 [1.336]
 [1.336]
 [1.336]] [[0.578]
 [0.684]
 [0.578]
 [0.578]
 [0.578]] [[1.217]
 [1.297]
 [1.217]
 [1.217]
 [1.217]]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.126]
 [1.191]
 [1.191]
 [1.039]
 [1.191]] [[1.701]
 [0.639]
 [0.639]
 [0.635]
 [0.639]] [[0.921]
 [0.77 ]
 [0.77 ]
 [0.689]
 [0.77 ]]
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.912]
 [0.799]
 [0.799]
 [0.799]] [[0.147]
 [1.148]
 [0.147]
 [0.147]
 [0.147]] [[0.561]
 [1.008]
 [0.561]
 [0.561]
 [0.561]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.411]
 [1.411]
 [1.411]
 [1.243]
 [1.302]] [[2.004]
 [2.004]
 [2.004]
 [1.841]
 [2.25 ]] [[1.213]
 [1.213]
 [1.213]
 [1.046]
 [1.246]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]] [[-1.111]
 [-1.111]
 [-1.111]
 [-1.111]
 [-1.111]] [[0.66]
 [0.66]
 [0.66]
 [0.66]
 [0.66]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6031 1.0 1.0
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.877]
 [1.112]
 [0.877]
 [0.877]
 [0.877]] [[-0.918]
 [-1.098]
 [-0.918]
 [-0.918]
 [-0.918]] [[0.723]
 [0.929]
 [0.723]
 [0.723]
 [0.723]]
Printing some Q and Qe and total Qs values:  [[1.025]
 [1.178]
 [1.025]
 [1.025]
 [1.025]] [[-0.206]
 [-0.201]
 [-0.206]
 [-0.206]
 [-0.206]] [[1.115]
 [1.268]
 [1.115]
 [1.115]
 [1.115]]
siam score:  -0.7784151
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.771]
 [0.788]
 [0.764]
 [0.74 ]] [[-1.569]
 [-1.252]
 [-1.725]
 [-1.777]
 [-1.497]] [[0.543]
 [0.681]
 [0.54 ]
 [0.499]
 [0.568]]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]] [[-0.243]
 [-0.243]
 [-0.243]
 [-0.243]
 [-0.243]] [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.899]
 [0.75 ]
 [0.75 ]
 [0.75 ]] [[-1.248]
 [-0.392]
 [-1.248]
 [-1.248]
 [-1.248]] [[0.346]
 [0.723]
 [0.346]
 [0.346]
 [0.346]]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.158]
 [1.326]
 [1.158]
 [1.158]
 [1.158]] [[0.357]
 [0.374]
 [0.357]
 [0.357]
 [0.357]] [[0.782]
 [0.959]
 [0.782]
 [0.782]
 [0.782]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 0.3816880454443244
maxi score, test score, baseline:  0.6031 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.709]
 [0.706]
 [0.701]
 [0.692]] [[-0.147]
 [-0.237]
 [-0.147]
 [-0.278]
 [ 0.061]] [[0.865]
 [0.809]
 [0.865]
 [0.775]
 [0.987]]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.993]] [[1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]] [[1.092]
 [1.092]
 [1.092]
 [1.092]
 [1.092]]
siam score:  -0.7904938
maxi score, test score, baseline:  0.6051 1.0 1.0
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.977]
 [0.951]
 [0.951]
 [0.925]] [[0.539]
 [0.464]
 [0.539]
 [0.539]
 [0.586]] [[0.951]
 [0.977]
 [0.951]
 [0.951]
 [0.925]]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6031 1.0 1.0
maxi score, test score, baseline:  0.6031 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.07852369414247058
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.079023195675313
Printing some Q and Qe and total Qs values:  [[0.971]
 [1.117]
 [0.971]
 [0.971]
 [0.971]] [[0.216]
 [0.476]
 [0.216]
 [0.216]
 [0.216]] [[0.793]
 [0.982]
 [0.793]
 [0.793]
 [0.793]]
Printing some Q and Qe and total Qs values:  [[0.993]
 [1.235]
 [0.993]
 [0.993]
 [0.993]] [[0.015]
 [0.409]
 [0.015]
 [0.015]
 [0.015]] [[0.846]
 [1.153]
 [0.846]
 [0.846]
 [0.846]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]] [[1.342]
 [1.342]
 [1.342]
 [1.342]
 [1.342]] [[1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]]
first move QE:  -0.07910235828241148
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.04 ]
 [1.315]
 [1.04 ]
 [1.04 ]
 [1.04 ]] [[1.611]
 [1.792]
 [1.611]
 [1.611]
 [1.611]] [[1.02 ]
 [1.244]
 [1.02 ]
 [1.02 ]
 [1.02 ]]
Printing some Q and Qe and total Qs values:  [[1.263]
 [1.406]
 [1.263]
 [1.263]
 [1.177]] [[1.25 ]
 [1.018]
 [1.25 ]
 [1.25 ]
 [1.454]] [[1.076]
 [1.104]
 [1.076]
 [1.076]
 [1.093]]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.984]
 [1.   ]
 [1.   ]
 [0.984]
 [1.   ]] [[0.72 ]
 [0.587]
 [0.044]
 [0.72 ]
 [0.381]] [[0.984]
 [1.   ]
 [1.   ]
 [0.984]
 [1.   ]]
siam score:  -0.7821671
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6051 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6051 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]] [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.07900423858027755
actor:  0 policy actor:  0  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
actor:  0 policy actor:  0  step number:  39 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6041 1.0 1.0
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.803]
 [0.023]
 [0.624]
 [0.707]] [[ 0.   ]
 [-0.95 ]
 [ 0.059]
 [ 0.044]
 [ 0.   ]] [[0.707]
 [0.803]
 [0.023]
 [0.624]
 [0.707]]
Printing some Q and Qe and total Qs values:  [[1.]
 [1.]
 [1.]
 [1.]
 [1.]] [[ 0.342]
 [ 0.   ]
 [-0.065]
 [-0.048]
 [-0.016]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.877]
 [0.799]
 [0.799]
 [0.799]] [[0.   ]
 [0.159]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.799]
 [0.877]
 [0.799]
 [0.799]
 [0.799]]
maxi score, test score, baseline:  0.6011 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.0006140629443937939
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6211 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]] [[1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]] [[1.087]
 [1.087]
 [1.087]
 [1.087]
 [1.087]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6211 1.0 1.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6211 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6211 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.965]] [[0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[0.73]
 [0.73]
 [0.73]
 [0.73]
 [0.73]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.78169626
maxi score, test score, baseline:  0.6211 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6211 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6211 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7781464
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.7779361
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6231 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6231 1.0 1.0
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6251 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6251 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.952]
 [0.984]
 [0.952]
 [0.952]
 [0.952]] [[0.307]
 [0.379]
 [0.307]
 [0.307]
 [0.307]] [[0.952]
 [0.984]
 [0.952]
 [0.952]
 [0.952]]
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.245]
 [0.249]
 [0.245]
 [0.247]] [[-1.006]
 [-1.306]
 [-0.692]
 [-1.306]
 [-1.004]] [[0.245]
 [0.245]
 [0.249]
 [0.245]
 [0.247]]
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.77624506
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6231 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7850102
maxi score, test score, baseline:  0.6271 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08537699216011553
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6271 1.0 1.0
maxi score, test score, baseline:  0.6271 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6291 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6291 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6291 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6291 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6291 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6291 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6291 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6291 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6291 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.299]
 [0.3  ]
 [0.269]
 [0.269]] [[ 0.   ]
 [-1.041]
 [-1.278]
 [ 0.   ]
 [ 0.   ]] [[0.269]
 [0.299]
 [0.3  ]
 [0.269]
 [0.269]]
maxi score, test score, baseline:  0.6291 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7903867
maxi score, test score, baseline:  0.6311 1.0 1.0
maxi score, test score, baseline:  0.6311 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6311 1.0 1.0
maxi score, test score, baseline:  0.6311 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]] [[0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]] [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]]
maxi score, test score, baseline:  0.6311 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6331 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6351 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]] [[-0.347]
 [-0.347]
 [-0.347]
 [-0.347]
 [-0.347]] [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7784221
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6361 1.0 1.0
maxi score, test score, baseline:  0.6361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6341 1.0 1.0
actor:  0 policy actor:  0  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6331 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6371 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.919]
 [1.05 ]
 [0.919]
 [0.919]
 [0.986]] [[0.279]
 [0.511]
 [0.279]
 [0.279]
 [0.604]] [[0.786]
 [1.021]
 [0.786]
 [0.786]
 [1.02 ]]
maxi score, test score, baseline:  0.6381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6391 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.865]
 [1.006]
 [0.865]
 [0.865]
 [0.865]] [[-0.45 ]
 [ 0.417]
 [-0.45 ]
 [-0.45 ]
 [-0.45 ]] [[0.714]
 [1.109]
 [0.714]
 [0.714]
 [0.714]]
maxi score, test score, baseline:  0.6391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6391 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.778]
 [0.636]
 [0.636]
 [0.636]] [[0.646]
 [0.502]
 [0.646]
 [0.646]
 [0.646]] [[0.636]
 [0.778]
 [0.636]
 [0.636]
 [0.636]]
Printing some Q and Qe and total Qs values:  [[1.   ]
 [0.999]
 [1.   ]
 [0.905]
 [1.   ]] [[0.522]
 [0.987]
 [0.932]
 [0.2  ]
 [1.157]] [[1.   ]
 [0.999]
 [1.   ]
 [0.905]
 [1.   ]]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.742]
 [0.742]
 [0.962]
 [0.742]
 [0.742]] [[-1.158]
 [-1.158]
 [-0.136]
 [-1.158]
 [-1.158]] [[0.551]
 [0.551]
 [1.057]
 [0.551]
 [0.551]]
maxi score, test score, baseline:  0.6401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.78935975
maxi score, test score, baseline:  0.6401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6401 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]] [[1.57]
 [1.57]
 [1.57]
 [1.57]
 [1.57]] [[0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]]
siam score:  -0.7914995
maxi score, test score, baseline:  0.6401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.704]
 [0.625]
 [0.625]
 [0.625]] [[0.08 ]
 [0.184]
 [0.08 ]
 [0.08 ]
 [0.08 ]] [[0.625]
 [0.704]
 [0.625]
 [0.625]
 [0.625]]
Printing some Q and Qe and total Qs values:  [[0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]] [[0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]] [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.514]
 [0.404]
 [0.49 ]
 [0.476]] [[-0.275]
 [ 0.158]
 [-0.464]
 [-0.275]
 [-0.377]] [[0.49 ]
 [0.514]
 [0.404]
 [0.49 ]
 [0.476]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6391 1.0 1.0
maxi score, test score, baseline:  0.6391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.012984550486669608
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
maxi score, test score, baseline:  0.6451 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.688]
 [0.625]
 [0.753]
 [0.698]] [[-0.565]
 [-0.255]
 [-0.577]
 [-0.565]
 [-0.5  ]] [[0.457]
 [0.443]
 [0.327]
 [0.457]
 [0.412]]
siam score:  -0.7797283
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.348]
 [1.43 ]
 [1.348]
 [1.348]
 [1.34 ]] [[0.249]
 [0.148]
 [0.249]
 [0.249]
 [0.441]] [[1.26 ]
 [1.319]
 [1.26 ]
 [1.26 ]
 [1.281]]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.77913195
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.721]
 [0.904]
 [0.719]
 [0.72 ]] [[-0.544]
 [-0.248]
 [ 1.152]
 [-0.691]
 [-0.666]] [[0.463]
 [0.537]
 [1.025]
 [0.425]
 [0.432]]
siam score:  -0.78182375
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.023]
 [1.087]
 [1.023]
 [1.023]
 [1.023]] [[2.543]
 [3.537]
 [2.543]
 [2.543]
 [2.543]] [[0.872]
 [1.2  ]
 [0.872]
 [0.872]
 [0.872]]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7611931
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.482]
 [1.092]
 [1.092]
 [1.092]
 [1.027]] [[2.64 ]
 [0.555]
 [0.555]
 [0.555]
 [0.67 ]] [[0.735]
 [0.996]
 [0.996]
 [0.996]
 [0.951]]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.751379
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.858]
 [1.076]
 [0.858]
 [0.858]
 [0.872]] [[2.725]
 [0.556]
 [2.725]
 [2.725]
 [1.575]] [[1.055]
 [0.607]
 [1.055]
 [1.055]
 [0.765]]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.947]
 [0.832]
 [0.832]
 [0.826]] [[0.127]
 [0.012]
 [0.127]
 [0.127]
 [0.319]] [[0.832]
 [0.947]
 [0.832]
 [0.832]
 [0.826]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.903]
 [0.786]
 [0.786]
 [0.702]] [[-0.131]
 [-0.346]
 [ 0.   ]
 [ 0.   ]
 [ 0.231]] [[0.015]
 [0.903]
 [0.786]
 [0.786]
 [0.702]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.901]
 [0.786]
 [0.786]
 [0.726]] [[-0.131]
 [-0.404]
 [ 0.   ]
 [ 0.   ]
 [ 0.105]] [[0.015]
 [0.901]
 [0.786]
 [0.786]
 [0.726]]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.77 ]
 [0.663]
 [0.663]
 [0.663]] [[-0.372]
 [ 0.256]
 [-0.372]
 [-0.372]
 [-0.372]] [[0.639]
 [1.146]
 [0.639]
 [0.639]
 [0.639]]
siam score:  -0.75178766
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.061]
 [0.942]
 [0.686]
 [0.701]] [[-0.459]
 [-0.079]
 [-0.716]
 [-0.744]
 [-0.671]] [[0.674]
 [0.061]
 [0.73 ]
 [0.465]
 [0.504]]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.3586862117764138
siam score:  -0.7491069
line 256 mcts: sample exp_bonus 1.9800379333194118
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  16 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
first move QE:  -0.08886174150261117
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.964]
 [0.917]
 [0.917]
 [0.896]] [[1.46 ]
 [0.344]
 [1.46 ]
 [1.46 ]
 [1.468]] [[0.917]
 [0.964]
 [0.917]
 [0.917]
 [0.896]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.75645804
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
first move QE:  -0.09089780122032465
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
maxi score, test score, baseline:  0.6461 1.0 1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.833 0.042 0.042 0.042]
maxi score, test score, baseline:  0.6501 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6511 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.997]
 [1.019]
 [0.997]
 [0.997]
 [0.998]] [[0.92 ]
 [0.828]
 [0.92 ]
 [0.92 ]
 [1.091]] [[1.099]
 [1.081]
 [1.099]
 [1.099]
 [1.168]]
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0927450681985963
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
maxi score, test score, baseline:  0.6461 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.75977063
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.815]
 [0.887]
 [0.757]
 [0.759]] [[-1.337]
 [-0.861]
 [-1.094]
 [-1.307]
 [-1.129]] [[0.411]
 [0.65 ]
 [0.614]
 [0.424]
 [0.498]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6481 1.0 1.0
siam score:  -0.7575214
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.043]
 [1.01 ]
 [0.027]
 [0.749]
 [0.94 ]] [[0.332]
 [1.35 ]
 [0.654]
 [3.414]
 [1.996]] [[0.006]
 [0.636]
 [0.082]
 [1.064]
 [0.775]]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6481 1.0 1.0
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75398445
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.5429790417289734
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6491 1.0 1.0
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6491 1.0 1.0
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.882]
 [0.876]
 [0.882]
 [0.882]
 [0.882]] [[-0.489]
 [ 0.61 ]
 [-0.489]
 [-0.489]
 [-0.489]] [[0.664]
 [1.109]
 [0.664]
 [0.664]
 [0.664]]
siam score:  -0.7539995
maxi score, test score, baseline:  0.6501 1.0 1.0
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.2467767969368229
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.797]
 [0.662]
 [0.662]
 [0.662]] [[-0.123]
 [-0.164]
 [-0.123]
 [-0.123]
 [-0.123]] [[0.662]
 [0.797]
 [0.662]
 [0.662]
 [0.662]]
Printing some Q and Qe and total Qs values:  [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]] [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.974]
 [0.853]
 [0.853]
 [0.853]] [[-0.324]
 [-0.105]
 [-0.324]
 [-0.324]
 [-0.324]] [[0.865]
 [1.058]
 [0.865]
 [0.865]
 [0.865]]
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6541 1.0 1.0
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6561 1.0 1.0
maxi score, test score, baseline:  0.6561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[1.901]
 [1.146]
 [1.146]
 [1.146]
 [1.146]] [[0.853]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.629]
 [0.43 ]
 [0.523]
 [0.274]] [[0.005]
 [1.691]
 [0.113]
 [0.005]
 [0.292]] [[0.523]
 [0.629]
 [0.43 ]
 [0.523]
 [0.274]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.748]
 [0.815]
 [0.748]
 [0.748]] [[-0.902]
 [-0.902]
 [-0.575]
 [-0.902]
 [-0.902]] [[0.69 ]
 [0.69 ]
 [0.865]
 [0.69 ]
 [0.69 ]]
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.09771541247318398
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6551 1.0 1.0
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.041]
 [1.271]
 [1.041]
 [1.041]
 [1.041]] [[-0.278]
 [-0.755]
 [-0.278]
 [-0.278]
 [-0.278]] [[0.774]
 [0.924]
 [0.774]
 [0.774]
 [0.774]]
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.963]
 [1.041]
 [0.963]
 [0.963]
 [0.913]] [[1.44 ]
 [1.317]
 [1.44 ]
 [1.44 ]
 [1.398]] [[1.17 ]
 [1.168]
 [1.17 ]
 [1.17 ]
 [1.115]]
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.204]
 [1.453]
 [1.204]
 [1.204]
 [1.204]] [[0.   ]
 [0.194]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.706]
 [0.987]
 [0.706]
 [0.706]
 [0.706]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6551 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]] [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]] [[0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]]
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6551 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.321]] [[0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[1.437]
 [1.437]
 [1.437]
 [1.437]
 [1.437]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.347]
 [0.233]
 [0.28 ]
 [0.273]] [[-0.401]
 [ 0.329]
 [-0.272]
 [-0.401]
 [-0.273]] [[0.28 ]
 [0.347]
 [0.233]
 [0.28 ]
 [0.273]]
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6541 1.0 1.0
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.809]
 [0.949]
 [0.809]
 [0.809]
 [0.809]] [[-1.23 ]
 [-0.268]
 [-1.23 ]
 [-1.23 ]
 [-1.23 ]] [[0.727]
 [1.028]
 [0.727]
 [0.727]
 [0.727]]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6551 1.0 1.0
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.93 ]
 [1.083]
 [0.93 ]
 [0.93 ]
 [0.93 ]] [[0.066]
 [0.014]
 [0.066]
 [0.066]
 [0.066]] [[0.988]
 [1.124]
 [0.988]
 [0.988]
 [0.988]]
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [1.414]
 [0.793]
 [1.128]
 [1.283]] [[0.04 ]
 [0.159]
 [0.007]
 [0.221]
 [0.462]] [[0.003]
 [1.168]
 [0.626]
 [0.953]
 [1.143]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.6551 1.0 1.0
maxi score, test score, baseline:  0.6551 1.0 1.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.042 0.625 0.    0.    0.333]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6521 1.0 1.0
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.159]
 [1.241]
 [1.159]
 [1.159]
 [1.159]] [[0.101]
 [0.262]
 [0.101]
 [0.101]
 [0.101]] [[1.211]
 [1.321]
 [1.211]
 [1.211]
 [1.211]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.94 ]
 [1.008]
 [0.94 ]
 [0.94 ]
 [0.94 ]] [[-1.103]
 [-0.148]
 [-1.103]
 [-1.103]
 [-1.103]] [[0.818]
 [1.047]
 [0.818]
 [0.818]
 [0.818]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6471 1.0 1.0
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6471 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6451 1.0 1.0
maxi score, test score, baseline:  0.6451 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6431 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7533374
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  10 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
siam score:  -0.75331664
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6431 1.0 1.0
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.908]
 [1.016]
 [0.908]
 [0.908]
 [0.908]] [[0.516]
 [0.664]
 [0.516]
 [0.516]
 [0.516]] [[1.011]
 [1.145]
 [1.011]
 [1.011]
 [1.011]]
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.75286996
Printing some Q and Qe and total Qs values:  [[1.035]
 [1.202]
 [0.807]
 [0.469]
 [1.045]] [[2.836]
 [1.301]
 [1.316]
 [2.015]
 [1.948]] [[1.302]
 [0.758]
 [0.407]
 [0.418]
 [0.909]]
maxi score, test score, baseline:  0.6431 1.0 1.0
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7505341
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7502956
maxi score, test score, baseline:  0.6431 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6411 1.0 1.0
maxi score, test score, baseline:  0.6411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.114]
 [1.219]
 [1.114]
 [1.114]
 [1.114]] [[-0.296]
 [ 0.195]
 [-0.296]
 [-0.296]
 [-0.296]] [[0.847]
 [1.035]
 [0.847]
 [0.847]
 [0.847]]
Printing some Q and Qe and total Qs values:  [[1.056]
 [1.056]
 [1.056]
 [1.056]
 [1.056]] [[0]
 [0]
 [0]
 [0]
 [0]] [[1.056]
 [1.056]
 [1.056]
 [1.056]
 [1.056]]
maxi score, test score, baseline:  0.6411 1.0 1.0
maxi score, test score, baseline:  0.6411 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6391 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6371 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6371 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6371 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6371 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5  reward:  0.5 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.971]
 [0.913]
 [0.913]
 [0.913]] [[0.657]
 [1.018]
 [0.657]
 [0.657]
 [0.657]] [[0.686]
 [0.925]
 [0.686]
 [0.686]
 [0.686]]
maxi score, test score, baseline:  0.6371 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6371 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Starting evaluation
maxi score, test score, baseline:  0.6371 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6371 1.0 1.0
maxi score, test score, baseline:  0.6371 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.95 ]
 [0.853]
 [0.853]
 [0.792]] [[-0.045]
 [-0.06 ]
 [-0.045]
 [-0.045]
 [-0.152]] [[0.853]
 [0.95 ]
 [0.853]
 [0.853]
 [0.792]]
line 256 mcts: sample exp_bonus 0.5338364843926775
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.74110365
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7399774
UNIT TEST: sample policy line 217 mcts : [0.042 0.5   0.042 0.083 0.333]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.928]
 [0.99 ]
 [0.928]
 [0.928]
 [0.928]] [[ 0.257]
 [-0.079]
 [ 0.257]
 [ 0.257]
 [ 0.257]] [[0.928]
 [0.99 ]
 [0.928]
 [0.928]
 [0.928]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6531 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.134]
 [1.134]
 [1.134]
 [1.134]
 [1.134]] [[0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]] [[0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]]
maxi score, test score, baseline:  0.6531 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6531 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6531 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74661285
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [0.991]
 [0.95 ]
 [0.95 ]
 [0.95 ]] [[0.693]
 [0.31 ]
 [0.693]
 [0.693]
 [0.693]] [[0.95 ]
 [0.991]
 [0.95 ]
 [0.95 ]
 [0.95 ]]
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7446335
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6511 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6491 1.0 1.0
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6491 1.0 1.0
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6491 1.0 1.0
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.04490677757263184
Printing some Q and Qe and total Qs values:  [[1.339]
 [1.273]
 [1.339]
 [1.339]
 [1.32 ]] [[1.126]
 [0.687]
 [1.126]
 [1.126]
 [0.578]] [[1.306]
 [1.138]
 [1.306]
 [1.306]
 [1.146]]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6511 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.131]
 [1.486]
 [1.226]
 [1.226]
 [1.226]] [[0.614]
 [0.249]
 [0.636]
 [0.636]
 [0.636]] [[0.189]
 [1.377]
 [1.25 ]
 [1.25 ]
 [1.25 ]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.792 0.042 0.083]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.79 ]
 [0.553]
 [0.553]
 [0.553]] [[0.213]
 [0.069]
 [0.213]
 [0.213]
 [0.213]] [[0.553]
 [0.79 ]
 [0.553]
 [0.553]
 [0.553]]
maxi score, test score, baseline:  0.6491 1.0 1.0
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6481 1.0 1.0
siam score:  -0.72634536
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  13 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6491 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.947]
 [1.172]
 [0.947]
 [0.928]
 [0.947]] [[ 0.   ]
 [-0.531]
 [ 0.   ]
 [ 0.907]
 [ 0.   ]] [[0.64 ]
 [0.777]
 [0.64 ]
 [0.773]
 [0.64 ]]
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72817093
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6511 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6511 1.0 1.0
siam score:  -0.7347728
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.73454684
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.962]
 [0.99 ]
 [0.962]
 [0.962]
 [0.962]] [[-0.085]
 [ 0.189]
 [-0.085]
 [-0.085]
 [-0.085]] [[1.005]
 [1.129]
 [1.005]
 [1.005]
 [1.005]]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6541 1.0 1.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6541 1.0 1.0
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.022]
 [1.154]
 [1.054]
 [0.933]
 [0.992]] [[-0.384]
 [ 0.327]
 [ 0.568]
 [ 0.348]
 [ 0.631]] [[0.005]
 [1.033]
 [1.046]
 [0.887]
 [1.026]]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.7740],
        [0.9276],
        [0.7573],
        [0.8704],
        [0.0000],
        [0.5537],
        [0.0000],
        [0.4682],
        [0.8704]], dtype=torch.float64)
0.0 0.0
0.0 0.7740319176005832
0.0 0.9275822513406682
0.0 0.7573165058320508
0.0 0.8703697114216408
0.0 0.0
0.0 0.5536784779472076
0.9509900498999999 0.9509900498999999
0.0 0.46817377638634067
0.0 0.8703697114216408
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.057]
 [1.111]
 [1.057]
 [1.057]
 [1.057]] [[-0.338]
 [ 0.124]
 [-0.338]
 [-0.338]
 [-0.338]] [[1.241]
 [1.438]
 [1.241]
 [1.241]
 [1.241]]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6541 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 0.5285854120954633
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6551 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6571 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6571 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6571 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6571 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6571 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6571 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6571 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.12567434014505444
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6591 1.0 1.0
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6571 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.6571 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6571 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.19616543924121915
maxi score, test score, baseline:  0.6571 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.858]
 [0.935]
 [0.784]
 [0.813]
 [0.826]] [[-1.448]
 [-0.577]
 [-1.176]
 [-1.366]
 [-1.299]] [[0.512]
 [1.11 ]
 [0.61 ]
 [0.521]
 [0.574]]
maxi score, test score, baseline:  0.6581 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.172]
 [1.3  ]
 [1.172]
 [1.172]
 [1.152]] [[0.534]
 [0.186]
 [0.534]
 [0.534]
 [0.631]] [[1.096]
 [1.107]
 [1.096]
 [1.096]
 [1.108]]
maxi score, test score, baseline:  0.6581 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6591 1.0 1.0
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6571 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6581 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.753]
 [0.852]
 [0.753]
 [0.753]] [[-0.983]
 [-0.983]
 [-0.219]
 [-0.983]
 [-0.983]] [[0.53 ]
 [0.53 ]
 [0.908]
 [0.53 ]
 [0.53 ]]
maxi score, test score, baseline:  0.6581 1.0 1.0
maxi score, test score, baseline:  0.6581 1.0 1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6581 1.0 1.0
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6581 1.0 1.0
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7565039
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6621 1.0 1.0
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  54 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.23]
 [1.3 ]
 [1.23]
 [1.23]
 [1.23]] [[0.081]
 [0.29 ]
 [0.081]
 [0.081]
 [0.081]] [[0.832]
 [0.937]
 [0.832]
 [0.832]
 [0.832]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.835]
 [0.776]
 [0.835]
 [0.766]] [[ 2.346]
 [ 0.   ]
 [-0.779]
 [ 0.   ]
 [-1.019]] [[1.106]
 [0.663]
 [0.44 ]
 [0.663]
 [0.378]]
maxi score, test score, baseline:  0.6661 1.0 1.0
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6641 1.0 1.0
siam score:  -0.75031114
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.047]
 [0.02 ]
 [0.69 ]
 [0.717]] [[ 3.052]
 [ 0.994]
 [-0.276]
 [ 2.545]
 [ 1.584]] [[0.748]
 [0.047]
 [0.02 ]
 [0.69 ]
 [0.717]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.017]
 [1.183]
 [0.231]
 [1.009]
 [1.057]] [[1.089]
 [1.283]
 [0.531]
 [1.249]
 [1.547]] [[0.272]
 [1.159]
 [0.164]
 [1.025]
 [1.194]]
Printing some Q and Qe and total Qs values:  [[1.206]
 [1.038]
 [1.038]
 [1.13 ]
 [1.168]] [[0.554]
 [0.671]
 [0.671]
 [1.287]
 [1.027]] [[0.861]
 [0.733]
 [0.733]
 [1.029]
 [0.981]]
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6611 1.0 1.0
maxi score, test score, baseline:  0.6611 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6611 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6611 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6611 1.0 1.0
maxi score, test score, baseline:  0.6611 1.0 1.0
maxi score, test score, baseline:  0.6611 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73831314
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6611 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6611 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6611 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6611 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.337]
 [0.282]
 [0.282]
 [0.282]] [[-0.576]
 [-0.278]
 [-0.576]
 [-0.576]
 [-0.576]] [[0.282]
 [0.337]
 [0.282]
 [0.282]
 [0.282]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.091]
 [1.231]
 [0.088]
 [0.976]
 [1.079]] [[1.798]
 [1.647]
 [1.036]
 [2.02 ]
 [2.042]] [[0.353]
 [1.031]
 [0.019]
 [1.027]
 [1.104]]
maxi score, test score, baseline:  0.6611 1.0 1.0
maxi score, test score, baseline:  0.6611 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.263]
 [0.288]
 [0.273]
 [0.274]] [[ 0.   ]
 [-0.197]
 [-0.057]
 [-0.517]
 [-0.49 ]] [[0.251]
 [0.263]
 [0.288]
 [0.273]
 [0.274]]
maxi score, test score, baseline:  0.6611 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7320514
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.268]
 [1.331]
 [1.268]
 [1.268]
 [1.268]] [[0.075]
 [0.007]
 [0.075]
 [0.075]
 [0.075]] [[1.309]
 [1.361]
 [1.309]
 [1.309]
 [1.309]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6631 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6631 1.0 1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7326324
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.942]
 [0.942]
 [0.931]
 [0.942]
 [0.841]] [[0.01 ]
 [0.01 ]
 [0.353]
 [0.01 ]
 [0.301]] [[0.852]
 [0.852]
 [1.013]
 [0.852]
 [0.896]]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.]
 [1.]
 [1.]
 [1.]
 [1.]] [[0.083]
 [0.212]
 [0.437]
 [0.66 ]
 [0.064]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7262619
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6621 1.0 1.0
line 256 mcts: sample exp_bonus 5.823598433479452
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.025]
 [0.793]
 [0.904]
 [0.755]] [[1.115]
 [1.469]
 [0.575]
 [5.181]
 [1.276]] [[0.285]
 [0.148]
 [0.196]
 [1.086]
 [0.316]]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.925]
 [1.091]
 [0.925]
 [0.925]
 [0.925]] [[1.307]
 [1.39 ]
 [1.307]
 [1.307]
 [1.307]] [[0.861]
 [0.962]
 [0.861]
 [0.861]
 [0.861]]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
maxi score, test score, baseline:  0.6621 1.0 1.0
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.922]
 [1.07 ]
 [0.922]
 [0.922]
 [0.922]] [[0.19 ]
 [1.164]
 [0.19 ]
 [0.19 ]
 [0.19 ]] [[0.553]
 [0.997]
 [0.553]
 [0.553]
 [0.553]]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6661 1.0 1.0
siam score:  -0.7368464
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6681 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6681 1.0 1.0
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.133]] [[0.92]
 [0.92]
 [0.92]
 [0.92]
 [0.92]] [[1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]]
siam score:  -0.73833114
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.837]
 [0.816]
 [0.816]
 [0.816]] [[1.175]
 [1.288]
 [1.175]
 [1.175]
 [1.175]] [[0.837]
 [0.891]
 [0.837]
 [0.837]
 [0.837]]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7356292
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  15 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.7287007
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  11 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
siam score:  -0.7272055
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.817]
 [1.015]
 [0.817]
 [0.817]
 [0.817]] [[0.671]
 [0.471]
 [0.671]
 [0.671]
 [0.671]] [[0.877]
 [0.951]
 [0.877]
 [0.877]
 [0.877]]
maxi score, test score, baseline:  0.6691 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6691 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6711 1.0 1.0
maxi score, test score, baseline:  0.6711 1.0 1.0
siam score:  -0.722271
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.72313124
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6711 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  8 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  0.5  reward:  0.5 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6721 1.0 1.0
siam score:  -0.72513354
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6721 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [0.912]
 [0.951]
 [0.92 ]
 [0.92 ]] [[-0.652]
 [-0.325]
 [ 0.231]
 [-0.652]
 [-0.652]] [[0.806]
 [0.907]
 [1.131]
 [0.806]
 [0.806]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6721 1.0 1.0
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  14 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6711 1.0 1.0
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6671 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6671 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.6671 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.269]
 [0.332]
 [0.253]
 [0.256]] [[0.114]
 [0.388]
 [0.553]
 [0.153]
 [0.257]] [[0.256]
 [0.269]
 [0.332]
 [0.253]
 [0.256]]
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.932]
 [0.863]
 [0.863]
 [0.863]] [[1.923]
 [0.93 ]
 [1.923]
 [1.923]
 [1.923]] [[0.863]
 [0.932]
 [0.863]
 [0.863]
 [0.863]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6671 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7289668
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6651 1.0 1.0
maxi score, test score, baseline:  0.6651 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6651 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6631 1.0 1.0
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6631 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7307354
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6591 1.0 1.0
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6591 1.0 1.0
Starting evaluation
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.864]
 [0.771]
 [0.771]
 [0.771]] [[0.026]
 [0.063]
 [0.026]
 [0.026]
 [0.026]] [[0.771]
 [0.864]
 [0.771]
 [0.771]
 [0.771]]
maxi score, test score, baseline:  0.6591 1.0 1.0
maxi score, test score, baseline:  0.6591 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.894]
 [0.935]
 [0.894]
 [0.894]
 [0.894]] [[0.35]
 [0.4 ]
 [0.35]
 [0.35]
 [0.35]] [[0.894]
 [0.935]
 [0.894]
 [0.894]
 [0.894]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7205167
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.213]
 [1.179]
 [1.179]
 [1.179]
 [1.179]] [[2.231]
 [1.733]
 [1.733]
 [1.733]
 [1.733]] [[1.151]
 [0.999]
 [0.999]
 [0.999]
 [0.999]]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7148039
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
first move QE:  -0.12539703404402192
start point for exploration sampling:  16339
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6024346653561835
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.153]
 [1.151]
 [1.153]
 [1.153]
 [1.153]] [[0.053]
 [0.008]
 [0.053]
 [0.053]
 [0.053]] [[1.278]
 [1.268]
 [1.278]
 [1.278]
 [1.278]]
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  9 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]] [[0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]] [[0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7323101
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.    0.708 0.042 0.    0.25 ]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6701 1.0 1.0
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.71155196
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  16339
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7167808
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.792]
 [0.027]
 [0.61 ]
 [0.7  ]] [[-0.165]
 [-0.49 ]
 [-0.215]
 [-0.121]
 [-0.237]] [[0.003]
 [0.733]
 [0.014]
 [0.612]
 [0.683]]
maxi score, test score, baseline:  0.6701 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.168]
 [1.29 ]
 [1.168]
 [1.168]
 [1.184]] [[0.515]
 [0.019]
 [0.515]
 [0.515]
 [0.864]] [[0.887]
 [0.844]
 [0.887]
 [0.887]
 [1.019]]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.12623378003493665
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.964]
 [0.838]
 [1.087]
 [0.804]
 [0.804]] [[ 0.   ]
 [-0.246]
 [-0.359]
 [-0.455]
 [-0.176]] [[0.955]
 [0.747]
 [0.957]
 [0.642]
 [0.736]]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.5830656594869318
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6721 1.0 1.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.12727229408998345
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6721 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6721 1.0 1.0
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6721 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.089]
 [1.07 ]
 [0.862]
 [0.846]] [[-1.067]
 [-0.258]
 [-0.438]
 [-0.884]
 [-0.86 ]] [[0.619]
 [0.03 ]
 [0.981]
 [0.699]
 [0.687]]
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6741 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.999]
 [0.872]
 [0.872]
 [0.872]] [[0.03]
 [0.57]
 [0.03]
 [0.03]
 [0.03]] [[0.531]
 [0.927]
 [0.531]
 [0.531]
 [0.531]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5  reward:  0.5 rdn_beta:  0.5
siam score:  -0.71854514
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6721 1.0 1.0
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.1986655894312392
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.084]
 [1.246]
 [1.084]
 [1.084]
 [1.068]] [[1.03 ]
 [0.973]
 [1.03 ]
 [1.03 ]
 [1.035]] [[1.092]
 [1.202]
 [1.092]
 [1.092]
 [1.082]]
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6721 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.23 ]
 [1.36 ]
 [1.23 ]
 [1.23 ]
 [1.307]] [[1.018]
 [0.796]
 [1.018]
 [1.018]
 [1.114]] [[1.247]
 [1.294]
 [1.247]
 [1.247]
 [1.339]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.6701 1.0 1.0
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  7 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.6691 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.728009
maxi score, test score, baseline:  0.6691 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.275]
 [1.275]
 [1.275]
 [1.275]
 [1.275]] [[2.208]
 [2.208]
 [2.208]
 [2.208]
 [2.208]] [[1.276]
 [1.276]
 [1.276]
 [1.276]
 [1.276]]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.038]
 [0.046]
 [0.032]
 [0.041]] [[-0.005]
 [-0.075]
 [-0.092]
 [-0.08 ]
 [-0.101]] [[0.052]
 [0.032]
 [0.032]
 [0.024]
 [0.023]]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.208 0.042 0.542 0.042 0.167]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6701 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  12 total reward:  0.5  reward:  0.5 rdn_beta:  0.167
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  16339
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7326933
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.13045090078788185
siam score:  -0.7322557
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.8062753720503734
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6701 1.0 1.0
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.495]
 [0.538]
 [0.516]
 [0.516]] [[-1.457]
 [-0.728]
 [-0.707]
 [-1.457]
 [-1.457]] [[0.516]
 [0.495]
 [0.538]
 [0.516]
 [0.516]]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.12195091635176669
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7223276
siam score:  -0.71805435
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.081]
 [1.215]
 [1.081]
 [0.428]
 [1.203]] [[1.142]
 [1.555]
 [1.142]
 [1.165]
 [1.55 ]] [[0.888]
 [1.279]
 [0.888]
 [0.277]
 [1.264]]
maxi score, test score, baseline:  0.6691 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6711 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
