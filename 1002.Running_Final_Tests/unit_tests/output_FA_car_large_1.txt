dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
res_block_channels:[32, 64, 64]
res_block_ds:[False, False, False]
reward_support:[-1, 1, 41]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 41]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
channels:3
env_size:[8, 60]
observable_size:[8, 10]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.
  0. 0. 0. 0. 0. 0. 2. 2. 2. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.
  1. 1. 1. 1. 2. 1. 1. 1. 0. 0. 2. 2. 2. 1. 1. 0. 0. 0. 2. 1. 1. 1. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 2. 1. 1. 1. 2. 2. 2. 2. 2.
  1. 1. 1. 1. 2. 2. 2. 1. 1. 1. 2. 2. 2. 0. 1. 1. 1. 1. 1. 1. 1. 2. 0. 0.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.
  1. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 0. 2. 2. 1. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.
  1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 2. 2. 1. 2. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2.
  2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:200
actions_size:7
optimal_score:0.86
total_frames:305000
exp_gamma:0.975
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
timesteps_in_obs:2
store_prev_actions:True
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 1, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
state_size:[6, 6]
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 480)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.
  0. 0. 0. 0. 0. 0. 2. 2. 2. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.
  1. 1. 1. 1. 2. 1. 1. 1. 0. 0. 2. 2. 2. 1. 1. 0. 0. 0. 2. 1. 1. 1. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 2. 1. 1. 1. 2. 2. 2. 2. 2.
  1. 1. 1. 1. 2. 2. 2. 1. 1. 1. 2. 2. 2. 0. 1. 1. 1. 1. 1. 1. 1. 2. 0. 0.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.
  1. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 0. 2. 2. 1. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.
  1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 2. 2. 1. 2. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2.
  2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
in main func line 156:  1
in main func line 156:  3
7 6
Starting evaluation
siam score:  -0.009640138342299244
siam score:  -0.019028269390507443
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.05223342002136633
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.11216936855550044, 0.18690852025081414, 0.14019655044124307, 0.14019655044124307, 0.2803324598699562, 0.14019655044124307]
first move QE:  -4.0736398589367374e-05
deleting a thread, now have 2 threads
Frames:  973 train batches done:  27 episodes:  21
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.11216936855550044, 0.18690852025081414, 0.14019655044124307, 0.14019655044124307, 0.2803324598699562, 0.14019655044124307]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.11216936855550044, 0.18690852025081414, 0.14019655044124307, 0.14019655044124307, 0.2803324598699562, 0.14019655044124307]
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.11216936855550044, 0.18690852025081414, 0.14019655044124307, 0.14019655044124307, 0.2803324598699562, 0.14019655044124307]
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.11216936855550044, 0.18690852025081414, 0.14019655044124307, 0.14019655044124307, 0.2803324598699562, 0.14019655044124307]
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.11216936855550044, 0.18690852025081414, 0.14019655044124307, 0.14019655044124307, 0.2803324598699562, 0.14019655044124307]
siam score:  -0.40600616
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
from probs:  [0.11216936855550044, 0.18690852025081414, 0.14019655044124307, 0.14019655044124307, 0.2803324598699562, 0.14019655044124307]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
from probs:  [0.1417141345015471, 0.21252537442958908, 0.1417141345015471, 0.17003863047276388, 0.21252537442958908, 0.1214823516649637]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.  0.  0.2 0.2 0.2]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.17219803350635088, 0.20661542717549725, 0.17219803350635088, 0.17219803350635088, 0.14761418088553208, 0.12917629141991796]
UNIT TEST: sample policy line 217 mcts : [0.  0.2 0.2 0.2 0.  0.2 0.2]
first move QE:  -0.2725635997353433
siam score:  -0.4871307
using another actor
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.2058512663279286, 0.2469950959722537, 0.12356360703927853, 0.17646281658198215, 0.12356360703927853, 0.12356360703927853]
siam score:  -0.4792084
siam score:  -0.48111436
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.21543462232181787, 0.2584939544052959, 0.12931595815486188, 0.16161045721747036, 0.11757250395027696, 0.11757250395027696]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.21543462232181787, 0.2584939544052959, 0.12931595815486188, 0.16161045721747036, 0.11757250395027696, 0.11757250395027696]
siam score:  -0.48739383
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.1871531968014928, 0.24948455444575107, 0.12482183915723456, 0.1871531968014928, 0.13615481327437243, 0.11523239951965634]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
siam score:  -0.5098315
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
from probs:  [0.2098011682820287, 0.2098011682820287, 0.1399269054969752, 0.1678766106109966, 0.15263131691243947, 0.11996283041553132]
using explorer policy with actor:  1
52 25
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.22853098655135146, 0.20316006779222434, 0.14070857546206522, 0.16625691323349392, 0.13067172848043251, 0.13067172848043251]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.22853098655135146, 0.20316006779222434, 0.14070857546206522, 0.16625691323349392, 0.13067172848043251, 0.13067172848043251]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.22853098655135146, 0.20316006779222434, 0.14070857546206522, 0.16625691323349392, 0.13067172848043251, 0.13067172848043251]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.22853098655135146, 0.20316006779222434, 0.14070857546206522, 0.16625691323349392, 0.13067172848043251, 0.13067172848043251]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.2397769388192257, 0.21315749245669274, 0.13710193142088425, 0.1599185997316268, 0.11294310615068628, 0.13710193142088425]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.2442351686292288, 0.21712075053402202, 0.1303546126293603, 0.16289191434360845, 0.11504294123442, 0.1303546126293603]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.2442351686292288, 0.21712075053402202, 0.1303546126293603, 0.16289191434360845, 0.11504294123442, 0.1303546126293603]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
siam score:  -0.53319067
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.21397685752898954, 0.23772710620001886, 0.14272611151590153, 0.16464941798146707, 0.10710073850935752, 0.1338197682642655]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.20978190068121347, 0.25634645876660295, 0.13582642607500664, 0.17754489892979, 0.10501164498908715, 0.11548867055829978]
first move QE:  -0.1481769784911137
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.20978190068121347, 0.25634645876660295, 0.13582642607500664, 0.17754489892979, 0.10501164498908715, 0.11548867055829978]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.20978190068121347, 0.25634645876660295, 0.13582642607500664, 0.17754489892979, 0.10501164498908715, 0.11548867055829978]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
from probs:  [0.20978190068121347, 0.25634645876660295, 0.13582642607500664, 0.17754489892979, 0.10501164498908715, 0.11548867055829978]
from probs:  [0.20978190068121347, 0.25634645876660295, 0.13582642607500664, 0.17754489892979, 0.10501164498908715, 0.11548867055829978]
using explorer policy with actor:  1
from probs:  [0.2097820267829472, 0.2563467210585172, 0.13582633587468895, 0.1775449307460141, 0.1050114646629147, 0.11548852087491794]
from probs:  [0.2097820267829472, 0.2563467210585172, 0.13582633587468895, 0.1775449307460141, 0.1050114646629147, 0.11548852087491794]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.21630717880886136, 0.23791308056941185, 0.1400510549480949, 0.18306732994647598, 0.1035807348407718, 0.11908062088638412]
siam score:  -0.55407834
UNIT TEST: sample policy line 217 mcts : [0.2 0.  0.2 0.  0.2 0.2 0.2]
using another actor
siam score:  -0.5559224
siam score:  -0.5529656
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.2165109090192494, 0.2165109090192494, 0.14443076214410733, 0.18561941750133137, 0.11309156785056729, 0.1238364344654953]
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
siam score:  -0.5470867
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.2165109090192494, 0.2165109090192494, 0.14443076214410733, 0.18561941750133137, 0.11309156785056729, 0.1238364344654953]
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.2165109090192494, 0.2165109090192494, 0.14443076214410733, 0.18561941750133137, 0.11309156785056729, 0.1238364344654953]
from probs:  [0.20325799813086745, 0.22017324435566465, 0.14687384404821002, 0.1887592156524698, 0.11500453956670798, 0.12593115824608012]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.20915705124214326, 0.22656327654564964, 0.14319661851306664, 0.19423742955342357, 0.11342281207285847, 0.11342281207285847]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.1995435640644985, 0.23275255133503572, 0.13976738697753152, 0.1995435640644985, 0.11187183767028026, 0.11652109588815548]
using explorer policy with actor:  1
siam score:  -0.57246214
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.18665940250919108, 0.2296632076456658, 0.13583672371153915, 0.2132808056889135, 0.11957346649629057, 0.11498639394839993]
siam score:  -0.5778884
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.18665940250919108, 0.2296632076456658, 0.13583672371153915, 0.2132808056889135, 0.11957346649629057, 0.11498639394839993]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
main train batch thing paused
add a thread
Adding thread: now have 3 threads
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 4 threads
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.18890034977315148, 0.2324204388908509, 0.13746751717950673, 0.2158413573222035, 0.12100901074954043, 0.10436132608474692]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0019264947321639264
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
from probs:  [0.19821414752198935, 0.2264835230168929, 0.1442453397589917, 0.19821414752198935, 0.12697532127483244, 0.10586752090530445]
siam score:  -0.568748
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.2015418784228257, 0.23028585613738817, 0.14666701187684283, 0.1897061228933, 0.12415424611438831, 0.107644884555255]
from probs:  [0.2015418784228257, 0.23028585613738817, 0.14666701187684283, 0.1897061228933, 0.12415424611438831, 0.107644884555255]
siam score:  -0.56794703
UNIT TEST: sample policy line 217 mcts : [0.286 0.286 0.102 0.061 0.041 0.061 0.163]
Printing some Q and Qe and total Qs values:  [[ 0.14 ]
 [-0.002]
 [ 0.11 ]
 [ 0.135]
 [ 0.118]
 [ 0.096]
 [ 0.129]] [[ 0.029]
 [ 0.081]
 [-0.006]
 [-0.36 ]
 [-0.095]
 [-0.073]
 [-0.   ]] [[ 0.14 ]
 [-0.002]
 [ 0.11 ]
 [ 0.135]
 [ 0.118]
 [ 0.096]
 [ 0.129]]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.19404433210486532, 0.2355521396377721, 0.15002089987299444, 0.18328304867040798, 0.12699325839786202, 0.11010632131609821]
siam score:  -0.5828176
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
main train batch thing paused
add a thread
Adding thread: now have 5 threads
main train batch thing paused
add a thread
from probs:  [0.19404433210486532, 0.2355521396377721, 0.15002089987299444, 0.18328304867040798, 0.12699325839786202, 0.11010632131609821]
Adding thread: now have 6 threads
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.614]
 [0.702]
 [0.667]
 [0.675]
 [0.692]
 [0.693]] [[0.372]
 [0.321]
 [0.001]
 [0.314]
 [0.227]
 [0.074]
 [0.141]] [[-0.019]
 [-0.194]
 [-0.231]
 [-0.094]
 [-0.134]
 [-0.202]
 [-0.156]]
Printing some Q and Qe and total Qs values:  [[1.129]
 [1.129]
 [1.129]
 [1.129]
 [1.129]
 [1.129]
 [1.129]] [[0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]] [[2.355]
 [2.355]
 [2.355]
 [2.355]
 [2.355]
 [2.355]
 [2.355]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.1953154443475287, 0.23709519931241135, 0.14445313395549772, 0.18448365602329989, 0.12782507094271836, 0.11082749541854389]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.18952799113789687, 0.22736306134212841, 0.14840291482894957, 0.18952799113789687, 0.1313201908236945, 0.11385785072943382]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
siam score:  -0.58935434
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.19070440109501954, 0.2287743155196191, 0.14311700806427013, 0.19070440109501954, 0.13213530198025106, 0.11456457224582049]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.002]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.  ]
 [0.07]
 [0.  ]
 [0.  ]
 [0.  ]
 [0.  ]
 [0.  ]] [[-1.689]
 [-1.637]
 [-1.689]
 [-1.689]
 [-1.689]
 [-1.689]
 [-1.689]]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.18760709479960785, 0.20963519984289514, 0.14859899211878663, 0.19800925551449353, 0.1371966236428543, 0.11895283408136251]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.18832750602624637, 0.21044019899085778, 0.149169612234747, 0.19876961103731286, 0.1377234586649241, 0.11556961304591204]
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.194]
 [0.273]
 [0.299]
 [0.3  ]
 [0.298]
 [0.283]] [[-0.052]
 [-0.048]
 [-0.453]
 [-0.405]
 [-0.354]
 [-0.26 ]
 [-0.338]] [[0.295]
 [0.194]
 [0.273]
 [0.299]
 [0.3  ]
 [0.298]
 [0.283]]
siam score:  -0.62394714
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.18832750602624637, 0.21044019899085778, 0.149169612234747, 0.19876961103731286, 0.1377234586649241, 0.11556961304591204]
using explorer policy with actor:  1
main train batch thing paused
using another actor
from probs:  [0.18832750602624637, 0.21044019899085778, 0.149169612234747, 0.19876961103731286, 0.1377234586649241, 0.11556961304591204]
main train batch thing paused
Training Flag: False
Self play flag: True
resampling flag: False
add more workers flag:  True
expV_train_flag:  False
expV_train_start_flag:  305000
main train batch thing paused
line 256 mcts: sample exp_bonus 0.1736480156624442
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.19246189941926123, 0.21506008709432156, 0.14636159656213815, 0.19246189941926123, 0.13554794527466485, 0.11810657223035306]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.218]
 [0.941]
 [1.257]
 [1.231]
 [1.215]
 [1.208]
 [1.205]] [[-0.006]
 [-0.175]
 [-0.205]
 [-0.289]
 [-0.273]
 [-0.221]
 [-0.153]] [[2.367]
 [1.757]
 [2.379]
 [2.3  ]
 [2.273]
 [2.276]
 [2.293]]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.19246193080547655, 0.2150601459767642, 0.14636157185604967, 0.19246193080547655, 0.1355479074111224, 0.11810651314511063]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
using explorer policy with actor:  1
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.327 0.02  0.265 0.082 0.143 0.061 0.102]
siam score:  -0.63201386
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.18463092938219708, 0.21714566537638152, 0.14778089525545468, 0.1943283067839714, 0.1368623666253088, 0.11925183657668637]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.606]
 [0.828]
 [0.838]
 [0.835]
 [0.842]
 [0.774]] [[0.166]
 [0.151]
 [0.034]
 [0.12 ]
 [0.219]
 [0.215]
 [0.285]] [[0.946]
 [0.683]
 [0.886]
 [1.018]
 [1.147]
 [1.152]
 [1.137]]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.18568361361272104, 0.2183837343089988, 0.14292191731758855, 0.19543628118880388, 0.1376426955527574, 0.11993175801913024]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.18568361361272104, 0.2183837343089988, 0.14292191731758855, 0.19543628118880388, 0.1376426955527574, 0.11993175801913024]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]] [[0.668]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[2.359]
 [1.934]
 [1.934]
 [1.934]
 [1.934]
 [1.934]
 [1.934]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.1920458644484311, 0.20213269676974424, 0.14235887560640714, 0.20213269676974424, 0.1372887747041598, 0.12404109170151348]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.1920458644484311, 0.20213269676974424, 0.14235887560640714, 0.20213269676974424, 0.1372887747041598, 0.12404109170151348]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.842]
 [0.808]
 [0.808]
 [0.896]
 [0.808]
 [0.841]
 [0.828]] [[0.574]
 [0.424]
 [0.424]
 [0.244]
 [0.424]
 [0.395]
 [1.186]] [[0.977]
 [0.708]
 [0.708]
 [0.644]
 [0.708]
 [0.735]
 [1.766]]
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.738]
 [0.733]
 [0.737]
 [0.743]
 [0.743]
 [0.743]] [[-0.147]
 [-0.288]
 [-0.355]
 [-0.392]
 [-0.164]
 [-0.13 ]
 [-0.092]] [[0.743]
 [0.738]
 [0.733]
 [0.737]
 [0.743]
 [0.743]
 [0.743]]
main train batch thing paused
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
main train batch thing paused
using explorer policy with actor:  1
first move QE:  -0.06807623319707559
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.18126392476875852, 0.2098196983928728, 0.14777258533306895, 0.189875983480793, 0.14250966056460346, 0.12875814745990324]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
siam score:  -0.6808374
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.18416140523578028, 0.202535820092182, 0.14478765911491956, 0.19291112659597157, 0.14478765911491956, 0.13081632984622704]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.18416140523578028, 0.202535820092182, 0.14478765911491956, 0.19291112659597157, 0.14478765911491956, 0.13081632984622704]
siam score:  -0.6848845
first move QE:  -0.06852031839991642
line 256 mcts: sample exp_bonus 0.06850118447126159
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.367]
 [0.42 ]
 [0.461]
 [0.464]
 [0.455]
 [0.463]] [[-0.03 ]
 [-0.07 ]
 [-0.047]
 [-0.151]
 [-0.076]
 [-0.006]
 [-0.047]] [[0.439]
 [0.367]
 [0.42 ]
 [0.461]
 [0.464]
 [0.455]
 [0.463]]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.403]
 [0.454]
 [0.435]
 [0.426]
 [0.425]
 [0.454]] [[-0.079]
 [-0.079]
 [-0.086]
 [-0.097]
 [-0.072]
 [ 0.011]
 [ 0.008]] [[0.454]
 [0.403]
 [0.454]
 [0.435]
 [0.426]
 [0.425]
 [0.454]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.18508284559312926, 0.20354923593815452, 0.1455120091395037, 0.19387636480504603, 0.14050879993272347, 0.13147074459144303]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]] [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[1.564]
 [1.564]
 [1.564]
 [1.564]
 [1.564]
 [1.564]
 [1.564]]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.18689061442242483, 0.19577002300934107, 0.14693327578130172, 0.19577002300934107, 0.14188119848184938, 0.1327548652957419]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.18689061442242483, 0.19577002300934107, 0.14693327578130172, 0.19577002300934107, 0.14188119848184938, 0.1327548652957419]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.18689061442242483, 0.19577002300934107, 0.14693327578130172, 0.19577002300934107, 0.14188119848184938, 0.1327548652957419]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
using explorer policy with actor:  1
138 57
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.18873409695976728, 0.19770111008284516, 0.14328061664623468, 0.19770111008284516, 0.13851882347053127, 0.13406424275777645]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.18873409695976728, 0.19770111008284516, 0.14328061664623468, 0.19770111008284516, 0.13851882347053127, 0.13406424275777645]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
first move QE:  -0.06582933036310623
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.1896371096543796, 0.19864702613627463, 0.13918157735576758, 0.19864702613627463, 0.13918157735576758, 0.13470568336153588]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
Printing some Q and Qe and total Qs values:  [[1.305]
 [1.312]
 [1.312]
 [1.312]
 [1.312]
 [1.312]
 [1.312]] [[-0.005]
 [ 0.019]
 [ 0.019]
 [ 0.019]
 [ 0.019]
 [ 0.019]
 [ 0.019]] [[1.598]
 [1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.1853839070927235, 0.20299777858041768, 0.14222992194787276, 0.1937905275754866, 0.14222992194787276, 0.13336794285562661]
144 57
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.802]
 [0.8  ]
 [0.804]
 [0.807]
 [0.808]
 [0.804]] [[-0.159]
 [-0.088]
 [-0.27 ]
 [-0.288]
 [-0.309]
 [-0.275]
 [-0.24 ]] [[0.496]
 [0.539]
 [0.474]
 [0.477]
 [0.474]
 [0.487]
 [0.493]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.18782193824709656, 0.2056674542615977, 0.14410042401156872, 0.18782193824709656, 0.1394663464658676, 0.1351218987667728]
siam score:  -0.6871068
using explorer policy with actor:  1
main train batch thing paused
main train batch thing paused
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]]
using explorer policy with actor:  0
using explorer policy with actor:  0
main train batch thing paused
line 256 mcts: sample exp_bonus 0.004725719475211037
main train batch thing paused
line 256 mcts: sample exp_bonus 0.0027564004309772916
using explorer policy with actor:  0
main train batch thing paused
in main func line 156:  147
Printing some Q and Qe and total Qs values:  [[1.38 ]
 [1.363]
 [1.363]
 [1.363]
 [1.359]
 [1.339]
 [1.349]] [[0.001]
 [0.002]
 [0.002]
 [0.002]
 [0.001]
 [0.001]
 [0.   ]] [[2.684]
 [2.651]
 [2.651]
 [2.651]
 [2.642]
 [2.602]
 [2.623]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.262]] [[0.002]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.271]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.262]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.176]
 [0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.141]
 [0.176]
 [0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]]
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.639]
 [0.791]
 [0.819]
 [0.824]
 [0.766]
 [0.77 ]] [[0.194]
 [0.126]
 [0.002]
 [0.014]
 [0.057]
 [0.191]
 [0.023]] [[ 0.361]
 [-0.029]
 [ 0.069]
 [ 0.144]
 [ 0.226]
 [ 0.332]
 [ 0.062]]
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.082]
 [0.139]
 [0.128]
 [0.148]
 [0.125]
 [0.131]] [[-0.005]
 [-0.004]
 [-0.005]
 [-0.005]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.131]
 [0.082]
 [0.139]
 [0.128]
 [0.148]
 [0.125]
 [0.131]]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.073]
 [0.116]
 [0.13 ]
 [0.154]
 [0.128]
 [0.135]] [[-0.006]
 [-0.005]
 [-0.005]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.139]
 [0.073]
 [0.116]
 [0.13 ]
 [0.154]
 [0.128]
 [0.135]]
siam score:  -0.68493575
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]] [[-0.005]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.678]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]]
line 256 mcts: sample exp_bonus -0.055923126464105216
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.115]
 [0.15 ]
 [0.161]
 [0.162]
 [0.16 ]
 [0.164]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.165]
 [0.115]
 [0.15 ]
 [0.161]
 [0.162]
 [0.16 ]
 [0.164]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
line 256 mcts: sample exp_bonus -0.010658237252940437
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.527]
 [0.535]
 [0.537]
 [0.549]
 [0.55 ]
 [0.551]] [[-0.006]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.006]
 [-0.005]
 [-0.005]] [[0.537]
 [0.527]
 [0.535]
 [0.537]
 [0.549]
 [0.55 ]
 [0.551]]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.048]
 [0.046]
 [0.064]
 [0.007]
 [0.003]
 [0.153]] [[0.878]
 [0.864]
 [0.926]
 [0.777]
 [1.069]
 [0.896]
 [0.708]] [[0.251]
 [0.207]
 [0.29 ]
 [0.114]
 [0.423]
 [0.178]
 [0.166]]
maxi score, test score, baseline:  -0.9879952380952381 -1.0 -0.9879952380952381
probs:  [0.18561967575629176, 0.20245280450190162, 0.13512028951946223, 0.1936703025476704, 0.14380835596880928, 0.13932857170586468]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.653]
 [0.624]
 [0.629]
 [0.626]
 [0.637]
 [0.641]] [[ 0.006]
 [-0.004]
 [ 0.006]
 [ 0.007]
 [ 0.006]
 [ 0.005]
 [ 0.004]] [[1.229]
 [1.266]
 [1.21 ]
 [1.221]
 [1.215]
 [1.236]
 [1.245]]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.18892025403013551, 0.1971140435196136, 0.13752284723250036, 0.1971140435196136, 0.13752284723250036, 0.14180596446563662]
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
rdn probs:  [0.19521404741386122, 0.19521404741386122, 0.14210425589731496, 0.18742461132476776, 0.13793878205287993, 0.14210425589731496]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.19521406809355502, 0.19521406809355502, 0.14210423810432932, 0.1874246263618019, 0.13793876124242926, 0.14210423810432932]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.1902748862453092, 0.19818278590713215, 0.14426528821288476, 0.18297528655747264, 0.14003646486431634, 0.14426528821288476]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.1902748862453092, 0.19818278590713215, 0.14426528821288476, 0.18297528655747264, 0.14003646486431634, 0.14426528821288476]
using explorer policy with actor:  1
using another actor
STARTED EXPV TRAINING ON FRAME NO.  11106
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.1917615836595958, 0.19973127105500604, 0.14539249335902735, 0.18440494914075564, 0.13331720942658767, 0.14539249335902735]
Printing some Q and Qe and total Qs values:  [[1.064]
 [1.062]
 [1.062]
 [1.062]
 [1.062]
 [1.062]
 [1.062]] [[-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]] [[1.334]
 [1.332]
 [1.332]
 [1.332]
 [1.332]
 [1.332]
 [1.332]]
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]] [[0.007]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[-0.185]
 [-0.188]
 [-0.188]
 [-0.188]
 [-0.188]
 [-0.188]
 [-0.188]]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.19307676439080262, 0.20110111121849408, 0.14638965557514333, 0.17881125891935118, 0.13423155432106537, 0.14638965557514333]
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
probs:  [0.188579747339548, 0.19610293581061525, 0.1486840509020702, 0.1816138320885598, 0.1363353829571366, 0.1486840509020702]
siam score:  -0.83728296
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.731]
 [0.771]
 [0.771]
 [0.771]
 [0.777]
 [0.776]] [[-0.003]
 [ 0.004]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.003]
 [-0.003]] [[1.533]
 [1.459]
 [1.529]
 [1.529]
 [1.529]
 [1.539]
 [1.537]]
Printing some Q and Qe and total Qs values:  [[0.805]
 [0.755]
 [0.798]
 [0.799]
 [0.799]
 [0.797]
 [0.799]] [[-0.004]
 [ 0.003]
 [-0.002]
 [-0.001]
 [-0.003]
 [-0.003]
 [-0.003]] [[1.288]
 [1.194]
 [1.275]
 [1.278]
 [1.277]
 [1.273]
 [1.276]]
siam score:  -0.8578541
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
line 256 mcts: sample exp_bonus 0.0038868180958596737
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.142]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.148]] [[0.004]
 [0.004]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.004]] [[-0.221]
 [-0.106]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.095]]
siam score:  -0.87080365
line 256 mcts: sample exp_bonus -0.0016206208976472273
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.742]
 [0.806]
 [0.806]
 [0.796]
 [0.806]
 [0.804]] [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[1.572]
 [1.449]
 [1.576]
 [1.576]
 [1.556]
 [1.576]
 [1.573]]
Printing some Q and Qe and total Qs values:  [[0.773]
 [0.763]
 [0.766]
 [0.774]
 [0.771]
 [0.771]
 [0.764]] [[0.011]
 [0.01 ]
 [0.012]
 [0.009]
 [0.005]
 [0.008]
 [0.007]] [[1.419]
 [1.398]
 [1.405]
 [1.416]
 [1.406]
 [1.41 ]
 [1.394]]
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
siam score:  -0.8723881
line 256 mcts: sample exp_bonus -0.032164124155447595
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.17721789409081246, 0.183106520393472, 0.148570522888685, 0.18940125885493564, 0.14467548557611504, 0.1570283181959798]
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.17265043872705715, 0.18412298739330496, 0.14939527251169002, 0.19045266941606237, 0.14547861293857556, 0.15790001901331]
Printing some Q and Qe and total Qs values:  [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]] [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.35 ]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]] [[-0.01 ]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[-0.045]
 [-0.047]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]]
siam score:  -0.8602914
from probs:  [0.1724253473743212, 0.17779554688724009, 0.1538473598701694, 0.18351221088486344, 0.14981398100413643, 0.16260555397926954]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.008]
 [-0.016]
 [-0.017]
 [-0.017]
 [-0.018]
 [-0.017]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[-0.351]
 [-0.339]
 [-0.356]
 [-0.358]
 [-0.356]
 [-0.359]
 [-0.357]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.07 ]
 [0.097]
 [0.097]
 [0.084]
 [0.083]
 [0.099]] [[-0.003]
 [-0.004]
 [-0.004]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.004]] [[0.104]
 [0.07 ]
 [0.097]
 [0.097]
 [0.084]
 [0.083]
 [0.099]]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.66 ]
 [0.653]
 [0.658]
 [0.668]
 [0.667]
 [0.668]] [[-0.002]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.972]
 [0.984]
 [0.969]
 [0.979]
 [1.   ]
 [0.998]
 [0.999]]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.368]
 [0.432]
 [0.45 ]
 [0.441]
 [0.45 ]
 [0.449]] [[-0.005]
 [-0.006]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.449]
 [0.279]
 [0.407]
 [0.444]
 [0.426]
 [0.443]
 [0.442]]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.02 ]
 [-0.012]
 [-0.011]
 [-0.008]
 [-0.008]
 [-0.01 ]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[-0.571]
 [-0.592]
 [-0.575]
 [-0.574]
 [-0.568]
 [-0.568]
 [-0.572]]
line 256 mcts: sample exp_bonus -0.003773366227997823
using explorer policy with actor:  1
deleting a thread, now have 5 threads
Frames:  12964 train batches done:  1515 episodes:  271
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.007824739767513352
first move QE:  -0.04834200582166384
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
using another actor
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.16749489092707323, 0.17703002395942466, 0.1631037112411219, 0.1822182581093806, 0.1512153955059854, 0.15893772025701425]
using explorer policy with actor:  1
using explorer policy with actor:  0
deleting a thread, now have 4 threads
Frames:  13245 train batches done:  1539 episodes:  279
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
from probs:  [0.17053216978462013, 0.18024022319834318, 0.16606135571251085, 0.17525136241629108, 0.15395744444411738, 0.15395744444411738]
Printing some Q and Qe and total Qs values:  [[1.274]
 [1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]] [[0.678]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[3.329]
 [2.784]
 [2.784]
 [2.784]
 [2.784]
 [2.784]
 [2.784]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using another actor
from probs:  [0.17134075983594285, 0.18109484465487205, 0.16684874709038333, 0.17134075983594285, 0.15468744429142953, 0.15468744429142953]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.1713407633094899, 0.18109485537715636, 0.16684874722569615, 0.1713407633094899, 0.15468743538908378, 0.15468743538908378]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.17275036554567946, 0.18258471729730272, 0.16822138776532666, 0.16822138776532666, 0.15226213272979772, 0.1559600088965666]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.17336161814182657, 0.1832307673482663, 0.1688166152178083, 0.1688166152178083, 0.14926253287028776, 0.15651185120400268]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
UNIT TEST: sample policy line 217 mcts : [0.857 0.02  0.041 0.02  0.02  0.02  0.02 ]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.16322649531374755, 0.1864498584963696, 0.17178247122313461, 0.1673947912696028, 0.15188485282921121, 0.15926153086793404]
using another actor
from probs:  [0.16322649531374755, 0.1864498584963696, 0.17178247122313461, 0.1673947912696028, 0.15188485282921121, 0.15926153086793404]
siam score:  -0.8878037
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.16378941683146064, 0.1870928707544005, 0.17237489985570162, 0.16797208804839855, 0.14895994615322616, 0.15981077835681237]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9417907560220243
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.1637894147480128, 0.18709288554523648, 0.17237490398909522, 0.16797208899366833, 0.14895993333159777, 0.15981077339238928]
from probs:  [0.16378941270405617, 0.1870929000557166, 0.17237490804414157, 0.16797208992102086, 0.14895992075299955, 0.15981076852206536]
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.693]
 [0.748]
 [0.74 ]
 [0.752]
 [0.728]
 [0.733]] [[1.209]
 [1.348]
 [1.465]
 [1.48 ]
 [1.451]
 [1.461]
 [1.502]] [[2.36 ]
 [2.268]
 [2.496]
 [2.495]
 [2.49 ]
 [2.45 ]
 [2.502]]
siam score:  -0.88492846
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.58 ]
 [0.579]
 [0.568]
 [0.586]
 [0.563]
 [0.583]] [[0.405]
 [0.645]
 [0.558]
 [0.521]
 [0.598]
 [0.532]
 [0.592]] [[0.564]
 [0.778]
 [0.691]
 [0.631]
 [0.745]
 [0.633]
 [0.733]]
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.16251423863111858, 0.1849917693078745, 0.17529094027895878, 0.16656019415293466, 0.14812861899799482, 0.16251423863111858]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.03 ]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]] [[-0.438]
 [-0.146]
 [-0.382]
 [-0.382]
 [-0.382]
 [-0.382]
 [-0.382]] [[-0.672]
 [-0.579]
 [-0.662]
 [-0.662]
 [-0.662]
 [-0.662]
 [-0.662]]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[-0.342]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.28 ]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [-0.051]
 [-0.053]
 [-0.053]
 [-0.052]
 [-0.052]
 [-0.051]] [[-0.437]
 [-0.243]
 [ 0.424]
 [ 0.306]
 [ 0.307]
 [-0.339]
 [-0.414]] [[-0.716]
 [-0.652]
 [-0.434]
 [-0.475]
 [-0.472]
 [-0.687]
 [-0.71 ]]
line 256 mcts: sample exp_bonus -0.27662845310624884
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.16441089565288486, 0.18715077106113373, 0.1728071573420844, 0.16850407322636965, 0.14661447142034226, 0.16051263129718504]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.64 ]
 [0.581]
 [0.618]
 [0.647]
 [0.569]
 [0.588]] [[0.646]
 [0.464]
 [0.377]
 [0.296]
 [0.387]
 [0.253]
 [0.318]] [[1.298]
 [1.324]
 [1.059]
 [0.999]
 [1.209]
 [0.829]
 [0.976]]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.474]
 [0.447]
 [0.467]
 [0.447]
 [0.438]
 [0.467]] [[-0.184]
 [-0.105]
 [-0.113]
 [-0.229]
 [-0.113]
 [-0.214]
 [-0.17 ]] [[0.141]
 [0.195]
 [0.138]
 [0.099]
 [0.138]
 [0.052]
 [0.139]]
siam score:  -0.85771596
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.85914123
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
using explorer policy with actor:  1
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.061 0.082 0.082 0.204 0.143 0.204 0.224]
from probs:  [0.1642148488557507, 0.17679294766268022, 0.17239061308025488, 0.17239061308025488, 0.14999612846530863, 0.1642148488557507]
siam score:  -0.87297106
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.381]
 [0.381]
 [0.455]
 [0.448]
 [0.435]
 [0.455]] [[1.948]
 [1.815]
 [1.815]
 [1.946]
 [1.874]
 [1.807]
 [1.713]] [[0.786]
 [0.513]
 [0.513]
 [0.75 ]
 [0.686]
 [0.617]
 [0.594]]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.16668353391102098, 0.17498221813335152, 0.17073167255606025, 0.17498221813335152, 0.14593682335519462, 0.16668353391102098]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.256]
 [0.312]
 [0.319]
 [0.295]
 [0.282]
 [0.276]] [[1.262]
 [1.002]
 [1.064]
 [1.144]
 [1.011]
 [0.988]
 [1.152]] [[0.916]
 [0.311]
 [0.531]
 [0.69 ]
 [0.398]
 [0.333]
 [0.625]]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.16789478842102018, 0.17197234681492124, 0.17197234681492124, 0.1762537831285174, 0.14401194639959958, 0.16789478842102018]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[1.608]
 [1.257]
 [1.257]
 [1.257]
 [1.257]
 [1.257]
 [1.257]] [[1.205]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.599]
 [0.686]
 [0.691]
 [0.475]
 [0.691]
 [0.644]] [[1.109]
 [0.975]
 [0.913]
 [0.884]
 [0.751]
 [0.808]
 [1.053]] [[1.505]
 [1.484]
 [1.618]
 [1.607]
 [1.086]
 [1.557]
 [1.626]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.16855009509676996, 0.1726435685244932, 0.1726435685244932, 0.17694171562360259, 0.1445740364486768, 0.1646470157819641]
first move QE:  0.05287340184398172
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.17061280841549356, 0.16666196327960012, 0.17061280841549356, 0.17910712545766444, 0.1463433311521482, 0.16666196327960012]
in main func line 156:  222
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.17128954688864798, 0.16732303066141171, 0.16732303066141171, 0.17981755677720593, 0.146923804349911, 0.16732303066141171]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.034]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.035]] [[3.361]
 [2.507]
 [2.197]
 [2.197]
 [2.197]
 [2.197]
 [2.899]] [[0.723]
 [0.276]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.472]]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
siam score:  -0.88033277
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.724]
 [0.724]
 [0.676]
 [0.724]
 [0.774]
 [0.714]] [[0.94 ]
 [1.395]
 [1.395]
 [1.359]
 [1.395]
 [1.474]
 [1.266]] [[1.414]
 [2.357]
 [2.357]
 [2.214]
 [2.357]
 [2.563]
 [2.166]]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.16912875501751642, 0.165301674679408, 0.16912875501751642, 0.1817581201332743, 0.1455539401347684, 0.16912875501751642]
first move QE:  0.06495305804097375
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.1691287567234114, 0.16530167373365273, 0.1691287567234114, 0.18175813058961507, 0.14555392550649793, 0.1691287567234114]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
siam score:  -0.8556446
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.663]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[-0.255]
 [ 0.181]
 [-0.265]
 [-0.265]
 [-0.265]
 [-0.265]
 [-0.265]] [[0.716]
 [1.384]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.473]
 [0.361]
 [0.36 ]
 [0.364]
 [0.379]
 [0.385]] [[0.318]
 [1.395]
 [0.336]
 [0.34 ]
 [0.338]
 [0.345]
 [0.375]] [[-0.044]
 [ 1.059]
 [-0.025]
 [-0.025]
 [-0.019]
 [ 0.012]
 [ 0.048]]
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[2.354]
 [2.354]
 [2.354]
 [2.354]
 [2.354]
 [2.354]
 [2.354]] [[1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.58 ]
 [0.647]
 [0.705]
 [0.672]
 [0.684]
 [0.695]] [[1.342]
 [1.361]
 [1.149]
 [1.297]
 [1.221]
 [1.193]
 [1.285]] [[1.306]
 [1.055]
 [0.977]
 [1.24 ]
 [1.099]
 [1.093]
 [1.209]]
using another actor
from probs:  [0.1795711082460802, 0.1679109813354389, 0.164355056107371, 0.17162494768475425, 0.14862692529091678, 0.1679109813354389]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.366]
 [0.349]
 [0.342]
 [0.336]
 [0.341]
 [0.352]] [[0.861]
 [1.056]
 [0.708]
 [0.81 ]
 [0.636]
 [0.747]
 [0.866]] [[0.358]
 [0.366]
 [0.349]
 [0.342]
 [0.336]
 [0.341]
 [0.352]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.109]
 [-0.089]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.109]] [[1.56 ]
 [1.504]
 [2.005]
 [1.499]
 [1.499]
 [1.499]
 [1.485]] [[-0.031]
 [-0.11 ]
 [ 0.586]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.136]]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
from probs:  [0.18021194752606792, 0.16851019357933694, 0.16494157358139772, 0.17223741891051791, 0.14915729282128184, 0.16494157358139772]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.18083038407701874, 0.16908846553899962, 0.1655075967649938, 0.17282848403629458, 0.14966913872612186, 0.16207593085657154]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.18083038407701874, 0.16908846553899962, 0.1655075967649938, 0.17282848403629458, 0.14966913872612186, 0.16207593085657154]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.18133979293193236, 0.16956478941465206, 0.16597383089519782, 0.17331534609052654, 0.14727374502030385, 0.16253249564738748]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
line 256 mcts: sample exp_bonus 3.579482317187908
line 256 mcts: sample exp_bonus 2.6810039831610633
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.539]
 [0.561]
 [0.56 ]
 [0.561]
 [0.568]
 [0.569]] [[2.09 ]
 [2.3  ]
 [2.163]
 [2.272]
 [2.443]
 [2.626]
 [2.741]] [[0.948]
 [1.091]
 [1.001]
 [1.103]
 [1.269]
 [1.458]
 [1.57 ]]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
first move QE:  0.14187341283079236
in main func line 156:  251
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
siam score:  -0.894652
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.759]
 [0.759]
 [0.287]
 [0.221]
 [0.759]
 [0.278]] [[0.423]
 [0.423]
 [0.423]
 [1.029]
 [1.015]
 [0.423]
 [1.128]] [[0.857]
 [0.857]
 [0.857]
 [0.315]
 [0.174]
 [0.857]
 [0.364]]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.013]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[1.636]
 [1.767]
 [1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.636]] [[0.547]
 [0.728]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]]
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.016]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.015]
 [-0.02 ]] [[1.715]
 [1.572]
 [0.858]
 [0.858]
 [0.858]
 [1.132]
 [0.858]] [[ 0.469]
 [ 0.33 ]
 [-0.34 ]
 [-0.34 ]
 [-0.34 ]
 [-0.078]
 [-0.34 ]]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.1710807887848367, 0.16760630652350483, 0.16760630652350483, 0.1710807887848367, 0.15501950285981203, 0.16760630652350483]
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.74 ]
 [0.71 ]
 [0.707]
 [0.726]
 [0.743]
 [0.765]] [[1.392]
 [1.738]
 [1.411]
 [1.313]
 [1.411]
 [1.211]
 [1.49 ]] [[0.807]
 [1.249]
 [0.862]
 [0.757]
 [0.893]
 [0.729]
 [1.05 ]]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.17165333900705598, 0.16482056303027562, 0.16816722881482113, 0.17165333900705598, 0.15553830132597024, 0.16816722881482113]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
line 256 mcts: sample exp_bonus 2.8856150277070323
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.17444531541302824, 0.1675013982620837, 0.16109162858428874, 0.17090250054009737, 0.1551566566604045, 0.17090250054009737]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.735]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]] [[0.691]
 [2.148]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]] [[0.993]
 [1.917]
 [0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.993]]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
siam score:  -0.8864338
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.023]
 [-0.024]
 [-0.024]
 [-0.025]
 [-0.028]
 [-0.025]] [[1.881]
 [1.755]
 [1.719]
 [1.931]
 [2.151]
 [2.126]
 [2.763]] [[-0.209]
 [-0.319]
 [-0.351]
 [-0.171]
 [ 0.016]
 [-0.01 ]
 [ 0.536]]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.17291375585639054, 0.16317569992084016, 0.166296871695055, 0.17291375585639054, 0.15178616081493326, 0.17291375585639054]
from probs:  [0.17291375585639054, 0.16317569992084016, 0.166296871695055, 0.17291375585639054, 0.15178616081493326, 0.17291375585639054]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.061]
 [-0.071]
 [-0.071]
 [-0.056]
 [-0.056]
 [-0.056]] [[3.411]
 [3.17 ]
 [3.457]
 [3.572]
 [3.411]
 [3.411]
 [3.411]] [[0.925]
 [0.663]
 [0.95 ]
 [1.072]
 [0.925]
 [0.925]
 [0.925]]
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.061]
 [-0.071]
 [-0.071]
 [-0.056]
 [-0.056]
 [-0.071]] [[3.482]
 [3.17 ]
 [3.457]
 [3.572]
 [3.411]
 [3.411]
 [3.682]] [[0.989]
 [0.661]
 [0.947]
 [1.068]
 [0.921]
 [0.921]
 [1.183]]
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.012]] [[1.017]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [1.105]] [[0.325]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.476]]
first move QE:  0.17600831422480878
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[2.934]
 [2.934]
 [2.934]
 [2.934]
 [2.934]
 [2.934]
 [2.934]] [[0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]]
siam score:  -0.87195635
first move QE:  0.1810056581473752
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.17383798234058148, 0.16731190072473098, 0.16731190072473098, 0.16731190072473098, 0.15038833314464414, 0.17383798234058148]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[1.028]
 [1.028]
 [1.028]
 [1.028]
 [1.028]
 [1.028]
 [1.028]] [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.354]
 [0.431]
 [0.432]
 [0.444]
 [0.555]
 [0.426]] [[1.032]
 [0.866]
 [0.926]
 [0.964]
 [0.997]
 [1.152]
 [1.029]] [[0.575]
 [0.363]
 [0.496]
 [0.526]
 [0.565]
 [0.808]
 [0.568]]
line 256 mcts: sample exp_bonus 0.9670915246156768
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 0.9611099504080702
using explorer policy with actor:  1
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.131]
 [0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]] [[2.459]
 [1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.159]] [[2.307]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
277 129
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.3376453797276622
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.17439021009346956, 0.17111712790935435, 0.17111712790935435, 0.15917324906205663, 0.1464106365840974, 0.17779164844166775]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.17439021009346956, 0.17111712790935435, 0.17111712790935435, 0.15917324906205663, 0.1464106365840974, 0.17779164844166775]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.17479388347793223, 0.17151322485840714, 0.17151322485840714, 0.1595416986678594, 0.14443477276073963, 0.1782031953766544]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
Printing some Q and Qe and total Qs values:  [[1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.347]] [[0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.202]] [[1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.93 ]]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.588]] [[3.652]
 [3.652]
 [3.652]
 [3.652]
 [3.652]
 [3.652]
 [3.923]] [[1.611]
 [1.611]
 [1.611]
 [1.611]
 [1.611]
 [1.611]
 [1.878]]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.17701512784899695, 0.16443765702418847, 0.1736927770650853, 0.1615691110466006, 0.14627019916613182, 0.17701512784899695]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.674]
 [0.612]
 [0.624]
 [0.639]
 [0.625]
 [0.633]] [[2.688]
 [3.364]
 [3.334]
 [3.463]
 [3.544]
 [2.893]
 [3.022]] [[1.033]
 [1.66 ]
 [1.559]
 [1.68 ]
 [1.766]
 [1.212]
 [1.328]]
using another actor
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.18044272879113482, 0.16469762578979316, 0.17705604625877075, 0.15395998177576342, 0.14678757112576704, 0.17705604625877075]
using explorer policy with actor:  1
first move QE:  0.21141347840160685
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.386]
 [0.336]
 [0.337]
 [0.345]
 [0.334]
 [0.341]] [[0.244]
 [0.824]
 [0.279]
 [0.196]
 [0.26 ]
 [0.003]
 [0.057]] [[0.34 ]
 [0.386]
 [0.336]
 [0.337]
 [0.345]
 [0.334]
 [0.341]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.18103313337053173, 0.16523650228195091, 0.17763536743827096, 0.15446371780186957, 0.14726783441869026, 0.1743634446886865]
using another actor
284 139
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.364]
 [0.4  ]
 [0.396]
 [0.396]
 [0.396]
 [0.403]] [[0.889]
 [0.61 ]
 [0.396]
 [0.39 ]
 [0.414]
 [0.435]
 [0.38 ]] [[0.96 ]
 [0.433]
 [0.218]
 [0.203]
 [0.233]
 [0.263]
 [0.203]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.17868493834143717, 0.1662127971893994, 0.17868493834143717, 0.15288571087774613, 0.14813793637921965, 0.17539367887076054]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.123]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[-0.049]
 [ 0.541]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]] [[0.664]
 [1.159]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.18119373572090167, 0.16016767551235342, 0.18119373572090167, 0.15258685108342107, 0.1502178434493797, 0.1746401585130425]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.18119373572090167, 0.16016767551235342, 0.18119373572090167, 0.15258685108342107, 0.1502178434493797, 0.1746401585130425]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.18119374588677878, 0.1601676709644336, 0.18119374588677878, 0.15258684123052682, 0.15021783193868096, 0.17464016409280106]
Printing some Q and Qe and total Qs values:  [[1.291]
 [1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]] [[2.979]
 [2.866]
 [2.866]
 [2.866]
 [2.866]
 [2.866]
 [2.866]] [[2.757]
 [2.304]
 [2.304]
 [2.304]
 [2.304]
 [2.304]
 [2.304]]
siam score:  -0.88817716
Printing some Q and Qe and total Qs values:  [[1.418]
 [1.35 ]
 [1.37 ]
 [1.383]
 [1.373]
 [1.375]
 [1.38 ]] [[0.265]
 [0.251]
 [0.247]
 [0.226]
 [0.22 ]
 [0.209]
 [0.208]] [[2.412]
 [2.272]
 [2.31 ]
 [2.329]
 [2.308]
 [2.309]
 [2.317]]
from probs:  [0.1798971404028045, 0.15936582362907548, 0.18327290946802677, 0.1543377460518357, 0.1496190886331953, 0.17350729181506228]
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[0.73]
 [0.05]
 [0.05]
 [0.05]
 [0.05]
 [0.05]
 [0.05]] [[0.222]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]]
Printing some Q and Qe and total Qs values:  [[0.874]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[0.853]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]] [[1.716]
 [1.156]
 [1.156]
 [1.156]
 [1.156]
 [1.156]
 [1.156]]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.047]
 [-0.053]
 [-0.054]
 [-0.054]
 [-0.052]
 [-0.052]] [[1.964]
 [2.048]
 [1.88 ]
 [1.863]
 [1.879]
 [1.913]
 [2.013]] [[ 0.132]
 [ 0.233]
 [-0.004]
 [-0.028]
 [-0.006]
 [ 0.043]
 [ 0.175]]
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[1.888]
 [2.198]
 [2.198]
 [2.198]
 [2.198]
 [2.198]
 [2.198]] [[0.185]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]]
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.574]
 [0.532]
 [0.528]
 [0.529]
 [0.536]
 [0.542]] [[-0.558]
 [ 0.849]
 [-0.364]
 [-0.518]
 [-0.56 ]
 [-0.519]
 [-0.526]] [[0.528]
 [0.574]
 [0.532]
 [0.528]
 [0.529]
 [0.536]
 [0.542]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.375]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.477]] [[4.606]
 [2.897]
 [4.606]
 [4.606]
 [4.606]
 [4.606]
 [4.275]] [[2.302]
 [0.381]
 [2.302]
 [2.302]
 [2.302]
 [2.302]
 [1.931]]
line 256 mcts: sample exp_bonus 0.1408457051399943
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
probs:  [0.17982101769403525, 0.16223195813598076, 0.17662776580998368, 0.15467415910712926, 0.15001733344288742, 0.17662776580998368]
using explorer policy with actor:  0
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.864]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]] [[1.343]
 [1.969]
 [1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.236]] [[1.653]
 [2.255]
 [1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]]
from probs:  [0.1831939704184797, 0.16262564964024348, 0.1799408176423301, 0.15056533361888963, 0.1528312111744167, 0.17084301750564054]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.1837708283469476, 0.1631377401591231, 0.17735854028857606, 0.15103944751167803, 0.15331246006968285, 0.17138098362399246]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.1837708283469476, 0.1631377401591231, 0.17735854028857606, 0.15103944751167803, 0.15331246006968285, 0.17138098362399246]
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]] [[1.512]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]] [[1.851]
 [1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]]
Printing some Q and Qe and total Qs values:  [[0.901]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.981]] [[1.773]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.745]] [[1.924]
 [1.704]
 [1.704]
 [1.704]
 [1.704]
 [1.704]
 [2.075]]
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.32 ]
 [0.387]
 [0.406]
 [0.393]
 [0.411]
 [0.403]] [[1.492]
 [1.492]
 [1.645]
 [1.664]
 [1.64 ]
 [1.675]
 [1.694]] [[0.494]
 [0.395]
 [0.732]
 [0.796]
 [0.739]
 [0.821]
 [0.831]]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.18396806984092157, 0.1636427235861733, 0.17766020376186173, 0.14950335227852227, 0.1562516885844466, 0.16897396194807446]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[1.022]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]] [[1.488]
 [1.353]
 [1.353]
 [1.353]
 [1.353]
 [1.353]
 [1.353]]
from probs:  [0.1851714361629348, 0.16471313837378507, 0.17882230926285383, 0.1462718558596219, 0.15494201107150457, 0.17007924926929974]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.717]
 [0.774]
 [0.782]
 [0.506]
 [0.773]
 [0.75 ]] [[1.729]
 [1.377]
 [0.938]
 [0.85 ]
 [1.169]
 [0.391]
 [1.24 ]] [[2.052]
 [2.021]
 [1.719]
 [1.656]
 [1.645]
 [1.273]
 [1.942]]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.187428525439949, 0.16162358555586864, 0.1810020078442968, 0.14601318982352374, 0.15453987656808188, 0.16939281476827994]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.18784636207094477, 0.1619838949288395, 0.18140551776352393, 0.14633869875645483, 0.152655082304312, 0.16977044417592496]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.5  ]
 [0.491]
 [0.499]
 [0.491]
 [0.491]
 [0.495]] [[1.831]
 [2.477]
 [2.006]
 [1.85 ]
 [2.006]
 [2.006]
 [2.366]] [[0.489]
 [0.5  ]
 [0.491]
 [0.499]
 [0.491]
 [0.491]
 [0.495]]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.189632964303123, 0.16106309996358, 0.1800451115247679, 0.14376674966072156, 0.15410695908090868, 0.17138511546689877]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.18694331407434053, 0.16159767602246913, 0.18064270348517475, 0.14424390582479144, 0.1546184423560118, 0.1719539582372123]
first move QE:  0.27084846456372763
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.847]
 [0.782]
 [0.771]
 [0.734]
 [0.724]
 [0.83 ]] [[1.604]
 [2.355]
 [1.7  ]
 [1.589]
 [1.35 ]
 [1.634]
 [2.076]] [[0.969]
 [1.593]
 [1.085]
 [0.998]
 [0.802]
 [0.991]
 [1.386]]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.387]
 [0.365]
 [0.354]
 [0.356]
 [0.352]
 [0.341]] [[ 0.012]
 [ 0.505]
 [-0.026]
 [ 0.017]
 [-0.065]
 [-0.079]
 [-0.13 ]] [[0.183]
 [0.414]
 [0.162]
 [0.166]
 [0.133]
 [0.123]
 [0.087]]
maxi score, test score, baseline:  -0.9930034482758621 -1.0 -0.9930034482758621
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
line 256 mcts: sample exp_bonus 1.0843781108327046
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.39 ]
 [0.379]
 [0.379]
 [0.379]
 [0.384]
 [0.4  ]] [[2.751]
 [3.077]
 [3.189]
 [3.189]
 [3.189]
 [2.96 ]
 [2.764]] [[1.032]
 [1.446]
 [1.573]
 [1.573]
 [1.573]
 [1.278]
 [1.047]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.955092300845249
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.339]
 [0.418]
 [0.42 ]
 [0.408]
 [0.402]
 [0.429]] [[6.639]
 [4.039]
 [6.403]
 [5.946]
 [5.484]
 [5.025]
 [4.133]] [[0.415]
 [0.339]
 [0.418]
 [0.42 ]
 [0.408]
 [0.402]
 [0.429]]
siam score:  -0.8542418
using explorer policy with actor:  1
siam score:  -0.84852004
rdn probs:  [0.19122752976953197, 0.15816149153428688, 0.17876000715624282, 0.14003875903996982, 0.15591772465403808, 0.1758944878459304]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.19122754273156375, 0.15816148704566813, 0.17876001353852117, 0.14003874498705224, 0.15591771898126805, 0.17589449271592666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.329]
 [0.289]
 [0.284]
 [0.284]
 [0.294]
 [0.291]] [[ 0.022]
 [ 0.246]
 [-0.138]
 [-0.643]
 [-0.502]
 [-0.226]
 [-0.261]] [[0.304]
 [0.329]
 [0.289]
 [0.284]
 [0.284]
 [0.294]
 [0.291]]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.1900710037400846, 0.1576777082806211, 0.17507410769403667, 0.14161949343746827, 0.1576777082806211, 0.17787997856716822]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[ 0.004]
 [-0.003]
 [ 0.004]
 [ 0.011]
 [ 0.005]
 [ 0.004]
 [ 0.012]] [[-0.232]
 [ 0.264]
 [-0.117]
 [-0.382]
 [-0.173]
 [ 0.078]
 [ 0.023]] [[ 0.004]
 [-0.003]
 [ 0.004]
 [ 0.011]
 [ 0.005]
 [ 0.004]
 [ 0.012]]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.19322201565378572, 0.15375067722816332, 0.1752132174970955, 0.14396718308847914, 0.15587043429176156, 0.17797647224071464]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.617]
 [0.568]
 [0.558]
 [0.556]
 [0.551]
 [0.561]] [[ 0.166]
 [ 0.809]
 [ 0.009]
 [-0.002]
 [-0.033]
 [-0.03 ]
 [-0.148]] [[0.539]
 [0.617]
 [0.568]
 [0.558]
 [0.556]
 [0.551]
 [0.561]]
using explorer policy with actor:  0
siam score:  -0.8851909
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.412]
 [0.41 ]
 [0.411]
 [0.409]
 [0.396]
 [0.379]] [[2.394]
 [2.633]
 [2.604]
 [2.536]
 [2.566]
 [2.698]
 [2.398]] [[0.428]
 [0.412]
 [0.41 ]
 [0.411]
 [0.409]
 [0.396]
 [0.379]]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.734]
 [0.532]
 [0.496]
 [0.469]
 [0.484]
 [0.487]] [[ 0.088]
 [ 1.694]
 [-0.36 ]
 [-0.461]
 [ 0.013]
 [-0.152]
 [-0.483]] [[0.624]
 [1.795]
 [0.501]
 [0.398]
 [0.589]
 [0.532]
 [0.375]]
siam score:  -0.88551575
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.202]
 [0.182]
 [0.163]
 [0.182]
 [0.182]
 [0.182]] [[ 0.   ]
 [-0.637]
 [ 0.   ]
 [-1.041]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.182]
 [0.202]
 [0.182]
 [0.163]
 [0.182]
 [0.182]
 [0.182]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.194]
 [0.169]
 [0.164]
 [0.162]
 [0.165]
 [0.179]] [[-0.634]
 [-0.318]
 [-0.777]
 [-0.853]
 [-0.765]
 [-0.557]
 [-0.285]] [[0.165]
 [0.194]
 [0.169]
 [0.164]
 [0.162]
 [0.165]
 [0.179]]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.18934643266828208, 0.1537404116134627, 0.17758372928410068, 0.14591491248053534, 0.15583078466951863, 0.17758372928410068]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.646]
 [0.556]
 [0.648]
 [0.648]
 [0.542]
 [0.648]] [[-0.816]
 [-0.319]
 [-0.714]
 [ 0.465]
 [ 0.465]
 [-0.609]
 [ 0.465]] [[0.465]
 [0.965]
 [0.654]
 [1.231]
 [1.231]
 [0.661]
 [1.231]]
Printing some Q and Qe and total Qs values:  [[0.566]
 [1.003]
 [0.532]
 [0.544]
 [0.553]
 [0.555]
 [0.555]] [[0.525]
 [1.889]
 [0.657]
 [0.592]
 [0.572]
 [0.608]
 [0.678]] [[1.053]
 [2.364]
 [1.106]
 [1.075]
 [1.071]
 [1.096]
 [1.141]]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.589]
 [0.607]
 [0.563]
 [0.563]
 [0.539]
 [0.607]] [[ 1.043]
 [ 1.168]
 [ 1.24 ]
 [-0.101]
 [ 0.084]
 [ 0.023]
 [ 1.105]] [[0.791]
 [0.821]
 [0.879]
 [0.344]
 [0.406]
 [0.338]
 [0.834]]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.18807929774243012, 0.15318662295511534, 0.17931718632820487, 0.1454887815011327, 0.15735189333090047, 0.17657621814221647]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.1884192402670682, 0.15346349897482198, 0.17964129183988584, 0.1439443015693442, 0.15763629783749716, 0.17689536951138263]
in main func line 156:  332
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.056]
 [-0.056]
 [-0.054]
 [-0.054]
 [-0.049]
 [-0.052]] [[1.523]
 [1.683]
 [1.46 ]
 [1.376]
 [1.425]
 [1.823]
 [1.813]] [[-1.018]
 [-0.971]
 [-1.044]
 [-1.068]
 [-1.052]
 [-0.91 ]
 [-0.918]]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]] [[1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]] [[1.138]
 [1.138]
 [1.138]
 [1.138]
 [1.138]
 [1.138]
 [1.138]]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.801]
 [0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]] [[2.157]
 [2.51 ]
 [2.157]
 [2.157]
 [2.157]
 [2.157]
 [2.157]] [[2.327]
 [2.505]
 [2.327]
 [2.327]
 [2.327]
 [2.327]
 [2.327]]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.18027717273982036, 0.15080816623111037, 0.1858734504181571, 0.1471140457570841, 0.16091707698773647, 0.17501008786609165]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8562380792191865
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.24 ]
 [0.238]
 [0.276]
 [0.434]
 [0.399]
 [0.254]] [[0.854]
 [1.019]
 [1.059]
 [1.241]
 [1.008]
 [1.557]
 [1.006]] [[1.796]
 [1.619]
 [1.654]
 [1.863]
 [1.792]
 [2.28 ]
 [1.62 ]]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.18053426681824075, 0.14851955783446374, 0.18317077226396358, 0.14851955783446374, 0.16619167719350864, 0.17306416805535946]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.17749088139627647, 0.14846564191238104, 0.18526549911517706, 0.15021801606489515, 0.16589715321896864, 0.1726628082923017]
Printing some Q and Qe and total Qs values:  [[1.35]
 [1.35]
 [1.35]
 [1.35]
 [1.35]
 [1.35]
 [1.35]] [[0.403]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[2.683]
 [2.723]
 [2.723]
 [2.723]
 [2.723]
 [2.723]
 [2.723]]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.17874378535378246, 0.1495136573869923, 0.18127905155498367, 0.1495136573869923, 0.1670682173219349, 0.17388163099531442]
siam score:  -0.8802179
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]] [[0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]] [[2.103]
 [2.103]
 [2.103]
 [2.103]
 [2.103]
 [2.103]
 [2.103]]
from probs:  [0.17829847395367177, 0.14948308765264567, 0.18079155566583296, 0.15122653959690943, 0.1689822212398062, 0.17121812189113392]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[0.719]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[0.341]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.35 ]
 [0.359]
 [0.357]
 [0.348]
 [0.348]
 [0.355]] [[1.39 ]
 [1.476]
 [1.268]
 [1.18 ]
 [1.124]
 [1.171]
 [1.32 ]] [[ 0.016]
 [ 0.058]
 [-0.063]
 [-0.125]
 [-0.181]
 [-0.15 ]
 [-0.037]]
from probs:  [0.17930701311688824, 0.14861562592045624, 0.18181419685969535, 0.15032863357793153, 0.16774791923771257, 0.17218661128731602]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.18005989022691507, 0.1475585307501359, 0.18005989022691507, 0.1509598358116593, 0.16845226184235107, 0.17290959114202364]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.392]
 [0.373]
 [0.375]
 [0.374]
 [0.372]
 [0.368]] [[-0.836]
 [-0.459]
 [-1.191]
 [-1.403]
 [-1.336]
 [-1.12 ]
 [-1.11 ]] [[0.372]
 [0.392]
 [0.373]
 [0.375]
 [0.374]
 [0.372]
 [0.368]]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
using explorer policy with actor:  1
first move QE:  0.4002195276055631
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.225]
 [0.224]
 [0.217]
 [0.217]
 [0.21 ]
 [0.228]] [[-0.39 ]
 [ 0.23 ]
 [-0.332]
 [-0.219]
 [-0.219]
 [-0.357]
 [-0.147]] [[0.22 ]
 [0.225]
 [0.224]
 [0.217]
 [0.217]
 [0.21 ]
 [0.228]]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.18050189108535641, 0.14792074910526049, 0.17804714751151357, 0.15133040349852633, 0.16886576894960786, 0.1733340398497353]
siam score:  -0.86993706
siam score:  -0.8661515
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.1767688281691352, 0.1475292119650196, 0.17913960786136077, 0.15432912271016277, 0.1700212244297239, 0.17221200486459767]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.49 ]
 [0.662]
 [0.675]
 [0.68 ]
 [0.68 ]
 [0.68 ]] [[0.315]
 [1.03 ]
 [0.515]
 [0.357]
 [0.424]
 [0.527]
 [0.521]] [[0.933]
 [0.824]
 [0.994]
 [0.968]
 [1.   ]
 [1.035]
 [1.033]]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.147]] [[2.704]
 [2.704]
 [2.704]
 [2.704]
 [2.704]
 [2.704]
 [3.919]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [ 1.226]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
probs:  [0.1746825575512168, 0.14642183743376816, 0.18170954741825812, 0.15654311859211023, 0.16818259192420362, 0.17246034708044308]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
from probs:  [0.17468256169696147, 0.14642182696332956, 0.18170955519829698, 0.15654311335630472, 0.16818259270822614, 0.172460350076881]
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
probs:  [0.17535654267636855, 0.14698676902385716, 0.18241064855753356, 0.1553563957336019, 0.16676388695280955, 0.1731257570558292]
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.014]
 [-0.024]
 [ 0.089]
 [-0.014]
 [-0.02 ]
 [ 0.022]] [[1.876]
 [1.705]
 [2.26 ]
 [3.812]
 [1.899]
 [2.117]
 [2.028]] [[-0.638]
 [-0.729]
 [-0.38 ]
 [ 0.88 ]
 [-0.599]
 [-0.466]
 [-0.442]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[0.401]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[0.325]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.329]
 [0.33 ]]
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.877]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.878]] [[3.988]
 [3.561]
 [3.647]
 [3.647]
 [3.647]
 [3.647]
 [2.878]] [[1.895]
 [1.726]
 [1.764]
 [1.764]
 [1.764]
 [1.764]
 [1.457]]
from probs:  [0.17491617508242016, 0.1456901284190837, 0.18664115157320124, 0.15541846136741339, 0.16458702912625586, 0.17274705443162564]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.3496240139469553
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
358 261
line 256 mcts: sample exp_bonus 1.100516487357419
Printing some Q and Qe and total Qs values:  [[0.929]
 [1.167]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]] [[2.026]
 [1.763]
 [2.026]
 [2.026]
 [2.026]
 [2.026]
 [2.026]] [[2.748]
 [2.954]
 [2.748]
 [2.748]
 [2.748]
 [2.748]
 [2.748]]
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.317856311456321
siam score:  -0.87728226
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.387]
 [0.421]
 [0.419]
 [0.421]
 [0.423]
 [0.421]] [[0.678]
 [1.01 ]
 [0.675]
 [0.5  ]
 [0.55 ]
 [0.594]
 [0.507]] [[0.407]
 [0.387]
 [0.421]
 [0.419]
 [0.421]
 [0.423]
 [0.421]]
Printing some Q and Qe and total Qs values:  [[-0.078]
 [-0.078]
 [-0.074]
 [-0.061]
 [-0.062]
 [-0.059]
 [-0.061]] [[4.459]
 [4.172]
 [4.729]
 [5.965]
 [4.344]
 [4.142]
 [3.717]] [[0.698]
 [0.529]
 [0.859]
 [1.595]
 [0.641]
 [0.525]
 [0.274]]
line 256 mcts: sample exp_bonus 3.01722579914194
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.17711899620929025, 0.1460027510086004, 0.18899164938602406, 0.15228966500055563, 0.1628165907545272, 0.17278034764100234]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.341]
 [0.465]
 [0.473]
 [0.472]
 [0.473]
 [0.478]] [[-0.044]
 [ 0.891]
 [ 0.15 ]
 [ 0.062]
 [ 0.219]
 [ 0.223]
 [ 0.303]] [[0.579]
 [0.578]
 [0.579]
 [0.564]
 [0.616]
 [0.62 ]
 [0.656]]
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.17771363167446422, 0.14499720470322042, 0.18962614449807774, 0.15280094181290718, 0.16150166019956946, 0.17336041711176098]
Printing some Q and Qe and total Qs values:  [[0.875]
 [1.106]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]] [[2.509]
 [3.622]
 [2.509]
 [2.509]
 [2.509]
 [2.509]
 [2.509]] [[1.778]
 [2.513]
 [1.778]
 [1.778]
 [1.778]
 [1.778]
 [1.778]]
Printing some Q and Qe and total Qs values:  [[0.93]
 [1.26]
 [0.93]
 [0.93]
 [0.93]
 [0.93]
 [0.93]] [[2.375]
 [4.266]
 [2.375]
 [2.375]
 [2.375]
 [2.375]
 [2.375]] [[1.368]
 [2.225]
 [1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.368]]
using another actor
from probs:  [0.17663094008626534, 0.1459234045144249, 0.1908374470042392, 0.15377699417474217, 0.16253329529026828, 0.1702979189300601]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.17714809101258766, 0.1434227865463054, 0.19139619261498572, 0.15422723191307774, 0.1630091702653804, 0.17079652764766323]
using another actor
deleting a thread, now have 3 threads
Frames:  23750 train batches done:  2780 episodes:  638
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.235]
 [0.227]
 [0.225]
 [0.216]
 [0.224]
 [0.221]] [[0.424]
 [1.053]
 [0.631]
 [0.453]
 [0.267]
 [0.559]
 [0.549]] [[0.224]
 [0.235]
 [0.227]
 [0.225]
 [0.216]
 [0.224]
 [0.221]]
rdn beta is 0 so we're just using the maxi policy
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.3584],
        [-0.5082],
        [-0.4882],
        [-0.5667],
        [-0.5096],
        [-0.5892],
        [-0.5811],
        [-0.3929],
        [-0.0000],
        [-0.5231]], dtype=torch.float64)
-0.0727797758985 -0.4312219149008954
-0.0727797758985 -0.5809648896737769
-0.0727797758985 -0.5609548212587735
-0.024259925299500003 -0.5909275978474999
-0.0727797758985 -0.5823306442089375
-0.024259925299500003 -0.6134261589726925
-0.024259925299500003 -0.6053812841420082
-0.0727797758985 -0.4657155964773229
-0.9514752239519999 -0.9514752239519999
-0.0727797758985 -0.5958566410840364
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.61 ]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[2.594]
 [0.97 ]
 [2.594]
 [2.594]
 [2.594]
 [2.594]
 [2.594]] [[1.822]
 [0.724]
 [1.822]
 [1.822]
 [1.822]
 [1.822]
 [1.822]]
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
line 256 mcts: sample exp_bonus 1.8486864011264226
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.66 ]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[1.045]
 [2.506]
 [1.045]
 [1.045]
 [1.045]
 [1.045]
 [1.045]] [[0.63]
 [1.22]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]]
line 256 mcts: sample exp_bonus 3.7016132944427804
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.04 ]
 [0.079]
 [0.12 ]
 [0.099]
 [0.144]
 [0.117]] [[3.411]
 [3.251]
 [3.299]
 [3.251]
 [2.841]
 [3.438]
 [2.962]] [[0.795]
 [0.595]
 [0.721]
 [0.755]
 [0.303]
 [0.991]
 [0.46 ]]
using explorer policy with actor:  1
siam score:  -0.8847021
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.514]
 [0.514]
 [0.514]
 [0.264]
 [0.514]
 [0.501]] [[1.223]
 [0.734]
 [0.734]
 [0.734]
 [0.777]
 [0.734]
 [0.775]] [[1.932]
 [1.261]
 [1.261]
 [1.261]
 [0.833]
 [1.261]
 [1.304]]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.17810973741272162, 0.14487941139424168, 0.18716847675918988, 0.15555925861406672, 0.16240792254551006, 0.17187519327426998]
siam score:  -0.87471145
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
siam score:  -0.8718245
using explorer policy with actor:  1
siam score:  -0.8718901
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.17959409242814017, 0.14327885997021428, 0.18872832672158674, 0.1568556793997732, 0.160233487132248, 0.1713095543480375]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.331]
 [0.303]
 [0.331]
 [0.309]
 [0.331]
 [0.301]] [[4.59 ]
 [4.59 ]
 [5.223]
 [4.59 ]
 [5.195]
 [4.59 ]
 [5.495]] [[1.03 ]
 [1.03 ]
 [1.422]
 [1.03 ]
 [1.409]
 [1.03 ]
 [1.598]]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.17959409890934525, 0.14327884824466483, 0.1887283377822685, 0.15685567448100432, 0.1602334839069535, 0.17130955667576367]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.17959409890934525, 0.14327884824466483, 0.1887283377822685, 0.15685567448100432, 0.1602334839069535, 0.17130955667576367]
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.263]
 [0.271]
 [0.272]
 [0.262]
 [0.261]
 [0.258]] [[4.453]
 [4.888]
 [4.273]
 [4.394]
 [4.534]
 [4.464]
 [4.405]] [[0.724]
 [0.956]
 [0.561]
 [0.643]
 [0.717]
 [0.668]
 [0.623]]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.17959410532179193, 0.14327883664351032, 0.1887283487256088, 0.15685566961441802, 0.16023348071587573, 0.17130955897879524]
using another actor
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.18014726333147996, 0.14235243980078482, 0.18930964479346662, 0.1573387818197258, 0.15901467383044612, 0.17183719642409667]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.1808197708320246, 0.14153721261568047, 0.1876299006273732, 0.15792614300936333, 0.1596082912902399, 0.17247868162531857]
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.1808197777523428, 0.1415372003283535, 0.18762991087758035, 0.15792613873558678, 0.1596082878389685, 0.1724786844671681]
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.148]
 [0.2  ]
 [0.187]
 [0.09 ]
 [0.212]
 [0.129]] [[1.394]
 [1.405]
 [1.302]
 [1.206]
 [0.969]
 [0.351]
 [1.925]] [[0.074]
 [0.148]
 [0.2  ]
 [0.187]
 [0.09 ]
 [0.212]
 [0.129]]
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.18105899927034366, 0.1404014685149565, 0.18787814208691175, 0.15813507235507218, 0.15981944691336272, 0.17270687085935327]
Printing some Q and Qe and total Qs values:  [[0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]] [[4.958]
 [4.958]
 [4.958]
 [4.958]
 [4.958]
 [4.958]
 [4.958]] [[0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.436]
 [0.468]
 [0.509]
 [0.488]
 [0.517]
 [0.521]] [[2.693]
 [2.969]
 [2.194]
 [3.113]
 [3.136]
 [2.566]
 [3.669]] [[1.024]
 [1.235]
 [0.728]
 [1.383]
 [1.385]
 [1.016]
 [1.77 ]]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
Printing some Q and Qe and total Qs values:  [[0.951]
 [1.144]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]] [[4.45 ]
 [5.376]
 [4.45 ]
 [4.45 ]
 [4.45 ]
 [4.45 ]
 [4.45 ]] [[2.285]
 [2.884]
 [2.285]
 [2.285]
 [2.285]
 [2.285]
 [2.285]]
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.729]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[1.257]
 [3.325]
 [1.257]
 [1.257]
 [1.257]
 [1.257]
 [1.257]] [[0.65 ]
 [1.258]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]]
from probs:  [0.1781962378764628, 0.14152342773643217, 0.18937954591686337, 0.15939876265961922, 0.15939876265961922, 0.17210326315100316]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.17861572166528156, 0.1418565652598518, 0.18747133661749874, 0.15977398772439397, 0.15977398772439397, 0.17250840100858006]
from probs:  [0.17861572166528156, 0.1418565652598518, 0.18747133661749874, 0.15977398772439397, 0.15977398772439397, 0.17250840100858006]
siam score:  -0.8726997
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
using explorer policy with actor:  1
siam score:  -0.8746933
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.628]
 [0.679]
 [0.688]
 [0.673]
 [0.68 ]
 [0.646]] [[4.39 ]
 [3.985]
 [3.847]
 [4.119]
 [3.174]
 [3.312]
 [4.469]] [[1.512]
 [1.349]
 [1.356]
 [1.494]
 [1.034]
 [1.108]
 [1.599]]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.17817458024101185, 0.1405531125445241, 0.18463765316448055, 0.16126058088810433, 0.16126058088810433, 0.1741134922737748]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.619]
 [0.649]
 [0.642]
 [0.644]
 [0.674]
 [0.666]] [[1.589]
 [1.455]
 [2.439]
 [1.605]
 [1.548]
 [1.676]
 [1.729]] [[1.487]
 [1.298]
 [2.237]
 [1.458]
 [1.408]
 [1.554]
 [1.596]]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.17745083319370414, 0.14032963579816887, 0.18380946679681576, 0.1624787313764485, 0.1624787313764485, 0.1734526014584143]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.609]
 [0.598]
 [0.604]
 [0.602]
 [0.607]
 [0.605]] [[6.587]
 [5.963]
 [7.241]
 [7.256]
 [7.169]
 [7.342]
 [7.263]] [[1.497]
 [1.246]
 [1.76 ]
 [1.769]
 [1.733]
 [1.805]
 [1.772]]
siam score:  -0.87569785
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]] [[7.528]
 [7.528]
 [7.528]
 [7.528]
 [7.528]
 [7.528]
 [7.528]] [[1.967]
 [1.967]
 [1.967]
 [1.967]
 [1.967]
 [1.967]
 [1.967]]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
using explorer policy with actor:  0
from probs:  [0.17745083824008573, 0.1403296234739241, 0.18380947481867915, 0.162478729416731, 0.162478729416731, 0.17345260463384896]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.659]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]] [[2.793]
 [3.047]
 [2.64 ]
 [2.64 ]
 [2.64 ]
 [2.64 ]
 [2.64 ]] [[0.791]
 [0.853]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.704]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[0.228]
 [6.456]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]] [[0.368]
 [2.139]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.519]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[5.238]
 [5.663]
 [5.238]
 [5.238]
 [5.238]
 [5.238]
 [5.238]] [[1.621]
 [1.873]
 [1.621]
 [1.621]
 [1.621]
 [1.621]
 [1.621]]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.17843487220173343, 0.14110777353463796, 0.18482877543072254, 0.1633797241731917, 0.1616761153173304, 0.17057273934238384]
siam score:  -0.865051
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[1.778]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]] [[2.047]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.048]
 [0.125]
 [0.169]
 [0.161]
 [0.152]
 [0.176]] [[4.532]
 [4.022]
 [4.104]
 [4.399]
 [4.239]
 [4.039]
 [4.556]] [[1.   ]
 [0.746]
 [0.858]
 [1.062]
 [0.965]
 [0.843]
 [1.158]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.211]
 [0.228]
 [0.246]
 [0.257]
 [0.231]
 [0.23 ]] [[3.579]
 [3.231]
 [2.813]
 [3.081]
 [3.022]
 [3.011]
 [3.162]] [[1.269]
 [0.824]
 [0.362]
 [0.709]
 [0.659]
 [0.6  ]
 [0.776]]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.17860775608029636, 0.1378702078452304, 0.1871403658125715, 0.16369814328495239, 0.16369814328495239, 0.16898538369199692]
siam score:  -0.86513627
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.811]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]] [[5.098]
 [6.167]
 [5.114]
 [5.114]
 [5.114]
 [5.114]
 [5.114]] [[1.607]
 [1.917]
 [1.617]
 [1.617]
 [1.617]
 [1.617]
 [1.617]]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.1791244795700787, 0.13706987794521538, 0.18768177467982328, 0.16247786642690362, 0.16417173232568286, 0.16947426905229604]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
407 296
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.18214889206419585, 0.1358509397616199, 0.1842484503662894, 0.16353425144769623, 0.16188172722970087, 0.17233573913049768]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[4.128]
 [4.128]
 [4.128]
 [4.128]
 [4.128]
 [4.128]
 [4.128]] [[2.09]
 [2.09]
 [2.09]
 [2.09]
 [2.09]
 [2.09]
 [2.09]]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.18214889206419585, 0.1358509397616199, 0.1842484503662894, 0.16353425144769623, 0.16188172722970087, 0.17233573913049768]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.043]
 [-0.043]
 [-0.048]
 [-0.044]
 [-0.046]
 [-0.044]] [[3.7  ]
 [3.7  ]
 [3.7  ]
 [3.77 ]
 [3.831]
 [4.052]
 [3.978]] [[0.349]
 [0.35 ]
 [0.35 ]
 [0.401]
 [0.457]
 [0.64 ]
 [0.58 ]]
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]] [[4.298]
 [4.298]
 [4.298]
 [4.298]
 [4.298]
 [4.298]
 [4.298]] [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
from probs:  [0.18253212891084944, 0.13613676684066037, 0.18253212891084944, 0.1638783235424229, 0.1622223224535932, 0.17269832934162457]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.18327735486656574, 0.13669257393490902, 0.17919464372873514, 0.1645473913991986, 0.16288462933627926, 0.1734034067343124]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 3.3667372269040494
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.1821338411278157, 0.1362365746812676, 0.18010542261070234, 0.16538370471330227, 0.16371248928872495, 0.17242796757818724]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.501]
 [0.603]
 [0.603]
 [0.603]
 [0.596]
 [0.669]] [[4.631]
 [3.013]
 [2.844]
 [2.844]
 [2.844]
 [1.968]
 [2.398]] [[1.966]
 [0.829]
 [0.819]
 [0.819]
 [0.819]
 [0.269]
 [0.604]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 1.1667320412492035
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.968]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]] [[2.141]
 [2.26 ]
 [2.141]
 [2.141]
 [2.141]
 [2.141]
 [2.141]] [[1.476]
 [1.646]
 [1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.476]]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.18466050887315744, 0.13697943266809517, 0.17670251719236865, 0.16269645183418038, 0.1659835896223266, 0.17297749980987176]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.18500879452745733, 0.13723778769821807, 0.17514970709651825, 0.16300331138160132, 0.16629664899526683, 0.17330375030093817]
line 256 mcts: sample exp_bonus 3.710413168237726
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.18555379597005253, 0.13650866885959084, 0.17566566550423365, 0.16348348877034477, 0.16678652794309015, 0.17200185295268813]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
first move QE:  0.5907726292128495
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.714]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[0.837]
 [4.907]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.837]] [[0.541]
 [2.183]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.18588367614692175, 0.1367513559121093, 0.17597796642216118, 0.16377413204125615, 0.1670830434040088, 0.17052982607354286]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.859660671976654
first move QE:  0.5966121759486065
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.1865544970778238, 0.13724486708951128, 0.1747516601125362, 0.16436516358308317, 0.1676860162149491, 0.1693977959220965]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]] [[3.269]
 [3.269]
 [3.269]
 [3.269]
 [3.269]
 [3.269]
 [3.269]] [[-0.357]
 [-0.357]
 [-0.357]
 [-0.357]
 [-0.357]
 [-0.357]
 [-0.357]]
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.735]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]] [[3.562]
 [3.99 ]
 [3.562]
 [3.562]
 [3.562]
 [3.562]
 [3.562]] [[1.538]
 [1.675]
 [1.538]
 [1.538]
 [1.538]
 [1.538]
 [1.538]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.03 ]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.03 ]
 [-0.031]] [[4.228]
 [3.596]
 [3.567]
 [3.521]
 [3.548]
 [3.759]
 [3.656]] [[-0.091]
 [-0.329]
 [-0.342]
 [-0.359]
 [-0.349]
 [-0.27 ]
 [-0.308]]
first move QE:  0.5993284806662136
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[4.622]
 [5.709]
 [5.709]
 [5.709]
 [5.709]
 [5.709]
 [5.709]] [[1.062]
 [1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.18538067216690216, 0.13679112216964082, 0.17560784046864977, 0.16517045621491627, 0.1668221634873926, 0.1702277454924985]
from probs:  [0.18538067216690216, 0.13679112216964082, 0.17560784046864977, 0.16517045621491627, 0.1668221634873926, 0.1702277454924985]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.18558624648709204, 0.13583388333687768, 0.17580257739565724, 0.16535361880600488, 0.16700715770739766, 0.17041651626697046]
using explorer policy with actor:  1
in main func line 156:  436
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.572]
 [0.581]
 [0.579]
 [0.566]
 [0.563]
 [0.572]] [[2.976]
 [3.319]
 [2.977]
 [2.943]
 [2.995]
 [3.118]
 [3.22 ]] [[0.892]
 [1.069]
 [0.895]
 [0.876]
 [0.896]
 [0.959]
 [1.017]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.1868952182841014, 0.13679192230291015, 0.17338883735987878, 0.16651987791841696, 0.16651987791841696, 0.16988426621627584]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.392]
 [0.384]
 [0.376]
 [0.373]
 [0.357]
 [0.369]] [[2.695]
 [1.457]
 [1.844]
 [1.687]
 [1.789]
 [2.104]
 [1.995]] [[0.829]
 [0.058]
 [0.306]
 [0.191]
 [0.255]
 [0.444]
 [0.388]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5022113082743855
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
using explorer policy with actor:  1
from probs:  [0.18722667509791774, 0.13703452141881375, 0.17192285067092447, 0.16681519926841545, 0.16681519926841545, 0.17018555427551324]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.188061265539053, 0.13653992470439208, 0.17268922206382367, 0.16755880255396588, 0.16591639102770778, 0.1692343941110575]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
447 309
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.903]
 [1.112]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]] [[2.685]
 [3.596]
 [2.685]
 [2.685]
 [2.685]
 [2.685]
 [2.685]] [[1.386]
 [2.024]
 [1.386]
 [1.386]
 [1.386]
 [1.386]
 [1.386]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.18375049203525343, 0.13714803423462368, 0.17100763248039372, 0.16768806402492606, 0.16768806402492606, 0.17271771319987705]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.18394821942597686, 0.13621954982143014, 0.17119164773167073, 0.16786850720626323, 0.16786850720626323, 0.17290356860839579]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.775878798941645
first move QE:  0.6508314022563728
siam score:  -0.8670569
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.356613297570727
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.032]] [[2.907]
 [2.907]
 [2.907]
 [2.907]
 [2.907]
 [2.907]
 [3.505]] [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [1.474]]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[4.885]
 [3.651]
 [3.651]
 [3.651]
 [3.651]
 [3.651]
 [3.651]] [[1.111]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.1832877335706893, 0.13717618223732492, 0.17239387956818197, 0.16904740154126793, 0.16904740154126793, 0.16904740154126793]
first move QE:  0.6532954646104804
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.204]] [[3.443]
 [3.443]
 [3.443]
 [3.443]
 [3.443]
 [3.443]
 [3.868]] [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.863]]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.1837823028569629, 0.1364768932553299, 0.17285905373076765, 0.16950354582925667, 0.16787465849842614, 0.16950354582925667]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.1837823028569629, 0.1364768932553299, 0.17285905373076765, 0.16950354582925667, 0.16787465849842614, 0.16950354582925667]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.536568584138317
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.18416705136209896, 0.13466910643741734, 0.17322093439875508, 0.16985840173681283, 0.16822610432810303, 0.16985840173681283]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.65 ]
 [0.649]
 [0.629]
 [0.644]
 [0.644]
 [0.659]] [[1.632]
 [1.674]
 [1.841]
 [1.175]
 [1.539]
 [1.562]
 [1.422]] [[0.681]
 [0.695]
 [0.749]
 [0.486]
 [0.638]
 [0.645]
 [0.629]]
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.812]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]] [[5.345]
 [5.316]
 [5.345]
 [5.345]
 [5.345]
 [5.345]
 [5.345]] [[2.345]
 [2.313]
 [2.345]
 [2.345]
 [2.345]
 [2.345]
 [2.345]]
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.1827846317648665, 0.13404094338418449, 0.17373223249416841, 0.1703597700207711, 0.16872265231523842, 0.1703597700207711]
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.18309643154768612, 0.13426959464787436, 0.17232275890784082, 0.17065037508302824, 0.1690104647305421, 0.17065037508302824]
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.18309643154768612, 0.13426959464787436, 0.17232275890784082, 0.17065037508302824, 0.1690104647305421, 0.17065037508302824]
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
first move QE:  0.6716997918069711
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.18339718649243192, 0.13449014643131446, 0.17260581697399582, 0.1709306860846961, 0.16928808200878084, 0.16928808200878084]
siam score:  -0.8587118
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.18184905126643552, 0.1347451160809357, 0.17293304722724653, 0.17125474058457568, 0.1696090224204033, 0.1696090224204033]
siam score:  -0.8604135
siam score:  -0.8580394
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.18184905126643552, 0.1347451160809357, 0.17293304722724653, 0.17125474058457568, 0.1696090224204033, 0.1696090224204033]
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.528]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.5  ]] [[1.636]
 [3.578]
 [2.798]
 [2.798]
 [2.798]
 [2.798]
 [2.474]] [[0.488]
 [0.528]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.5  ]]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
siam score:  -0.86064774
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.849]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[1.885]
 [1.855]
 [1.885]
 [1.885]
 [1.885]
 [1.885]
 [1.885]] [[0.701]
 [0.849]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 4.163309718931449
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
from probs:  [0.1791209780738286, 0.13546161697416179, 0.17385267645598515, 0.17216544260451894, 0.1705109705754113, 0.16888831531609422]
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]] [[1.1  ]
 [1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.023]] [[0.075]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]]
Printing some Q and Qe and total Qs values:  [[0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]] [[4.96]
 [4.96]
 [4.96]
 [4.96]
 [4.96]
 [4.96]
 [4.96]] [[0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]]
line 256 mcts: sample exp_bonus 4.570213173937216
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[3.295]
 [3.295]
 [3.295]
 [3.295]
 [3.295]
 [3.295]
 [3.295]] [[0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]]
line 256 mcts: sample exp_bonus 0.7456503056481273
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.169293999585094
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.17811939643738164, 0.13503968985195355, 0.17462716774759757, 0.17293240970696705, 0.1696406681280501, 0.1696406681280501]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.227]
 [0.219]
 [0.219]
 [0.218]
 [0.217]
 [0.212]] [[0.774]
 [2.065]
 [1.178]
 [1.044]
 [1.059]
 [1.063]
 [1.442]] [[0.216]
 [0.227]
 [0.219]
 [0.219]
 [0.218]
 [0.217]
 [0.212]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.447]
 [0.457]
 [0.527]
 [0.557]
 [0.461]
 [0.609]] [[4.214]
 [3.825]
 [4.318]
 [3.869]
 [3.49 ]
 [3.892]
 [3.672]] [[0.584]
 [0.447]
 [0.457]
 [0.527]
 [0.557]
 [0.461]
 [0.609]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.253]] [[2.629]
 [2.629]
 [2.629]
 [2.629]
 [2.629]
 [2.629]
 [2.453]] [[-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.313]]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.17811941972987297, 0.13503962552920276, 0.17462718393761567, 0.17293242245019666, 0.16964067417655598, 0.16964067417655598]
from probs:  [0.17811941972987297, 0.13503962552920276, 0.17462718393761567, 0.17293242245019666, 0.16964067417655598, 0.16964067417655598]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.469]
 [0.484]
 [0.479]
 [0.523]
 [0.498]
 [0.515]] [[5.059]
 [5.382]
 [4.777]
 [4.548]
 [4.305]
 [4.565]
 [4.386]] [[0.48 ]
 [0.469]
 [0.484]
 [0.479]
 [0.523]
 [0.498]
 [0.515]]
from probs:  [0.17841593013657522, 0.13526439035553048, 0.1749178783350178, 0.17155567320536544, 0.16992306398375542, 0.16992306398375542]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.361670950307085
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]] [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.483]] [[2.099]
 [3.427]
 [3.427]
 [3.427]
 [3.427]
 [3.427]
 [2.991]] [[0.656]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.483]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.825421493302451
rdn probs:  [0.17696203613020703, 0.13550371185505813, 0.17522743361342502, 0.1718592733866638, 0.170223772507323, 0.170223772507323]
in main func line 156:  480
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
from probs:  [0.17470437657520568, 0.13642321094052498, 0.17470437657520568, 0.17302549050284438, 0.16976365470511387, 0.17137889070110543]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
first move QE:  0.7213418747460782
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
483 322
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.17590821953551591, 0.13532453542633666, 0.1742177622895025, 0.1742177622895025, 0.1677719066224607, 0.17255981383668165]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
using another actor
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.1745127744094775, 0.13555366034831273, 0.1745127744094775, 0.1745127744094775, 0.16805599911958977, 0.17285201730366506]
using another actor
from probs:  [0.17479754416080637, 0.13577484318305108, 0.17479754416080637, 0.17479754416080637, 0.1683302304785117, 0.17150229385601815]
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]] [[0.192]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[0.182]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.17564176168180184, 0.1364305664970069, 0.17397025881014233, 0.17564176168180184, 0.16759378489233004, 0.1707218664369172]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]] [[1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]] [[2.802]
 [2.802]
 [2.802]
 [2.802]
 [2.802]
 [2.802]
 [2.802]]
siam score:  -0.8642041
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
siam score:  -0.86613816
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.17713908525031727, 0.13688484213529167, 0.17383152419095727, 0.17546955442987838, 0.1675772269150766, 0.16909776707847876]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.468]
 [0.44 ]
 [0.448]
 [0.448]
 [0.45 ]
 [0.452]] [[0.24 ]
 [0.   ]
 [0.247]
 [0.03 ]
 [0.054]
 [0.206]
 [0.097]] [[0.448]
 [0.4  ]
 [0.426]
 [0.368]
 [0.378]
 [0.431]
 [0.399]]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.17713908525031727, 0.13688484213529167, 0.17383152419095727, 0.17546955442987838, 0.1675772269150766, 0.16909776707847876]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.1773151572366854, 0.1360269264466399, 0.1740043085412572, 0.17564396694280257, 0.16774379464444758, 0.16926584618816734]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.1773151572366854, 0.1360269264466399, 0.1740043085412572, 0.17564396694280257, 0.16774379464444758, 0.16926584618816734]
first move QE:  0.7622806773146792
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
siam score:  -0.8588718
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
from probs:  [0.17508333435785675, 0.13715315944690687, 0.1734794967150172, 0.17508333435785675, 0.1688413716397785, 0.1703593034825839]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.73 ]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[1.328]
 [3.694]
 [1.328]
 [1.328]
 [1.328]
 [1.328]
 [1.328]] [[0.518]
 [1.778]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
using explorer policy with actor:  1
siam score:  -0.8698996
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.633]
 [0.722]
 [0.727]
 [0.719]
 [0.718]
 [0.715]] [[2.564]
 [3.775]
 [2.406]
 [1.856]
 [2.128]
 [2.013]
 [2.33 ]] [[0.887]
 [1.35 ]
 [0.819]
 [0.585]
 [0.697]
 [0.647]
 [0.781]]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.362]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.341]] [[3.983]
 [4.24 ]
 [5.147]
 [5.147]
 [5.147]
 [5.147]
 [4.331]] [[0.978]
 [1.187]
 [1.838]
 [1.838]
 [1.838]
 [1.838]
 [1.231]]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[3.467]
 [3.467]
 [3.467]
 [3.467]
 [3.467]
 [3.467]
 [3.467]] [[1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.279]]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.17585520087788478, 0.1360927183126135, 0.17585520087788478, 0.17585520087788478, 0.16964231297706114, 0.16669936607667102]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.255]] [[4.986]
 [4.986]
 [4.986]
 [4.986]
 [4.986]
 [4.986]
 [5.062]] [[1.51 ]
 [1.51 ]
 [1.51 ]
 [1.51 ]
 [1.51 ]
 [1.51 ]
 [1.578]]
line 256 mcts: sample exp_bonus 9.41200244704657
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.17585520087788478, 0.1360927183126135, 0.17585520087788478, 0.17585520087788478, 0.16964231297706114, 0.16669936607667102]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.17585520087788478, 0.1360927183126135, 0.17585520087788478, 0.17585520087788478, 0.16964231297706114, 0.16669936607667102]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.17610757839188446, 0.13628803094340142, 0.17610757839188446, 0.17610757839188446, 0.16988577410305897, 0.16550345977788625]
siam score:  -0.8753365
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.45 ]
 [0.472]
 [0.534]
 [0.55 ]
 [0.504]
 [0.502]] [[6.26 ]
 [5.532]
 [4.052]
 [5.344]
 [5.173]
 [4.474]
 [5.787]] [[1.87 ]
 [1.549]
 [0.936]
 [1.512]
 [1.448]
 [1.13 ]
 [1.683]]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
first move QE:  0.8049679915589092
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.387160379674219
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.314]
 [0.802]
 [0.317]
 [0.33 ]
 [0.802]
 [0.802]] [[1.497]
 [1.181]
 [3.16 ]
 [0.764]
 [0.802]
 [3.16 ]
 [3.16 ]] [[ 0.163]
 [-0.04 ]
 [ 2.254]
 [-0.311]
 [-0.261]
 [ 2.254]
 [ 2.254]]
line 256 mcts: sample exp_bonus 3.918150037219196
using another actor
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.242]
 [0.295]
 [0.302]
 [0.302]
 [0.306]
 [0.295]] [[6.031]
 [3.989]
 [6.102]
 [6.318]
 [6.269]
 [6.298]
 [6.104]] [[1.326]
 [0.032]
 [1.369]
 [1.507]
 [1.477]
 [1.5  ]
 [1.37 ]]
Printing some Q and Qe and total Qs values:  [[1.001]
 [1.359]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]] [[0.925]
 [1.502]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]] [[1.463]
 [2.406]
 [1.463]
 [1.463]
 [1.463]
 [1.463]
 [1.463]]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.719]
 [0.671]
 [0.648]
 [0.664]
 [0.652]
 [0.675]] [[-0.243]
 [-0.016]
 [-0.663]
 [-0.594]
 [-0.484]
 [-0.687]
 [-0.762]] [[0.659]
 [0.719]
 [0.671]
 [0.648]
 [0.664]
 [0.652]
 [0.675]]
using another actor
from probs:  [0.17735117183402652, 0.13471355379246897, 0.1757564141722124, 0.1757564141722124, 0.16965716557088828, 0.16676528045819153]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]] [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.17762934352288712, 0.1349248492487287, 0.17446360495121205, 0.17603208451626923, 0.16992326936815172, 0.1670268483927512]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.17806697775922167, 0.13433779548337965, 0.17334917163571192, 0.17646578295366683, 0.17034191510084312, 0.16743835706717672]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.573]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[0.863]
 [1.348]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]] [[0.253]
 [0.602]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
from probs:  [0.17806698192927312, 0.1343377836579957, 0.17334917408006537, 0.17646578653802686, 0.17034191644519023, 0.16743835734944876]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.16 ]
 [0.162]
 [0.195]
 [0.137]
 [0.171]
 [0.165]] [[3.45 ]
 [3.114]
 [3.583]
 [3.717]
 [2.491]
 [4.081]
 [4.275]] [[1.069]
 [0.725]
 [1.166]
 [1.322]
 [0.118]
 [1.641]
 [1.817]]
siam score:  -0.87550825
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.513]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[5.839]
 [4.441]
 [5.839]
 [5.839]
 [5.839]
 [5.839]
 [5.839]] [[1.938]
 [1.06 ]
 [1.938]
 [1.938]
 [1.938]
 [1.938]
 [1.938]]
siam score:  -0.87567556
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.019]
 [-0.011]
 [-0.012]
 [-0.012]
 [-0.015]
 [-0.013]] [[7.471]
 [7.583]
 [7.76 ]
 [8.038]
 [7.578]
 [8.088]
 [7.992]] [[1.216]
 [1.262]
 [1.347]
 [1.473]
 [1.263]
 [1.495]
 [1.452]]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[4.775]
 [4.775]
 [4.775]
 [4.775]
 [4.775]
 [4.775]
 [4.775]] [[1.982]
 [1.982]
 [1.982]
 [1.982]
 [1.982]
 [1.982]
 [1.982]]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.17845594702424417, 0.1349357071940479, 0.17226300841292969, 0.17845594702424417, 0.17078208831022404, 0.16510730203431]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.17845595128280692, 0.13493569573208597, 0.1722630104344587, 0.17845595128280692, 0.17078208979681025, 0.16510730147103117]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.17714755431021478, 0.1351505845982709, 0.17253736064258787, 0.17874016666812226, 0.17105408094082963, 0.16537025283997456]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.599]
 [0.502]
 [0.502]
 [0.501]
 [0.502]
 [0.502]] [[-0.067]
 [ 0.694]
 [-0.562]
 [-0.567]
 [-0.615]
 [-0.56 ]
 [-0.418]] [[0.514]
 [0.599]
 [0.502]
 [0.502]
 [0.501]
 [0.502]
 [0.502]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.17715873167316065, 0.13636316804815538, 0.1725887938553212, 0.17873693842706917, 0.16967236630205612, 0.16548000169423757]
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.17743876673322423, 0.13657871750255807, 0.17286160520815214, 0.17743876673322423, 0.1699405676536902, 0.16574157616915117]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.17809638018475438, 0.13616584470898083, 0.17350225195001745, 0.17809638018475438, 0.16914172345602985, 0.16499741951546312]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.218]
 [0.261]] [[3.257]
 [3.257]
 [3.257]
 [3.257]
 [3.257]
 [3.615]
 [3.32 ]] [[1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.48 ]
 [1.234]]
siam score:  -0.8649815
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.17856969366535158, 0.13652770853539833, 0.17396335442502625, 0.17856969366535158, 0.16959123582403954, 0.16277831388483255]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.17884916739058124, 0.13674138369224192, 0.17423561891580666, 0.1772840993239173, 0.16985665765161387, 0.16303307302583914]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.475]
 [0.437]
 [0.452]
 [0.481]
 [0.465]
 [0.447]] [[5.184]
 [5.416]
 [5.364]
 [5.33 ]
 [5.511]
 [5.386]
 [5.244]] [[1.505]
 [1.708]
 [1.645]
 [1.631]
 [1.779]
 [1.68 ]
 [1.566]]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[5.518]
 [5.518]
 [5.518]
 [5.518]
 [5.518]
 [5.518]
 [5.518]] [[2.104]
 [2.104]
 [2.104]
 [2.104]
 [2.104]
 [2.104]
 [2.104]]
siam score:  -0.8717717
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.1784332741195005, 0.13790749428780158, 0.17689930598264844, 0.17689930598264844, 0.168227948911518, 0.161632670715883]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.499]
 [0.493]
 [0.493]
 [0.49 ]
 [0.492]
 [0.486]] [[3.26 ]
 [2.78 ]
 [2.842]
 [2.421]
 [2.818]
 [2.657]
 [2.398]] [[0.484]
 [0.499]
 [0.493]
 [0.493]
 [0.49 ]
 [0.492]
 [0.486]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.151]
 [0.159]
 [0.152]
 [0.153]
 [0.154]
 [0.152]] [[0.575]
 [1.508]
 [1.094]
 [0.528]
 [0.808]
 [0.653]
 [0.686]] [[0.153]
 [0.151]
 [0.159]
 [0.152]
 [0.153]
 [0.154]
 [0.152]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.6357110220298257
from probs:  [0.17859615337594023, 0.13712057033700809, 0.17706078396654867, 0.17706078396654867, 0.1683815056523024, 0.16178020270165208]
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.1791367513768098, 0.13753562466377314, 0.17608326966240462, 0.17608326966240462, 0.16889118463260738, 0.16226990000200037]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.567]
 [0.562]
 [0.556]
 [0.551]
 [0.554]
 [0.554]] [[1.051]
 [1.637]
 [1.096]
 [1.043]
 [1.099]
 [1.068]
 [1.079]] [[0.557]
 [0.567]
 [0.562]
 [0.556]
 [0.551]
 [0.554]
 [0.554]]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.578]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]] [[1.043]
 [1.533]
 [1.043]
 [1.043]
 [1.043]
 [1.043]
 [1.043]] [[0.556]
 [0.578]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9961121212121212 -1.0 -0.9961121212121212
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[3.662]
 [3.215]
 [3.215]
 [3.215]
 [3.215]
 [3.215]
 [3.215]] [[1.625]
 [1.353]
 [1.353]
 [1.353]
 [1.353]
 [1.353]
 [1.353]]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.858]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[1.553]
 [1.51 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]] [[1.476]
 [2.06 ]
 [1.589]
 [1.589]
 [1.589]
 [1.589]
 [1.589]]
siam score:  -0.877439
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.818]
 [0.54 ]
 [0.536]
 [0.212]
 [0.554]
 [0.458]] [[1.661]
 [1.869]
 [1.22 ]
 [1.561]
 [1.854]
 [1.15 ]
 [2.12 ]] [[1.214]
 [1.676]
 [1.218]
 [1.362]
 [1.279]
 [1.196]
 [1.552]]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
siam score:  -0.8802935
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.24501146832507506
using explorer policy with actor:  1
siam score:  -0.87672776
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.17806035052875016, 0.13934358937474187, 0.176568949867964, 0.17224265538915462, 0.1708478391364237, 0.16293661570296566]
545 390
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
from probs:  [0.17753294499235583, 0.13739810137326888, 0.17753294499235583, 0.1731830292376543, 0.1717805973823134, 0.16257238202205165]
from probs:  [0.17801516121718605, 0.13777130305561527, 0.17653681540716915, 0.17365343019093787, 0.17224718904039885, 0.1617761010886929]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.567]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.587]] [[3.319]
 [3.309]
 [2.998]
 [2.998]
 [2.998]
 [2.998]
 [4.261]] [[1.176]
 [1.136]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [1.907]]
maxi score, test score, baseline:  -0.996168656716418 -1.0 -0.996168656716418
probs:  [0.17827432801122317, 0.13797185337786202, 0.17533800485936402, 0.1739062439836641, 0.17249795459772974, 0.16201161517015703]
Printing some Q and Qe and total Qs values:  [[0.742]
 [0.719]
 [0.738]
 [0.732]
 [0.732]
 [0.886]
 [0.733]] [[2.818]
 [3.541]
 [2.588]
 [2.727]
 [2.588]
 [2.391]
 [3.096]] [[0.742]
 [0.719]
 [0.738]
 [0.732]
 [0.732]
 [0.886]
 [0.733]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.17705596376915111, 0.1381764134563624, 0.17559798063242152, 0.17416409639051392, 0.172753718447654, 0.16225182730389695]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.17705596376915111, 0.1381764134563624, 0.17559798063242152, 0.17416409639051392, 0.172753718447654, 0.16225182730389695]
siam score:  -0.8627704
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
first move QE:  0.8862521241040411
line 256 mcts: sample exp_bonus 2.0961854998996774
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.17705596376915111, 0.1381764134563624, 0.17559798063242152, 0.17416409639051392, 0.172753718447654, 0.16225182730389695]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.468]
 [0.476]
 [0.471]
 [0.471]
 [0.481]
 [0.473]] [[2.241]
 [3.112]
 [2.195]
 [2.014]
 [1.978]
 [2.209]
 [2.314]] [[0.478]
 [0.468]
 [0.476]
 [0.471]
 [0.471]
 [0.481]
 [0.473]]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.1773060322862236, 0.13837156966613842, 0.1758459899379704, 0.17299771060285352, 0.17299771060285352, 0.16248098690396043]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.1773060322862236, 0.13837156966613842, 0.1758459899379704, 0.17299771060285352, 0.17299771060285352, 0.16248098690396043]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.1773060322862236, 0.13837156966613842, 0.1758459899379704, 0.17299771060285352, 0.17299771060285352, 0.16248098690396043]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.507]] [[5.602]
 [5.967]
 [5.967]
 [5.967]
 [5.967]
 [5.967]
 [5.845]] [[1.282]
 [1.563]
 [1.563]
 [1.563]
 [1.563]
 [1.563]
 [1.456]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1749337301040086
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9961962962962964 -1.0 -0.9961962962962964
probs:  [0.17780840072715814, 0.1387636107114365, 0.17490424279210445, 0.17348787068443483, 0.17209452901754033, 0.1629413460673257]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.208]
 [0.25 ]
 [0.249]
 [0.249]
 [0.25 ]
 [0.254]] [[-0.948]
 [ 2.023]
 [ 1.083]
 [ 0.889]
 [ 0.874]
 [ 0.96 ]
 [ 0.916]] [[0.279]
 [0.208]
 [0.25 ]
 [0.249]
 [0.249]
 [0.25 ]
 [0.254]]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.1782108533660751, 0.13818827310690412, 0.1753001202563172, 0.1738805414036074, 0.1711100729975125, 0.16331013886958376]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.514]
 [0.535]
 [0.541]
 [0.552]
 [0.591]
 [0.573]] [[3.686]
 [3.209]
 [2.921]
 [2.938]
 [3.169]
 [2.961]
 [2.887]] [[0.46 ]
 [0.281]
 [0.228]
 [0.245]
 [0.343]
 [0.352]
 [0.291]]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.177003109866166, 0.1383913616841774, 0.17555775030855142, 0.17413608516991416, 0.1713615451412834, 0.1635501478299075]
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]] [[6.317]
 [6.317]
 [6.317]
 [6.317]
 [6.317]
 [6.317]
 [6.317]] [[0.22]
 [0.22]
 [0.22]
 [0.22]
 [0.22]
 [0.22]
 [0.22]]
siam score:  -0.8482154
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.177003109866166, 0.1383913616841774, 0.17555775030855142, 0.17413608516991416, 0.1713615451412834, 0.1635501478299075]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.17725100401540872, 0.1385851797978432, 0.17580362022116827, 0.17297945672021137, 0.17160153823788965, 0.16377920100747867]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.17789971974809066, 0.13909237108434874, 0.1764470382473089, 0.17361253775797855, 0.17222957582568435, 0.16071875733658883]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.17694880195556484, 0.1394879090576616, 0.17694880195556484, 0.17271934630580157, 0.17271934630580157, 0.16117579441960558]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.17720272110909924, 0.13968807233316713, 0.17576773454389966, 0.17296719624730045, 0.17296719624730045, 0.16140707951923308]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[4.237]
 [4.237]
 [4.237]
 [4.237]
 [4.237]
 [4.237]
 [4.237]] [[1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
564 404
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.604]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[1.802]
 [3.722]
 [1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]] [[1.452]
 [2.213]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]]
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.7  ]
 [0.642]
 [0.673]
 [0.639]
 [0.643]
 [0.673]] [[1.697]
 [2.632]
 [1.444]
 [2.311]
 [1.388]
 [1.641]
 [2.311]] [[0.611]
 [1.244]
 [0.453]
 [1.011]
 [0.416]
 [0.57 ]
 [1.011]]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.59 ]
 [0.58 ]
 [0.576]
 [0.574]
 [0.574]
 [0.573]] [[6.919]
 [6.452]
 [6.746]
 [6.563]
 [6.648]
 [6.691]
 [6.91 ]] [[0.988]
 [0.86 ]
 [0.937]
 [0.868]
 [0.893]
 [0.907]
 [0.979]]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[6.258]
 [6.866]
 [6.866]
 [6.866]
 [6.866]
 [6.866]
 [6.866]] [[0.751]
 [0.96 ]
 [0.96 ]
 [0.96 ]
 [0.96 ]
 [0.96 ]
 [0.96 ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8695095
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.685]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]] [[4.266]
 [5.143]
 [4.266]
 [4.266]
 [4.266]
 [4.266]
 [4.266]] [[1.946]
 [2.496]
 [1.946]
 [1.946]
 [1.946]
 [1.946]
 [1.946]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
start point for exploration sampling:  11106
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.018]
 [-0.017]
 [-0.017]
 [-0.016]
 [-0.018]
 [-0.017]] [[4.572]
 [4.543]
 [4.588]
 [4.499]
 [4.525]
 [4.716]
 [4.631]] [[0.108]
 [0.089]
 [0.12 ]
 [0.061]
 [0.08 ]
 [0.205]
 [0.149]]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 7.393183975537695
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
from probs:  [0.1753773676081756, 0.13896948352350189, 0.1753773676081756, 0.17399166147452863, 0.17262795067633635, 0.16365616910928202]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.534]
 [0.543]
 [0.56 ]
 [0.458]
 [0.548]
 [0.545]] [[6.406]
 [5.897]
 [6.094]
 [5.668]
 [7.063]
 [6.53 ]
 [5.918]] [[1.681]
 [1.407]
 [1.516]
 [1.316]
 [1.922]
 [1.741]
 [1.429]]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.17471412588634752, 0.13954652558013678, 0.17334475256093207, 0.17471412588634752, 0.17334475256093207, 0.16433571752530413]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.17471412588634752, 0.13954652558013678, 0.17334475256093207, 0.17471412588634752, 0.17334475256093207, 0.16433571752530413]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.17471412588634752, 0.13954652558013678, 0.17334475256093207, 0.17471412588634752, 0.17334475256093207, 0.16433571752530413]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.17471412588634752, 0.13954652558013678, 0.17334475256093207, 0.17471412588634752, 0.17334475256093207, 0.16433571752530413]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.17471412588634752, 0.13954652558013678, 0.17334475256093207, 0.17471412588634752, 0.17334475256093207, 0.16433571752530413]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.17471412870234754, 0.13954651609014543, 0.17334475489775536, 0.17471412870234754, 0.17334475489775536, 0.1643357167096488]
575 405
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.17492581855929348, 0.13971559561592756, 0.17355478557265946, 0.17492581855929348, 0.17355478557265946, 0.16332319612016663]
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]] [[2.265]
 [2.265]
 [2.265]
 [2.265]
 [2.265]
 [2.265]
 [2.265]] [[0.87]
 [0.87]
 [0.87]
 [0.87]
 [0.87]
 [0.87]
 [0.87]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.17516218976025055, 0.13990438846194744, 0.17243803877411198, 0.17516218976025055, 0.17378930414422042, 0.16354388909921924]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.359]] [[5.027]
 [5.027]
 [5.027]
 [5.027]
 [5.027]
 [5.027]
 [5.594]] [[0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.298]]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.17516218976025055, 0.13990438846194744, 0.17243803877411198, 0.17516218976025055, 0.17378930414422042, 0.16354388909921924]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.328]
 [0.401]
 [0.403]
 [0.408]
 [0.409]
 [0.509]] [[3.458]
 [4.846]
 [3.796]
 [3.616]
 [3.678]
 [3.87 ]
 [3.837]] [[0.088]
 [0.379]
 [0.175]
 [0.118]
 [0.148]
 [0.215]
 [0.403]]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.776]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]] [[3.275]
 [3.372]
 [3.789]
 [3.789]
 [3.789]
 [3.789]
 [3.789]] [[1.112]
 [1.277]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.1740282249910939, 0.14009672524088354, 0.1726751019378089, 0.17540299801323142, 0.1740282249910939, 0.16376872482588825]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.1740282249910939, 0.14009672524088354, 0.1726751019378089, 0.17540299801323142, 0.1740282249910939, 0.16376872482588825]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.1731436713242073, 0.14047689027826216, 0.1731436713242073, 0.1758789697904544, 0.1731436713242073, 0.16421312595866158]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.74 ]
 [0.731]
 [0.736]
 [0.731]
 [0.731]
 [0.731]] [[1.39 ]
 [1.526]
 [0.95 ]
 [1.012]
 [0.95 ]
 [0.95 ]
 [0.95 ]] [[0.892]
 [0.953]
 [0.711]
 [0.741]
 [0.711]
 [0.711]
 [0.711]]
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.788]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[1.779]
 [2.81 ]
 [2.176]
 [2.176]
 [2.176]
 [2.176]
 [2.176]] [[0.998]
 [1.435]
 [1.161]
 [1.161]
 [1.161]
 [1.161]
 [1.161]]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[5.581]
 [5.537]
 [5.537]
 [5.537]
 [5.537]
 [5.537]
 [5.537]] [[1.548]
 [1.532]
 [1.532]
 [1.532]
 [1.532]
 [1.532]
 [1.532]]
using another actor
from probs:  [0.1731436713242073, 0.14047689027826216, 0.1731436713242073, 0.1758789697904544, 0.1731436713242073, 0.16421312595866158]
siam score:  -0.855423
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.17338267997135862, 0.1406708054889346, 0.17338267997135862, 0.1747413477739355, 0.17338267997135862, 0.16443980682305415]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.17338267997135862, 0.1406708054889346, 0.17338267997135862, 0.1747413477739355, 0.17338267997135862, 0.16443980682305415]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.17338267997135862, 0.1406708054889346, 0.17338267997135862, 0.1747413477739355, 0.17338267997135862, 0.16443980682305415]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.17338267997135862, 0.1406708054889346, 0.17338267997135862, 0.1747413477739355, 0.17338267997135862, 0.16443980682305415]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.17400432409303399, 0.1402955536770344, 0.17266208978715497, 0.17400432409303399, 0.17400432409303399, 0.16502938425670863]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.562]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.576]] [[5.809]
 [5.278]
 [5.439]
 [5.439]
 [5.439]
 [5.439]
 [5.785]] [[1.741]
 [1.464]
 [1.554]
 [1.554]
 [1.554]
 [1.554]
 [1.708]]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.17400432409303399, 0.1402955536770344, 0.17266208978715497, 0.17400432409303399, 0.17400432409303399, 0.16502938425670863]
start point for exploration sampling:  11106
from probs:  [0.17400432409303399, 0.1402955536770344, 0.17266208978715497, 0.17400432409303399, 0.17400432409303399, 0.16502938425670863]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.17438622302468423, 0.13973302750150782, 0.17171671813699768, 0.17438622302468423, 0.17438622302468423, 0.16539158528744186]
from probs:  [0.17438622302468423, 0.13973302750150782, 0.17171671813699768, 0.17438622302468423, 0.17438622302468423, 0.16539158528744186]
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.167]
 [0.287]
 [0.288]
 [0.288]
 [0.283]
 [0.286]] [[-0.277]
 [ 1.561]
 [-0.138]
 [-0.539]
 [-0.498]
 [-0.146]
 [-0.22 ]] [[0.281]
 [0.167]
 [0.287]
 [0.288]
 [0.288]
 [0.283]
 [0.286]]
siam score:  -0.8601511
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.538]
 [0.532]
 [0.53 ]
 [0.535]
 [0.532]
 [0.525]] [[6.128]
 [5.119]
 [6.618]
 [6.54 ]
 [6.627]
 [6.619]
 [6.554]] [[1.423]
 [0.692]
 [1.591]
 [1.541]
 [1.6  ]
 [1.592]
 [1.544]]
siam score:  -0.8564138
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]] [[4.698]
 [4.016]
 [4.016]
 [4.016]
 [4.016]
 [4.016]
 [4.016]] [[1.979]
 [1.546]
 [1.546]
 [1.546]
 [1.546]
 [1.546]
 [1.546]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.17450202756421612, 0.13918988741595498, 0.1718515592360716, 0.17450202756421612, 0.17316652026708904, 0.16678797795245215]
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.17450202756421612, 0.13918988741595498, 0.1718515592360716, 0.17450202756421612, 0.17316652026708904, 0.16678797795245215]
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.500187316613391
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.17450202756421612, 0.13918988741595498, 0.1718515592360716, 0.17450202756421612, 0.17316652026708904, 0.16678797795245215]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.485]
 [0.443]
 [0.46 ]
 [0.463]
 [0.471]
 [0.457]] [[3.901]
 [3.832]
 [3.658]
 [3.753]
 [3.757]
 [3.988]
 [4.186]] [[1.303]
 [1.29 ]
 [1.138]
 [1.216]
 [1.221]
 [1.391]
 [1.519]]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.823]
 [0.785]
 [0.763]
 [0.777]
 [0.773]
 [0.772]
 [0.776]] [[2.191]
 [1.934]
 [2.024]
 [2.221]
 [2.206]
 [2.359]
 [2.246]] [[1.274]
 [1.027]
 [1.042]
 [1.201]
 [1.183]
 [1.284]
 [1.215]]
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.737]] [[2.112]
 [2.112]
 [2.112]
 [2.112]
 [2.112]
 [2.112]
 [3.727]] [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [1.524]]
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.252]
 [0.203]
 [0.227]
 [0.234]
 [0.25 ]
 [0.257]] [[2.843]
 [2.755]
 [2.56 ]
 [2.639]
 [2.642]
 [2.73 ]
 [3.964]] [[0.447]
 [0.421]
 [0.303]
 [0.353]
 [0.358]
 [0.408]
 [1.01 ]]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.313]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[2.574]
 [2.355]
 [3.436]
 [3.436]
 [3.436]
 [3.436]
 [3.436]] [[-0.155]
 [-0.328]
 [ 0.338]
 [ 0.338]
 [ 0.338]
 [ 0.338]
 [ 0.338]]
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.17495864273430148, 0.13955410217611416, 0.1710029654061487, 0.17495864273430148, 0.17230123899077318, 0.16722440795836097]
from probs:  [0.17495864273430148, 0.13955410217611416, 0.1710029654061487, 0.17495864273430148, 0.17230123899077318, 0.16722440795836097]
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.17510708025259145, 0.1388240870611152, 0.1711480468688785, 0.17510708025259145, 0.17244742192814838, 0.16736628363667508]
siam score:  -0.8480355
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.17510708025259145, 0.1388240870611152, 0.1711480468688785, 0.17510708025259145, 0.17244742192814838, 0.16736628363667508]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.066]
 [0.115]
 [0.116]
 [0.117]
 [0.117]
 [0.111]] [[1.119]
 [1.414]
 [0.674]
 [0.94 ]
 [0.79 ]
 [0.647]
 [0.572]] [[0.115]
 [0.066]
 [0.115]
 [0.116]
 [0.117]
 [0.117]
 [0.111]]
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.17555986012499822, 0.139183048859438, 0.1703075932577783, 0.17555986012499822, 0.17159058974442745, 0.1677990478883598]
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.545]
 [0.563]
 [0.565]
 [0.564]
 [0.56 ]
 [0.562]] [[1.446]
 [1.88 ]
 [1.701]
 [1.785]
 [1.78 ]
 [1.699]
 [1.621]] [[0.06 ]
 [0.31 ]
 [0.227]
 [0.285]
 [0.282]
 [0.218]
 [0.171]]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.638]
 [0.629]
 [0.666]
 [0.591]
 [0.701]
 [0.629]] [[3.187]
 [4.12 ]
 [4.372]
 [4.238]
 [4.306]
 [4.176]
 [4.422]] [[1.296]
 [1.929]
 [2.068]
 [2.055]
 [1.957]
 [2.082]
 [2.099]]
first move QE:  0.9818961168446189
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
line 256 mcts: sample exp_bonus 8.876304663362562
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.507]
 [0.516]
 [0.505]
 [0.513]
 [0.501]
 [0.502]] [[3.124]
 [3.444]
 [3.129]
 [2.667]
 [2.652]
 [2.623]
 [3.166]] [[0.516]
 [0.507]
 [0.516]
 [0.505]
 [0.513]
 [0.501]
 [0.502]]
line 256 mcts: sample exp_bonus 8.138179895063281
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.515]
 [0.555]
 [0.563]
 [0.531]
 [0.536]
 [0.528]] [[3.509]
 [3.881]
 [3.394]
 [3.437]
 [3.509]
 [3.444]
 [3.512]] [[0.534]
 [0.515]
 [0.555]
 [0.563]
 [0.531]
 [0.536]
 [0.528]]
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.17637035677203988, 0.13756320029508218, 0.16987379770066538, 0.17637035677203988, 0.1724133617012936, 0.16740892675887917]
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.17637035677203988, 0.13756320029508218, 0.16987379770066538, 0.17637035677203988, 0.1724133617012936, 0.16740892675887917]
siam score:  -0.8564617
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.17637035677203988, 0.13756320029508218, 0.16987379770066538, 0.17637035677203988, 0.1724133617012936, 0.16740892675887917]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.025]] [[3.979]
 [4.044]
 [4.044]
 [4.044]
 [4.044]
 [4.044]
 [4.119]] [[0.958]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.104]]
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.518]
 [0.481]] [[5.284]
 [4.869]
 [4.869]
 [4.869]
 [4.869]
 [5.046]
 [4.936]] [[1.795]
 [1.574]
 [1.574]
 [1.574]
 [1.574]
 [1.707]
 [1.6  ]]
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.1765832220689474, 0.13772921565727864, 0.17007882002746189, 0.1765832220689474, 0.1726214499164062, 0.1664040702609584]
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]] [[5.7  ]
 [4.472]
 [4.472]
 [4.472]
 [4.472]
 [4.472]
 [4.472]] [[1.646]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]]
Printing some Q and Qe and total Qs values:  [[ 0.002]
 [-0.006]
 [ 0.106]
 [ 0.492]
 [ 0.088]
 [ 0.163]
 [ 0.145]] [[1.648]
 [2.005]
 [2.248]
 [1.935]
 [2.119]
 [1.924]
 [2.5  ]] [[0.632]
 [0.84 ]
 [1.065]
 [1.156]
 [0.975]
 [0.913]
 [1.245]]
rdn beta is 0 so we're just using the maxi policy
602 422
maxi score, test score, baseline:  -0.9963539007092199 -1.0 -0.9963539007092199
maxi score, test score, baseline:  -0.9963539007092199 -1.0 -0.9963539007092199
maxi score, test score, baseline:  -0.9963539007092199 -1.0 -0.9963539007092199
probs:  [0.1770333559402541, 0.13808029289505006, 0.17051237120295695, 0.1770333559402541, 0.17051237120295695, 0.16682825281852776]
maxi score, test score, baseline:  -0.9963539007092199 -1.0 -0.9963539007092199
probs:  [0.1777152121309959, 0.1386121186746037, 0.16991807878113271, 0.17636570828198112, 0.16991807878113271, 0.16747080335015377]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
probs:  [0.17786041441553413, 0.13790833014397583, 0.17005690789699585, 0.1765098075180948, 0.17005690789699585, 0.16760763212840354]
siam score:  -0.84522104
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
probs:  [0.17696713474262563, 0.1382656430590554, 0.17049751601273955, 0.17696713474262563, 0.16926067713790838, 0.16804189430504554]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 6.2688570300985695
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
probs:  [0.17586821032671837, 0.13845025718256454, 0.17072516656122563, 0.1772034236119905, 0.16948667624269703, 0.1682662660748039]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[6.1]
 [6.1]
 [6.1]
 [6.1]
 [6.1]
 [6.1]
 [6.1]] [[1.39]
 [1.39]
 [1.39]
 [1.39]
 [1.39]
 [1.39]
 [1.39]]
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.02 ]
 [-0.02 ]
 [-0.019]
 [-0.02 ]
 [-0.019]
 [-0.019]] [[6.257]
 [6.233]
 [6.23 ]
 [6.098]
 [6.188]
 [6.245]
 [6.237]] [[1.358]
 [1.345]
 [1.343]
 [1.272]
 [1.32 ]
 [1.351]
 [1.347]]
siam score:  -0.84338653
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
probs:  [0.17586821032671837, 0.13845025718256454, 0.17072516656122563, 0.1772034236119905, 0.16948667624269703, 0.1682662660748039]
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
probs:  [0.17586821032671837, 0.13845025718256454, 0.17072516656122563, 0.1772034236119905, 0.16948667624269703, 0.1682662660748039]
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.129]
 [0.143]
 [0.151]
 [0.145]
 [0.146]
 [0.147]] [[5.624]
 [4.721]
 [5.692]
 [5.483]
 [5.675]
 [5.639]
 [5.63 ]] [[1.028]
 [0.383]
 [1.054]
 [0.93 ]
 [1.047]
 [1.025]
 [1.021]]
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
probs:  [0.17607998578673473, 0.13861697501547904, 0.17093074891529464, 0.17741680689758935, 0.16969076724220888, 0.16726471614269328]
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.043]
 [-0.046]
 [-0.044]
 [-0.043]
 [-0.042]
 [-0.039]] [[5.408]
 [2.653]
 [2.47 ]
 [2.772]
 [2.739]
 [2.718]
 [2.859]] [[0.783]
 [0.134]
 [0.091]
 [0.162]
 [0.155]
 [0.15 ]
 [0.184]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.02  0.02  0.02  0.878 0.02 ]
siam score:  -0.84989107
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.232905134521058
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.266]
 [0.378]] [[4.041]
 [3.997]
 [3.997]
 [3.997]
 [3.997]
 [3.388]
 [4.378]] [[0.118]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.155]
 [1.039]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.101]
 [-0.097]
 [-0.102]
 [-0.101]
 [-0.104]
 [-0.104]
 [-0.095]] [[3.418]
 [2.966]
 [3.423]
 [3.401]
 [3.324]
 [3.361]
 [3.444]] [[-0.573]
 [-0.865]
 [-0.572]
 [-0.585]
 [-0.641]
 [-0.618]
 [-0.545]]
from probs:  [0.17758316205636213, 0.1392141795520063, 0.1711870700823671, 0.18027936082693846, 0.16875747543325248, 0.16297875204907353]
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
probs:  [0.17772626718144297, 0.13852051658826242, 0.1713250209263136, 0.18042463867975903, 0.16889346839199107, 0.16311008823223092]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.546]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[1.227]
 [2.481]
 [1.227]
 [1.227]
 [1.227]
 [1.227]
 [1.227]] [[1.668]
 [2.583]
 [1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]]
Starting evaluation
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
Printing some Q and Qe and total Qs values:  [[0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]] [[-0.215]
 [-0.215]
 [-0.215]
 [-0.215]
 [-0.215]
 [-0.215]
 [-0.215]] [[0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
probs:  [0.17770582462654705, 0.13953998921500202, 0.1725859256169099, 0.18038302372700618, 0.16659167765668334, 0.16319355915785147]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.28387552554548656
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]] [[0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]]
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.215]
 [0.249]
 [0.251]
 [0.266]
 [0.266]
 [0.256]] [[-0.037]
 [ 1.688]
 [ 0.365]
 [ 0.393]
 [ 0.17 ]
 [ 0.17 ]
 [ 0.593]] [[0.252]
 [0.215]
 [0.249]
 [0.251]
 [0.266]
 [0.266]
 [0.256]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]] [[0.306]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[0.246]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.497]
 [0.55 ]
 [0.532]
 [0.531]
 [0.528]
 [0.528]] [[2.548]
 [3.091]
 [2.019]
 [1.998]
 [2.608]
 [2.367]
 [2.64 ]] [[0.537]
 [0.497]
 [0.55 ]
 [0.532]
 [0.531]
 [0.528]
 [0.528]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
Printing some Q and Qe and total Qs values:  [[0.823]
 [0.82 ]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]] [[2.663]
 [5.738]
 [2.663]
 [2.663]
 [2.663]
 [2.663]
 [2.663]] [[1.285]
 [2.167]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]]
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.837]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]] [[4.96 ]
 [5.246]
 [4.96 ]
 [4.96 ]
 [4.96 ]
 [4.96 ]
 [4.96 ]] [[2.065]
 [2.167]
 [2.065]
 [2.065]
 [2.065]
 [2.065]
 [2.065]]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]] [[4.684]
 [4.684]
 [4.684]
 [4.684]
 [4.684]
 [4.684]
 [4.684]] [[0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]]
from probs:  [0.177945846965408, 0.13972839980672083, 0.17281902430454643, 0.1792761020876391, 0.16681667030899547, 0.16341395652669005]
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.191]
 [0.237]
 [0.233]
 [0.237]
 [0.237]
 [0.237]] [[-1.279]
 [ 1.012]
 [-0.513]
 [-0.687]
 [-0.508]
 [-0.286]
 [-0.035]] [[0.251]
 [0.191]
 [0.237]
 [0.233]
 [0.237]
 [0.237]
 [0.237]]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.107]
 [0.243]
 [0.233]
 [0.232]
 [0.24 ]
 [0.242]] [[-0.89 ]
 [ 1.393]
 [-0.773]
 [-0.687]
 [-0.647]
 [-0.676]
 [-0.551]] [[0.244]
 [0.107]
 [0.243]
 [0.233]
 [0.232]
 [0.24 ]
 [0.242]]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
probs:  [0.17794585461619247, 0.1397283815342162, 0.17281902847775416, 0.1792761106407494, 0.1668166704107445, 0.16341395432034317]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.17726718502979172, 0.14022808132268327, 0.17343710739584497, 0.1799172892512599, 0.16625905980215458, 0.1628912771982656]
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.084]
 [0.213]
 [0.222]
 [0.223]
 [0.224]
 [0.224]] [[ 0.4  ]
 [ 1.528]
 [ 0.103]
 [-0.267]
 [-0.204]
 [-0.033]
 [ 0.07 ]] [[0.224]
 [0.084]
 [0.213]
 [0.222]
 [0.223]
 [0.224]
 [0.224]]
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.831]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.818]] [[2.634]
 [2.795]
 [3.099]
 [3.099]
 [3.099]
 [3.099]
 [2.859]] [[1.228]
 [1.257]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.271]]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.089]
 [0.227]
 [0.231]
 [0.234]
 [0.234]
 [0.234]] [[ 0.264]
 [ 1.5  ]
 [ 0.397]
 [-0.395]
 [-0.259]
 [-0.073]
 [ 0.057]] [[0.234]
 [0.089]
 [0.227]
 [0.231]
 [0.234]
 [0.234]
 [0.234]]
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.815]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[2.541]
 [4.953]
 [3.295]
 [3.295]
 [3.295]
 [3.295]
 [3.295]] [[0.622]
 [1.455]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.233]] [[ 0.22 ]
 [ 0.263]
 [ 0.263]
 [ 0.263]
 [ 0.263]
 [ 0.263]
 [-0.024]] [[0.232]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.233]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.543]
 [0.555]
 [0.548]
 [0.544]
 [0.547]
 [0.542]] [[3.811]
 [4.327]
 [3.933]
 [4.014]
 [4.107]
 [3.973]
 [4.125]] [[0.598]
 [0.543]
 [0.555]
 [0.548]
 [0.544]
 [0.547]
 [0.542]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.306]
 [0.304]
 [0.321]
 [0.31 ]
 [0.301]
 [0.3  ]] [[4.344]
 [4.479]
 [4.717]
 [4.608]
 [4.102]
 [4.159]
 [4.291]] [[0.492]
 [0.57 ]
 [0.725]
 [0.686]
 [0.328]
 [0.348]
 [0.432]]
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.811]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]] [[3.359]
 [5.703]
 [3.359]
 [3.359]
 [3.359]
 [3.359]
 [3.359]] [[0.759]
 [1.582]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]]
maxi score, test score, baseline:  -0.9965777408637874 -1.0 -0.9965777408637874
probs:  [0.17656159125599197, 0.13988921308310015, 0.17277476959683466, 0.1805205411723837, 0.16681648376948924, 0.1634374011222002]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[2.736]
 [2.736]
 [2.736]
 [2.736]
 [2.736]
 [2.736]
 [2.736]] [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]]
Printing some Q and Qe and total Qs values:  [[1.383]
 [1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]] [[2.234]
 [1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.909]] [[1.101]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.17564617367484078, 0.13937960697716456, 0.17313496213216753, 0.18089688871861206, 0.16716424937336405, 0.16377811912385085]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[4.594]
 [4.594]
 [4.594]
 [4.594]
 [4.594]
 [4.594]
 [4.594]] [[1.692]
 [1.692]
 [1.692]
 [1.692]
 [1.692]
 [1.692]
 [1.692]]
from probs:  [0.17578538939806923, 0.13869749143254448, 0.1732721867548903, 0.1810402676519888, 0.1672967399109684, 0.16390792485153874]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.1760218487597152, 0.138884061661153, 0.17350526545848086, 0.17993863630374157, 0.1675217806863153, 0.16412840713059404]
rdn probs:  [0.1749761506156921, 0.13906031731222193, 0.17372545835291023, 0.18016699369129807, 0.16773438003119287, 0.16433669999668468]
in main func line 156:  629
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.534]
 [0.564]
 [0.565]
 [0.564]
 [0.565]
 [0.564]] [[0.915]
 [2.136]
 [0.702]
 [0.76 ]
 [0.772]
 [1.078]
 [1.65 ]] [[0.789]
 [1.163]
 [0.744]
 [0.765]
 [0.767]
 [0.872]
 [1.061]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.572]
 [0.572]
 [0.572]
 [0.582]
 [0.593]
 [0.572]] [[2.482]
 [2.712]
 [2.712]
 [2.712]
 [3.16 ]
 [3.383]
 [2.712]] [[0.619]
 [0.724]
 [0.724]
 [0.724]
 [0.893]
 [0.989]
 [0.724]]
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[3.589]
 [3.589]
 [3.589]
 [3.589]
 [3.589]
 [3.589]
 [3.589]] [[0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]]
line 256 mcts: sample exp_bonus 5.591812669737132
630 442
UNIT TEST: sample policy line 217 mcts : [0.02  0.816 0.02  0.041 0.02  0.061 0.02 ]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
line 256 mcts: sample exp_bonus 8.492595127365362
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.071]
 [-0.071]
 [-0.069]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]] [[4.984]
 [5.962]
 [5.962]
 [5.242]
 [5.273]
 [5.439]
 [5.95 ]] [[0.815]
 [1.257]
 [1.257]
 [0.932]
 [0.946]
 [1.021]
 [1.253]]
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.09 ]
 [-0.082]
 [-0.079]
 [-0.076]
 [-0.077]
 [-0.076]] [[4.346]
 [3.199]
 [4.06 ]
 [4.244]
 [6.995]
 [4.445]
 [4.576]] [[ 0.441]
 [-0.066]
 [ 0.315]
 [ 0.397]
 [ 1.598]
 [ 0.486]
 [ 0.543]]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.17472910488312682, 0.13907360122368673, 0.17348928521151558, 0.17987343128175248, 0.16754848261837849, 0.16528609478153994]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.17393359525191, 0.13942977190766964, 0.17393359525191, 0.17901606319849017, 0.16797757812701136, 0.16570939626300887]
using another actor
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.82 ]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]] [[2.863]
 [3.309]
 [2.863]
 [2.863]
 [2.863]
 [2.863]
 [2.863]] [[1.099]
 [1.443]
 [1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]]
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
probs:  [0.17403337921168407, 0.13893035979341123, 0.17526801778434917, 0.1777913666919858, 0.16811562881166875, 0.165861247706901]
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
probs:  [0.17403337921168407, 0.13893035979341123, 0.17526801778434917, 0.1777913666919858, 0.16811562881166875, 0.165861247706901]
using explorer policy with actor:  1
in main func line 156:  646
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.526]
 [0.526]
 [0.48 ]
 [0.526]
 [0.526]
 [0.518]] [[2.026]
 [2.026]
 [2.026]
 [1.14 ]
 [2.026]
 [2.026]
 [1.183]] [[0.526]
 [0.526]
 [0.526]
 [0.48 ]
 [0.526]
 [0.526]
 [0.518]]
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.1750265841901188, 0.13972322240320972, 0.17626826917702845, 0.17752794959853102, 0.16793367131968956, 0.1635203033114225]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.469]
 [0.479]
 [0.48 ]
 [0.477]
 [0.49 ]
 [0.475]] [[4.345]
 [4.82 ]
 [3.861]
 [4.191]
 [4.123]
 [4.186]
 [4.354]] [[0.476]
 [0.585]
 [0.284]
 [0.397]
 [0.367]
 [0.415]
 [0.441]]
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.1750265841901188, 0.13972322240320972, 0.17626826917702845, 0.17752794959853102, 0.16793367131968956, 0.1635203033114225]
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.507]
 [0.604]
 [0.619]
 [0.62 ]
 [0.645]
 [0.652]] [[2.602]
 [4.033]
 [2.32 ]
 [2.366]
 [2.055]
 [2.272]
 [2.408]] [[0.784]
 [1.562]
 [0.623]
 [0.665]
 [0.479]
 [0.632]
 [0.719]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.023]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[2.694]
 [4.159]
 [2.694]
 [2.694]
 [2.694]
 [2.694]
 [2.694]] [[-0.974]
 [-0.494]
 [-0.974]
 [-0.974]
 [-0.974]
 [-0.974]
 [-0.974]]
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.174201586675355, 0.14004404649901314, 0.17667300582069362, 0.1779355786449427, 0.16831927055888127, 0.1628265118011144]
using explorer policy with actor:  1
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.674]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]] [[3.705]
 [3.457]
 [3.705]
 [3.705]
 [3.705]
 [3.705]
 [3.705]] [[1.838]
 [1.691]
 [1.838]
 [1.838]
 [1.838]
 [1.838]
 [1.838]]
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.397]
 [0.388]
 [0.417]
 [0.387]
 [0.438]
 [0.43 ]] [[4.873]
 [5.179]
 [5.214]
 [4.75 ]
 [4.731]
 [5.648]
 [5.621]] [[0.622]
 [0.397]
 [0.388]
 [0.417]
 [0.387]
 [0.438]
 [0.43 ]]
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.17333640977502587, 0.13954389005184414, 0.17702476084861468, 0.1782898474487345, 0.16865439334371224, 0.16315069853206873]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 6.552138345161618
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.17333640977502587, 0.13954389005184414, 0.17702476084861468, 0.1782898474487345, 0.16865439334371224, 0.16315069853206873]
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.17355597518841936, 0.13972064064691306, 0.17724899936961974, 0.17724899936961974, 0.16886802668442982, 0.16335735874099844]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.136]
 [0.217]
 [0.268]
 [0.268]
 [0.207]
 [0.268]] [[5.397]
 [5.641]
 [5.194]
 [5.397]
 [5.397]
 [5.29 ]
 [5.397]] [[1.204]
 [1.189]
 [0.974]
 [1.204]
 [1.204]
 [1.032]
 [1.204]]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.795]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]] [[3.149]
 [4.071]
 [3.149]
 [3.149]
 [3.149]
 [3.149]
 [3.149]] [[1.701]
 [2.205]
 [1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.701]]
line 256 mcts: sample exp_bonus 3.248371320714659
UNIT TEST: sample policy line 217 mcts : [0.02  0.612 0.    0.    0.    0.347 0.02 ]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.1739573334108366, 0.1400437527028602, 0.17765889792387576, 0.17640741658851491, 0.16925854375501523, 0.16267405561889708]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.1739573334108366, 0.1400437527028602, 0.17765889792387576, 0.17640741658851491, 0.16925854375501523, 0.16267405561889708]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[5.534]
 [5.534]
 [5.534]
 [5.534]
 [5.534]
 [5.534]
 [5.534]] [[1.756]
 [1.756]
 [1.756]
 [1.756]
 [1.756]
 [1.756]
 [1.756]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.17331673270921866, 0.13972119757617307, 0.17697796041555466, 0.17697796041555466, 0.16980596641547183, 0.1632001824680271]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.6173204390625475
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.17353151613819617, 0.13989434760875882, 0.1759580271810492, 0.17719728103507773, 0.1700163991137892, 0.16340242892312876]
line 256 mcts: sample exp_bonus 4.55145054050919
first move QE:  1.0926890404600467
siam score:  -0.8564254
siam score:  -0.8564766
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.766]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.695]] [[5.086]
 [6.919]
 [6.061]
 [6.061]
 [6.061]
 [6.061]
 [5.201]] [[1.133]
 [1.865]
 [1.505]
 [1.505]
 [1.505]
 [1.505]
 [1.168]]
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.142]
 [0.191]
 [0.2  ]
 [0.201]
 [0.196]
 [0.193]] [[0.677]
 [1.96 ]
 [1.216]
 [0.587]
 [0.767]
 [0.849]
 [0.902]] [[0.216]
 [0.142]
 [0.191]
 [0.2  ]
 [0.201]
 [0.196]
 [0.193]]
siam score:  -0.8550146
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.877]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]] [[3.287]
 [5.929]
 [3.287]
 [3.287]
 [3.287]
 [3.287]
 [3.287]] [[0.737]
 [1.829]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
using explorer policy with actor:  0
663 447
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
line 256 mcts: sample exp_bonus 2.085603601008675
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.135]
 [0.135]
 [0.135]
 [0.169]
 [0.135]
 [0.175]] [[6.935]
 [6.53 ]
 [6.53 ]
 [6.53 ]
 [8.421]
 [6.53 ]
 [6.762]] [[1.373]
 [1.22 ]
 [1.22 ]
 [1.22 ]
 [1.933]
 [1.22 ]
 [1.324]]
using explorer policy with actor:  1
siam score:  -0.8593298
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.1734389577648926, 0.1392467840907825, 0.17584685731940738, 0.17707642304937238, 0.1699499604512079, 0.1644410173243373]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
in main func line 156:  667
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.17284225541467943, 0.139717518733639, 0.17644132137197557, 0.17767504374504398, 0.16938909753673312, 0.16393476319792905]
using explorer policy with actor:  1
siam score:  -0.85873294
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.528]
 [0.548]] [[3.07 ]
 [3.271]
 [3.271]
 [3.271]
 [3.271]
 [3.103]
 [3.222]] [[1.174]
 [1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.144]
 [1.303]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.204]
 [0.173]
 [0.183]
 [0.204]
 [0.204]
 [0.181]] [[4.476]
 [3.811]
 [4.179]
 [4.424]
 [3.811]
 [3.811]
 [4.594]] [[1.595]
 [1.088]
 [1.354]
 [1.555]
 [1.088]
 [1.088]
 [1.689]]
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.173052768485872, 0.1398876875612643, 0.17543826881211752, 0.17789144290648345, 0.16959540483736077, 0.16413442739690187]
line 256 mcts: sample exp_bonus 4.29794604685664
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
Printing some Q and Qe and total Qs values:  [[-0.103]
 [-0.094]
 [-0.103]
 [-0.102]
 [-0.103]
 [-0.097]
 [-0.093]] [[5.192]
 [4.749]
 [5.368]
 [6.779]
 [5.375]
 [5.186]
 [4.897]] [[0.666]
 [0.44 ]
 [0.758]
 [1.509]
 [0.763]
 [0.669]
 [0.52 ]]
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.17287421456755164, 0.14069314180105433, 0.174049182741582, 0.178915717590048, 0.16944393835477856, 0.16402380494498553]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.447]
 [0.429]
 [0.427]
 [0.427]
 [0.43 ]
 [0.433]] [[5.534]
 [5.647]
 [6.104]
 [6.001]
 [5.929]
 [5.963]
 [5.884]] [[1.257]
 [1.33 ]
 [1.581]
 [1.521]
 [1.48 ]
 [1.501]
 [1.458]]
Printing some Q and Qe and total Qs values:  [[-0.109]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.109]] [[5.057]
 [5.085]
 [5.085]
 [5.085]
 [5.085]
 [5.085]
 [5.302]] [[0.841]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [1.018]]
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.539]
 [0.459]
 [0.398]
 [0.459]
 [0.459]
 [0.459]] [[5.734]
 [6.233]
 [5.271]
 [5.711]
 [5.271]
 [5.271]
 [5.271]] [[1.031]
 [1.649]
 [0.916]
 [1.072]
 [0.916]
 [0.916]
 [0.916]]
from probs:  [0.17287421456755164, 0.14069314180105433, 0.174049182741582, 0.178915717590048, 0.16944393835477856, 0.16402380494498553]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.1730775752100618, 0.1408586462274085, 0.1730775752100618, 0.17912618515674267, 0.16964326379067124, 0.16421675440505404]
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.1730775752100618, 0.1408586462274085, 0.1730775752100618, 0.17912618515674267, 0.16964326379067124, 0.16421675440505404]
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.668]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]] [[6.237]
 [6.462]
 [6.237]
 [6.237]
 [6.237]
 [6.237]
 [6.237]] [[2.041]
 [2.145]
 [2.041]
 [2.041]
 [2.041]
 [2.041]
 [2.041]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.349]
 [0.349]
 [0.354]
 [0.354]
 [0.356]
 [0.362]] [[5.729]
 [5.845]
 [5.845]
 [5.699]
 [5.689]
 [5.729]
 [5.981]] [[1.901]
 [1.967]
 [1.967]
 [1.869]
 [1.862]
 [1.892]
 [2.076]]
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.76 ]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.718]] [[3.593]
 [4.983]
 [3.894]
 [3.894]
 [3.894]
 [3.894]
 [3.203]] [[0.818]
 [1.557]
 [1.035]
 [1.035]
 [1.035]
 [1.035]
 [0.649]]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.249]
 [0.251]
 [0.255]
 [0.255]
 [0.255]
 [0.255]] [[4.101]
 [6.44 ]
 [4.494]
 [3.797]
 [3.745]
 [3.756]
 [3.878]] [[-0.131]
 [ 0.625]
 [-0.022]
 [-0.247]
 [-0.264]
 [-0.261]
 [-0.219]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.880450551124013
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
siam score:  -0.85037285
rdn beta is 0 so we're just using the maxi policy
first move QE:  1.1406828099106388
siam score:  -0.8512383
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[4.765]
 [4.612]
 [4.612]
 [4.612]
 [4.612]
 [4.612]
 [4.612]] [[0.98 ]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]]
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.1731566969938652, 0.14034286047921268, 0.1731566969938652, 0.17916486424302694, 0.1708663352505606, 0.16331254603946946]
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.17350983959132296, 0.1406290812442463, 0.17350983959132296, 0.17953026013374546, 0.17121480679394313, 0.16160617264541927]
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
siam score:  -0.84785676
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.17383651647020973, 0.1401377806404534, 0.17383651647020973, 0.17986827376368136, 0.17041047832751782, 0.16191043432792782]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.326]
 [0.287]
 [0.313]
 [0.313]
 [0.337]
 [0.313]] [[4.52 ]
 [4.939]
 [4.548]
 [4.579]
 [4.614]
 [4.54 ]
 [4.693]] [[1.652]
 [2.023]
 [1.642]
 [1.696]
 [1.727]
 [1.687]
 [1.796]]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.494]
 [0.527]
 [0.541]
 [0.537]
 [0.528]
 [0.524]] [[4.559]
 [5.451]
 [4.711]
 [4.657]
 [4.685]
 [4.826]
 [4.887]] [[1.333]
 [1.895]
 [1.422]
 [1.396]
 [1.411]
 [1.5  ]
 [1.538]]
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.655]
 [0.714]
 [0.712]
 [0.722]
 [0.704]
 [0.695]] [[4.667]
 [4.364]
 [4.694]
 [4.699]
 [4.555]
 [4.78 ]
 [4.958]] [[1.836]
 [1.526]
 [1.858]
 [1.86 ]
 [1.757]
 [1.915]
 [2.046]]
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.17424631876879237, 0.14046814152084167, 0.17424631876879237, 0.17904927637281334, 0.16969782249478577, 0.1622921220739744]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.325]
 [0.32 ]
 [0.318]
 [0.318]
 [0.318]
 [0.314]] [[5.592]
 [5.557]
 [5.195]
 [5.254]
 [5.291]
 [5.31 ]
 [5.432]] [[1.816]
 [1.792]
 [1.544]
 [1.582]
 [1.608]
 [1.62 ]
 [1.701]]
siam score:  -0.8443454
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.566]
 [0.728]
 [0.68 ]
 [0.393]
 [0.728]
 [0.558]] [[4.184]
 [4.033]
 [3.811]
 [4.882]
 [4.757]
 [3.302]
 [3.699]] [[1.363]
 [1.532]
 [1.547]
 [2.273]
 [1.87 ]
 [1.178]
 [1.281]]
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.17482853277110694, 0.14093749157340177, 0.17482853277110694, 0.178417688175725, 0.16916144529013102, 0.16182630941852824]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.17482853277110694, 0.14093749157340177, 0.17482853277110694, 0.178417688175725, 0.16916144529013102, 0.16182630941852824]
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.065]
 [0.108]
 [0.111]
 [0.108]
 [0.109]
 [0.123]] [[4.591]
 [3.829]
 [3.686]
 [3.65 ]
 [3.739]
 [3.682]
 [3.592]] [[0.853]
 [0.353]
 [0.32 ]
 [0.304]
 [0.351]
 [0.319]
 [0.284]]
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.443]
 [0.463]
 [0.463]
 [0.458]
 [0.459]
 [0.46 ]] [[2.123]
 [2.602]
 [2.027]
 [2.407]
 [2.19 ]
 [2.096]
 [2.038]] [[0.476]
 [0.443]
 [0.463]
 [0.463]
 [0.458]
 [0.459]
 [0.46 ]]
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.381]
 [0.363]
 [0.363]
 [0.357]
 [0.36 ]
 [0.361]] [[2.457]
 [2.807]
 [2.231]
 [2.054]
 [2.148]
 [2.211]
 [2.147]] [[0.367]
 [0.381]
 [0.363]
 [0.363]
 [0.357]
 [0.36 ]
 [0.361]]
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
probs:  [0.17565792814260903, 0.14160609625480045, 0.17333448100037602, 0.17684352274600876, 0.16996395406049192, 0.16259401779571392]
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
probs:  [0.17469267092867652, 0.14177190885931637, 0.17353744572087346, 0.17705059635282247, 0.17016297208755402, 0.16278440605075703]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.1749002795591623, 0.14194037465079148, 0.17374368078673047, 0.17607261436931423, 0.17036519489883759, 0.16297785573516402]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
main train batch thing paused
add a thread
Adding thread: now have 4 threads
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 5 threads
main train batch thing paused
add a thread
Adding thread: now have 6 threads
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.17546888027747734, 0.1424018227353101, 0.17316363398025195, 0.17664502634749027, 0.16981876680388575, 0.16250186985558462]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.17546888293428925, 0.14240181541134264, 0.17316363594126097, 0.17664502935930368, 0.16981876775529836, 0.16250186859850518]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.57 ]] [[1.988]
 [1.988]
 [1.988]
 [1.988]
 [1.988]
 [1.988]
 [2.591]] [[1.07 ]
 [1.07 ]
 [1.07 ]
 [1.07 ]
 [1.07 ]
 [1.07 ]
 [1.639]]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.17546888293428925, 0.14240181541134264, 0.17316363594126097, 0.17664502935930368, 0.16981876775529836, 0.16250186859850518]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.17579338390767743, 0.14190383250072447, 0.17348387373772212, 0.17697170542296073, 0.1690448152292366, 0.1628023892016788]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.17579338390767743, 0.14190383250072447, 0.17348387373772212, 0.17697170542296073, 0.1690448152292366, 0.1628023892016788]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.17579338390767743, 0.14190383250072447, 0.17348387373772212, 0.17697170542296073, 0.1690448152292366, 0.1628023892016788]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.17579338390767743, 0.14190383250072447, 0.17348387373772212, 0.17697170542296073, 0.1690448152292366, 0.1628023892016788]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.17579338390767743, 0.14190383250072447, 0.17348387373772212, 0.17697170542296073, 0.1690448152292366, 0.1628023892016788]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.17579338390767743, 0.14190383250072447, 0.17348387373772212, 0.17697170542296073, 0.1690448152292366, 0.1628023892016788]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.17579338390767743, 0.14190383250072447, 0.17348387373772212, 0.17697170542296073, 0.1690448152292366, 0.1628023892016788]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.651]
 [0.663]
 [0.663]
 [0.663]
 [0.683]
 [0.697]] [[3.332]
 [4.263]
 [3.319]
 [3.319]
 [3.319]
 [3.258]
 [3.36 ]] [[1.25 ]
 [1.869]
 [1.248]
 [1.248]
 [1.248]
 [1.223]
 [1.303]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[6.515]
 [5.825]
 [5.825]
 [5.825]
 [5.825]
 [5.825]
 [5.825]] [[ 0.147]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]]
Printing some Q and Qe and total Qs values:  [[0.76 ]
 [0.763]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]] [[8.094]
 [8.356]
 [8.094]
 [8.094]
 [8.094]
 [8.094]
 [8.094]] [[1.877]
 [1.97 ]
 [1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]]
Printing some Q and Qe and total Qs values:  [[1.006]
 [1.353]
 [1.006]
 [1.006]
 [1.006]
 [1.006]
 [1.006]] [[4.733]
 [7.003]
 [4.733]
 [4.733]
 [4.733]
 [4.733]
 [4.733]] [[0.942]
 [1.63 ]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]]
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[7.003]
 [7.966]
 [7.966]
 [7.966]
 [7.966]
 [7.966]
 [7.966]] [[1.117]
 [1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.17574862580976008, 0.14205417479885593, 0.1734552213075462, 0.17691857202541647, 0.16797898299296507, 0.16384442306545627]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
siam score:  -0.8324846
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.68]
 [0.68]
 [0.68]
 [0.68]
 [0.68]
 [0.68]
 [0.68]] [[8.724]
 [8.724]
 [8.724]
 [8.724]
 [8.724]
 [8.724]
 [8.724]] [[1.961]
 [1.961]
 [1.961]
 [1.961]
 [1.961]
 [1.961]
 [1.961]]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.696]
 [0.702]
 [0.702]
 [0.696]
 [0.702]
 [0.702]] [[5.444]
 [2.582]
 [5.075]
 [5.075]
 [5.548]
 [5.075]
 [5.075]] [[1.927]
 [0.612]
 [1.757]
 [1.757]
 [1.971]
 [1.757]
 [1.757]]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.17539367754514998, 0.14270458510366515, 0.17200573239015016, 0.17655330977941167, 0.16874809281803493, 0.16459460236358803]
using explorer policy with actor:  1
siam score:  -0.8374445
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
probs:  [0.1748280933969936, 0.14317850316978176, 0.17257696914015616, 0.1759761667679807, 0.16930850988263255, 0.16413175764245544]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
probs:  [0.1750025716846242, 0.14332139519344442, 0.17274920080770445, 0.17615179083185328, 0.16947747963063828, 0.1632975618517355]
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.821]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[5.14]
 [4.29]
 [5.14]
 [5.14]
 [5.14]
 [5.14]
 [5.14]] [[1.262]
 [0.999]
 [1.262]
 [1.262]
 [1.262]
 [1.262]
 [1.262]]
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.739]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]] [[8.722]
 [8.545]
 [8.722]
 [8.722]
 [8.722]
 [8.722]
 [8.722]] [[2.108]
 [2.047]
 [2.108]
 [2.108]
 [2.108]
 [2.108]
 [2.108]]
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.17423783447550986, 0.1436258537458204, 0.17311618336935172, 0.17652600273207253, 0.16983751090519708, 0.1626566147720486]
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
probs:  [0.17443873106438346, 0.1437914458584158, 0.17331578636976985, 0.17557654919866078, 0.17003333264705323, 0.16284415486171677]
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
probs:  [0.17443873106438346, 0.1437914458584158, 0.17331578636976985, 0.17557654919866078, 0.17003333264705323, 0.16284415486171677]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.707]
 [0.728]
 [0.725]
 [0.699]
 [0.73 ]
 [0.701]] [[8.666]
 [8.391]
 [6.952]
 [8.008]
 [8.136]
 [7.051]
 [8.412]] [[1.997]
 [1.89 ]
 [1.308]
 [1.743]
 [1.78 ]
 [1.351]
 [1.895]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
probs:  [0.17443873106438346, 0.1437914458584158, 0.17331578636976985, 0.17557654919866078, 0.17003333264705323, 0.16284415486171677]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.023]] [[8.051]
 [7.936]
 [7.936]
 [7.936]
 [7.936]
 [7.936]
 [8.588]] [[1.094]
 [1.054]
 [1.054]
 [1.054]
 [1.054]
 [1.054]
 [1.345]]
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
722 457
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
probs:  [0.17480916083676873, 0.14409679451714127, 0.1736838315103415, 0.17594939518738703, 0.16827085500347636, 0.16318996294488514]
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.581]
 [0.372]
 [0.376]
 [0.377]
 [0.373]
 [0.374]] [[1.405]
 [2.362]
 [0.831]
 [1.214]
 [1.245]
 [0.948]
 [0.971]] [[0.372]
 [0.581]
 [0.372]
 [0.376]
 [0.377]
 [0.373]
 [0.374]]
siam score:  -0.83760434
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.06 ]] [[4.69 ]
 [4.69 ]
 [4.69 ]
 [4.69 ]
 [4.69 ]
 [4.69 ]
 [5.012]] [[0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [1.149]]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.038]
 [0.039]] [[5.499]
 [5.499]
 [5.499]
 [5.499]
 [5.499]
 [5.509]
 [5.404]] [[1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.46 ]
 [1.393]]
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.489234095982315
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.244]
 [0.238]
 [0.244]
 [0.244]
 [0.244]
 [0.239]] [[1.315]
 [1.938]
 [1.559]
 [1.938]
 [1.938]
 [1.938]
 [2.   ]] [[0.24 ]
 [0.244]
 [0.238]
 [0.244]
 [0.244]
 [0.244]
 [0.239]]
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.827]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[3.539]
 [5.009]
 [3.539]
 [3.539]
 [3.539]
 [3.539]
 [3.539]] [[0.65 ]
 [0.827]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]]
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.887]
 [0.76 ]
 [0.661]
 [0.661]
 [0.739]
 [0.735]] [[3.418]
 [4.716]
 [4.247]
 [4.372]
 [4.372]
 [4.101]
 [4.102]] [[1.636]
 [2.122]
 [1.849]
 [1.794]
 [1.794]
 [1.782]
 [1.778]]
using explorer policy with actor:  0
start point for exploration sampling:  11106
first move QE:  1.2599380955836335
siam score:  -0.8332748
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[4.247]
 [4.666]
 [4.666]
 [4.666]
 [4.666]
 [4.666]
 [4.666]] [[0.57 ]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968418960244648 -1.0 -0.9968418960244648
probs:  [0.17503828228613078, 0.1429455036786156, 0.17391890522152728, 0.17503828228613078, 0.16958309026243018, 0.16347593626516538]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]] [[5.114]
 [4.883]
 [4.883]
 [4.883]
 [4.883]
 [4.883]
 [4.883]] [[1.562]
 [1.242]
 [1.242]
 [1.242]
 [1.242]
 [1.242]
 [1.242]]
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.17319922447507569, 0.14326415374742055, 0.17430662181801138, 0.17542849494320767, 0.1699611385735802, 0.16384036644270458]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.627]] [[4.624]
 [4.624]
 [4.624]
 [4.624]
 [4.624]
 [4.624]
 [5.231]] [[1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.632]]
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.17352298544313627, 0.14278662936338735, 0.1746324528417368, 0.1746324528417368, 0.17027884659406373, 0.1641466329159388]
using explorer policy with actor:  1
siam score:  -0.8470096
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.683]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.679]] [[1.69 ]
 [2.437]
 [1.63 ]
 [1.63 ]
 [1.63 ]
 [1.63 ]
 [1.785]] [[1.128]
 [1.574]
 [1.129]
 [1.129]
 [1.129]
 [1.129]
 [1.21 ]]
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
siam score:  -0.844947
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.47 ]] [[6.191]
 [6.191]
 [6.191]
 [6.191]
 [6.191]
 [6.191]
 [6.643]] [[1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.887]]
first move QE:  1.2819391572383791
from probs:  [0.17222093156320942, 0.14277409833148597, 0.17440857508775584, 0.1755237050661772, 0.17008832309588431, 0.16498436685548729]
745 459
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.691]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]] [[4.411]
 [4.885]
 [4.411]
 [4.411]
 [4.411]
 [4.411]
 [4.411]] [[1.508]
 [1.774]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.043]
 [-0.043]
 [-0.042]
 [-0.043]
 [-0.042]
 [-0.042]] [[6.543]
 [6.543]
 [6.543]
 [6.297]
 [6.395]
 [6.569]
 [6.433]] [[1.192]
 [1.192]
 [1.192]
 [1.033]
 [1.096]
 [1.21 ]
 [1.122]]
siam score:  -0.84211105
Starting evaluation
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]] [[0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.13 ]
 [0.128]] [[1.875]
 [1.926]
 [1.926]
 [1.926]
 [1.926]
 [1.764]
 [1.901]] [[0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.13 ]
 [0.128]]
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.031]
 [0.072]
 [0.085]
 [0.085]
 [0.085]
 [0.085]] [[0.021]
 [2.126]
 [1.128]
 [1.415]
 [1.332]
 [1.35 ]
 [1.549]] [[0.088]
 [0.031]
 [0.072]
 [0.085]
 [0.085]
 [0.085]
 [0.085]]
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.185]
 [0.205]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[1.102]
 [1.708]
 [1.491]
 [1.51 ]
 [1.391]
 [1.487]
 [1.603]] [[0.208]
 [0.185]
 [0.205]
 [0.206]
 [0.206]
 [0.206]
 [0.206]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.413]
 [0.406]
 [0.407]
 [0.41 ]
 [0.414]
 [0.421]] [[0.818]
 [3.178]
 [3.135]
 [3.199]
 [3.278]
 [3.025]
 [3.147]] [[0.496]
 [0.413]
 [0.406]
 [0.407]
 [0.41 ]
 [0.414]
 [0.421]]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[2.759]
 [2.7  ]
 [2.7  ]
 [2.7  ]
 [2.7  ]
 [2.7  ]
 [2.7  ]] [[0.525]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]]
siam score:  -0.84389067
maxi score, test score, baseline:  -0.9969414201183432 -1.0 -0.9969414201183432
probs:  [0.17325201294004086, 0.14289093863801158, 0.17434533216924292, 0.17545275874333785, 0.1700540541946249, 0.16400490331474174]
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]] [[1.885]
 [1.885]
 [1.885]
 [1.885]
 [1.885]
 [1.885]
 [1.885]] [[0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.669]
 [1.261]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]] [[3.345]
 [4.406]
 [3.345]
 [3.345]
 [3.345]
 [3.345]
 [3.345]] [[1.29 ]
 [2.495]
 [1.29 ]
 [1.29 ]
 [1.29 ]
 [1.29 ]
 [1.29 ]]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.536]
 [0.543]
 [0.538]
 [0.547]
 [0.572]
 [0.521]] [[4.714]
 [5.517]
 [4.908]
 [5.032]
 [4.759]
 [5.096]
 [5.177]] [[1.188]
 [1.646]
 [1.299]
 [1.367]
 [1.215]
 [1.427]
 [1.438]]
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.181]
 [0.177]
 [0.173]
 [0.175]
 [0.182]
 [0.204]] [[5.06 ]
 [5.956]
 [4.929]
 [4.96 ]
 [5.287]
 [5.493]
 [5.536]] [[0.638]
 [1.061]
 [0.558]
 [0.569]
 [0.729]
 [0.836]
 [0.879]]
from probs:  [0.1734441027411857, 0.1430493092720304, 0.17453863621641358, 0.17453863621641358, 0.17024259232614417, 0.16418672322781255]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970264367816092 -1.0 -0.9970264367816092
probs:  [0.1736247921695996, 0.14319831050390364, 0.17472046675446912, 0.17472046675446912, 0.1693782024742047, 0.1643577613433538]
siam score:  -0.8260063
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.009]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[4.072]
 [4.624]
 [5.134]
 [5.134]
 [5.134]
 [5.134]
 [5.134]] [[-0.635]
 [-0.445]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.289]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9970428571428571 -1.0 -0.9970428571428571
probs:  [0.17379302991859796, 0.14333705021408874, 0.1748897667443071, 0.1748897667443071, 0.16954232321522839, 0.1635480631634707]
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.052]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.044]
 [-0.043]] [[2.635]
 [2.302]
 [2.754]
 [2.53 ]
 [2.562]
 [2.587]
 [2.644]] [[-0.606]
 [-0.744]
 [-0.575]
 [-0.649]
 [-0.64 ]
 [-0.633]
 [-0.612]]
755 459
maxi score, test score, baseline:  -0.9970509971509972 -1.0 -0.9970509971509972
using another actor
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
probs:  [0.17340291170249877, 0.14317653652112952, 0.17340291170249877, 0.17559124655725855, 0.17022235048502188, 0.16420404303159247]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.727]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[4.862]
 [6.25 ]
 [4.862]
 [4.862]
 [4.862]
 [4.862]
 [4.862]] [[1.325]
 [2.041]
 [1.325]
 [1.325]
 [1.325]
 [1.325]
 [1.325]]
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.185]
 [0.185]
 [0.121]
 [0.116]
 [0.121]
 [0.159]] [[1.863]
 [1.863]
 [1.863]
 [1.714]
 [1.679]
 [1.687]
 [2.159]] [[-0.312]
 [-0.312]
 [-0.312]
 [-0.488]
 [-0.509]
 [-0.497]
 [-0.264]]
maxi score, test score, baseline:  -0.9970751412429378 -1.0 -0.9970751412429378
using explorer policy with actor:  0
UNIT TEST: sample policy line 217 mcts : [0.02  0.51  0.061 0.347 0.02  0.02  0.02 ]
maxi score, test score, baseline:  -0.9970751412429378 -1.0 -0.9970751412429378
probs:  [0.17395393637457515, 0.14363150283581433, 0.17181352930125085, 0.17614922568054878, 0.16972597178529258, 0.1647258340225183]
using explorer policy with actor:  1
rdn probs:  [0.17336791350645886, 0.14330718202574708, 0.17230142064326448, 0.1766494300085954, 0.1691804568656957, 0.16519359695023852]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.471]
 [0.462]
 [0.479]
 [0.483]
 [0.46 ]
 [0.466]] [[1.295]
 [1.848]
 [1.284]
 [1.295]
 [1.477]
 [1.259]
 [1.318]] [[0.504]
 [0.471]
 [0.462]
 [0.479]
 [0.483]
 [0.46 ]
 [0.466]]
in main func line 156:  765
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
line 256 mcts: sample exp_bonus 2.846339155782432
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.573]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[1.645]
 [2.823]
 [2.059]
 [2.059]
 [2.059]
 [2.059]
 [2.059]] [[0.833]
 [1.807]
 [1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.169]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.17302054305357237, 0.14390527860945732, 0.17302054305357237, 0.17627430451142656, 0.16886736785940074, 0.16491196291257063]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.322]
 [0.38 ]
 [0.382]
 [0.31 ]
 [0.365]
 [0.399]] [[2.336]
 [3.917]
 [2.904]
 [2.683]
 [3.629]
 [2.44 ]
 [3.378]] [[0.359]
 [0.322]
 [0.38 ]
 [0.382]
 [0.31 ]
 [0.365]
 [0.399]]
line 256 mcts: sample exp_bonus 5.522620536325642
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.1731949209564749, 0.1440503128363627, 0.1731949209564749, 0.17645196170180302, 0.16802971492600505, 0.16507816862287944]
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[3.143]
 [3.262]
 [3.262]
 [3.262]
 [3.262]
 [3.262]
 [3.262]] [[0.638]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]]
maxi score, test score, baseline:  -0.9971067039106145 -1.0 -0.9971067039106145
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.82152617
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.794]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]] [[2.316]
 [2.898]
 [2.316]
 [2.316]
 [2.316]
 [2.316]
 [2.316]] [[0.589]
 [1.069]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
siam score:  -0.8193966
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.532]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[3.384]
 [4.004]
 [3.519]
 [3.519]
 [3.519]
 [3.519]
 [3.519]] [[1.196]
 [1.678]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]]
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.485]
 [0.533]
 [0.558]
 [0.507]
 [0.507]
 [0.538]] [[3.092]
 [3.456]
 [3.306]
 [3.188]
 [3.13 ]
 [3.13 ]
 [3.269]] [[1.323]
 [1.674]
 [1.576]
 [1.484]
 [1.358]
 [1.358]
 [1.544]]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.485]
 [0.59 ]
 [0.594]
 [0.583]
 [0.592]
 [0.62 ]] [[3.376]
 [2.968]
 [2.982]
 [2.946]
 [3.069]
 [3.037]
 [3.329]] [[1.854]
 [1.173]
 [1.398]
 [1.369]
 [1.471]
 [1.456]
 [1.804]]
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.17146054127614857, 0.14450414339311715, 0.1746235702212946, 0.17355612981123497, 0.1694157548873673, 0.16643986041083744]
siam score:  -0.81779325
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.372]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[3.934]
 [4.782]
 [3.934]
 [3.934]
 [3.934]
 [3.934]
 [3.934]] [[0.349]
 [0.372]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]]
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.17216945105911155, 0.14510160097821295, 0.17427370388046773, 0.17321508286231632, 0.16811214921194576, 0.16712801200794572]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.534]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.591]] [[2.956]
 [3.231]
 [2.746]
 [2.746]
 [2.746]
 [2.746]
 [2.69 ]] [[0.564]
 [0.534]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.591]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.17149302241619602, 0.1454038190623071, 0.17463669039796897, 0.17252805420569592, 0.16846230058945685, 0.16747611332837506]
Printing some Q and Qe and total Qs values:  [[ 0.182]
 [-0.003]
 [-0.004]
 [ 0.149]
 [-0.003]
 [-0.001]
 [ 0.007]] [[5.274]
 [3.172]
 [3.8  ]
 [5.231]
 [4.013]
 [3.974]
 [4.115]] [[0.85 ]
 [0.035]
 [0.242]
 [0.814]
 [0.313]
 [0.301]
 [0.352]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
line 256 mcts: sample exp_bonus 2.382375157427275
UNIT TEST: sample policy line 217 mcts : [0.102 0.327 0.061 0.061 0.041 0.163 0.245]
Printing some Q and Qe and total Qs values:  [[ 0.169]
 [ 0.14 ]
 [-0.006]
 [-0.003]
 [-0.005]
 [-0.001]
 [ 0.056]] [[4.002]
 [3.603]
 [2.788]
 [3.311]
 [3.432]
 [3.465]
 [3.606]] [[0.968]
 [0.839]
 [0.527]
 [0.677]
 [0.71 ]
 [0.721]
 [0.793]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using another actor
from probs:  [0.17185740636591532, 0.14368207125374954, 0.17185740636591532, 0.17393166416558395, 0.16983283138779562, 0.1688386204610404]
maxi score, test score, baseline:  -0.9971451790633609 -1.0 -0.9971451790633609
probs:  [0.17101317923641313, 0.14382853829935047, 0.17203260327155298, 0.17410897615294213, 0.1700059638723528, 0.16901073916738843]
from probs:  [0.17101317923641313, 0.14382853829935047, 0.17203260327155298, 0.17410897615294213, 0.1700059638723528, 0.16901073916738843]
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.244]
 [0.227]
 [0.208]
 [0.205]
 [0.209]
 [0.218]] [[-0.677]
 [-0.117]
 [-0.632]
 [-1.086]
 [-1.01 ]
 [-0.459]
 [-0.439]] [[0.222]
 [0.244]
 [0.227]
 [0.208]
 [0.205]
 [0.209]
 [0.218]]
maxi score, test score, baseline:  -0.9971451790633609 -1.0 -0.9971451790633609
probs:  [0.1711919921560898, 0.14397892671374618, 0.1722124821101777, 0.17324541706370566, 0.1701837236385778, 0.1691874583177029]
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.17119199331471727, 0.14397892090496223, 0.17221248353008306, 0.17324541874807528, 0.17018372453905642, 0.16918745896310583]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.311]
 [0.474]
 [0.476]
 [0.474]
 [0.49 ]
 [0.474]] [[2.664]
 [2.179]
 [2.546]
 [2.583]
 [2.593]
 [2.481]
 [2.589]] [[1.375]
 [0.839]
 [1.269]
 [1.297]
 [1.302]
 [1.241]
 [1.299]]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.362]
 [0.389]
 [0.409]
 [0.399]
 [0.401]
 [0.406]] [[3.447]
 [3.797]
 [3.271]
 [3.189]
 [3.271]
 [3.267]
 [2.796]] [[0.391]
 [0.362]
 [0.389]
 [0.409]
 [0.399]
 [0.401]
 [0.406]]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.375]] [[2.354]
 [2.388]
 [2.388]
 [2.388]
 [2.388]
 [2.388]
 [2.402]] [[1.873]
 [1.941]
 [1.941]
 [1.941]
 [1.941]
 [1.941]
 [1.959]]
from probs:  [0.171395699219509, 0.1450042632126166, 0.171395699219509, 0.17241114895315934, 0.17039233817316402, 0.16940085122204204]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.17173628377441355, 0.1452924047043251, 0.1707309289249747, 0.1727537513328818, 0.1707309289249747, 0.16875570233843018]
maxi score, test score, baseline:  -0.997175204359673 -1.0 -0.997175204359673
probs:  [0.17173628506279642, 0.14529239927231066, 0.1707309299578586, 0.17275375287984193, 0.1707309299578586, 0.1687557028693337]
maxi score, test score, baseline:  -0.997175204359673 -1.0 -0.997175204359673
probs:  [0.17173628506279642, 0.14529239927231066, 0.1707309299578586, 0.17275375287984193, 0.1707309299578586, 0.1687557028693337]
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.291]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[4.144]
 [4.866]
 [4.144]
 [4.144]
 [4.144]
 [4.144]
 [4.144]] [[1.106]
 [1.57 ]
 [1.106]
 [1.106]
 [1.106]
 [1.106]
 [1.106]]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.722]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[1.744]
 [2.165]
 [1.744]
 [1.744]
 [1.744]
 [1.744]
 [1.744]] [[1.828]
 [2.231]
 [1.828]
 [1.828]
 [1.828]
 [1.828]
 [1.828]]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.361]
 [0.355]] [[7.42 ]
 [7.42 ]
 [7.42 ]
 [7.42 ]
 [7.42 ]
 [6.585]
 [6.962]] [[1.847]
 [1.847]
 [1.847]
 [1.847]
 [1.847]
 [1.392]
 [1.601]]
Printing some Q and Qe and total Qs values:  [[0.173]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.175]] [[2.887]
 [2.894]
 [2.894]
 [2.894]
 [2.894]
 [2.894]
 [2.834]] [[1.122]
 [1.143]
 [1.143]
 [1.143]
 [1.143]
 [1.143]
 [1.056]]
maxi score, test score, baseline:  -0.997189972899729 -1.0 -0.997189972899729
line 256 mcts: sample exp_bonus 4.130028002545687
799 489
from probs:  [0.17301827104597903, 0.1449417799893281, 0.17001543777788802, 0.17100453670445842, 0.17100453670445842, 0.17001543777788802]
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.127]
 [0.127]
 [0.104]
 [0.127]
 [0.127]
 [0.108]] [[ 0.157]
 [ 0.397]
 [ 0.397]
 [-0.045]
 [ 0.397]
 [ 0.397]
 [ 0.127]] [[0.107]
 [0.127]
 [0.127]
 [0.104]
 [0.127]
 [0.127]
 [0.108]]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.628]
 [0.537]
 [0.528]
 [0.5  ]
 [0.503]
 [0.491]] [[2.259]
 [1.826]
 [2.122]
 [2.227]
 [2.078]
 [2.202]
 [2.353]] [[0.537]
 [0.357]
 [0.47 ]
 [0.558]
 [0.353]
 [0.483]
 [0.611]]
maxi score, test score, baseline:  -0.9972045822102426 -1.0 -0.9972045822102426
maxi score, test score, baseline:  -0.9972045822102426 -1.0 -0.9972045822102426
probs:  [0.1736522870151549, 0.14476342099629697, 0.16965733547075143, 0.1716311724764182, 0.16965733547075143, 0.170638448570627]
maxi score, test score, baseline:  -0.9972045822102426 -1.0 -0.9972045822102426
probs:  [0.1736522870151549, 0.14476342099629697, 0.16965733547075143, 0.1716311724764182, 0.16965733547075143, 0.170638448570627]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
probs:  [0.17365228876208716, 0.1447634155188327, 0.16965733621864434, 0.17163117371791933, 0.16965733621864434, 0.17063844956387222]
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[3.849]
 [3.849]
 [3.849]
 [3.849]
 [3.849]
 [3.849]
 [3.849]] [[1.531]
 [1.531]
 [1.531]
 [1.531]
 [1.531]
 [1.531]
 [1.531]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
probs:  [0.17416295793702014, 0.14518912953901913, 0.16918370049174125, 0.17114025577023909, 0.16918370049174125, 0.17114025577023909]
Printing some Q and Qe and total Qs values:  [[0.904]
 [1.152]
 [0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]] [[2.436]
 [2.943]
 [2.436]
 [2.436]
 [2.436]
 [2.436]
 [2.436]] [[1.361]
 [2.027]
 [1.361]
 [1.361]
 [1.361]
 [1.361]
 [1.361]]
siam score:  -0.80013573
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.21 ]
 [0.241]
 [0.228]
 [0.228]
 [0.228]
 [0.241]] [[3.232]
 [3.808]
 [3.297]
 [4.07 ]
 [4.07 ]
 [4.07 ]
 [3.249]] [[-0.229]
 [-0.097]
 [-0.207]
 [ 0.026]
 [ 0.026]
 [ 0.026]
 [-0.222]]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.609]] [[1.812]
 [1.812]
 [1.812]
 [1.812]
 [1.812]
 [1.812]
 [2.325]] [[1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.815]]
from probs:  [0.17433450475167234, 0.1453321308245078, 0.16935034165372426, 0.1703238565512182, 0.16935034165372426, 0.17130882456515326]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[3.037]
 [3.037]
 [3.037]
 [3.037]
 [3.037]
 [3.037]
 [3.037]] [[-0.13]
 [-0.13]
 [-0.13]
 [-0.13]
 [-0.13]
 [-0.13]
 [-0.13]]
from probs:  [0.17450638796204226, 0.1454754194594892, 0.16951731078265345, 0.17049178550658084, 0.16951731078265345, 0.17049178550658084]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.488]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[0.538]
 [1.6  ]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]] [[0.449]
 [0.488]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
Printing some Q and Qe and total Qs values:  [[ 0.033]
 [-0.011]
 [ 0.037]
 [ 0.094]
 [ 0.128]
 [ 0.048]
 [ 0.037]] [[3.891]
 [3.88 ]
 [3.998]
 [4.601]
 [4.844]
 [4.177]
 [3.77 ]] [[0.524]
 [0.491]
 [0.581]
 [0.924]
 [1.068]
 [0.679]
 [0.464]]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.504]
 [0.451]
 [0.451]
 [0.455]
 [0.455]
 [0.456]] [[0.834]
 [1.731]
 [0.813]
 [0.801]
 [0.914]
 [0.903]
 [1.043]] [[0.459]
 [0.504]
 [0.451]
 [0.451]
 [0.455]
 [0.455]
 [0.456]]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.17383179155833023, 0.14576637609488444, 0.16985635215864553, 0.16985635215864553, 0.16985635215864553, 0.17083277587084877]
first move QE:  1.3436206448857662
809 495
using explorer policy with actor:  1
from probs:  [0.17412017861178825, 0.1461501233006539, 0.16825009947116576, 0.16920033258875694, 0.17016155106030872, 0.1721177149673264]
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.1747435418354384, 0.14596520682148267, 0.16885244737375804, 0.16885244737375804, 0.16885244737375804, 0.17273390922180473]
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.502]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[3.368]
 [3.143]
 [3.368]
 [3.368]
 [3.368]
 [3.368]
 [3.368]] [[0.485]
 [0.502]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.544]
 [0.312]
 [0.313]
 [0.422]
 [0.315]
 [0.422]] [[0.434]
 [0.731]
 [0.404]
 [0.475]
 [0.897]
 [1.002]
 [0.897]] [[0.313]
 [0.544]
 [0.312]
 [0.313]
 [0.422]
 [0.315]
 [0.422]]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.458]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[1.286]
 [1.501]
 [1.286]
 [1.286]
 [1.286]
 [1.286]
 [1.286]] [[0.435]
 [0.458]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.859]
 [1.106]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]] [[1.809]
 [2.399]
 [1.809]
 [1.809]
 [1.809]
 [1.809]
 [1.809]] [[1.899]
 [2.589]
 [1.899]
 [1.899]
 [1.899]
 [1.899]
 [1.899]]
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.484]
 [0.451]
 [0.452]
 [0.453]
 [0.456]
 [0.455]] [[1.452]
 [1.805]
 [1.299]
 [1.335]
 [1.328]
 [1.315]
 [1.391]] [[0.463]
 [0.484]
 [0.451]
 [0.452]
 [0.453]
 [0.456]
 [0.455]]
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.242]
 [0.181]
 [0.218]
 [0.205]
 [0.128]
 [0.16 ]] [[2.1  ]
 [2.573]
 [2.614]
 [1.855]
 [1.761]
 [2.622]
 [1.905]] [[0.66 ]
 [1.147]
 [1.098]
 [0.401]
 [0.291]
 [1.027]
 [0.366]]
siam score:  -0.78010637
first move QE:  1.3428852028428324
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]] [[2.446]
 [2.592]
 [2.592]
 [2.592]
 [2.592]
 [2.592]
 [2.592]] [[1.76 ]
 [1.731]
 [1.731]
 [1.731]
 [1.731]
 [1.731]
 [1.731]]
first move QE:  1.3428386099529634
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.68 ]
 [0.523]
 [0.517]
 [0.516]
 [0.514]
 [0.513]] [[-0.95 ]
 [ 1.079]
 [-1.323]
 [-1.434]
 [-1.322]
 [-1.235]
 [-1.074]] [[0.317]
 [1.341]
 [0.147]
 [0.092]
 [0.143]
 [0.183]
 [0.256]]
siam score:  -0.7686392
820 523
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[3.631]
 [3.631]
 [3.631]
 [3.631]
 [3.631]
 [3.631]
 [3.631]] [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]]
maxi score, test score, baseline:  -0.9972753280839896 -1.0 -0.9972753280839896
probs:  [0.1727005632255536, 0.14786432247032716, 0.16890806463723226, 0.16798627678590417, 0.16984020965542923, 0.1727005632255536]
deleting a thread, now have 5 threads
Frames:  55109 train batches done:  6457 episodes:  1347
maxi score, test score, baseline:  -0.9972753280839896 -1.0 -0.9972753280839896
probs:  [0.17301777231400578, 0.14813591340245488, 0.16738155012962133, 0.16829482687246142, 0.17015216496745075, 0.17301777231400578]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.362]
 [0.423]
 [0.412]
 [0.347]
 [0.461]
 [0.37 ]] [[3.666]
 [3.608]
 [2.633]
 [2.677]
 [2.889]
 [2.937]
 [3.705]] [[0.394]
 [0.362]
 [0.423]
 [0.412]
 [0.347]
 [0.461]
 [0.37 ]]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.467]
 [0.471]
 [0.473]
 [0.47 ]
 [0.474]
 [0.468]] [[1.064]
 [2.04 ]
 [1.059]
 [1.111]
 [0.938]
 [1.211]
 [1.41 ]] [[0.468]
 [0.467]
 [0.471]
 [0.473]
 [0.47 ]
 [0.474]
 [0.468]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
probs:  [0.17317949883686184, 0.1482743759383356, 0.16753800690957138, 0.16845213754593788, 0.1693764819324314, 0.17317949883686184]
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.17333621119032136, 0.14840854541793075, 0.1667847092886033, 0.16860457092796943, 0.1695297519848539, 0.17333621119032136]
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.17333621119032136, 0.14840854541793075, 0.1667847092886033, 0.16860457092796943, 0.1695297519848539, 0.17333621119032136]
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.17333621119032136, 0.14840854541793075, 0.1667847092886033, 0.16860457092796943, 0.1695297519848539, 0.17333621119032136]
deleting a thread, now have 4 threads
Frames:  55351 train batches done:  6487 episodes:  1352
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.17333621119032136, 0.14840854541793075, 0.1667847092886033, 0.16860457092796943, 0.1695297519848539, 0.17333621119032136]
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.262]
 [0.509]
 [0.468]
 [0.3  ]
 [0.516]
 [0.353]] [[3.572]
 [2.946]
 [2.64 ]
 [2.128]
 [2.479]
 [2.773]
 [2.724]] [[1.877]
 [1.705]
 [1.73 ]
 [1.505]
 [1.543]
 [1.787]
 [1.671]]
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.775]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[1.236]
 [1.883]
 [1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.236]] [[1.412]
 [1.819]
 [1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.482]
 [0.558]
 [0.551]
 [0.539]
 [0.54 ]
 [0.537]] [[3.082]
 [3.034]
 [3.173]
 [3.126]
 [3.017]
 [3.016]
 [3.227]] [[1.755]
 [1.568]
 [1.906]
 [1.83 ]
 [1.66 ]
 [1.662]
 [1.937]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972958333333334 -1.0 -0.9972958333333334
siam score:  -0.7569251
maxi score, test score, baseline:  -0.9972958333333334 -1.0 -0.9972958333333334
line 256 mcts: sample exp_bonus 2.498406557197322
maxi score, test score, baseline:  -0.9972958333333334 -1.0 -0.9972958333333334
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.609]
 [0.714]
 [1.093]
 [1.028]
 [0.812]
 [0.836]] [[1.591]
 [1.702]
 [1.847]
 [1.383]
 [1.614]
 [1.527]
 [1.406]] [[0.021]
 [0.632]
 [0.935]
 [1.04 ]
 [1.173]
 [0.762]
 [0.676]]
using explorer policy with actor:  1
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.631]
 [0.6  ]
 [0.54 ]
 [0.348]
 [0.682]
 [0.476]] [[1.75 ]
 [1.991]
 [0.31 ]
 [0.498]
 [1.32 ]
 [2.021]
 [0.971]] [[0.866]
 [1.445]
 [0.823]
 [0.764]
 [0.654]
 [1.556]
 [0.794]]
siam score:  -0.76316833
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
using explorer policy with actor:  1
siam score:  -0.762515
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.1702017796328749, 0.14961202228668366, 0.16930342586942446, 0.16841468016761518, 0.16666540291326046, 0.17580268913014135]
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.566]] [[6.616]
 [6.616]
 [6.616]
 [6.616]
 [6.616]
 [6.616]
 [8.747]] [[1.089]
 [1.089]
 [1.089]
 [1.089]
 [1.089]
 [1.089]
 [2.044]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9973093264248705 -1.0 -0.9973093264248705
probs:  [0.1699147610208675, 0.15015224915446393, 0.1699147610208675, 0.16814033974265, 0.16640327470186864, 0.17547461435928238]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]] [[1.517]
 [1.517]
 [1.517]
 [1.517]
 [1.517]
 [1.517]
 [1.517]] [[-1.094]
 [-1.094]
 [-1.094]
 [-1.094]
 [-1.094]
 [-1.094]
 [-1.094]]
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
probs:  [0.1694700042286424, 0.15054951528816002, 0.1694700042286424, 0.16858520299494517, 0.16598638890015915, 0.17593888435945085]
start point for exploration sampling:  11106
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.8820020415766548
line 256 mcts: sample exp_bonus 2.9022836399484198
siam score:  -0.7461501
from probs:  [0.1688520210506919, 0.15009130039815985, 0.16973822265014304, 0.1688520210506919, 0.1662490938397385, 0.17621734101057485]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.6902148764100073
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.388]
 [0.48 ]
 [0.414]
 [0.243]
 [0.7  ]
 [0.339]] [[2.932]
 [3.208]
 [3.13 ]
 [2.24 ]
 [2.592]
 [3.468]
 [3.398]] [[0.98 ]
 [1.149]
 [1.243]
 [0.73 ]
 [0.653]
 [1.713]
 [1.168]]
842 568
deleting a thread, now have 3 threads
Frames:  56592 train batches done:  6632 episodes:  1411
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
siam score:  -0.74913824
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.49 ]
 [0.548]
 [0.554]
 [0.399]
 [0.541]
 [0.423]] [[2.952]
 [1.613]
 [0.674]
 [2.255]
 [1.977]
 [0.969]
 [2.749]] [[0.374]
 [0.49 ]
 [0.548]
 [0.554]
 [0.399]
 [0.541]
 [0.423]]
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
probs:  [0.16952776555976273, 0.1493130197185457, 0.16864743271279073, 0.16952776555976273, 0.16606145497481045, 0.1769225614743276]
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
deleting a thread, now have 2 threads
Frames:  56787 train batches done:  6655 episodes:  1416
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.1700020692604496, 0.14973075671387645, 0.1691192729875626, 0.1691192729875626, 0.1665260589359571, 0.17550256911459158]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
probs:  [0.17011804884172108, 0.14915068211812182, 0.16923465008695449, 0.16923465008695449, 0.16663966624482768, 0.1756223026214205]
siam score:  -0.7482265
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
siam score:  -0.74099135
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.336]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]] [[-0.693]
 [ 0.196]
 [-0.693]
 [-0.693]
 [-0.693]
 [-0.693]
 [-0.693]] [[0.302]
 [0.336]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]]
maxi score, test score, baseline:  -0.9973424552429667 -1.0 -0.9973424552429667
probs:  [0.17052681448904874, 0.14883153449365397, 0.1687650925044603, 0.16964129285658364, 0.16619097022646062, 0.17604429542979277]
847 576
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.17081846664424208, 0.14908607591881776, 0.168185222592246, 0.16993143028810231, 0.16563341907794052, 0.1763453854786514]
siam score:  -0.73207426
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.17081846664424208, 0.14908607591881776, 0.168185222592246, 0.16993143028810231, 0.16563341907794052, 0.1763453854786514]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.17081846767167752, 0.14908607156819334, 0.1681852229680392, 0.1699314310960251, 0.16563341882224536, 0.17634538787381948]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.422]
 [0.427]
 [0.424]
 [0.41 ]
 [0.423]
 [0.423]] [[-0.591]
 [ 0.546]
 [-0.517]
 [-0.624]
 [-0.691]
 [-0.542]
 [-0.559]] [[0.414]
 [0.422]
 [0.427]
 [0.424]
 [0.41 ]
 [0.423]
 [0.423]]
maxi score, test score, baseline:  -0.9973683544303797 -1.0 -0.9973683544303797
Printing some Q and Qe and total Qs values:  [[1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.343]] [[1.272]
 [1.298]
 [1.298]
 [1.298]
 [1.315]
 [1.315]
 [1.298]] [[2.303]
 [2.33 ]
 [2.33 ]
 [2.33 ]
 [2.347]
 [2.347]
 [2.33 ]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]] [[4.204]
 [3.318]
 [3.318]
 [3.318]
 [3.318]
 [3.318]
 [3.318]] [[0.914]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.579]] [[0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.65 ]] [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.579]]
maxi score, test score, baseline:  -0.9973683544303797 -1.0 -0.9973683544303797
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.309916698279234
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.495]
 [0.532]
 [0.531]
 [0.53 ]
 [0.531]
 [0.531]] [[2.546]
 [3.123]
 [2.655]
 [2.698]
 [2.682]
 [2.534]
 [2.704]] [[0.538]
 [0.495]
 [0.532]
 [0.531]
 [0.53 ]
 [0.531]
 [0.531]]
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.1718630931585731, 0.150100687749058, 0.16751622094404364, 0.17009703408083757, 0.16583957023272516, 0.17458339383476248]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9973811083123426 -1.0 -0.9973811083123426
maxi score, test score, baseline:  -0.9973874371859297 -1.0 -0.9973874371859297
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.224]
 [0.235]
 [0.234]
 [0.237]
 [0.231]
 [0.23 ]] [[-2.375]
 [ 0.128]
 [-2.234]
 [-2.381]
 [-2.419]
 [-1.762]
 [-1.997]] [[0.247]
 [0.224]
 [0.235]
 [0.234]
 [0.237]
 [0.231]
 [0.23 ]]
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.386]
 [0.37 ]
 [0.376]
 [0.376]
 [0.374]
 [0.385]] [[ 0.224]
 [ 0.565]
 [-1.392]
 [ 0.107]
 [-1.539]
 [-0.675]
 [ 0.654]] [[0.383]
 [0.386]
 [0.37 ]
 [0.376]
 [0.376]
 [0.374]
 [0.385]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9973874371859297 -1.0 -0.9973874371859297
probs:  [0.1716747397515173, 0.15003802719993045, 0.1682013941430888, 0.16991997660559247, 0.16486838573096038, 0.1752974765689105]
maxi score, test score, baseline:  -0.9973874371859297 -1.0 -0.9973874371859297
maxi score, test score, baseline:  -0.9973937343358396 -1.0 -0.9973937343358396
maxi score, test score, baseline:  -0.9973937343358396 -1.0 -0.9973937343358396
from probs:  [0.17182315214229854, 0.15016772958100533, 0.16834680301242969, 0.16920238116615388, 0.16501091243326257, 0.17544902166484994]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.74263245
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.17222609191864827, 0.14984731356340314, 0.16874158963653046, 0.16874158963653046, 0.16458294997822406, 0.1758604652666636]
line 256 mcts: sample exp_bonus 2.618425295632889
maxi score, test score, baseline:  -0.9974062344139651 -1.0 -0.9974062344139651
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9974062344139651 -1.0 -0.9974062344139651
probs:  [0.17248842134191686, 0.15017962823046868, 0.16733393037270422, 0.16901684964641298, 0.16487266093490524, 0.17610850947359202]
maxi score, test score, baseline:  -0.9974062344139651 -1.0 -0.9974062344139651
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.517]
 [0.341]
 [0.524]
 [0.441]
 [0.221]
 [0.45 ]] [[1.657]
 [2.816]
 [2.228]
 [2.199]
 [1.857]
 [2.209]
 [2.43 ]] [[-0.094]
 [ 1.714]
 [ 0.565]
 [ 0.839]
 [ 0.206]
 [ 0.333]
 [ 1.044]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8879508399498148
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.1724482245264532, 0.15024773585432338, 0.16649706251366775, 0.16815423888271025, 0.16568090315191433, 0.17697183507093106]
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.583]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[-1.526]
 [ 2.524]
 [-1.532]
 [-1.532]
 [-1.532]
 [-1.532]
 [-1.532]] [[0.241]
 [0.583]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]]
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
from probs:  [0.17298921646437415, 0.15005491074354366, 0.16701938211091177, 0.16701938211091177, 0.1653900883001587, 0.1775270202701]
maxi score, test score, baseline:  -0.9974308641975309 -1.0 -0.9974308641975309
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.5369],
        [-0.5813],
        [-0.3111],
        [-0.4611],
        [-0.0000],
        [-0.0000],
        [-0.4475],
        [-0.5486],
        [-0.4663]], dtype=torch.float64)
-0.97515 -0.97515
-0.024259925299500003 -0.5611529607767253
-0.024259925299500003 -0.6055158046587261
-0.024259925299500003 -0.33537111258490043
-0.0727797758985 -0.5339058203421653
-0.7204229999999998 -0.7204229999999998
-0.9454499999999999 -0.9454499999999999
-0.0727797758985 -0.5203182753340816
-0.024259925299500003 -0.5728659787580707
-0.0530787758985 -0.5194029002747663
siam score:  -0.7505548
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9974369458128078 -1.0 -0.9974369458128078
maxi score, test score, baseline:  -0.9974369458128078 -1.0 -0.9974369458128078
probs:  [0.17278988217649105, 0.14933206234583676, 0.16767927918534184, 0.16685732387093202, 0.16604354721636708, 0.17729790520503133]
from probs:  [0.1729491140364344, 0.14946967701248848, 0.16783380144765447, 0.16701108867295905, 0.16619656209502676, 0.17653975673543687]
from probs:  [0.17322991932251539, 0.14971235496094581, 0.16810630017809172, 0.16565863063632497, 0.1664664021766427, 0.1768263927254794]
maxi score, test score, baseline:  -0.9974429975429976 -1.0 -0.9974429975429976
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.549]
 [0.565]
 [0.555]
 [0.548]
 [0.535]
 [0.58 ]] [[1.359]
 [2.438]
 [1.35 ]
 [1.379]
 [1.426]
 [1.399]
 [1.973]] [[0.275]
 [0.637]
 [0.307]
 [0.296]
 [0.297]
 [0.262]
 [0.543]]
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.399]
 [0.385]
 [0.384]
 [0.385]
 [0.412]
 [0.39 ]] [[-0.138]
 [ 0.624]
 [-0.353]
 [-0.22 ]
 [-0.197]
 [ 0.098]
 [-0.349]] [[0.38 ]
 [0.399]
 [0.385]
 [0.384]
 [0.385]
 [0.412]
 [0.39 ]]
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.616]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[2.234]
 [3.318]
 [2.234]
 [2.234]
 [2.234]
 [2.234]
 [2.234]] [[0.453]
 [0.83 ]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.49 ]
 [0.603]
 [0.622]
 [0.939]
 [0.558]
 [0.531]] [[2.103]
 [2.253]
 [2.197]
 [1.917]
 [1.701]
 [2.251]
 [2.048]] [[0.578]
 [0.3  ]
 [0.507]
 [0.452]
 [1.013]
 [0.434]
 [0.314]]
line 256 mcts: sample exp_bonus 2.484239317596976
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.576]
 [0.513]
 [0.516]
 [0.212]
 [0.582]
 [0.513]] [[ 3.455]
 [ 0.48 ]
 [ 1.332]
 [-1.261]
 [ 0.763]
 [-1.949]
 [ 1.332]] [[1.786]
 [0.993]
 [1.244]
 [0.352]
 [0.84 ]
 [0.161]
 [1.244]]
maxi score, test score, baseline:  -0.9974786924939467 -1.0 -0.9974786924939467
probs:  [0.17258795604354263, 0.1508716898107952, 0.1667808003335686, 0.1667808003335686, 0.1659835810479485, 0.17699517243057653]
maxi score, test score, baseline:  -0.9974786924939467 -1.0 -0.9974786924939467
probs:  [0.17258795604354263, 0.1508716898107952, 0.1667808003335686, 0.1667808003335686, 0.1659835810479485, 0.17699517243057653]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.705]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[-0.239]
 [ 0.449]
 [-0.239]
 [-0.239]
 [-0.239]
 [-0.239]
 [-0.239]] [[0.621]
 [1.094]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.395]
 [0.466]
 [0.466]
 [0.466]
 [0.423]
 [0.467]] [[1.677]
 [1.816]
 [1.677]
 [1.677]
 [1.677]
 [1.531]
 [2.16 ]] [[0.158]
 [0.063]
 [0.158]
 [0.158]
 [0.158]
 [0.024]
 [0.322]]
maxi score, test score, baseline:  -0.9974786924939467 -1.0 -0.9974786924939467
probs:  [0.17188013477023958, 0.15100075512035563, 0.16692347531554022, 0.16692347531554022, 0.1661255740374667, 0.17714658544085765]
maxi score, test score, baseline:  -0.9974845410628019 -1.0 -0.9974845410628019
siam score:  -0.7446919
maxi score, test score, baseline:  -0.9974845410628019 -1.0 -0.9974845410628019
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
from probs:  [0.17298871546142053, 0.1513200320405964, 0.16640178390932536, 0.16640178390932536, 0.16640178390932536, 0.17648590077000714]
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
probs:  [0.17310103738710617, 0.15076898240688025, 0.16650982892692298, 0.16650982892692298, 0.16650982892692298, 0.17660049342524464]
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
siam score:  -0.7466679
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
probs:  [0.17310103738710617, 0.15076898240688025, 0.16650982892692298, 0.16650982892692298, 0.16650982892692298, 0.17660049342524464]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9646922347690311
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.195]
 [0.234]
 [0.234]
 [0.221]
 [0.236]
 [0.222]] [[-2.83 ]
 [-0.762]
 [-2.423]
 [-2.355]
 [-1.946]
 [-2.076]
 [-1.756]] [[0.238]
 [0.195]
 [0.234]
 [0.234]
 [0.221]
 [0.236]
 [0.222]]
maxi score, test score, baseline:  -0.9975019184652278 -1.0 -0.9975019184652278
probs:  [0.17310103889415754, 0.15076897868334177, 0.16650982889018862, 0.16650982889018862, 0.16650982889018862, 0.17660049575193487]
maxi score, test score, baseline:  -0.9975019184652278 -1.0 -0.9975019184652278
probs:  [0.17310103889415754, 0.15076897868334177, 0.16650982889018862, 0.16650982889018862, 0.16650982889018862, 0.17660049575193487]
maxi score, test score, baseline:  -0.9975019184652278 -1.0 -0.9975019184652278
maxi score, test score, baseline:  -0.9975019184652278 -1.0 -0.9975019184652278
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9975076555023924 -1.0 -0.9975076555023924
Printing some Q and Qe and total Qs values:  [[1.102]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]] [[3.402]
 [2.237]
 [2.237]
 [2.237]
 [2.237]
 [2.237]
 [2.237]] [[1.715]
 [1.286]
 [1.286]
 [1.286]
 [1.286]
 [1.286]
 [1.286]]
maxi score, test score, baseline:  -0.9975133651551312 -1.0 -0.9975133651551312
probs:  [0.1732375669177661, 0.15088788298596473, 0.16585245396638826, 0.16664115534954513, 0.16664115534954513, 0.17673978543079064]
start point for exploration sampling:  11106
siam score:  -0.74490845
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.879]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]] [[1.604]
 [1.546]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]] [[1.979]
 [1.956]
 [1.979]
 [1.979]
 [1.979]
 [1.979]
 [1.979]]
siam score:  -0.74184257
siam score:  -0.7364102
first move QE:  1.272137626616415
from probs:  [0.17345070641049543, 0.15053094261119018, 0.16609277975656214, 0.16531434945133475, 0.16767231309435365, 0.17693890867606382]
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.543]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[2.001]
 [2.082]
 [2.001]
 [2.001]
 [2.001]
 [2.001]
 [2.001]] [[0.527]
 [0.543]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
maxi score, test score, baseline:  -0.9975190476190476 -1.0 -0.9975190476190476
probs:  [0.1735612522278742, 0.1499895481526127, 0.16619863612624128, 0.16541970970187928, 0.16777917615237392, 0.17705167763901866]
first move QE:  1.2701878824953206
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.319]
 [0.263]
 [0.319]
 [0.319]
 [0.319]
 [0.319]] [[1.739]
 [1.739]
 [2.25 ]
 [1.739]
 [1.739]
 [1.739]
 [1.739]] [[1.252]
 [1.252]
 [1.651]
 [1.252]
 [1.252]
 [1.252]
 [1.252]]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.798]
 [0.553]
 [0.548]
 [0.546]
 [0.551]
 [0.557]] [[-0.602]
 [-0.737]
 [-0.369]
 [-0.197]
 [-0.119]
 [-0.095]
 [-0.181]] [[0.574]
 [1.036]
 [0.668]
 [0.716]
 [0.737]
 [0.756]
 [0.738]]
maxi score, test score, baseline:  -0.997524703087886 -1.0 -0.997524703087886
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.717]
 [0.688]
 [0.688]
 [0.688]
 [0.732]
 [0.728]] [[2.865]
 [1.793]
 [2.865]
 [2.865]
 [2.865]
 [1.902]
 [1.911]] [[0.688]
 [0.717]
 [0.688]
 [0.688]
 [0.688]
 [0.732]
 [0.728]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[1.622]
 [1.622]
 [1.622]
 [1.622]
 [1.622]
 [1.622]
 [1.622]] [[1.364]
 [1.364]
 [1.364]
 [1.364]
 [1.364]
 [1.364]
 [1.364]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.557]] [[0.722]
 [1.624]
 [1.624]
 [1.624]
 [1.624]
 [1.624]
 [1.161]] [[0.634]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.781]]
siam score:  -0.73137605
maxi score, test score, baseline:  -0.9975303317535545 -1.0 -0.9975303317535545
maxi score, test score, baseline:  -0.9975303317535545 -1.0 -0.9975303317535545
probs:  [0.1729398116330105, 0.14965869166135026, 0.1664511741289473, 0.1664511741289473, 0.16723495786591647, 0.17726419058182816]
maxi score, test score, baseline:  -0.9975303317535545 -1.0 -0.9975303317535545
maxi score, test score, baseline:  -0.9975303317535545 -1.0 -0.9975303317535545
probs:  [0.17253159919584116, 0.1500334665978781, 0.1668680007597742, 0.1660897375733215, 0.1676537472460967, 0.1768234486270884]
line 256 mcts: sample exp_bonus -1.8758796248289134
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.474]
 [0.438]
 [0.468]
 [0.491]
 [0.435]
 [0.491]] [[-0.599]
 [ 1.241]
 [-0.24 ]
 [ 0.511]
 [ 0.919]
 [-0.082]
 [ 0.919]] [[0.435]
 [0.474]
 [0.438]
 [0.468]
 [0.491]
 [0.435]
 [0.491]]
using explorer policy with actor:  0
siam score:  -0.73829573
maxi score, test score, baseline:  -0.9975359338061466 -1.0 -0.9975359338061466
Starting evaluation
line 256 mcts: sample exp_bonus -2.2677334922249788
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.63 ]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[0.086]
 [1.342]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]] [[0.511]
 [0.63 ]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]]
maxi score, test score, baseline:  -0.9975415094339622 -1.0 -0.9975415094339622
probs:  [0.17348012729278076, 0.15022835302272475, 0.16700284731755088, 0.16545991034231922, 0.16778538951551525, 0.17604337250910918]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.145]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.169]] [[-4.975]
 [-1.618]
 [-3.627]
 [-3.627]
 [-3.627]
 [-3.627]
 [-3.29 ]] [[0.214]
 [0.145]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.169]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -1.4634589744160873
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]] [[-3.631]
 [-3.631]
 [-3.631]
 [-3.631]
 [-3.631]
 [-3.631]
 [-3.631]] [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.624]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.607]] [[1.155]
 [1.226]
 [1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.626]] [[0.278]
 [0.391]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.489]]
Printing some Q and Qe and total Qs values:  [[0.17 ]
 [0.137]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]] [[-3.631]
 [-1.467]
 [-3.729]
 [-3.729]
 [-3.729]
 [-3.729]
 [-3.729]] [[0.17 ]
 [0.137]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.3904406451966764
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[-3.681]
 [-3.821]
 [-3.821]
 [-3.821]
 [-3.821]
 [-3.821]
 [-3.821]] [[0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 3.4266252670778776
from probs:  [0.17348013352820177, 0.15022833797900292, 0.16700284762521067, 0.16545990923794154, 0.167785390539328, 0.1760433810903151]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.734]] [[1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]
 [2.157]] [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.734]]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.414]] [[2.59 ]
 [2.59 ]
 [2.59 ]
 [2.59 ]
 [2.59 ]
 [2.59 ]
 [1.571]] [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.414]]
maxi score, test score, baseline:  -0.9976116704805492 -1.0 -0.9976116704805492
probs:  [0.17305708092233119, 0.15058900393726382, 0.16662683476485585, 0.16585717676369577, 0.16740382284221744, 0.17646608076963594]
maxi score, test score, baseline:  -0.9976116704805492 -1.0 -0.9976116704805492
maxi score, test score, baseline:  -0.9976116704805492 -1.0 -0.9976116704805492
probs:  [0.17305708092233119, 0.15058900393726382, 0.16662683476485585, 0.16585717676369577, 0.16740382284221744, 0.17646608076963594]
maxi score, test score, baseline:  -0.9976116704805492 -1.0 -0.9976116704805492
maxi score, test score, baseline:  -0.9976220956719818 -1.0 -0.9976220956719818
maxi score, test score, baseline:  -0.9976220956719818 -1.0 -0.9976220956719818
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9976272727272727 -1.0 -0.9976272727272727
using explorer policy with actor:  0
using explorer policy with actor:  0
siam score:  -0.73357236
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.17358531071329789, 0.15104862543164338, 0.1671354292200618, 0.16409048516627825, 0.1671354292200618, 0.17700472024865674]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.433]
 [0.416]
 [0.417]
 [0.412]
 [0.416]
 [0.56 ]] [[1.518]
 [2.553]
 [1.258]
 [1.246]
 [1.239]
 [1.473]
 [0.605]] [[0.42 ]
 [0.433]
 [0.416]
 [0.417]
 [0.412]
 [0.416]
 [0.56 ]]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.636]
 [0.618]
 [0.619]
 [0.622]
 [0.619]
 [0.619]] [[0.291]
 [0.862]
 [1.205]
 [1.161]
 [1.111]
 [1.05 ]
 [1.074]] [[0.652]
 [0.636]
 [0.618]
 [0.619]
 [0.622]
 [0.619]
 [0.619]]
line 256 mcts: sample exp_bonus 1.4978191672536973
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.151]
 [0.343]
 [0.355]
 [0.371]
 [0.376]
 [0.366]] [[1.309]
 [2.32 ]
 [1.794]
 [1.887]
 [1.638]
 [1.528]
 [1.617]] [[0.136]
 [0.327]
 [0.363]
 [0.448]
 [0.313]
 [0.25 ]
 [0.289]]
maxi score, test score, baseline:  -0.9976578475336323 -1.0 -0.9976578475336323
probs:  [0.17434734045142417, 0.15045554651248735, 0.1670937426346286, 0.16332395165457356, 0.1678691404001944, 0.17691027834669193]
siam score:  -0.7354469
maxi score, test score, baseline:  -0.9976578475336323 -1.0 -0.9976578475336323
probs:  [0.17434734045142417, 0.15045554651248735, 0.1670937426346286, 0.16332395165457356, 0.1678691404001944, 0.17691027834669193]
rdn probs:  [0.17434734045142417, 0.15045554651248735, 0.1670937426346286, 0.16332395165457356, 0.1678691404001944, 0.17691027834669193]
maxi score, test score, baseline:  -0.9976628635346756 -1.0 -0.9976628635346756
probs:  [0.17445552713795473, 0.14992838836305133, 0.16719742683178532, 0.16342529584829227, 0.16797330590611517, 0.17702005591280123]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9976628635346756 -1.0 -0.9976628635346756
probs:  [0.1737630734212711, 0.1500541459365936, 0.1673376694031252, 0.16356237441550492, 0.16811419927261675, 0.17716853755088838]
902 680
maxi score, test score, baseline:  -0.9976678571428571 -1.0 -0.9976678571428571
probs:  [0.17389811201530164, 0.15017075455527784, 0.16746771325255075, 0.16368948356475935, 0.16746771325255075, 0.17730622335955962]
902 682
maxi score, test score, baseline:  -0.9976678571428571 -1.0 -0.9976678571428571
siam score:  -0.7353968
maxi score, test score, baseline:  -0.9976678571428571 -1.0 -0.9976678571428571
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9976678571428571 -1.0 -0.9976678571428571
probs:  [0.1740320882640755, 0.15028645054609507, 0.16759673533192987, 0.1638155947842406, 0.16682630575554622, 0.1774428253181127]
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.17416016974294088, 0.1503970513168315, 0.16772007932900385, 0.16320019935958935, 0.16694908258930716, 0.17757341766232748]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.398]
 [0.349]
 [0.382]
 [0.384]
 [0.384]
 [0.37 ]] [[1.91 ]
 [1.741]
 [1.908]
 [1.792]
 [1.925]
 [1.925]
 [1.937]] [[0.341]
 [0.398]
 [0.349]
 [0.382]
 [0.384]
 [0.384]
 [0.37 ]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.17455366057453206, 0.15011823427959872, 0.16732628096617147, 0.16356892765295653, 0.16732628096617147, 0.17710661556056986]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.375]
 [0.404]
 [0.4  ]
 [0.397]
 [0.394]
 [0.388]] [[2.457]
 [3.009]
 [2.115]
 [2.366]
 [2.417]
 [2.269]
 [2.116]] [[0.387]
 [0.375]
 [0.404]
 [0.4  ]
 [0.397]
 [0.394]
 [0.388]]
maxi score, test score, baseline:  -0.9976777777777778 -1.0 -0.9976777777777778
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.478]
 [0.457]
 [0.456]
 [0.502]
 [0.464]
 [0.464]] [[2.037]
 [2.682]
 [1.818]
 [0.864]
 [0.9  ]
 [1.997]
 [2.283]] [[0.474]
 [0.478]
 [0.457]
 [0.456]
 [0.502]
 [0.464]
 [0.464]]
maxi score, test score, baseline:  -0.9976777777777778 -1.0 -0.9976777777777778
maxi score, test score, baseline:  -0.9976777777777778 -1.0 -0.9976777777777778
probs:  [0.17455366222047375, 0.150118230826096, 0.1673262811038268, 0.1635689270064874, 0.1673262811038268, 0.17710661773928932]
maxi score, test score, baseline:  -0.9976777777777778 -1.0 -0.9976777777777778
from probs:  [0.17455366222047375, 0.150118230826096, 0.1673262811038268, 0.1635689270064874, 0.1673262811038268, 0.17710661773928932]
maxi score, test score, baseline:  -0.9976876106194691 -1.0 -0.9976876106194691
maxi score, test score, baseline:  -0.9976876106194691 -1.0 -0.9976876106194691
maxi score, test score, baseline:  -0.9976924944812362 -1.0 -0.9976924944812362
probs:  [0.17522751643213322, 0.15069773894933428, 0.16720375744243263, 0.1627396297136174, 0.16720375744243263, 0.17692760002005]
maxi score, test score, baseline:  -0.9976924944812362 -1.0 -0.9976924944812362
probs:  [0.17522751643213322, 0.15069773894933428, 0.16720375744243263, 0.1627396297136174, 0.16720375744243263, 0.17692760002005]
siam score:  -0.7262938
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
Printing some Q and Qe and total Qs values:  [[0.865]
 [0.73 ]
 [0.665]
 [0.618]
 [0.736]
 [0.265]
 [0.624]] [[2.217]
 [1.795]
 [1.508]
 [1.555]
 [1.466]
 [1.681]
 [1.879]] [[1.659]
 [1.089]
 [0.733]
 [0.725]
 [0.775]
 [0.434]
 [1.048]]
909 695
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
probs:  [0.17607436256130735, 0.14898156927018333, 0.16724682085494602, 0.1635261214580499, 0.16724682085494602, 0.1769243050005674]
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
siam score:  -0.73274976
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.448]
 [0.433]] [[1.522]
 [1.385]
 [1.385]
 [1.385]
 [1.385]
 [1.762]
 [1.759]] [[0.195]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.221]
 [0.191]]
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.382]
 [0.328]
 [0.335]
 [0.328]
 [0.334]
 [0.337]] [[0.761]
 [1.77 ]
 [0.96 ]
 [1.049]
 [0.96 ]
 [1.195]
 [1.331]] [[0.329]
 [0.382]
 [0.328]
 [0.335]
 [0.328]
 [0.334]
 [0.337]]
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
probs:  [0.17426197616832442, 0.148971317925108, 0.16790990386537705, 0.16417445225540517, 0.16790990386537705, 0.17677244592040842]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.404]
 [0.408]
 [0.408]
 [0.408]
 [0.411]
 [0.417]] [[1.969]
 [2.364]
 [1.969]
 [1.969]
 [1.969]
 [1.922]
 [1.668]] [[0.408]
 [0.404]
 [0.408]
 [0.408]
 [0.408]
 [0.411]
 [0.417]]
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
probs:  [0.17426197616832442, 0.148971317925108, 0.16790990386537705, 0.16417445225540517, 0.16790990386537705, 0.17677244592040842]
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
probs:  [0.17439468067168784, 0.14908475306910252, 0.16803776862266642, 0.1642994709120203, 0.16727626353346073, 0.1769070631910621]
siam score:  -0.7228744
first move QE:  1.231981848409015
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.702]
 [0.654]
 [0.643]
 [0.642]
 [0.643]
 [0.668]] [[1.063]
 [1.491]
 [0.981]
 [0.712]
 [0.721]
 [0.75 ]
 [1.192]] [[0.82 ]
 [1.033]
 [0.767]
 [0.655]
 [0.655]
 [0.668]
 [0.866]]
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
probs:  [0.17544581534754827, 0.14938329847717632, 0.16828449314442803, 0.16383286042356954, 0.16677338845936598, 0.1762801441479118]
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
in main func line 156:  916
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
probs:  [0.17489435694625236, 0.14961842513672602, 0.16854936994879924, 0.16409073043707542, 0.16628951156614472, 0.1765576059650022]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.579]
 [0.628]
 [0.597]
 [0.613]
 [0.612]
 [0.68 ]] [[2.373]
 [2.767]
 [2.408]
 [2.343]
 [2.337]
 [2.606]
 [2.33 ]] [[0.832]
 [1.096]
 [0.835]
 [0.708]
 [0.734]
 [1.   ]
 [0.861]]
siam score:  -0.7324157
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.186]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.25 ]] [[-1.475]
 [ 1.092]
 [-1.459]
 [-1.459]
 [-1.459]
 [-1.459]
 [-1.267]] [[0.237]
 [0.186]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.25 ]]
first move QE:  1.23236671140087
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
probs:  [0.17539059655971462, 0.14944517995126216, 0.16750982905528758, 0.16383449353752788, 0.16676133606856575, 0.17705856482764187]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.426]
 [0.595]
 [0.598]
 [0.598]
 [0.603]
 [0.598]] [[-1.379]
 [ 1.772]
 [-1.36 ]
 [-1.544]
 [-1.414]
 [-1.4  ]
 [-1.387]] [[0.897]
 [1.955]
 [0.902]
 [0.835]
 [0.884]
 [0.893]
 [0.894]]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.604]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[-1.195]
 [-0.115]
 [-1.195]
 [-1.195]
 [-1.195]
 [-1.195]
 [-1.195]] [[1.023]
 [1.871]
 [1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.023]]
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.17552197717410992, 0.14955711535973581, 0.1668862493461773, 0.16395721324636484, 0.1668862493461773, 0.17719119552743495]
line 256 mcts: sample exp_bonus 2.748435479984043
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
maxi score, test score, baseline:  -0.9977260869565218 -1.0 -0.9977260869565218
probs:  [0.17667215989829857, 0.1505371471583251, 0.16575870802368098, 0.1621872826147438, 0.16649238546063003, 0.17835231684432157]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977260869565218 -1.0 -0.9977260869565218
using explorer policy with actor:  1
924 722
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.491]
 [0.5  ]] [[4.56 ]
 [4.395]
 [4.395]
 [4.395]
 [4.395]
 [4.893]
 [4.757]] [[1.18 ]
 [1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.397]
 [1.311]]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.354]
 [0.468]
 [0.46 ]
 [0.46 ]
 [0.465]
 [0.456]] [[5.156]
 [4.606]
 [4.998]
 [5.032]
 [5.072]
 [5.382]
 [5.14 ]] [[1.53 ]
 [1.041]
 [1.422]
 [1.439]
 [1.467]
 [1.694]
 [1.512]]
maxi score, test score, baseline:  -0.9977260869565218 -1.0 -0.9977260869565218
probs:  [0.1763967790041023, 0.15101025194261908, 0.16555026831390707, 0.16269700118457942, 0.16627965078953472, 0.17806604876525728]
maxi score, test score, baseline:  -0.9977260869565218 -1.0 -0.9977260869565218
probs:  [0.1757186332236968, 0.15113459211339822, 0.1656865805733059, 0.16283096409539047, 0.1664165636143946, 0.17821266637981403]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9977260869565218 -1.0 -0.9977260869565218
siam score:  -0.7285268
maxi score, test score, baseline:  -0.9977260869565218 -1.0 -0.9977260869565218
probs:  [0.17598923187471732, 0.1513673324665306, 0.16594173032249762, 0.16238239272808783, 0.16667283750405207, 0.17764647510411447]
maxi score, test score, baseline:  -0.9977260869565218 -1.0 -0.9977260869565218
probs:  [0.17598923187471732, 0.1513673324665306, 0.16594173032249762, 0.16238239272808783, 0.16667283750405207, 0.17764647510411447]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977308026030369 -1.0 -0.9977308026030369
probs:  [0.1754439714676183, 0.1516020012906906, 0.16619899753987266, 0.16263414099975143, 0.16619899753987266, 0.17792189116219434]
siam score:  -0.73745346
maxi score, test score, baseline:  -0.9977308026030369 -1.0 -0.9977308026030369
probs:  [0.1756978469824794, 0.15182137637556636, 0.1649924490502295, 0.16286948009937918, 0.1664394951512575, 0.17817935234108814]
maxi score, test score, baseline:  -0.9977308026030369 -1.0 -0.9977308026030369
maxi score, test score, baseline:  -0.9977308026030369 -1.0 -0.9977308026030369
from probs:  [0.17552462964492338, 0.15237553240454077, 0.1648781555855895, 0.1620743636594046, 0.16631760233340756, 0.17882971637213418]
siam score:  -0.7397775
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[3.42 ]
 [2.324]
 [2.324]
 [2.324]
 [2.324]
 [2.324]
 [2.324]] [[0.543]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]]
maxi score, test score, baseline:  -0.9977308026030369 -1.0 -0.9977308026030369
probs:  [0.1756493778725988, 0.15248382818985035, 0.16428462081958844, 0.16218955256285958, 0.16643580697605112, 0.17895681357905183]
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[0.81 ]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[0.3  ]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]]
maxi score, test score, baseline:  -0.9977354978354979 -1.0 -0.9977354978354979
maxi score, test score, baseline:  -0.9977354978354979 -1.0 -0.9977354978354979
probs:  [0.17577320882069333, 0.15259132312152499, 0.16369545869566143, 0.1623038918334295, 0.16655314064488774, 0.17908297688380304]
start point for exploration sampling:  11106
siam score:  -0.74518085
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977401727861771 -1.0 -0.9977401727861771
probs:  [0.17634068160291003, 0.1530839500949158, 0.16422393685127026, 0.1628278771298857, 0.16636456175739328, 0.17715899256362502]
Printing some Q and Qe and total Qs values:  [[0.749]
 [1.139]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]] [[0.731]
 [1.699]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]] [[0.938]
 [2.043]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]]
maxi score, test score, baseline:  -0.9977448275862069 -1.0 -0.9977448275862069
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.419]
 [0.525]
 [0.515]
 [0.414]
 [0.429]
 [0.409]] [[1.315]
 [1.978]
 [1.381]
 [1.657]
 [1.486]
 [1.542]
 [1.693]] [[0.056]
 [0.204]
 [0.217]
 [0.29 ]
 [0.031]
 [0.078]
 [0.089]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9977494623655914 -1.0 -0.9977494623655914
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
probs:  [0.1756724647449086, 0.15320813651561205, 0.1643571678366486, 0.1629599746531939, 0.16649953071794582, 0.17730272553169107]
Printing some Q and Qe and total Qs values:  [[1.067]
 [1.079]
 [1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.053]] [[2.097]
 [2.173]
 [2.306]
 [2.306]
 [2.306]
 [2.306]
 [2.184]] [[2.944]
 [2.993]
 [3.026]
 [3.026]
 [3.026]
 [3.026]
 [2.945]]
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
probs:  [0.17551772584050418, 0.15377671508723909, 0.16426287099922382, 0.162186544330229, 0.167117435664153, 0.17713870807865084]
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
probs:  [0.17551772584050418, 0.15377671508723909, 0.16426287099922382, 0.162186544330229, 0.167117435664153, 0.17713870807865084]
siam score:  -0.73768747
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
Printing some Q and Qe and total Qs values:  [[0.895]
 [1.111]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]] [[1.374]
 [1.474]
 [1.374]
 [1.374]
 [1.374]
 [1.374]
 [1.374]] [[0.885]
 [1.35 ]
 [0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.885]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]] [[1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]] [[1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]]
siam score:  -0.7368883
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[1.473]
 [1.475]
 [1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]] [[1.209]
 [1.212]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]] [[2.785]
 [2.791]
 [2.785]
 [2.785]
 [2.785]
 [2.785]
 [2.785]]
from probs:  [0.17184309342258064, 0.15544361939583737, 0.16322285707518994, 0.16322285707518994, 0.16814056900024324, 0.17812700403095888]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.204]
 [0.313]
 [0.238]
 [0.13 ]
 [0.271]
 [0.211]] [[ 2.709]
 [ 2.296]
 [ 0.126]
 [ 0.958]
 [ 1.606]
 [-2.992]
 [ 1.259]] [[0.134]
 [0.204]
 [0.313]
 [0.238]
 [0.13 ]
 [0.271]
 [0.211]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4539],
        [-0.4518],
        [-0.0000],
        [-0.5306],
        [-0.4798],
        [-0.5456],
        [-0.4486],
        [-0.0000],
        [-0.5279],
        [-0.5134]], dtype=torch.float64)
-0.0727797758985 -0.526648712831207
-0.0727797758985 -0.5246230867299393
-0.9608890648499999 -0.9608890648499999
-0.024259925299500003 -0.5548685299739193
-0.0727797758985 -0.5525301690012062
-0.024259925299500003 -0.5698205289972308
-0.024259925299500003 -0.47283317699984256
-0.936486045 -0.936486045
-0.043375785898500004 -0.5712531429699523
-0.0727797758985 -0.5861529768591098
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.542]
 [0.621]
 [0.62 ]
 [0.62 ]
 [0.619]
 [0.623]] [[0.96 ]
 [1.733]
 [0.927]
 [0.756]
 [0.914]
 [1.072]
 [0.969]] [[0.626]
 [0.542]
 [0.621]
 [0.62 ]
 [0.62 ]
 [0.619]
 [0.623]]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.944]
 [1.248]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]] [[1.478]
 [1.623]
 [1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.375]] [[1.205]
 [1.861]
 [1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.133]]
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.801]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]] [[2.053]
 [1.986]
 [2.053]
 [2.053]
 [2.053]
 [2.053]
 [2.053]] [[0.583]
 [0.641]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]]
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.528]
 [0.373]
 [0.373]
 [0.373]
 [0.353]
 [0.373]] [[-0.743]
 [ 1.316]
 [-1.449]
 [-1.449]
 [-1.449]
 [-1.007]
 [-1.449]] [[0.367]
 [0.528]
 [0.373]
 [0.373]
 [0.373]
 [0.353]
 [0.373]]
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
probs:  [0.17243892493129961, 0.15536630241001412, 0.1631078286051979, 0.16243263193492585, 0.16872356126367186, 0.1779307508548908]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.331]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[-1.409]
 [-0.556]
 [-1.409]
 [-1.409]
 [-1.409]
 [-1.409]
 [-1.409]] [[0.333]
 [0.331]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
line 256 mcts: sample exp_bonus 1.7697515260106016
using another actor
maxi score, test score, baseline:  -0.997772340425532 -1.0 -0.997772340425532
probs:  [0.17203397423895195, 0.15507178740803443, 0.1627651453672777, 0.1627651453672777, 0.1690689540915916, 0.17829499352686648]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
start point for exploration sampling:  11106
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.81 ]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]] [[1.791]
 [2.479]
 [2.29 ]
 [2.29 ]
 [2.29 ]
 [2.29 ]
 [2.29 ]] [[1.199]
 [1.408]
 [1.301]
 [1.301]
 [1.301]
 [1.301]
 [1.301]]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.17 ]
 [0.205]
 [0.171]
 [0.176]
 [0.169]
 [0.191]] [[1.069]
 [1.532]
 [1.245]
 [1.583]
 [1.497]
 [1.381]
 [1.227]] [[-0.504]
 [-0.423]
 [-0.45 ]
 [-0.404]
 [-0.423]
 [-0.475]
 [-0.484]]
from probs:  [0.17176681803775576, 0.15551000000959458, 0.1625523346828346, 0.1625523346828346, 0.16881967558820607, 0.17879883699877433]
rdn beta is 0 so we're just using the maxi policy
using another actor
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]] [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]] [[0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]]
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]] [[1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.958]] [[2.382]
 [2.382]
 [2.382]
 [2.382]
 [2.382]
 [2.382]
 [2.382]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
first move QE:  1.1922500962298601
line 256 mcts: sample exp_bonus 2.1748775852482938
maxi score, test score, baseline:  -0.9978035639412998 -1.0 -0.9978035639412998
from probs:  [0.1701271027639039, 0.1568242486955513, 0.16253730013154502, 0.16253730013154502, 0.16869360555826243, 0.17928044271919233]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]] [[1.144]
 [1.147]
 [1.155]
 [1.155]
 [1.155]
 [1.149]
 [1.155]] [[0.566]
 [0.571]
 [0.582]
 [0.582]
 [0.582]
 [0.574]
 [0.582]]
siam score:  -0.74419004
maxi score, test score, baseline:  -0.9978035639412998 -1.0 -0.9978035639412998
line 256 mcts: sample exp_bonus -0.25871033880642286
siam score:  -0.7413507
maxi score, test score, baseline:  -0.9978035639412998 -1.0 -0.9978035639412998
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[2.087]
 [2.452]
 [2.452]
 [2.452]
 [2.452]
 [2.452]
 [2.452]] [[1.481]
 [1.45 ]
 [1.45 ]
 [1.45 ]
 [1.45 ]
 [1.45 ]
 [1.45 ]]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9978123173277662 -1.0 -0.9978123173277662
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.054]
 [0.04 ]
 [0.04 ]
 [0.03 ]
 [0.053]
 [0.033]] [[-2.146]
 [-2.011]
 [-2.277]
 [-2.458]
 [-2.433]
 [-2.296]
 [-2.245]] [[0.047]
 [0.054]
 [0.04 ]
 [0.04 ]
 [0.03 ]
 [0.053]
 [0.033]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.576]
 [0.656]
 [0.661]
 [0.662]
 [0.661]
 [0.656]] [[1.391]
 [1.417]
 [1.378]
 [1.393]
 [1.412]
 [1.353]
 [1.337]] [[0.549]
 [0.397]
 [0.543]
 [0.558]
 [0.567]
 [0.545]
 [0.53 ]]
siam score:  -0.7364313
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.16570372459309013, 0.15867017104032483, 0.16052702917825487, 0.16437772679215898, 0.17123438208757227, 0.1794869663085988]
maxi score, test score, baseline:  -0.9978253112033195 -1.0 -0.9978253112033195
maxi score, test score, baseline:  -0.9978253112033195 -1.0 -0.9978253112033195
probs:  [0.1661543432135731, 0.15910166100240394, 0.16033798776452127, 0.16351675474435537, 0.17170004204628733, 0.17918921122885897]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[1.311]
 [1.311]
 [1.311]
 [1.311]
 [1.311]
 [1.311]
 [1.311]] [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]]
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.902]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[-0.577]
 [ 0.388]
 [-0.577]
 [-0.577]
 [-0.577]
 [-0.577]
 [-0.577]] [[1.322]
 [1.734]
 [1.322]
 [1.322]
 [1.322]
 [1.322]
 [1.322]]
from probs:  [0.16649491502104607, 0.159427775260523, 0.16004474777929886, 0.16320454630663253, 0.17205198218316678, 0.17877603344933285]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9978338842975206 -1.0 -0.9978338842975206
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.421]
 [0.414]
 [0.413]
 [0.403]
 [0.386]
 [0.395]] [[-0.766]
 [ 1.876]
 [ 0.637]
 [ 0.775]
 [ 0.217]
 [-0.438]
 [-0.447]] [[0.379]
 [0.421]
 [0.414]
 [0.413]
 [0.403]
 [0.386]
 [0.395]]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.73893046
first move QE:  1.1549347333663431
Printing some Q and Qe and total Qs values:  [[1.092]
 [1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]] [[2.87 ]
 [2.831]
 [2.831]
 [2.831]
 [2.831]
 [2.831]
 [2.831]] [[0.94 ]
 [0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]]
start point for exploration sampling:  11106
siam score:  -0.73552614
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
using explorer policy with actor:  1
siam score:  -0.73428506
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[1.086]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[4.493]
 [2.114]
 [2.114]
 [2.114]
 [2.114]
 [2.114]
 [2.114]] [[1.421]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]]
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
line 256 mcts: sample exp_bonus 6.895487937946799
siam score:  -0.73056746
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
using another actor
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.481]] [[1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.627]] [[1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.461]]
maxi score, test score, baseline:  -0.9978633401221996 -1.0 -0.9978633401221996
probs:  [0.16756233114211028, 0.15879187241632867, 0.15879187241632867, 0.1636847087918635, 0.1730353490200039, 0.17813386621336494]
maxi score, test score, baseline:  -0.9978633401221996 -1.0 -0.9978633401221996
maxi score, test score, baseline:  -0.9978633401221996 -1.0 -0.9978633401221996
maxi score, test score, baseline:  -0.9978633401221996 -1.0 -0.9978633401221996
probs:  [0.1677723805661121, 0.15899092754470912, 0.15899092754470912, 0.16263633745443437, 0.1732522591999105, 0.17835716769012466]
maxi score, test score, baseline:  -0.9978633401221996 -1.0 -0.9978633401221996
maxi score, test score, baseline:  -0.9978674796747967 -1.0 -0.9978674796747967
probs:  [0.16787636929050737, 0.15908947158175854, 0.15908947158175854, 0.1621173227732498, 0.17335964556542294, 0.17846771920730278]
using another actor
maxi score, test score, baseline:  -0.9978716024340771 -1.0 -0.9978716024340771
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9978716024340771 -1.0 -0.9978716024340771
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.342]
 [0.4  ]
 [0.403]
 [0.403]
 [0.403]
 [0.423]] [[1.163]
 [1.294]
 [1.311]
 [1.305]
 [1.318]
 [1.298]
 [1.015]] [[0.296]
 [0.237]
 [0.357]
 [0.362]
 [0.366]
 [0.36 ]
 [0.305]]
maxi score, test score, baseline:  -0.9978716024340771 -1.0 -0.9978716024340771
probs:  [0.16752864876386991, 0.15938708301205884, 0.15879427376178018, 0.1618044530287594, 0.1736839554748465, 0.17880158595868528]
maxi score, test score, baseline:  -0.9978716024340771 -1.0 -0.9978716024340771
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9978716024340771 -1.0 -0.9978716024340771
probs:  [0.16720646872016945, 0.1591123683214563, 0.1591123683214563, 0.1621285775391749, 0.17403187684525095, 0.1784083402524921]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.563]
 [0.572]
 [0.586]
 [0.572]
 [0.564]
 [0.566]] [[0.937]
 [1.47 ]
 [1.237]
 [1.12 ]
 [1.107]
 [1.131]
 [1.054]] [[0.565]
 [0.563]
 [0.572]
 [0.586]
 [0.572]
 [0.564]
 [0.566]]
maxi score, test score, baseline:  -0.9978716024340771 -1.0 -0.9978716024340771
probs:  [0.16678088930484153, 0.15932915876216422, 0.15932915876216422, 0.1623494775618875, 0.17355989417352707, 0.1786514214354154]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.469]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]] [[ 2.117]
 [-0.47 ]
 [-0.5  ]
 [-0.5  ]
 [-0.5  ]
 [-0.5  ]
 [-0.5  ]] [[2.01 ]
 [1.323]
 [1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]]
maxi score, test score, baseline:  -0.9978757085020243 -1.0 -0.9978757085020243
maxi score, test score, baseline:  -0.9978757085020243 -1.0 -0.9978757085020243
probs:  [0.16590419021788969, 0.1591627997462734, 0.1591627997462734, 0.16276865976597513, 0.173960291694344, 0.1790412588292445]
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.528]
 [0.49 ]
 [0.457]
 [0.457]
 [0.482]
 [0.49 ]] [[0.374]
 [1.528]
 [0.551]
 [0.443]
 [0.443]
 [0.531]
 [0.766]] [[0.48 ]
 [0.528]
 [0.49 ]
 [0.457]
 [0.457]
 [0.482]
 [0.49 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997879797979798 -1.0 -0.997879797979798
probs:  [0.16653332827686154, 0.15859614634338365, 0.15859614634338365, 0.16338590669918546, 0.17391546252268358, 0.1789730098145021]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9978838709677419 -1.0 -0.9978838709677419
probs:  [0.16450471641485176, 0.15740503713399528, 0.15854432891450446, 0.16512616766973293, 0.1757678784144749, 0.17865187145244069]
using another actor
maxi score, test score, baseline:  -0.9978838709677419 -1.0 -0.9978838709677419
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.2601181701604856
maxi score, test score, baseline:  -0.9978838709677419 -1.0 -0.9978838709677419
probs:  [0.16425005741561546, 0.15721467769813519, 0.15949006072943922, 0.1661111607711117, 0.175401422603056, 0.1775326207826425]
Printing some Q and Qe and total Qs values:  [[0.57]
 [0.57]
 [0.57]
 [0.57]
 [0.57]
 [0.57]
 [0.57]] [[1.519]
 [1.519]
 [1.519]
 [1.519]
 [1.519]
 [1.519]
 [1.519]] [[1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.105]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[-3.279]
 [ 0.872]
 [-3.372]
 [-3.372]
 [-3.372]
 [-3.372]
 [-3.372]] [[0.201]
 [0.105]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]]
992 921
maxi score, test score, baseline:  -0.997895991983968 -1.0 -0.997895991983968
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.51 ]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.246]] [[-3.902]
 [-0.485]
 [-3.888]
 [-3.888]
 [-3.888]
 [-3.888]
 [-4.031]] [[0.221]
 [0.51 ]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.246]]
siam score:  -0.7359409
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.663]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[1.305]
 [1.366]
 [1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.345]] [[1.81 ]
 [1.928]
 [1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.73437136
Printing some Q and Qe and total Qs values:  [[0.459]
 [1.365]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[1.539]
 [1.07 ]
 [1.539]
 [1.539]
 [1.539]
 [1.539]
 [1.539]] [[1.858]
 [2.408]
 [1.858]
 [1.858]
 [1.858]
 [1.858]
 [1.858]]
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.122]
 [0.139]
 [0.143]
 [0.138]
 [0.14 ]
 [0.141]] [[-4.72 ]
 [-3.038]
 [-4.455]
 [-4.367]
 [-4.192]
 [-4.276]
 [-4.301]] [[0.14 ]
 [0.122]
 [0.139]
 [0.143]
 [0.138]
 [0.14 ]
 [0.141]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16684955014654188, 0.15857149177152585, 0.15913472775585305, 0.16260241198771833, 0.17537318456206094, 0.1774686337763]
line 256 mcts: sample exp_bonus 2.8930866409535945
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16578065783252444, 0.1582004944923022, 0.15932002630870426, 0.16219141809706877, 0.17620103125804087, 0.1783063720113595]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16527189644576581, 0.15829697547625382, 0.15941719005620575, 0.162290333006638, 0.17630849014382768, 0.17841511487130887]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]] [[1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]] [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.807]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]] [[2.086]
 [2.272]
 [2.086]
 [2.086]
 [2.086]
 [2.086]
 [2.086]] [[2.039]
 [2.115]
 [2.039]
 [2.039]
 [2.039]
 [2.039]
 [2.039]]
UNIT TEST: sample policy line 217 mcts : [0.082 0.122 0.143 0.082 0.061 0.184 0.327]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16545657901819888, 0.15791918184203793, 0.1590325654828867, 0.16247168384017502, 0.1765055055239485, 0.17861448429275306]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16545657901819888, 0.15791918184203793, 0.1590325654828867, 0.16247168384017502, 0.1765055055239485, 0.17861448429275306]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.716]
 [0.674]
 [0.717]
 [0.743]
 [0.649]
 [0.751]] [[2.197]
 [2.3  ]
 [2.47 ]
 [2.505]
 [2.676]
 [2.457]
 [2.423]] [[0.942]
 [1.054]
 [1.139]
 [1.217]
 [1.383]
 [1.099]
 [1.193]]
first move QE:  1.117431911281008
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1649639927751071, 0.1580533191731101, 0.1597246505605496, 0.16201044201690065, 0.17727363012866426, 0.17797396534566826]
1004 946
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.234]
 [0.219]
 [0.217]
 [0.217]
 [0.218]
 [0.215]] [[-3.587]
 [-1.495]
 [-3.95 ]
 [-4.047]
 [-4.138]
 [-4.129]
 [-3.761]] [[0.214]
 [0.234]
 [0.219]
 [0.217]
 [0.217]
 [0.218]
 [0.215]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1647856094424364, 0.15845829180486032, 0.15957133488124392, 0.1624255537995437, 0.1777278500647511, 0.17703136000716466]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.352]
 [0.381]
 [0.46 ]
 [0.516]
 [0.353]
 [0.342]] [[0.452]
 [1.34 ]
 [0.755]
 [0.523]
 [0.631]
 [1.06 ]
 [0.841]] [[0.346]
 [0.352]
 [0.381]
 [0.46 ]
 [0.516]
 [0.353]
 [0.342]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1649719892862912, 0.15808633365752353, 0.1597518171489583, 0.16202940141074945, 0.17792886815762823, 0.17723159033884944]
from probs:  [0.1649719892862912, 0.15808633365752353, 0.1597518171489583, 0.16202940141074945, 0.17792886815762823, 0.17723159033884944]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [1.067]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]] [[-2.352]
 [ 0.279]
 [-2.352]
 [-2.352]
 [-2.352]
 [-2.352]
 [-2.352]] [[0.823]
 [1.782]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]]
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.962]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[-0.667]
 [ 1.512]
 [-0.667]
 [-0.667]
 [-0.667]
 [-0.667]
 [-0.667]] [[1.299]
 [2.047]
 [1.299]
 [1.299]
 [1.299]
 [1.299]
 [1.299]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.012]
 [0.006]
 [0.007]
 [0.007]
 [0.007]
 [0.006]] [[1.596]
 [1.54 ]
 [1.733]
 [2.087]
 [1.861]
 [1.835]
 [1.714]] [[0.283]
 [0.241]
 [0.404]
 [0.715]
 [0.517]
 [0.494]
 [0.386]]
line 256 mcts: sample exp_bonus 1.4426153917775766
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.471]
 [0.482]
 [0.5  ]
 [0.487]
 [0.518]
 [0.465]] [[2.741]
 [2.219]
 [2.508]
 [2.065]
 [2.424]
 [1.71 ]
 [2.906]] [[0.49 ]
 [0.471]
 [0.482]
 [0.5  ]
 [0.487]
 [0.518]
 [0.465]]
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.485]
 [0.466]
 [0.469]
 [0.469]
 [0.487]
 [0.497]] [[1.465]
 [2.132]
 [1.444]
 [1.463]
 [1.488]
 [1.582]
 [1.371]] [[0.489]
 [0.485]
 [0.466]
 [0.469]
 [0.469]
 [0.487]
 [0.497]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16430793327574558, 0.15806789737254534, 0.1608428394955635, 0.16198107030093242, 0.17774556695529176, 0.1770546925999214]
line 256 mcts: sample exp_bonus 1.7191352094377512
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 1.113847516743231
first move QE:  1.1064458741056757
1008 966
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16391704153224368, 0.1577368938175562, 0.1610472541696354, 0.16218269177831954, 0.17721372297899557, 0.17790239572324962]
from probs:  [0.1642546061216405, 0.158061731243223, 0.1613789088268881, 0.16251668471307276, 0.17689403454758784, 0.17689403454758784]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.395]
 [0.552]
 [0.552]
 [0.535]
 [0.542]
 [0.532]] [[-2.578]
 [ 1.021]
 [-2.332]
 [-2.384]
 [-2.336]
 [-2.186]
 [-2.326]] [[0.367]
 [1.617]
 [0.444]
 [0.422]
 [0.421]
 [0.491]
 [0.422]]
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.345]
 [0.345]
 [0.345]
 [0.399]
 [0.437]
 [0.28 ]] [[2.427]
 [3.02 ]
 [3.02 ]
 [3.02 ]
 [2.33 ]
 [2.577]
 [3.354]] [[1.176]
 [0.446]
 [0.446]
 [0.446]
 [0.094]
 [0.334]
 [0.539]]
siam score:  -0.74243075
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.629]
 [0.633]
 [0.63 ]
 [0.636]
 [0.649]
 [0.648]] [[-1.178]
 [ 0.36 ]
 [-1.04 ]
 [-1.106]
 [-1.068]
 [-1.08 ]
 [-1.235]] [[0.635]
 [0.629]
 [0.633]
 [0.63 ]
 [0.636]
 [0.649]
 [0.648]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1649393922553824, 0.15817916681038308, 0.16092550839741407, 0.16205170602662822, 0.1776315151264286, 0.17627271138376366]
1011 982
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16485584416467133, 0.15812332369869636, 0.1614174497838341, 0.16198024551880985, 0.1768115684169942, 0.1768115684169942]
using explorer policy with actor:  1
using another actor
siam score:  -0.73882645
from probs:  [0.16534813881209712, 0.15752478484149282, 0.16189947665362667, 0.16189947665362667, 0.17666406151957836, 0.17666406151957836]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16544086158851354, 0.15761312049201684, 0.16199026551332316, 0.1614294924856825, 0.17676312996023197, 0.17676312996023197]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16555188447332408, 0.15771889039162323, 0.16209897279649269, 0.16153782344924963, 0.17688175091292707, 0.17621067797638337]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.36 ]
 [0.353]
 [0.288]
 [0.353]
 [0.288]
 [0.288]] [[ 0.343]
 [ 0.954]
 [ 0.728]
 [-0.068]
 [ 0.728]
 [ 0.103]
 [ 0.222]] [[0.321]
 [0.36 ]
 [0.353]
 [0.288]
 [0.353]
 [0.288]
 [0.288]]
line 256 mcts: sample exp_bonus 1.2252183532262035
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.239]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]] [[-1.023]
 [-0.44 ]
 [-0.713]
 [-0.713]
 [-0.713]
 [-0.713]
 [-0.713]] [[0.177]
 [0.239]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1659593582903357, 0.15810708481470193, 0.1613768765051634, 0.1619354174344949, 0.17597687661141004, 0.17664438634389407]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1659169810790458, 0.15703860218493101, 0.16135230217765462, 0.16190872358629052, 0.17655684066944932, 0.17722655030262877]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16648126438813976, 0.15704831987253537, 0.16024954538688074, 0.1619010610232568, 0.17649051189595344, 0.177829297433234]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16610163199446373, 0.15724467365500938, 0.16044990158554617, 0.16210348207267825, 0.17604867751415865, 0.17805163317814393]
first move QE:  1.0875735414222643
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16621346498191875, 0.15735054342525231, 0.16055792937320676, 0.16221262318381227, 0.17616720765325186, 0.17749823138255805]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.604]
 [0.507]
 [0.531]
 [0.575]
 [0.519]
 [0.526]] [[1.189]
 [1.432]
 [1.307]
 [1.269]
 [1.466]
 [1.385]
 [1.519]] [[0.549]
 [0.604]
 [0.507]
 [0.531]
 [0.575]
 [0.519]
 [0.526]]
siam score:  -0.7377425
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.619]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[1.203]
 [1.525]
 [1.084]
 [1.084]
 [1.084]
 [1.084]
 [1.084]] [[0.587]
 [0.619]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16505047454190747, 0.15790182421622515, 0.16003267190945739, 0.16278093836892651, 0.17678441300548312, 0.17744967795800037]
line 256 mcts: sample exp_bonus 0.4679321188938528
siam score:  -0.7403281
using another actor
from probs:  [0.16542755426221123, 0.1577377543373724, 0.15985881630378865, 0.16259416470238003, 0.17652662508968803, 0.17785508530455973]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16602484658545885, 0.15830728188779905, 0.15936432683166116, 0.16262447597430948, 0.17585101261007297, 0.17782805611069846]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16622462978905148, 0.15849777828986203, 0.15955609521095868, 0.16226664466796317, 0.17541280953443508, 0.17804204250772945]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1665336533622645, 0.15879243708895585, 0.15932076373774576, 0.16256831006142633, 0.17573891459635313, 0.17704592115325427]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16673484819847664, 0.15898427950474878, 0.15951324444250567, 0.16221336163349453, 0.17595123061431378, 0.17660303560646043]
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.092]
 [0.086]
 [0.088]
 [0.092]
 [0.092]
 [0.076]] [[-2.706]
 [-2.303]
 [-2.217]
 [-2.556]
 [-2.303]
 [-2.303]
 [-2.235]] [[0.11 ]
 [0.092]
 [0.086]
 [0.088]
 [0.092]
 [0.092]
 [0.076]]
1031 1037
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1673025233823107, 0.15952556665059575, 0.15795478653851916, 0.16276564270059346, 0.1759011962969993, 0.17655028443098147]
siam score:  -0.72232133
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1673025233823107, 0.15952556665059575, 0.15795478653851916, 0.16276564270059346, 0.1759011962969993, 0.17655028443098147]
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.84 ]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]] [[2.524]
 [2.13 ]
 [2.524]
 [2.524]
 [2.524]
 [2.524]
 [2.524]] [[1.496]
 [1.568]
 [1.496]
 [1.496]
 [1.496]
 [1.496]
 [1.496]]
siam score:  -0.7192254
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16642831564041308, 0.15979847468672942, 0.15822500736579603, 0.16249376593092082, 0.17620211890798188, 0.17685231746815872]
1035 1043
from probs:  [0.16671327103120825, 0.16007207859017344, 0.15797760600214789, 0.16222452965814435, 0.17585739408206913, 0.17715512063625696]
siam score:  -0.7175344
Printing some Q and Qe and total Qs values:  [[ 0.113]
 [ 0.009]
 [-0.004]
 [ 0.011]
 [-0.008]
 [-0.006]
 [-0.017]] [[1.735]
 [1.466]
 [2.135]
 [1.387]
 [1.441]
 [2.495]
 [1.804]] [[ 0.138]
 [-0.159]
 [ 0.038]
 [-0.181]
 [-0.202]
 [ 0.155]
 [-0.098]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16652291089043825, 0.1604426939282022, 0.15834337200184706, 0.16151399738546593, 0.1762645572058732, 0.17691246858817333]
1036 1047
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.699]] [[3.388]
 [3.388]
 [3.388]
 [3.388]
 [3.388]
 [3.388]
 [4.646]] [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [1.09 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.437]
 [0.454]
 [0.435]
 [0.439]
 [0.45 ]
 [0.501]] [[2.894]
 [2.208]
 [2.729]
 [2.704]
 [2.724]
 [3.109]
 [3.631]] [[ 0.63 ]
 [-0.009]
 [ 0.501]
 [ 0.452]
 [ 0.476]
 [ 0.851]
 [ 1.41 ]]
from probs:  [0.16682554316434556, 0.1612885664057457, 0.15814415619300237, 0.1612885664057457, 0.17654739598993213, 0.17590577184122855]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]] [[0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
in main func line 156:  1041
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.476]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[1.185]
 [1.433]
 [1.185]
 [1.185]
 [1.185]
 [1.185]
 [1.185]] [[0.867]
 [1.173]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16757471689732867, 0.16201287489645833, 0.15681815117947387, 0.16147750030929972, 0.17669572271592007, 0.1754210340015194]
using explorer policy with actor:  1
from probs:  [0.16785964326594857, 0.16175205961412098, 0.15708478821862, 0.16121941056220507, 0.17699615748052103, 0.17508794085858448]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1681528546424747, 0.16205593534200916, 0.15689490678364187, 0.1615241401696098, 0.17663106228925002, 0.17474110077301439]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1672896853434472, 0.1617951810408799, 0.15715817970487722, 0.1617951810408799, 0.17692745289046893, 0.17503431997944682]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1672896853434472, 0.1617951810408799, 0.15715817970487722, 0.1617951810408799, 0.17692745289046893, 0.17503431997944682]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1672896853434472, 0.1617951810408799, 0.15715817970487722, 0.1617951810408799, 0.17692745289046893, 0.17503431997944682]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.765]
 [1.334]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]] [[0.418]
 [0.951]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[1.625]
 [1.984]
 [1.625]
 [1.625]
 [1.625]
 [1.625]
 [1.625]]
line 256 mcts: sample exp_bonus 0.4021817268949669
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.701]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[0.926]
 [0.851]
 [1.265]
 [1.265]
 [1.265]
 [1.265]
 [1.265]] [[0.535]
 [0.701]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]]
line 256 mcts: sample exp_bonus 0.9555792310664487
using explorer policy with actor:  0
siam score:  -0.7110718
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]] [[0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.438]] [[0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.489]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[1.106]
 [1.951]
 [1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]] [[0.458]
 [0.489]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.584]
 [0.356]
 [0.482]
 [0.465]
 [0.461]
 [0.549]] [[1.5  ]
 [2.399]
 [1.577]
 [1.603]
 [1.404]
 [2.306]
 [3.102]] [[0.408]
 [0.584]
 [0.356]
 [0.482]
 [0.465]
 [0.461]
 [0.549]]
Printing some Q and Qe and total Qs values:  [[0.857]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]] [[4.142]
 [1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.362]] [[0.857]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16709560581995667, 0.16162648627430015, 0.15700990595193712, 0.16215676914916616, 0.17668573721514483, 0.1754254955894951]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[-1.197]
 [ 0.428]
 [ 0.428]
 [ 0.428]
 [ 0.428]
 [ 0.428]
 [ 0.428]] [[0.602]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.6  ]
 [0.601]] [[2.867]
 [1.617]
 [1.617]
 [1.617]
 [1.617]
 [3.179]
 [3.501]] [[0.608]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.6  ]
 [0.601]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.647]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]] [[1.321]
 [1.069]
 [1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.321]] [[0.62 ]
 [0.647]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.655]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]] [[1.55 ]
 [2.573]
 [1.55 ]
 [1.55 ]
 [1.55 ]
 [1.55 ]
 [1.55 ]] [[-0.175]
 [ 1.312]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.574]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.487]
 [0.57 ]] [[1.949]
 [2.333]
 [3.25 ]
 [3.25 ]
 [3.25 ]
 [2.928]
 [3.106]] [[0.524]
 [0.574]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.487]
 [0.57 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16730338170298917, 0.16182746155274802, 0.15720514072004446, 0.16235840381055858, 0.17690543799595818, 0.17440017421770174]
Printing some Q and Qe and total Qs values:  [[0.851]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]] [[3.588]
 [2.419]
 [2.419]
 [2.419]
 [2.419]
 [2.419]
 [2.419]] [[0.851]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.754]
 [0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.72 ]] [[0.554]
 [0.98 ]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [1.303]] [[0.198]
 [0.438]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.584]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7417546242742024
using explorer policy with actor:  0
using explorer policy with actor:  0
siam score:  -0.7105863
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.51 ]
 [0.53 ]] [[3.306]
 [3.013]
 [3.013]
 [3.013]
 [3.013]
 [2.752]
 [2.605]] [[0.519]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.51 ]
 [0.53 ]]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.621]
 [0.604]
 [0.596]
 [0.599]
 [0.601]
 [0.6  ]] [[2.757]
 [4.06 ]
 [3.237]
 [2.638]
 [2.832]
 [2.723]
 [2.637]] [[0.598]
 [0.621]
 [0.604]
 [0.596]
 [0.599]
 [0.601]
 [0.6  ]]
1054 1078
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.5  ]
 [0.531]
 [0.527]
 [0.521]
 [0.525]
 [0.541]] [[2.665]
 [1.745]
 [2.675]
 [2.657]
 [2.501]
 [2.418]
 [2.206]] [[0.534]
 [0.5  ]
 [0.531]
 [0.527]
 [0.521]
 [0.525]
 [0.541]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.617]
 [0.597]
 [0.608]
 [0.598]
 [0.599]
 [0.603]] [[2.662]
 [3.103]
 [2.675]
 [2.593]
 [2.692]
 [2.497]
 [2.382]] [[0.617]
 [0.617]
 [0.597]
 [0.608]
 [0.598]
 [0.599]
 [0.603]]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]] [[1.145]
 [1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.476]] [[0.798]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]]
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.324]] [[2.013]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [2.14 ]] [[0.262]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.439]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5561334453696851
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[0.412]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[0.372]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[2.983]
 [2.983]
 [2.983]
 [2.983]
 [2.983]
 [2.983]
 [2.983]] [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]]
using another actor
from probs:  [0.16768373921096683, 0.16221431890976656, 0.15759648913103333, 0.16168749849279476, 0.17727131126836493, 0.1735466429870736]
line 256 mcts: sample exp_bonus 2.589168669734515
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16778528336285364, 0.16231255094773933, 0.15769192475230734, 0.1617854115044106, 0.1773786613611128, 0.1730461680715764]
using explorer policy with actor:  0
using another actor
from probs:  [0.16778528336285364, 0.16231255094773933, 0.15769192475230734, 0.1617854115044106, 0.1773786613611128, 0.1730461680715764]
Printing some Q and Qe and total Qs values:  [[0.44]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]] [[1.501]
 [2.211]
 [2.211]
 [2.211]
 [2.211]
 [2.211]
 [2.211]] [[0.44]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]]
first move QE:  1.0509047379364227
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]] [[2.296]
 [2.296]
 [2.296]
 [2.296]
 [2.296]
 [2.296]
 [2.296]] [[0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.635]
 [0.611]
 [0.602]
 [0.597]
 [0.607]
 [0.602]] [[1.097]
 [2.311]
 [1.744]
 [1.237]
 [1.279]
 [1.803]
 [1.531]] [[0.613]
 [0.635]
 [0.611]
 [0.602]
 [0.597]
 [0.607]
 [0.602]]
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.645]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[-0.796]
 [-1.205]
 [-0.702]
 [-0.702]
 [-0.702]
 [-0.702]
 [-0.702]] [[0.431]
 [0.645]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
line 256 mcts: sample exp_bonus 1.9648689097815946
siam score:  -0.7067471
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]] [[1.256]
 [1.186]
 [1.186]
 [1.186]
 [1.186]
 [1.186]
 [1.186]] [[0.626]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.532]] [[4.754]
 [4.754]
 [4.754]
 [4.754]
 [4.754]
 [4.754]
 [4.54 ]] [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.532]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[-0.83]
 [-0.25]
 [-0.25]
 [-0.25]
 [-0.25]
 [-0.25]
 [-0.25]] [[0.43 ]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.603]] [[4.745]
 [4.745]
 [4.745]
 [4.745]
 [4.745]
 [4.745]
 [3.95 ]] [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.603]]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[3.405]
 [3.405]
 [3.405]
 [3.405]
 [3.405]
 [3.405]
 [3.405]] [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]]
using explorer policy with actor:  0
siam score:  -0.7082109
Printing some Q and Qe and total Qs values:  [[1.072]
 [1.091]
 [1.072]
 [1.072]
 [1.072]
 [1.072]
 [1.072]] [[1.115]
 [1.396]
 [1.115]
 [1.115]
 [1.115]
 [1.115]
 [1.115]] [[1.827]
 [1.957]
 [1.827]
 [1.827]
 [1.827]
 [1.827]
 [1.827]]
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]] [[0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]] [[0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.301]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.29 ]] [[ 2.385]
 [ 0.745]
 [-1.325]
 [-1.325]
 [-1.325]
 [-1.325]
 [ 0.593]] [[0.148]
 [0.301]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.29 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.564]] [[5.842]
 [5.842]
 [5.842]
 [5.842]
 [5.842]
 [5.842]
 [5.97 ]] [[0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [1.158]]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.564]] [[6.305]
 [6.305]
 [6.305]
 [6.305]
 [6.305]
 [6.305]
 [6.63 ]] [[1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.553]]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.402]
 [0.396]
 [0.391]
 [0.396]
 [0.409]
 [0.374]] [[-0.622]
 [-0.607]
 [ 0.011]
 [-0.01 ]
 [ 0.157]
 [ 0.306]
 [ 0.538]] [[0.531]
 [0.402]
 [0.396]
 [0.391]
 [0.396]
 [0.409]
 [0.374]]
rdn probs:  [0.16755486368008962, 0.16265357213156584, 0.15607313440438114, 0.16265357213156584, 0.17769457265084518, 0.17337028500155238]
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.111]
 [0.111]
 [0.121]
 [0.121]
 [0.127]
 [0.116]] [[-2.81 ]
 [-2.75 ]
 [-2.75 ]
 [-2.593]
 [-2.378]
 [-2.084]
 [-1.98 ]] [[0.123]
 [0.111]
 [0.111]
 [0.121]
 [0.121]
 [0.127]
 [0.116]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.1443805762268473
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.641]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.608]] [[1.326]
 [1.849]
 [1.745]
 [1.745]
 [1.745]
 [1.745]
 [1.361]] [[0.282]
 [0.411]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.181]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.16754552779917503, 0.16321927507591188, 0.15569796150046034, 0.16269460949316517, 0.17694770880785526, 0.17389491732343226]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.238]
 [0.238]
 [0.232]
 [0.238]
 [0.238]
 [0.203]] [[5.964]
 [5.964]
 [5.964]
 [6.369]
 [5.964]
 [5.964]
 [6.788]] [[1.086]
 [1.086]
 [1.086]
 [1.283]
 [1.086]
 [1.086]
 [1.477]]
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.623]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[2.249]
 [2.892]
 [1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]] [[1.005]
 [1.246]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.599]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.629]] [[ 2.067]
 [ 1.517]
 [-0.298]
 [-0.298]
 [-0.298]
 [-0.298]
 [ 0.398]] [[1.771]
 [1.695]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.41 ]]
using explorer policy with actor:  1
1066 1116
siam score:  -0.6946225
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16654740764175086, 0.16281041487446615, 0.15630953414729098, 0.1633336645713045, 0.17702097318788176, 0.17397800557730583]
UNIT TEST: sample policy line 217 mcts : [0.388 0.571 0.    0.    0.    0.02  0.02 ]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.023]
 [0.013]
 [0.01 ]
 [0.003]
 [0.009]
 [0.36 ]] [[-2.575]
 [-0.848]
 [-2.314]
 [-2.751]
 [-2.852]
 [-2.833]
 [-0.948]] [[0.042]
 [0.023]
 [0.013]
 [0.01 ]
 [0.003]
 [0.009]
 [0.36 ]]
siam score:  -0.69019705
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.694]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]] [[2.517]
 [2.458]
 [2.517]
 [2.517]
 [2.517]
 [2.517]
 [2.517]] [[2.051]
 [2.02 ]
 [2.051]
 [2.051]
 [2.051]
 [2.051]
 [2.051]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16733084434199902, 0.1635889868755813, 0.15707825606087245, 0.1635889868755813, 0.17658371405051754, 0.17182921179544833]
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.037]] [[6.677]
 [6.677]
 [6.677]
 [6.677]
 [6.677]
 [6.677]
 [7.265]] [[1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.452]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.046]
 [0.069]
 [0.068]
 [0.057]
 [0.066]
 [0.066]] [[-1.23 ]
 [-0.307]
 [-1.397]
 [-1.324]
 [-1.144]
 [-1.078]
 [-0.873]] [[0.336]
 [0.046]
 [0.069]
 [0.068]
 [0.057]
 [0.066]
 [0.066]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16750522735242798, 0.16323839780492444, 0.1572419543872183, 0.16323839780492444, 0.17676773989327202, 0.17200828275723276]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.217]
 [0.547]
 [0.579]
 [0.555]
 [0.555]
 [0.4  ]] [[2.099]
 [2.621]
 [2.011]
 [0.182]
 [2.099]
 [2.099]
 [2.118]] [[2.139]
 [1.998]
 [2.107]
 [1.612]
 [2.139]
 [2.139]
 [2.012]]
from probs:  [0.1683043311171706, 0.16195679997331244, 0.1575105340541476, 0.16298026183527, 0.17699842297644444, 0.17224965004365486]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16838491049141135, 0.16203434032838981, 0.1571071738225972, 0.1630582921950659, 0.1770831648370393, 0.1723321183254963]
siam score:  -0.68651915
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16857828408878192, 0.1622204209171755, 0.15728759604265327, 0.16324554869240193, 0.1772865275286152, 0.1713816227303721]
1075 1140
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16875457318192447, 0.162390061337039, 0.15697525399400755, 0.16341626112961827, 0.17747192318223304, 0.17099192717517772]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16884148700590787, 0.16247369724059751, 0.15705610110432955, 0.1629853946324528, 0.1775633267110537, 0.1710799933056586]
first move QE:  1.028205145968713
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.591752021968426
Printing some Q and Qe and total Qs values:  [[1.027]
 [0.936]
 [1.027]
 [1.027]
 [1.027]
 [1.027]
 [1.339]] [[1.311]
 [1.47 ]
 [1.311]
 [1.311]
 [1.311]
 [1.311]
 [2.153]] [[1.456]
 [1.47 ]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [2.283]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.348]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[-2.253]
 [-1.729]
 [-2.253]
 [-2.253]
 [-2.253]
 [-2.253]
 [-2.253]] [[0.327]
 [0.348]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]]
1077 1146
first move QE:  1.0270843229957773
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1690294642276952, 0.16216842382431362, 0.15726885507231816, 0.16318639247764086, 0.17651642410647297, 0.1718304402915593]
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]] [[3.585]
 [2.887]
 [2.887]
 [2.887]
 [2.887]
 [2.887]
 [2.887]] [[0.769]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]]
first move QE:  1.0260937964795926
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.349]
 [0.258]
 [0.256]
 [0.254]
 [0.348]
 [0.257]] [[0.582]
 [1.672]
 [0.554]
 [0.483]
 [0.568]
 [1.607]
 [0.985]] [[0.27 ]
 [0.349]
 [0.258]
 [0.256]
 [0.254]
 [0.348]
 [0.257]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
line 256 mcts: sample exp_bonus 1.250997954089927
1081 1153
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
siam score:  -0.6812488
siam score:  -0.6816268
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6836011
1083 1155
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16920617060858245, 0.16135778689978497, 0.15747096830114243, 0.16388925046928923, 0.17607555732513272, 0.17200026639606816]
first move QE:  1.021614534532231
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.664]
 [0.662]
 [0.693]
 [0.735]
 [0.671]
 [0.693]] [[0.972]
 [2.342]
 [1.034]
 [1.865]
 [1.705]
 [0.978]
 [1.865]] [[0.666]
 [0.664]
 [0.662]
 [0.693]
 [0.735]
 [0.671]
 [0.693]]
siam score:  -0.68416303
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16930679188008757, 0.1614537409990162, 0.15756461103886657, 0.16398670994555378, 0.1755855969132405, 0.17210254922323545]
using another actor
siam score:  -0.6839128
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.516]
 [0.545]
 [0.516]
 [0.516]
 [0.536]
 [0.552]] [[-0.611]
 [ 0.105]
 [ 0.672]
 [ 0.105]
 [ 0.105]
 [ 0.764]
 [ 0.524]] [[0.572]
 [0.516]
 [0.545]
 [0.516]
 [0.516]
 [0.536]
 [0.552]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16928424018698365, 0.16195641397054045, 0.1571099680601944, 0.16347112311036563, 0.1755398782867352, 0.17263837638518068]
line 256 mcts: sample exp_bonus 1.8090805983685587
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[1.41 ]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[1.856]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.384]]
line 256 mcts: sample exp_bonus 0.7649187137090642
siam score:  -0.6878002
from probs:  [0.16935519004251634, 0.1620482748488512, 0.15674739925590087, 0.1635588390475416, 0.1755912642164202, 0.1726990325887699]
siam score:  -0.6849771
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.476]
 [0.497]
 [0.623]
 [0.487]
 [0.455]
 [0.455]] [[-0.815]
 [ 0.017]
 [-0.475]
 [-0.029]
 [-0.456]
 [-0.442]
 [-0.341]] [[0.45 ]
 [0.476]
 [0.497]
 [0.623]
 [0.487]
 [0.455]
 [0.455]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16869307495409744, 0.16195730463729935, 0.15621357553976703, 0.1639700644065201, 0.17603274191998777, 0.17313323854232837]
using explorer policy with actor:  1
in main func line 156:  1098
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1689341927113107, 0.16025152787871152, 0.15647605123417704, 0.16473036856159073, 0.17567182867730025, 0.1739360309369097]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6856404
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.254]
 [-0.017]
 [ 0.024]
 [ 0.254]
 [-0.005]
 [-0.017]] [[1.351]
 [1.681]
 [1.237]
 [1.013]
 [1.681]
 [1.202]
 [1.303]] [[0.284]
 [1.034]
 [0.195]
 [0.128]
 [1.034]
 [0.196]
 [0.239]]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[1.628]
 [1.628]
 [1.628]
 [1.628]
 [1.628]
 [1.628]
 [1.628]] [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
start point for exploration sampling:  11106
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16892646287694513, 0.15931353437879353, 0.15696835505393653, 0.16524864205477782, 0.17505973841711917, 0.17448326721842794]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.498]
 [0.502]
 [0.504]
 [0.5  ]
 [0.496]
 [0.498]] [[1.871]
 [2.486]
 [2.141]
 [2.393]
 [2.339]
 [2.152]
 [2.198]] [[-0.144]
 [ 0.046]
 [-0.061]
 [ 0.027]
 [ 0.   ]
 [-0.071]
 [-0.05 ]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6877654
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.495]
 [0.623]
 [0.624]
 [0.609]
 [0.623]
 [0.623]] [[-3.601]
 [ 0.909]
 [-2.884]
 [-3.092]
 [-2.899]
 [-2.919]
 [-2.935]] [[0.219]
 [1.561]
 [0.438]
 [0.374]
 [0.428]
 [0.427]
 [0.422]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.507]
 [0.594]
 [0.594]
 [0.594]
 [0.765]
 [1.178]] [[2.1  ]
 [1.914]
 [2.218]
 [2.218]
 [2.218]
 [1.639]
 [1.17 ]] [[1.561]
 [1.421]
 [1.792]
 [1.792]
 [1.792]
 [1.748]
 [2.255]]
using another actor
from probs:  [0.16855456510595515, 0.1600284204400246, 0.1563164391765198, 0.16646509082586242, 0.1757353681850914, 0.17290011626654667]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.168738276884285, 0.1602028393629643, 0.15648681231951508, 0.16613191435904096, 0.17535159270591433, 0.17308856436828035]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.727]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.535]
 [0.669]] [[3.239]
 [4.146]
 [2.74 ]
 [2.74 ]
 [2.74 ]
 [2.819]
 [3.219]] [[1.145]
 [1.82 ]
 [0.941]
 [0.941]
 [0.941]
 [0.971]
 [1.287]]
using explorer policy with actor:  1
using another actor
from probs:  [0.168738276884285, 0.1602028393629643, 0.15648681231951508, 0.16613191435904096, 0.17535159270591433, 0.17308856436828035]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1684816676651724, 0.16046130959520907, 0.15673928713759136, 0.16588778201952953, 0.175062129235998, 0.17336782434649972]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.421]
 [0.402]
 [0.419]
 [0.405]
 [0.406]
 [0.414]] [[0.877]
 [1.147]
 [0.912]
 [0.8  ]
 [0.627]
 [0.824]
 [1.075]] [[0.425]
 [0.421]
 [0.402]
 [0.419]
 [0.405]
 [0.406]
 [0.414]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16856745618322558, 0.16054301425866482, 0.15681909659906326, 0.16546306369817004, 0.17515126843341047, 0.17345610082746588]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1687376728170475, 0.1607051279435331, 0.1569774499333692, 0.1646203620799945, 0.17532813329212893, 0.1736312539339269]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.421]
 [0.421]
 [0.421]
 [0.166]
 [0.421]
 [0.421]] [[-2.661]
 [ 0.038]
 [ 0.038]
 [ 0.038]
 [ 1.148]
 [ 0.038]
 [ 0.038]] [[0.114]
 [1.3  ]
 [1.3  ]
 [1.3  ]
 [1.7  ]
 [1.3  ]
 [1.3  ]]
line 256 mcts: sample exp_bonus 1.436105884787969
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16786466095356062, 0.16087390436780977, 0.15714231145976143, 0.16479325037814985, 0.1755122671513954, 0.17381360568932291]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16786466095356062, 0.16087390436780977, 0.15714231145976143, 0.16479325037814985, 0.1755122671513954, 0.17381360568932291]
Printing some Q and Qe and total Qs values:  [[1.236]
 [1.408]
 [1.262]
 [1.262]
 [1.262]
 [1.224]
 [1.262]] [[1.065]
 [1.106]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.006]
 [1.14 ]] [[1.89 ]
 [2.052]
 [1.943]
 [1.943]
 [1.943]
 [1.854]
 [1.943]]
siam score:  -0.68237627
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16857561059346132, 0.16205733318707982, 0.1573864979576902, 0.16400689919586858, 0.17565880537506254, 0.17231485369083752]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1686711768444924, 0.16214920419813691, 0.15747572105182447, 0.16409987542442386, 0.17519148274598553, 0.17241253973513673]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1113 1221
from probs:  [0.16869545610731482, 0.16219312302226857, 0.1579862564246631, 0.16463188626725725, 0.1735217779911209, 0.17297150018737537]
first move QE:  0.9876118321965477
using explorer policy with actor:  1
using explorer policy with actor:  0
siam score:  -0.6859022
siam score:  -0.68775034
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1686957329670641, 0.16221384200675604, 0.15847428952965517, 0.1641528692171046, 0.173505823996433, 0.17295744228298707]
using another actor
1118 1230
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1690547593661782, 0.16255907333275238, 0.15881156215962206, 0.16401195458756596, 0.17332553865751102, 0.1722371118963704]
using explorer policy with actor:  0
using explorer policy with actor:  0
siam score:  -0.6820273
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16897959024729733, 0.16250715502644414, 0.15922944744665304, 0.16444352381204377, 0.17214995926118426, 0.1726903242063775]
from probs:  [0.16897959024729733, 0.16250715502644414, 0.15922944744665304, 0.16444352381204377, 0.17214995926118426, 0.1726903242063775]
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.837]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]] [[1.159]
 [1.129]
 [1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.159]] [[0.827]
 [0.837]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]]
UNIT TEST: sample policy line 217 mcts : [0.163 0.061 0.306 0.102 0.102 0.143 0.122]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.399]
 [0.5  ]
 [0.387]
 [0.372]
 [0.39 ]
 [0.402]] [[3.341]
 [2.961]
 [3.966]
 [4.097]
 [3.935]
 [4.925]
 [4.581]] [[0.412]
 [0.217]
 [0.804]
 [0.785]
 [0.691]
 [1.208]
 [1.043]]
using explorer policy with actor:  0
1125 1244
from probs:  [0.16946158960797497, 0.16299098908373777, 0.15880205709017103, 0.1634705729897895, 0.17156067849762677, 0.17371411273069995]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.599]
 [0.587]
 [0.429]
 [0.462]
 [0.462]
 [0.439]] [[ 1.185]
 [-2.36 ]
 [-3.073]
 [-2.352]
 [ 0.348]
 [ 0.348]
 [ 0.696]] [[1.653]
 [0.396]
 [0.113]
 [0.32 ]
 [1.384]
 [1.384]
 [1.509]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.17021929196005006, 0.16276500932594737, 0.15951209818237333, 0.16229193473235165, 0.17126683559371023, 0.1739448302055672]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.671]
 [0.617]
 [0.617]
 [0.617]
 [0.657]
 [0.617]] [[2.063]
 [2.045]
 [2.248]
 [2.248]
 [2.248]
 [2.371]
 [2.248]] [[1.321]
 [1.365]
 [1.392]
 [1.392]
 [1.392]
 [1.555]
 [1.392]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [1.318]] [[1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.146]] [[1.789]
 [1.789]
 [1.789]
 [1.789]
 [1.789]
 [1.789]
 [2.573]]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.182]
 [0.152]
 [0.152]
 [0.158]
 [0.154]
 [0.157]] [[-3.637]
 [-1.877]
 [-3.487]
 [-3.606]
 [-3.275]
 [-3.208]
 [-3.331]] [[0.151]
 [0.182]
 [0.152]
 [0.152]
 [0.158]
 [0.154]
 [0.157]]
siam score:  -0.66745085
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.33 ]
 [0.33 ]
 [0.34 ]
 [0.338]
 [0.33 ]
 [0.328]] [[1.869]
 [1.869]
 [1.869]
 [2.17 ]
 [2.244]
 [1.869]
 [2.206]] [[-0.646]
 [-0.646]
 [-0.646]
 [-0.426]
 [-0.381]
 [-0.646]
 [-0.426]]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.528]] [[1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.04 ]] [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.528]]
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]] [[1.868]
 [2.159]
 [2.159]
 [2.159]
 [2.159]
 [2.159]
 [2.08 ]] [[-0.443]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.178]]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.384]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.479]] [[2.528]
 [2.496]
 [2.528]
 [2.528]
 [2.528]
 [2.528]
 [2.559]] [[0.035]
 [0.046]
 [0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.256]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.022]
 [0.04 ]
 [0.065]
 [0.069]
 [0.053]
 [0.048]] [[1.447]
 [2.082]
 [1.767]
 [1.884]
 [2.126]
 [1.975]
 [2.067]] [[-1.219]
 [-0.777]
 [-0.951]
 [-0.823]
 [-0.653]
 [-0.786]
 [-0.734]]
1134 1265
from probs:  [0.1692449572981815, 0.16192441525198334, 0.16008136113682286, 0.16239205584836736, 0.1718395797955682, 0.17451763066907663]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.323]
 [0.318]
 [0.319]
 [0.319]
 [0.312]
 [0.316]] [[ 0.445]
 [ 1.415]
 [-0.272]
 [-0.162]
 [-0.016]
 [ 0.285]
 [ 0.702]] [[0.311]
 [0.323]
 [0.318]
 [0.319]
 [0.319]
 [0.312]
 [0.316]]
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.424]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[0.201]
 [0.575]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]] [[0.379]
 [0.424]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]]
siam score:  -0.67123306
line 256 mcts: sample exp_bonus 4.402938416668101
rdn beta is 0 so we're just using the maxi policy
using another actor
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.848]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.715]] [[0.94 ]
 [1.91 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [1.595]] [[1.671]
 [2.137]
 [1.671]
 [1.671]
 [1.671]
 [1.671]
 [1.896]]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.418]
 [0.375]] [[-1.478]
 [-1.255]
 [-1.255]
 [-1.255]
 [-1.255]
 [-0.229]
 [-0.429]] [[0.391]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.418]
 [0.375]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6685845
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17023163363560326, 0.16196130199113523, 0.15923695744942812, 0.16289097446085724, 0.17178260676531448, 0.17389652569766162]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.563]
 [0.569]
 [0.569]
 [0.569]
 [0.544]
 [0.548]] [[ 0.   ]
 [-0.37 ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-1.462]
 [-1.508]] [[0.569]
 [0.563]
 [0.569]
 [0.569]
 [0.569]
 [0.544]
 [0.548]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1696946715033695, 0.16242174759043096, 0.15879995723839702, 0.16242174759043096, 0.1722709737045045, 0.174390902372867]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16953251195987548, 0.16323655445888624, 0.15871267933143973, 0.16277219198256895, 0.17261087194954186, 0.17313519031768776]
siam score:  -0.67741144
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]] [[1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]] [[1.571]
 [1.571]
 [1.571]
 [1.571]
 [1.571]
 [1.571]
 [1.571]]
siam score:  -0.6805352
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16953251195987548, 0.16323655445888624, 0.15871267933143973, 0.16277219198256895, 0.17261087194954186, 0.17313519031768776]
using another actor
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.429]
 [0.37 ]
 [0.416]
 [0.379]
 [0.356]
 [0.374]] [[-2.129]
 [-0.335]
 [-2.304]
 [-0.348]
 [-1.948]
 [-2.56 ]
 [-1.861]] [[0.382]
 [0.429]
 [0.37 ]
 [0.416]
 [0.379]
 [0.356]
 [0.374]]
line 256 mcts: sample exp_bonus 0.784401446966237
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16936673229878196, 0.16356132454610806, 0.1585894074779252, 0.16309603818945995, 0.17243220461678269, 0.1729542928709422]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16936673229878196, 0.16356132454610806, 0.1585894074779252, 0.16309603818945995, 0.17243220461678269, 0.1729542928709422]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.643]
 [0.65 ]
 [0.654]
 [0.622]
 [0.63 ]
 [0.624]] [[3.043]
 [3.382]
 [3.052]
 [3.034]
 [3.104]
 [3.099]
 [3.079]] [[1.452]
 [1.748]
 [1.54 ]
 [1.537]
 [1.52 ]
 [1.533]
 [1.506]]
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.435]
 [0.416]
 [0.413]
 [0.407]
 [0.412]
 [0.707]] [[1.492]
 [2.526]
 [1.689]
 [1.675]
 [1.56 ]
 [1.695]
 [1.005]] [[0.447]
 [0.435]
 [0.416]
 [0.413]
 [0.407]
 [0.412]
 [0.707]]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.581]] [[1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.529]] [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.581]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[ 1.362]
 [-1.184]
 [-1.184]
 [-1.184]
 [-1.184]
 [-1.184]
 [-1.184]] [[1.661]
 [0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]]
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.667]] [[1.458]
 [1.458]
 [1.458]
 [1.458]
 [1.458]
 [1.458]
 [1.228]] [[0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.667]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7240467404012456
first move QE:  0.9473177409202429
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1149 1311
in main func line 156:  1150
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[1.76]
 [1.76]
 [1.76]
 [1.76]
 [1.76]
 [1.76]
 [1.76]] [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]]
Printing some Q and Qe and total Qs values:  [[0.96 ]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]] [[0.918]
 [1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]] [[0.96 ]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16966627795989445, 0.16248215621402, 0.15846557905609931, 0.16340352158357888, 0.17220926429512137, 0.17377320089128598]
using explorer policy with actor:  1
using explorer policy with actor:  1
start point for exploration sampling:  11106
using explorer policy with actor:  1
siam score:  -0.6757835
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.338]] [[2.031]
 [2.031]
 [2.031]
 [2.031]
 [2.031]
 [2.031]
 [2.56 ]] [[1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.855]]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 0.8558386882977584
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.67142195
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
in main func line 156:  1156
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.621]
 [0.621]
 [0.615]
 [0.621]
 [0.615]
 [0.604]] [[9.359]
 [9.406]
 [9.406]
 [9.027]
 [9.406]
 [8.728]
 [8.887]] [[1.979]
 [1.991]
 [1.991]
 [1.826]
 [1.991]
 [1.699]
 [1.762]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16913473661346431, 0.1620584175466811, 0.15896136189485358, 0.16296637423467028, 0.1726605579495087, 0.17421855176082202]
from probs:  [0.16872609222648852, 0.16213812272345393, 0.15903954384702665, 0.16304652597169844, 0.17274547757603226, 0.1743042376553002]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.34 ]
 [0.381]
 [0.368]
 [0.367]
 [0.364]
 [0.347]] [[8.466]
 [6.902]
 [8.306]
 [8.606]
 [8.552]
 [8.391]
 [8.712]] [[1.563]
 [0.855]
 [1.479]
 [1.603]
 [1.579]
 [1.508]
 [1.639]]
first move QE:  0.9343217978765922
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16881434903466905, 0.1622229335135761, 0.15912273384051967, 0.16313181192704357, 0.17283583683391168, 0.17387233485027995]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16881434903466905, 0.1622229335135761, 0.15912273384051967, 0.16313181192704357, 0.17283583683391168, 0.17387233485027995]
siam score:  -0.67051893
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.401]] [[2.366]
 [2.366]
 [2.366]
 [2.366]
 [2.366]
 [2.366]
 [2.328]] [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.401]]
siam score:  -0.6728063
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.529]
 [0.482]
 [0.667]
 [0.667]
 [0.667]
 [0.667]] [[3.258]
 [4.368]
 [3.435]
 [3.258]
 [3.258]
 [3.258]
 [3.258]] [[0.419]
 [0.79 ]
 [0.216]
 [0.419]
 [0.419]
 [0.419]
 [0.419]]
siam score:  -0.6657894
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16936970122376438, 0.16096404002790562, 0.15964620331499557, 0.16321121823778115, 0.17340441859777658, 0.17340441859777658]
using another actor
first move QE:  0.9308188063876984
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.4  ]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[1.582]
 [1.81 ]
 [1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]] [[0.751]
 [0.868]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.401]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.361]] [[2.768]
 [2.601]
 [2.768]
 [2.768]
 [2.768]
 [2.768]
 [2.876]] [[0.802]
 [0.748]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.843]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1696953948530915, 0.16127356978446217, 0.15951803441804366, 0.16306957858872317, 0.17322171117783966, 0.17322171117783966]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17003180355303807, 0.1611497810333941, 0.15983426724582936, 0.16339285209288254, 0.17305109078327918, 0.1725402052915768]
siam score:  -0.6668048
line 256 mcts: sample exp_bonus 3.748315916764064
using explorer policy with actor:  1
siam score:  -0.66756254
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16970892036534888, 0.16131116445407348, 0.15999433324545095, 0.16355648183669114, 0.17322439276857393, 0.17220470732986168]
line 256 mcts: sample exp_bonus -0.958111769442621
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.26923457613118257
from probs:  [0.16970892036534888, 0.16131116445407348, 0.15999433324545095, 0.16355648183669114, 0.17322439276857393, 0.17220470732986168]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.477]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]] [[2.312]
 [3.115]
 [2.312]
 [2.312]
 [2.312]
 [2.312]
 [2.312]] [[0.469]
 [0.477]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.597]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.615]] [[2.918]
 [3.742]
 [3.683]
 [3.683]
 [3.683]
 [3.683]
 [4.244]] [[1.061]
 [1.433]
 [1.402]
 [1.402]
 [1.402]
 [1.402]
 [1.688]]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.425]
 [0.4  ]
 [0.405]
 [0.414]
 [0.398]
 [0.389]] [[0.553]
 [1.549]
 [0.737]
 [0.657]
 [1.107]
 [0.808]
 [0.984]] [[0.41 ]
 [0.425]
 [0.4  ]
 [0.405]
 [0.414]
 [0.398]
 [0.389]]
line 256 mcts: sample exp_bonus 1.4052937372040148
siam score:  -0.6697846
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1700170304278211, 0.16116179612054166, 0.1594176426435366, 0.1638534220194933, 0.17353888524405925, 0.17201122354454815]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.502]
 [0.629]
 [0.626]
 [0.611]
 [0.611]
 [0.614]] [[2.146]
 [1.902]
 [2.297]
 [2.412]
 [2.431]
 [2.222]
 [2.201]] [[1.017]
 [0.656]
 [1.042]
 [1.074]
 [1.051]
 [0.982]
 [0.981]]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[1.033]
 [1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]] [[0.438]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16895911198180727, 0.16155062457225922, 0.15937127728086806, 0.16424874444655788, 0.1734440137539365, 0.17242622796457113]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16895911198180727, 0.16155062457225922, 0.15937127728086806, 0.16424874444655788, 0.1734440137539365, 0.17242622796457113]
line 256 mcts: sample exp_bonus 1.151191147830151
first move QE:  0.9342782486180364
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.523]
 [0.537]
 [0.531]
 [0.534]
 [0.534]
 [0.52 ]] [[3.577]
 [3.577]
 [3.595]
 [3.513]
 [3.499]
 [3.558]
 [3.5  ]] [[0.144]
 [0.144]
 [0.179]
 [0.139]
 [0.14 ]
 [0.159]
 [0.113]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1689405127793329, 0.16199649228992888, 0.15895398257179372, 0.16378986682702043, 0.17392270634185247, 0.17239643919007153]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16814087131566222, 0.1621523645399858, 0.15910692733355208, 0.1639474646534793, 0.17409005393802743, 0.17256231821929313]
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.665]
 [0.656]
 [0.67 ]
 [0.667]
 [0.654]
 [0.655]] [[1.636]
 [3.096]
 [1.585]
 [1.784]
 [1.752]
 [1.73 ]
 [1.911]] [[0.652]
 [0.665]
 [0.656]
 [0.67 ]
 [0.667]
 [0.654]
 [0.655]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16774399749191504, 0.16222972623107698, 0.1591828360691278, 0.16402568277352758, 0.17417311101223665, 0.17264464642211602]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16800026744942745, 0.16247757179139571, 0.15942602675997863, 0.16427627209644713, 0.17341557735855298, 0.17240428454419823]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6661437
1187 1354
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.978]] [[1.847]
 [1.847]
 [1.847]
 [1.847]
 [1.847]
 [1.847]
 [2.467]] [[1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.547]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16760485712789205, 0.16255478973557383, 0.15950179444839965, 0.16435434487823894, 0.1734979936220683, 0.17248622018782722]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.729]
 [0.697]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[0.333]
 [0.853]
 [1.686]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[-0.143]
 [ 0.165]
 [ 0.61 ]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.37 ]] [[1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.443]] [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.641]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.26978206268802
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.426]
 [0.414]
 [0.414]
 [0.414]
 [0.415]
 [0.411]] [[5.11 ]
 [4.889]
 [4.949]
 [5.145]
 [5.124]
 [5.013]
 [5.114]] [[1.263]
 [1.156]
 [1.182]
 [1.287]
 [1.275]
 [1.216]
 [1.268]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1669777409250008, 0.1624177506316668, 0.15980335619682198, 0.1646650810914188, 0.1738260172564442, 0.1723100538986472]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1669777409250008, 0.1624177506316668, 0.15980335619682198, 0.1646650810914188, 0.1738260172564442, 0.1723100538986472]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.064]
 [-0.059]
 [-0.059]
 [-0.049]
 [-0.059]
 [-0.069]] [[7.081]
 [4.426]
 [7.081]
 [7.849]
 [6.941]
 [7.081]
 [8.332]] [[ 0.844]
 [-0.137]
 [ 0.844]
 [ 1.127]
 [ 0.796]
 [ 0.844]
 [ 1.302]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.232]
 [0.677]
 [0.68 ]
 [0.632]
 [0.637]
 [0.36 ]] [[2.16 ]
 [2.543]
 [2.146]
 [1.538]
 [1.736]
 [4.239]
 [1.761]] [[0.686]
 [0.722]
 [0.749]
 [0.407]
 [0.492]
 [1.906]
 [0.353]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666146889642547, 0.16212337518241496, 0.1599488719444516, 0.16481502388957878, 0.17398430193494238, 0.17246695815218666]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.599]
 [0.597]
 [0.578]
 [0.548]
 [0.507]
 [0.557]] [[2.468]
 [2.685]
 [3.067]
 [2.664]
 [2.022]
 [1.318]
 [2.88 ]] [[ 0.754]
 [ 0.971]
 [ 1.214]
 [ 0.93 ]
 [ 0.477]
 [-0.031]
 [ 1.042]]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.426]
 [0.36 ]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[2.311]
 [3.186]
 [5.06 ]
 [3.338]
 [3.338]
 [3.338]
 [3.338]] [[0.32 ]
 [0.786]
 [1.702]
 [0.833]
 [0.833]
 [0.833]
 [0.833]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.234]
 [0.234]
 [0.267]
 [0.234]
 [0.246]
 [0.247]] [[6.702]
 [6.702]
 [6.702]
 [6.065]
 [6.702]
 [6.517]
 [6.098]] [[1.38 ]
 [1.38 ]
 [1.38 ]
 [1.073]
 [1.38 ]
 [1.292]
 [1.08 ]]
using explorer policy with actor:  1
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]] [[-1.773]
 [-1.773]
 [-1.773]
 [-1.773]
 [-1.773]
 [-1.773]
 [-1.773]] [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.497]
 [0.606]
 [0.583]
 [0.616]
 [0.653]
 [0.638]] [[-0.102]
 [ 0.148]
 [ 0.277]
 [-0.071]
 [-0.256]
 [ 0.094]
 [-0.523]] [[1.913]
 [1.824]
 [2.045]
 [1.907]
 [1.911]
 [2.072]
 [1.872]]
line 256 mcts: sample exp_bonus -0.03494249346971024
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]] [[1.5  ]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]] [[0.864]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16656688078980247, 0.1616205751441726, 0.15947135393326153, 0.1656443606433396, 0.1748597778033847, 0.17183705168603908]
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[3.015]
 [2.583]
 [2.583]
 [2.583]
 [2.583]
 [2.583]
 [2.583]] [[0.818]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.434]
 [0.428]
 [0.434]
 [0.434]
 [0.429]
 [0.426]] [[-0.04 ]
 [ 0.44 ]
 [ 0.34 ]
 [ 0.44 ]
 [ 0.44 ]
 [ 0.492]
 [ 0.526]] [[0.632]
 [0.434]
 [0.428]
 [0.434]
 [0.434]
 [0.429]
 [0.426]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.971]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]] [[2.243]
 [1.129]
 [1.129]
 [1.129]
 [1.129]
 [1.129]
 [1.129]] [[0.971]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]]
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]] [[1.478]
 [1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.146]] [[0.77 ]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.572]] [[3.357]
 [2.879]
 [2.879]
 [2.879]
 [2.879]
 [2.879]
 [3.406]] [[ 0.277]
 [-0.225]
 [-0.225]
 [-0.225]
 [-0.225]
 [-0.225]
 [ 0.32 ]]
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]] [[2.874]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]] [[0.914]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]]
Printing some Q and Qe and total Qs values:  [[0.966]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[2.058]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]] [[0.966]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]]
Printing some Q and Qe and total Qs values:  [[0.898]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]] [[2.302]
 [1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.146]] [[0.898]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]]
using another actor
from probs:  [0.16656688078980247, 0.1616205751441726, 0.15947135393326153, 0.1656443606433396, 0.1748597778033847, 0.17183705168603908]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[0.52 ]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[0.672]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16664308774291953, 0.16169451908509508, 0.15954431457186558, 0.1652626298887628, 0.17493977888658885, 0.1719156698247682]
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.631]
 [0.629]
 [0.637]
 [0.629]
 [0.629]
 [0.629]] [[2.905]
 [3.631]
 [3.793]
 [3.54 ]
 [3.793]
 [3.793]
 [3.455]] [[0.633]
 [0.631]
 [0.629]
 [0.637]
 [0.629]
 [0.629]
 [0.629]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.438]
 [0.465]
 [0.443]
 [0.434]
 [0.432]
 [0.435]] [[4.156]
 [3.754]
 [4.219]
 [4.281]
 [4.372]
 [4.21 ]
 [4.244]] [[0.455]
 [0.438]
 [0.465]
 [0.443]
 [0.434]
 [0.432]
 [0.435]]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.454]
 [0.466]
 [0.476]
 [0.465]
 [0.468]
 [0.471]] [[4.95 ]
 [2.573]
 [2.922]
 [3.82 ]
 [2.978]
 [3.419]
 [4.099]] [[0.567]
 [0.454]
 [0.466]
 [0.476]
 [0.465]
 [0.468]
 [0.471]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.455]
 [0.464]
 [0.471]
 [0.461]
 [0.456]
 [0.463]] [[0.717]
 [2.244]
 [0.764]
 [0.734]
 [0.554]
 [0.782]
 [0.644]] [[0.49 ]
 [0.455]
 [0.464]
 [0.471]
 [0.461]
 [0.456]
 [0.463]]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]] [[2.607]
 [1.439]
 [1.439]
 [1.439]
 [1.439]
 [1.439]
 [1.439]] [[0.581]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.744]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.724]] [[4.818]
 [4.259]
 [4.818]
 [4.818]
 [4.818]
 [4.818]
 [5.479]] [[1.618]
 [1.581]
 [1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.949]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.549]
 [0.573]
 [0.664]
 [0.515]
 [0.573]
 [0.397]] [[2.284]
 [1.709]
 [1.253]
 [2.209]
 [2.235]
 [1.776]
 [2.395]] [[0.863]
 [0.926]
 [0.821]
 [1.322]
 [1.033]
 [0.995]
 [0.851]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]] [[0.837]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[0.486]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.69 ]
 [0.657]
 [0.292]
 [0.292]
 [0.292]
 [0.446]] [[1.127]
 [1.667]
 [1.006]
 [2.63 ]
 [2.63 ]
 [2.63 ]
 [1.281]] [[-0.074]
 [ 0.315]
 [ 0.028]
 [-0.159]
 [-0.159]
 [-0.159]
 [-0.303]]
line 256 mcts: sample exp_bonus 2.5436191402848727
using another actor
from probs:  [0.1660244961285965, 0.16156868901381516, 0.16027981918722553, 0.1646612463597682, 0.17574625710630115, 0.1717194922042933]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16564319017890758, 0.16164256061090457, 0.1603531014939449, 0.16473653191987003, 0.17582661089746063, 0.17179800489891214]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.679]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.647]] [[2.23 ]
 [3.3  ]
 [2.612]
 [2.612]
 [2.612]
 [2.612]
 [2.535]] [[0.635]
 [0.679]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.647]]
from probs:  [0.16564319017890758, 0.16164256061090457, 0.1603531014939449, 0.16473653191987003, 0.17582661089746063, 0.17179800489891214]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]] [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]]
siam score:  -0.6732534
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.775]
 [0.754]
 [0.74 ]
 [0.775]
 [0.746]
 [0.744]] [[1.484]
 [1.484]
 [1.508]
 [1.609]
 [1.484]
 [1.45 ]
 [1.613]] [[0.775]
 [0.775]
 [0.754]
 [0.74 ]
 [0.775]
 [0.746]
 [0.744]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.392]
 [0.385]] [[0.646]
 [2.305]
 [2.305]
 [2.305]
 [2.305]
 [1.477]
 [1.645]] [[0.409]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.392]
 [0.385]]
Printing some Q and Qe and total Qs values:  [[0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]] [[1.878]
 [1.878]
 [1.878]
 [1.878]
 [1.878]
 [1.878]
 [1.878]] [[0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.01 ]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.006]
 [-0.006]] [[1.2  ]
 [2.905]
 [1.952]
 [1.695]
 [1.428]
 [2.181]
 [1.943]] [[-1.131]
 [-0.567]
 [-0.877]
 [-0.964]
 [-1.052]
 [-0.803]
 [-0.881]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.443]
 [0.414]
 [0.443]
 [0.402]
 [0.411]
 [0.409]] [[0.664]
 [0.408]
 [0.611]
 [0.408]
 [0.716]
 [0.605]
 [0.374]] [[0.427]
 [0.443]
 [0.414]
 [0.443]
 [0.402]
 [0.411]
 [0.409]]
line 256 mcts: sample exp_bonus 0.5713809412916444
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.508]
 [0.471]] [[4.583]
 [4.583]
 [4.583]
 [4.583]
 [4.583]
 [4.007]
 [5.703]] [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.508]
 [0.471]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.403]
 [0.427]] [[5.788]
 [5.788]
 [5.788]
 [5.788]
 [5.788]
 [5.45 ]
 [5.54 ]] [[1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.256]
 [1.352]]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.419]
 [0.39 ]] [[0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.632]
 [0.691]] [[0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.419]
 [0.39 ]]
siam score:  -0.6690988
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.887245440195468
rdn probs:  [0.16579892872711352, 0.16179453774736666, 0.16007837018461799, 0.16489141802348814, 0.17547721508299716, 0.17195953023441637]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]] [[2.112]
 [2.112]
 [2.112]
 [2.112]
 [2.112]
 [2.112]
 [2.112]] [[0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]]
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.096]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]] [[3.1  ]
 [3.414]
 [3.1  ]
 [3.1  ]
 [3.1  ]
 [3.1  ]
 [3.1  ]] [[-0.335]
 [-0.107]
 [-0.335]
 [-0.335]
 [-0.335]
 [-0.335]
 [-0.335]]
siam score:  -0.6909994
line 256 mcts: sample exp_bonus 1.4399166632615843
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16557556756208364, 0.161588034080092, 0.1603026753020141, 0.16512246728288973, 0.1752107722943392, 0.17220048347858125]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16557556756208364, 0.161588034080092, 0.1603026753020141, 0.16512246728288973, 0.1752107722943392, 0.17220048347858125]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16557556756208364, 0.161588034080092, 0.1603026753020141, 0.16512246728288973, 0.1752107722943392, 0.17220048347858125]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.045]
 [0.111]
 [0.108]
 [0.074]
 [0.094]
 [0.093]] [[-4.999]
 [-2.121]
 [-3.782]
 [-4.165]
 [-3.527]
 [-2.86 ]
 [-3.227]] [[0.13 ]
 [0.045]
 [0.111]
 [0.108]
 [0.074]
 [0.094]
 [0.093]]
from probs:  [0.1654932043429975, 0.1619508355430862, 0.15981549369477902, 0.16504165238792406, 0.17560415986714192, 0.17209465416407116]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.302]
 [0.24 ]
 [0.247]
 [0.249]
 [0.238]
 [0.242]] [[-3.817]
 [-1.775]
 [-3.787]
 [-3.597]
 [-3.314]
 [-3.173]
 [-3.191]] [[0.261]
 [0.302]
 [0.24 ]
 [0.247]
 [0.249]
 [0.238]
 [0.242]]
using another actor
using explorer policy with actor:  0
using another actor
using another actor
using explorer policy with actor:  0
using another actor
siam score:  -0.69270295
siam score:  -0.69171846
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.309]
 [0.361]
 [0.366]
 [0.387]
 [0.365]
 [0.395]] [[1.737]
 [1.458]
 [0.719]
 [0.531]
 [0.543]
 [0.786]
 [0.384]] [[0.412]
 [0.309]
 [0.361]
 [0.366]
 [0.387]
 [0.365]
 [0.395]]
line 256 mcts: sample exp_bonus 0.41774358639796094
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.271]
 [0.357]
 [0.363]
 [0.357]
 [0.361]
 [0.363]] [[ 0.16 ]
 [ 1.531]
 [ 0.402]
 [ 0.362]
 [ 0.313]
 [ 0.271]
 [-0.096]] [[0.374]
 [0.271]
 [0.357]
 [0.363]
 [0.357]
 [0.361]
 [0.363]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.67978364
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.815897462557182
siam score:  -0.6793943
siam score:  -0.68000776
Printing some Q and Qe and total Qs values:  [[1.149]
 [1.405]
 [1.149]
 [1.149]
 [1.149]
 [1.149]
 [1.149]] [[1.17 ]
 [1.287]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]] [[2.346]
 [2.898]
 [2.346]
 [2.346]
 [2.346]
 [2.346]
 [2.346]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16650008212063727, 0.16168120286120044, 0.16041669105553003, 0.16471368950628038, 0.17506208222495132, 0.1716262522314005]
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.886]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]] [[1.421]
 [2.353]
 [1.421]
 [1.421]
 [1.421]
 [1.421]
 [1.421]] [[1.604]
 [2.292]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]]
siam score:  -0.675193
siam score:  -0.67357635
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.543]
 [0.44 ]
 [0.391]
 [0.415]
 [0.412]
 [0.424]] [[ 1.37 ]
 [ 3.693]
 [-0.034]
 [-0.68 ]
 [-0.301]
 [ 0.312]
 [ 1.506]] [[ 0.575]
 [ 1.489]
 [ 0.037]
 [-0.277]
 [-0.102]
 [ 0.097]
 [ 0.519]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16664018554583904, 0.16139310006009863, 0.16013436526702343, 0.16485228975069785, 0.17520939024444468, 0.17177066913189631]
from probs:  [0.16589036934964133, 0.1615383136380522, 0.16027844629728488, 0.16500061573749422, 0.175367035041149, 0.17192521993637824]
from probs:  [0.1659593155087041, 0.16160545102710308, 0.15992944728042227, 0.1650691921035768, 0.17543991982366963, 0.17199667425652418]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1665945049471667, 0.1622362416644119, 0.16014450835089722, 0.16526174321953974, 0.17360700124659917, 0.1721560005713852]
start point for exploration sampling:  11106
from probs:  [0.166663112336699, 0.1623030542224351, 0.1597986368416019, 0.1653298017476133, 0.17367849654089978, 0.17222689831075103]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]] [[2.051]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[0.151]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16674286618287495, 0.16238072163569084, 0.1598751058077883, 0.16540891756029683, 0.1737616074819052, 0.171830781331444]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1234 1431
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16636879167092095, 0.16288757717965466, 0.1595541065394766, 0.16548410286425616, 0.17381556353919494, 0.17188985820649658]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[ 1.193]
 [-0.981]
 [-0.981]
 [-0.981]
 [-0.981]
 [-0.981]
 [-0.981]] [[1.939]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]]
line 256 mcts: sample exp_bonus 0.8380478306614212
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.342]
 [0.508]
 [0.342]
 [0.342]
 [0.342]
 [0.306]] [[1.778]
 [1.778]
 [4.158]
 [1.778]
 [1.778]
 [1.778]
 [1.563]] [[1.438]
 [1.438]
 [2.113]
 [1.438]
 [1.438]
 [1.438]
 [1.366]]
line 256 mcts: sample exp_bonus 2.031938364260909
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1670136959418343, 0.16310372019188105, 0.1589759534334002, 0.1643857955736592, 0.17398103985745708, 0.17253979500176828]
from probs:  [0.16716009986807184, 0.1632466966399225, 0.15871406963163553, 0.16452989588683378, 0.17413355134570588, 0.17221568662783046]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.152]
 [0.196]
 [0.196]
 [0.192]
 [0.193]
 [0.191]] [[-2.834]
 [-0.469]
 [-2.393]
 [-2.508]
 [-2.511]
 [-2.395]
 [-1.839]] [[0.213]
 [0.152]
 [0.196]
 [0.196]
 [0.192]
 [0.193]
 [0.191]]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.137]
 [0.211]
 [0.182]
 [0.201]
 [0.204]
 [0.279]] [[ 1.109]
 [ 0.91 ]
 [-2.098]
 [ 0.095]
 [-1.559]
 [-1.235]
 [ 0.749]] [[0.089]
 [0.137]
 [0.211]
 [0.182]
 [0.201]
 [0.204]
 [0.279]]
from probs:  [0.16627587488621148, 0.16368149112816807, 0.15913679183124654, 0.16410804521451305, 0.17459734209861, 0.1722004548412509]
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.402]
 [0.401]
 [0.396]
 [0.409]
 [0.433]
 [0.411]] [[1.729]
 [3.004]
 [2.688]
 [2.628]
 [2.193]
 [2.089]
 [2.263]] [[0.716]
 [0.402]
 [0.401]
 [0.396]
 [0.409]
 [0.433]
 [0.411]]
line 256 mcts: sample exp_bonus -1.8491520366368144
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.763]] [[1.556]
 [1.556]
 [1.556]
 [1.556]
 [1.556]
 [1.556]
 [0.019]] [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.763]]
using another actor
from probs:  [0.1664275638952376, 0.1638308133506621, 0.15928196804679193, 0.1638308133506621, 0.17427129267140498, 0.17235754868524114]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.898948683915129
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16649826702244636, 0.1634755850760422, 0.15934963552317197, 0.16390041330493613, 0.1743453280360176, 0.17243077103738558]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.614]] [[5.211]
 [5.211]
 [5.211]
 [5.211]
 [5.211]
 [5.211]
 [4.552]] [[0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.614]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16649826702244636, 0.1634755850760422, 0.15934963552317197, 0.16390041330493613, 0.1743453280360176, 0.17243077103738558]
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.755]] [[2.071]
 [2.071]
 [2.071]
 [2.071]
 [2.071]
 [2.071]
 [3.348]] [[0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.755]]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.653]] [[1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.145]] [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.653]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16664549808616658, 0.16362014324300372, 0.15908944585572332, 0.16404534713894306, 0.1740163175812567, 0.17258324809490663]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.095]
 [0.435]
 [0.328]
 [0.328]
 [0.596]
 [0.328]] [[ 3.158]
 [ 2.049]
 [ 0.966]
 [ 1.299]
 [ 1.299]
 [-1.016]
 [ 1.299]] [[2.05 ]
 [1.446]
 [1.214]
 [1.283]
 [1.283]
 [0.258]
 [1.283]]
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[0.92]
 [0.92]
 [0.92]
 [0.92]
 [0.92]
 [0.92]
 [0.92]] [[2.468]
 [2.468]
 [2.468]
 [2.468]
 [2.468]
 [2.468]
 [2.468]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16728085682171429, 0.163829019203851, 0.15891954031212324, 0.16257234887745386, 0.17369911739242874, 0.17369911739242874]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.504]] [[1.637]
 [1.637]
 [1.637]
 [1.637]
 [1.637]
 [1.637]
 [1.369]] [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.504]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]] [[1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]] [[0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6572567989096976
siam score:  -0.6644116
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.078]
 [0.137]
 [0.128]
 [0.174]
 [0.114]
 [0.202]] [[2.48 ]
 [2.668]
 [2.821]
 [3.161]
 [3.165]
 [2.973]
 [3.17 ]] [[-0.645]
 [-0.612]
 [-0.393]
 [-0.184]
 [-0.089]
 [-0.337]
 [-0.03 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.704]
 [0.911]
 [0.707]
 [0.766]
 [0.793]
 [0.799]] [[2.395]
 [3.008]
 [3.737]
 [3.486]
 [3.467]
 [3.018]
 [3.806]] [[0.671]
 [0.633]
 [1.291]
 [0.798]
 [0.91 ]
 [0.813]
 [1.09 ]]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.512]] [[7.731]
 [7.731]
 [7.731]
 [7.731]
 [7.731]
 [7.731]
 [8.241]] [[1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]
 [2.009]]
first move QE:  0.919951652939259
using explorer policy with actor:  1
siam score:  -0.66216224
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.003]
 [ 0.   ]] [[7.844]
 [8.459]
 [7.897]
 [7.785]
 [7.783]
 [7.88 ]
 [7.781]] [[1.409]
 [1.711]
 [1.434]
 [1.379]
 [1.378]
 [1.426]
 [1.38 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16713373519945948, 0.16369446208650168, 0.15919823619368736, 0.1624422664193831, 0.17400373209590392, 0.17352756800506441]
using another actor
from probs:  [0.16683874695495926, 0.16341500527769198, 0.1593348986156636, 0.16258171365253699, 0.17415310417457572, 0.17367653132457236]
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.404]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]] [[-3.03 ]
 [-1.405]
 [-3.03 ]
 [-3.03 ]
 [-3.03 ]
 [-3.03 ]
 [-3.03 ]] [[0.383]
 [0.404]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16662326874715805, 0.16406632126337298, 0.1584052874906621, 0.16364805516263034, 0.17386435195006492, 0.17339271538611153]
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.364]
 [0.855]
 [0.564]
 [0.381]
 [0.676]
 [0.418]] [[ 1.432]
 [ 1.503]
 [ 1.515]
 [ 0.542]
 [ 1.309]
 [-0.21 ]
 [ 1.276]] [[1.796]
 [1.853]
 [2.1  ]
 [1.634]
 [1.798]
 [1.442]
 [1.805]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16646616868477274, 0.16433759340371623, 0.15827766989948394, 0.16308733550179041, 0.1741518244460635, 0.1736794080641732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16653070720584712, 0.16440130668142902, 0.15795133623800084, 0.16315056405761294, 0.17421934267683048, 0.17374674314027963]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6652112
siam score:  -0.66612154
from probs:  [0.1668953043497342, 0.1643412040704069, 0.15829714997428876, 0.16268304472675982, 0.17412713884850375, 0.1736561580303066]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16680762770471402, 0.1638453699320936, 0.15823662805804575, 0.16260897812024644, 0.17448667473967708, 0.17401472144522312]
Printing some Q and Qe and total Qs values:  [[0.017]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]] [[2.404]
 [0.894]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]] [[2.   ]
 [1.886]
 [1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.909]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16673876390181672, 0.16378578552812104, 0.1581938163546416, 0.16296190346041334, 0.17486537934596, 0.17345435140904747]
line 256 mcts: sample exp_bonus 4.918180424296133
siam score:  -0.6393399
from probs:  [0.16702363648170765, 0.16283090224192154, 0.1584640899223063, 0.16324032328779098, 0.1746903505201738, 0.17375069754609992]
first move QE:  0.9058653006314132
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16738134450176326, 0.16236541275967112, 0.1584176887033949, 0.1635899287332628, 0.17412281265095397, 0.17412281265095397]
UNIT TEST: sample policy line 217 mcts : [0.02  0.837 0.02  0.041 0.02  0.041 0.02 ]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.657]] [[2.152]
 [2.152]
 [2.152]
 [2.152]
 [2.152]
 [2.152]
 [0.956]] [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.657]]
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.64 ]
 [0.731]
 [0.814]
 [0.724]
 [0.808]
 [0.971]] [[1.591]
 [1.685]
 [2.047]
 [1.581]
 [1.875]
 [1.353]
 [2.371]] [[0.974]
 [0.974]
 [1.174]
 [1.037]
 [1.1  ]
 [0.942]
 [1.449]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1677208270863821, 0.16148664794697068, 0.15873899121515603, 0.16310159720975156, 0.17447596827086972, 0.17447596827086972]
from probs:  [0.16805176325618187, 0.16100863070180776, 0.15828339850740167, 0.16301574101523042, 0.1748202332596891, 0.1748202332596891]
from probs:  [0.16811590165690746, 0.1610700810304216, 0.15796215012751055, 0.16307795737475694, 0.1748869549052017, 0.1748869549052017]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1685614498142677, 0.1611004793528519, 0.15800003162777468, 0.1635101537505803, 0.17394763843466365, 0.17488024701986182]
siam score:  -0.6423275
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1682010405154695, 0.16117031266859924, 0.15806852097145058, 0.16358103160420698, 0.1740230406954583, 0.17495605354481547]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16803569508586585, 0.16103042624697525, 0.15756161885001801, 0.16384022014882835, 0.17429877424601797, 0.17523326542229462]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[3.795]
 [3.795]
 [3.795]
 [3.795]
 [3.795]
 [3.795]
 [3.795]] [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.3535152003133413
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1680363347545273, 0.16104949268168328, 0.1579660399904779, 0.16344552615178232, 0.17381956117616712, 0.17568304524536216]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.508]
 [0.601]
 [0.582]
 [0.579]
 [0.548]
 [0.578]] [[1.538]
 [1.607]
 [0.908]
 [1.284]
 [1.023]
 [1.112]
 [0.33 ]] [[0.588]
 [0.508]
 [0.601]
 [0.582]
 [0.579]
 [0.548]
 [0.578]]
from probs:  [0.16774765774943004, 0.16118373599798522, 0.1580977130849925, 0.16317695773806085, 0.17396444902363495, 0.17582948640589643]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[-1.378]
 [-1.378]
 [-1.378]
 [-1.378]
 [-1.378]
 [-1.378]
 [-1.378]] [[0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.432]
 [0.501]
 [0.479]
 [0.475]
 [0.483]
 [0.478]] [[-1.731]
 [ 1.472]
 [-1.57 ]
 [-1.591]
 [-1.598]
 [-1.689]
 [-2.   ]] [[0.489]
 [0.432]
 [0.501]
 [0.479]
 [0.475]
 [0.483]
 [0.478]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16787396764224669, 0.1613051034178994, 0.1574637812074983, 0.16329982600530563, 0.1740954400085756, 0.17596188171847427]
UNIT TEST: sample policy line 217 mcts : [0.122 0.204 0.082 0.143 0.204 0.122 0.122]
siam score:  -0.6442438
from probs:  [0.16787396764224669, 0.1613051034178994, 0.1574637812074983, 0.16329982600530563, 0.1740954400085756, 0.17596188171847427]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16787396764224669, 0.1613051034178994, 0.1574637812074983, 0.16329982600530563, 0.1740954400085756, 0.17596188171847427]
line 256 mcts: sample exp_bonus 0.2016676941107172
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.529]
 [0.344]
 [0.529]
 [0.55 ]
 [0.439]
 [0.533]] [[2.717]
 [2.72 ]
 [1.774]
 [1.896]
 [1.908]
 [2.354]
 [2.25 ]] [[1.432]
 [1.457]
 [0.423]
 [0.728]
 [0.762]
 [1.038]
 [1.047]]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.462]
 [0.507]
 [0.47 ]
 [0.487]
 [0.478]
 [0.476]] [[1.648]
 [1.689]
 [1.852]
 [1.611]
 [1.507]
 [1.531]
 [1.496]] [[0.495]
 [0.462]
 [0.507]
 [0.47 ]
 [0.487]
 [0.478]
 [0.476]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.6455338
siam score:  -0.6467961
1294 1556
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.397]
 [0.409]
 [0.405]
 [0.4  ]
 [0.399]
 [0.398]] [[-0.399]
 [ 0.93 ]
 [-0.286]
 [-0.746]
 [-0.586]
 [-0.319]
 [-0.7  ]] [[0.405]
 [0.397]
 [0.409]
 [0.405]
 [0.4  ]
 [0.399]
 [0.398]]
1294 1560
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16745549515433542, 0.161738234869425, 0.15789647795840056, 0.16292898350383694, 0.17452654897050637, 0.17545425954349575]
siam score:  -0.64780974
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.851]] [[ 0.47 ]
 [ 0.47 ]
 [ 0.47 ]
 [ 0.47 ]
 [ 0.47 ]
 [ 0.47 ]
 [-1.697]] [[0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.851]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16752131544458743, 0.161408746532018, 0.15795854096803436, 0.16299302459711254, 0.17459514861902395, 0.17552322383922378]
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]] [[-0.612]
 [-0.635]
 [-0.635]
 [-0.635]
 [-0.635]
 [-0.635]
 [-0.635]] [[0.341]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16766405040897636, 0.16115485888713266, 0.15809312806019135, 0.1631319012737896, 0.17428328461535944, 0.1756727767545506]
line 256 mcts: sample exp_bonus 2.109234790338256
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.517]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[0.02 ]
 [0.179]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]] [[0.476]
 [0.517]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]]
start point for exploration sampling:  11106
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16725265082297996, 0.16158699639749702, 0.15890105652171252, 0.16237184895860288, 0.17471349508977982, 0.175173952209428]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.016]
 [0.017]
 [0.023]
 [0.022]
 [0.018]
 [0.023]] [[2.347]
 [2.274]
 [2.344]
 [2.347]
 [2.343]
 [2.344]
 [2.351]] [[1.053]
 [0.93 ]
 [1.049]
 [1.066]
 [1.055]
 [1.049]
 [1.073]]
from probs:  [0.16731781704807094, 0.16126032767061504, 0.15896296873635027, 0.16243511348927314, 0.17478156826465283, 0.17524220479103783]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
in main func line 156:  1306
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16717646110063505, 0.16113980168607267, 0.15922695076047055, 0.1623106399100558, 0.17461292559683508, 0.1755332209459309]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]] [[0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]]
line 256 mcts: sample exp_bonus 0.9426610372750345
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1669833568221695, 0.1609848242007425, 0.15983843796642533, 0.16254022260911252, 0.17436965807168878, 0.17528350032986142]
Printing some Q and Qe and total Qs values:  [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]] [[0.877]
 [0.845]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]] [[2.07 ]
 [2.049]
 [2.07 ]
 [2.07 ]
 [2.07 ]
 [2.07 ]
 [2.07 ]]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.372]
 [0.361]
 [0.361]
 [0.384]
 [0.36 ]
 [0.384]] [[-2.952]
 [-1.752]
 [-3.355]
 [-3.399]
 [ 0.   ]
 [-3.241]
 [ 0.   ]] [[0.36 ]
 [0.372]
 [0.361]
 [0.361]
 [0.384]
 [0.36 ]
 [0.384]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.818]
 [0.822]] [[5.606]
 [5.606]
 [5.606]
 [5.606]
 [5.606]
 [5.144]
 [6.009]] [[1.792]
 [1.792]
 [1.792]
 [1.792]
 [1.792]
 [1.636]
 [1.993]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.055]
 [0.401]] [[1.273]
 [1.273]
 [1.273]
 [1.273]
 [1.273]
 [2.327]
 [1.273]] [[1.775]
 [1.775]
 [1.775]
 [1.775]
 [1.775]
 [2.016]
 [1.775]]
Printing some Q and Qe and total Qs values:  [[1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]] [[0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]] [[2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]]
from probs:  [0.1668561188283332, 0.16129105692488752, 0.1605273784793056, 0.16206233416097274, 0.17553675592868864, 0.17372635567781236]
using another actor
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.409]
 [0.433]
 [0.419]
 [0.422]
 [0.449]
 [0.485]] [[-0.378]
 [ 0.845]
 [-0.095]
 [-0.536]
 [-0.317]
 [-0.54 ]
 [-1.136]] [[0.432]
 [0.409]
 [0.433]
 [0.419]
 [0.422]
 [0.449]
 [0.485]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.276]
 [0.341]
 [0.335]
 [0.334]
 [0.245]
 [0.257]] [[1.069]
 [1.451]
 [1.119]
 [1.457]
 [1.986]
 [2.131]
 [1.749]] [[-0.263]
 [-0.216]
 [-0.196]
 [-0.095]
 [ 0.078]
 [-0.05 ]
 [-0.154]]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]] [[ 0.833]
 [-2.755]
 [-2.755]
 [-2.755]
 [-2.755]
 [-2.755]
 [-2.755]] [[0.014]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]]
siam score:  -0.6354492
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16678533534736967, 0.16126523113507452, 0.1601313978831066, 0.16280326796983666, 0.1763015103795011, 0.17271325728511142]
siam score:  -0.6282425
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.495]
 [0.527]
 [0.532]
 [0.535]
 [0.526]
 [0.535]] [[ 0.231]
 [ 1.283]
 [ 0.144]
 [ 0.395]
 [ 0.218]
 [ 0.362]
 [-0.245]] [[0.539]
 [0.495]
 [0.527]
 [0.532]
 [0.535]
 [0.526]
 [0.535]]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]] [[1.71 ]
 [1.329]
 [1.329]
 [1.329]
 [1.329]
 [1.329]
 [1.329]] [[1.602]
 [1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.379]]
siam score:  -0.6219399
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16591391668957423, 0.16161158413511834, 0.16047818929495974, 0.16276177000994593, 0.17618163308568247, 0.1730529067847192]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16591391668957423, 0.16161158413511834, 0.16047818929495974, 0.16276177000994593, 0.17618163308568247, 0.1730529067847192]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
from probs:  [0.16564320241064587, 0.16173874835418425, 0.16060446170128514, 0.1625042521544166, 0.17632026176075544, 0.17318907361871277]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16570524655125038, 0.1617993300230849, 0.160290053539386, 0.16256512055422276, 0.17638630515374049, 0.1732539441783155]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.366]
 [0.415]
 [0.414]
 [0.415]
 [0.416]
 [0.409]] [[0.527]
 [1.343]
 [0.632]
 [0.656]
 [0.632]
 [0.798]
 [0.816]] [[0.42 ]
 [0.366]
 [0.415]
 [0.414]
 [0.415]
 [0.416]
 [0.409]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1657670363952213, 0.16185966338949787, 0.15997693402354393, 0.16262573947633427, 0.1764520778585938, 0.17331854885680897]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]] [[1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.377]] [[0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]]
siam score:  -0.6276265
line 256 mcts: sample exp_bonus 4.192713144668478
siam score:  -0.62676585
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]] [[2.144]
 [2.144]
 [2.144]
 [2.144]
 [2.144]
 [2.144]
 [2.144]] [[0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]]
using explorer policy with actor:  0
siam score:  -0.62435937
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16667510860098153, 0.16160497808718685, 0.16048016652392585, 0.1631305210427645, 0.17515627930171254, 0.17295294644342865]
line 256 mcts: sample exp_bonus 1.7974982053809776
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16667510860098153, 0.16160497808718685, 0.16048016652392585, 0.1631305210427645, 0.17515627930171254, 0.17295294644342865]
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.182]
 [0.422]
 [0.467]
 [0.467]
 [0.467]
 [0.32 ]] [[2.   ]
 [2.438]
 [2.293]
 [2.   ]
 [2.   ]
 [2.   ]
 [2.045]] [[1.622]
 [1.489]
 [1.826]
 [1.622]
 [1.622]
 [1.622]
 [1.373]]
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.57 ]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.751]] [[2.087]
 [2.199]
 [2.087]
 [2.087]
 [2.087]
 [2.087]
 [1.883]] [[0.565]
 [0.57 ]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.751]]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.786]] [[1.983]
 [1.983]
 [1.983]
 [1.983]
 [1.983]
 [1.983]
 [1.08 ]] [[0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.786]]
1339 1638
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.438]
 [0.473]
 [0.467]
 [0.494]
 [0.501]
 [0.641]] [[2.596]
 [2.918]
 [2.185]
 [2.316]
 [2.232]
 [2.213]
 [1.22 ]] [[0.451]
 [0.438]
 [0.473]
 [0.467]
 [0.494]
 [0.501]
 [0.641]]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.732]] [[1.91 ]
 [1.91 ]
 [1.91 ]
 [1.91 ]
 [1.91 ]
 [1.91 ]
 [1.872]] [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.732]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16648370120771136, 0.16144488703151316, 0.1606977042064977, 0.16373049542744506, 0.17535352774519192, 0.17228968438164077]
first move QE:  0.8627496085744087
siam score:  -0.6108349
Printing some Q and Qe and total Qs values:  [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]] [[0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]] [[1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]]
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[1.601]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]] [[1.727]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]]
from probs:  [0.1669209694920955, 0.16150704253879508, 0.16150704253879508, 0.1641675630146204, 0.17402574156343747, 0.17187164085225634]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.434]
 [0.464]
 [0.434]
 [0.555]
 [0.275]
 [0.296]] [[2.913]
 [1.993]
 [1.578]
 [1.993]
 [0.785]
 [1.656]
 [1.748]] [[1.974]
 [1.738]
 [1.501]
 [1.738]
 [1.078]
 [1.369]
 [1.448]]
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]] [[2.103]
 [2.103]
 [2.103]
 [2.103]
 [2.103]
 [2.103]
 [2.103]] [[1.869]
 [1.869]
 [1.869]
 [1.869]
 [1.869]
 [1.869]
 [1.869]]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.47 ]
 [0.496]
 [0.496]
 [0.459]
 [0.496]
 [0.465]] [[1.973]
 [2.426]
 [1.671]
 [1.671]
 [1.97 ]
 [1.671]
 [2.339]] [[0.472]
 [0.47 ]
 [0.496]
 [0.496]
 [0.459]
 [0.496]
 [0.465]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.327]
 [0.312]
 [0.312]
 [0.32 ]
 [0.312]
 [0.312]] [[2.408]
 [3.813]
 [2.42 ]
 [2.42 ]
 [2.673]
 [2.42 ]
 [2.42 ]] [[0.331]
 [0.327]
 [0.312]
 [0.312]
 [0.32 ]
 [0.312]
 [0.312]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.8633952075981782
using explorer policy with actor:  0
from probs:  [0.16716525573912236, 0.161384547979279, 0.16175695895996123, 0.16288501256004223, 0.1746982405386682, 0.1721099842229268]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16730924735545327, 0.16152356025076822, 0.1618962920161662, 0.16302531729101577, 0.17441136612343808, 0.17183421696315834]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.01 ]
 [ 1.599]
 [ 1.599]
 [ 1.599]
 [-0.006]
 [ 1.599]
 [ 0.007]] [[-3.477]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-3.243]
 [ 0.   ]
 [-2.648]] [[ 0.01 ]
 [ 1.599]
 [ 1.599]
 [ 1.599]
 [-0.006]
 [ 1.599]
 [ 0.007]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.455]
 [0.539]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[1.62 ]
 [1.62 ]
 [2.122]
 [1.62 ]
 [1.62 ]
 [1.62 ]
 [1.62 ]] [[1.771]
 [1.771]
 [1.929]
 [1.771]
 [1.771]
 [1.771]
 [1.771]]
from probs:  [0.1677802209717742, 0.1612360376945772, 0.16197824722411117, 0.16348423169323825, 0.17446601869965117, 0.17105524371664796]
Starting evaluation
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.265]
 [0.252]
 [0.253]
 [0.253]
 [0.253]
 [0.254]] [[-3.055]
 [-1.255]
 [-2.941]
 [-2.909]
 [-2.909]
 [-2.909]
 [-3.006]] [[0.251]
 [0.265]
 [0.252]
 [0.253]
 [0.253]
 [0.253]
 [0.254]]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.745]
 [0.708]] [[-1.283]
 [-1.283]
 [-1.283]
 [-1.283]
 [-1.283]
 [-0.708]
 [-1.283]] [[0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.745]
 [0.708]]
siam score:  -0.6113488
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.227]
 [0.417]
 [0.362]
 [0.366]
 [0.427]
 [0.545]] [[-4.82 ]
 [ 0.746]
 [-4.364]
 [-4.724]
 [-4.525]
 [-3.469]
 [-3.479]] [[0.377]
 [0.227]
 [0.417]
 [0.362]
 [0.366]
 [0.427]
 [0.545]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.424]
 [0.467]
 [0.465]
 [0.467]
 [0.445]
 [0.467]] [[1.392]
 [2.025]
 [1.216]
 [1.28 ]
 [1.216]
 [1.347]
 [1.216]] [[0.484]
 [0.424]
 [0.467]
 [0.465]
 [0.467]
 [0.445]
 [0.467]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.274]
 [0.311]
 [0.321]
 [0.313]
 [0.328]
 [0.332]] [[0.519]
 [1.608]
 [0.569]
 [0.763]
 [0.729]
 [0.636]
 [0.296]] [[0.346]
 [0.274]
 [0.311]
 [0.321]
 [0.313]
 [0.328]
 [0.332]]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.453]
 [0.498]
 [0.498]
 [0.498]
 [0.504]
 [0.498]] [[ 0.677]
 [ 0.661]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.975]
 [-0.011]] [[0.445]
 [0.453]
 [0.498]
 [0.498]
 [0.498]
 [0.504]
 [0.498]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16817672425552543, 0.16088018056253728, 0.16236103910833113, 0.163112173382764, 0.17401039605781413, 0.171459486633028]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.346]
 [0.369]
 [0.382]
 [0.389]
 [0.392]
 [0.482]] [[-2.384]
 [-0.629]
 [-1.818]
 [-2.653]
 [-2.502]
 [-1.379]
 [-0.944]] [[0.375]
 [0.346]
 [0.369]
 [0.382]
 [0.389]
 [0.392]
 [0.482]]
line 256 mcts: sample exp_bonus -4.645787447340612
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[2.106]
 [1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.228]] [[0.791]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
Printing some Q and Qe and total Qs values:  [[0.838]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[-0.239]
 [-3.791]
 [-3.791]
 [-3.791]
 [-3.791]
 [-3.791]
 [-3.791]] [[0.838]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]]
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.902]] [[3.283]
 [3.279]
 [3.279]
 [3.279]
 [3.279]
 [3.279]
 [2.783]] [[0.913]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.902]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.959]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]] [[1.125]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]] [[0.959]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]]
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.426]
 [0.402]
 [0.404]
 [0.401]
 [0.427]
 [0.406]] [[-1.859]
 [ 0.   ]
 [-0.799]
 [-0.624]
 [-0.475]
 [-0.382]
 [-0.392]] [[0.406]
 [0.426]
 [0.402]
 [0.404]
 [0.401]
 [0.427]
 [0.406]]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[-0.975]
 [-3.336]
 [-3.336]
 [-3.336]
 [-3.336]
 [-3.336]
 [-3.336]] [[0.595]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]]
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]] [[-1.085]
 [-0.671]
 [-0.671]
 [-0.671]
 [-0.671]
 [-0.671]
 [-0.671]] [[0.822]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]]
Printing some Q and Qe and total Qs values:  [[0.956]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]] [[1.064]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]] [[0.956]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.318]
 [0.361]
 [0.365]
 [0.365]
 [0.365]
 [0.362]] [[-3.536]
 [-1.688]
 [-3.387]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-3.352]] [[0.36 ]
 [0.318]
 [0.361]
 [0.365]
 [0.365]
 [0.365]
 [0.362]]
start point for exploration sampling:  11106
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.962]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]] [[0.882]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]] [[0.962]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]]
siam score:  -0.61485666
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[-1.393]
 [-1.83 ]
 [-1.83 ]
 [-1.83 ]
 [-1.83 ]
 [-1.83 ]
 [-1.83 ]] [[0.417]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.962]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]] [[0.908]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]] [[0.962]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]]
Printing some Q and Qe and total Qs values:  [[0.963]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]] [[1.067]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]] [[0.963]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]]
Printing some Q and Qe and total Qs values:  [[0.96 ]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]] [[0.85 ]
 [0.838]
 [0.838]
 [0.838]
 [0.838]
 [0.838]
 [0.838]] [[0.96 ]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 0.9587438305535262
Printing some Q and Qe and total Qs values:  [[0.919]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[ 0.486]
 [-0.8  ]
 [-0.8  ]
 [-0.8  ]
 [-0.8  ]
 [-0.8  ]
 [-0.8  ]] [[0.919]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[ 0.046]
 [-2.657]
 [-2.657]
 [-2.657]
 [-2.657]
 [-2.657]
 [-2.657]] [[0.925]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]]
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[-2.327]
 [-2.327]
 [-2.327]
 [-2.327]
 [-2.327]
 [-2.327]
 [-2.327]] [[0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]]
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[-0.719]
 [-3.122]
 [-3.122]
 [-3.122]
 [-3.122]
 [-3.122]
 [-3.122]] [[0.917]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.391]
 [0.434]
 [0.393]
 [0.39 ]
 [0.397]
 [0.37 ]] [[ 0.535]
 [-0.012]
 [-1.936]
 [-1.93 ]
 [-2.307]
 [-2.464]
 [-0.703]] [[0.736]
 [0.391]
 [0.434]
 [0.393]
 [0.39 ]
 [0.397]
 [0.37 ]]
Printing some Q and Qe and total Qs values:  [[0.954]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]] [[0.829]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]] [[0.954]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]] [[-2.949]
 [-2.976]
 [-2.976]
 [-2.976]
 [-2.976]
 [-2.976]
 [-2.976]] [[0.385]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[ 0.481]
 [-0.859]
 [-0.859]
 [-0.859]
 [-0.859]
 [-0.859]
 [-0.859]] [[0.929]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16784287841439044, 0.16094474857928795, 0.1624262014562111, 0.16317763719257816, 0.17408023378504794, 0.17152830057248453]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 0.9549781824372005
using explorer policy with actor:  0
siam score:  -0.61047536
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.047]
 [0.027]
 [0.039]
 [0.029]
 [0.017]
 [0.029]] [[1.351]
 [1.501]
 [1.009]
 [0.865]
 [1.223]
 [1.618]
 [1.288]] [[0.666]
 [0.915]
 [0.217]
 [0.05 ]
 [0.507]
 [1.01 ]
 [0.593]]
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]] [[0.502]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[0.844]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]]
Printing some Q and Qe and total Qs values:  [[0.887]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[-1.149]
 [-4.054]
 [-4.054]
 [-4.054]
 [-4.054]
 [-4.054]
 [-4.054]] [[0.887]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]]
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]] [[0.948]
 [1.211]
 [1.211]
 [1.211]
 [1.211]
 [1.211]
 [1.211]] [[0.77 ]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.376]
 [0.442]
 [0.441]
 [0.439]
 [0.465]
 [0.444]] [[-4.533]
 [-2.478]
 [-5.003]
 [-4.909]
 [-4.881]
 [-4.393]
 [-4.481]] [[0.473]
 [0.376]
 [0.442]
 [0.441]
 [0.439]
 [0.465]
 [0.444]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.304]
 [0.374]
 [0.373]
 [0.371]
 [0.398]
 [0.377]] [[-2.43 ]
 [-2.478]
 [-5.003]
 [-4.909]
 [-4.881]
 [-4.393]
 [-4.481]] [[0.771]
 [0.304]
 [0.374]
 [0.373]
 [0.371]
 [0.398]
 [0.377]]
line 256 mcts: sample exp_bonus 1.1696504000289178
Printing some Q and Qe and total Qs values:  [[0.905]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]] [[ 0.474]
 [-0.191]
 [-0.191]
 [-0.191]
 [-0.191]
 [-0.191]
 [-0.191]] [[0.905]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]]
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]] [[0.856]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]] [[0.951]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]]
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]] [[0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]] [[0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[-1.109]
 [-2.846]
 [-2.846]
 [-2.846]
 [-2.846]
 [-2.846]
 [-2.846]] [[0.689]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]]
from probs:  [0.16804154129274548, 0.16113524666482454, 0.16261845302509745, 0.16261845302509745, 0.17385498038685657, 0.17173132560537835]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.943]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[0.897]
 [1.124]
 [1.124]
 [1.124]
 [1.124]
 [1.124]
 [1.124]] [[0.943]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]]
first move QE:  0.8602473513850866
Printing some Q and Qe and total Qs values:  [[0.834]
 [0.304]
 [0.374]
 [0.373]
 [0.371]
 [0.398]
 [0.377]] [[-1.758]
 [-2.465]
 [-4.953]
 [-4.866]
 [-4.836]
 [-4.369]
 [-4.472]] [[0.834]
 [0.304]
 [0.374]
 [0.373]
 [0.371]
 [0.398]
 [0.377]]
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.529]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.53 ]] [[-0.541]
 [ 0.172]
 [-2.104]
 [-2.104]
 [-2.104]
 [-2.104]
 [-2.173]] [[0.876]
 [0.529]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.53 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.5901264407860403
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.49 ]] [[1.892]
 [1.892]
 [1.892]
 [1.892]
 [1.892]
 [1.892]
 [1.688]] [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.49 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1681900313829895, 0.1616622836848385, 0.162402761031477, 0.16315034127832304, 0.17313873807132363, 0.17145584455104837]
siam score:  -0.60507876
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[-0.269]
 [-0.269]
 [-0.269]
 [-0.269]
 [-0.269]
 [-0.269]
 [-0.269]] [[0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.0666882603686227
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.301]
 [0.316]
 [0.312]
 [0.313]
 [0.314]
 [0.308]] [[1.727]
 [2.31 ]
 [2.115]
 [2.038]
 [1.422]
 [2.04 ]
 [1.975]] [[0.318]
 [0.301]
 [0.316]
 [0.312]
 [0.313]
 [0.314]
 [0.308]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 0.7539699632869731
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]] [[-3.062]
 [-3.062]
 [-3.062]
 [-3.062]
 [-3.062]
 [-3.062]
 [-3.062]] [[0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]]
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.21 ]
 [0.406]
 [0.462]
 [0.404]
 [0.486]
 [0.722]] [[-3.259]
 [ 1.079]
 [-3.887]
 [-4.248]
 [-4.324]
 [-3.157]
 [-1.129]] [[0.203]
 [0.21 ]
 [0.406]
 [0.462]
 [0.404]
 [0.486]
 [0.722]]
Printing some Q and Qe and total Qs values:  [[1.16]
 [1.16]
 [1.16]
 [1.16]
 [1.16]
 [1.16]
 [1.16]] [[2.333]
 [2.333]
 [2.333]
 [2.333]
 [2.333]
 [2.333]
 [2.333]] [[2.234]
 [2.234]
 [2.234]
 [2.234]
 [2.234]
 [2.234]
 [2.234]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 0.1858326614922905
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.394]
 [0.575]
 [0.528]
 [0.471]
 [0.466]
 [0.524]] [[0.376]
 [2.375]
 [1.542]
 [0.849]
 [0.543]
 [1.385]
 [0.914]] [[0.814]
 [0.394]
 [0.575]
 [0.528]
 [0.471]
 [0.466]
 [0.524]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
first move QE:  0.8574820037068028
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]] [[-0.204]
 [-0.264]
 [-0.264]
 [-0.264]
 [-0.264]
 [-0.264]
 [-0.264]] [[0.765]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[-5.454]
 [-4.979]
 [-4.979]
 [-4.979]
 [-4.979]
 [-4.979]
 [-4.979]] [[0.531]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.353]
 [0.339]] [[-1.08 ]
 [-1.08 ]
 [-1.08 ]
 [-1.08 ]
 [-1.08 ]
 [-1.349]
 [-1.233]] [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.353]
 [0.339]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -0.8932990214595812
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]] [[-0.038]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]] [[0.77 ]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]]
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.315]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]] [[-1.599]
 [-0.968]
 [-1.599]
 [-1.599]
 [-1.599]
 [-1.599]
 [-1.599]] [[0.302]
 [0.315]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16778106584153823, 0.16206465871914177, 0.16243310960475593, 0.16280331085087185, 0.17308647052779275, 0.1718313844558995]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.669]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.614]] [[2.536]
 [2.64 ]
 [2.515]
 [2.515]
 [2.515]
 [2.515]
 [2.748]] [[0.108]
 [0.214]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.14 ]]
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.788]
 [0.917]] [[-0.771]
 [-0.662]
 [-0.662]
 [-0.662]
 [-0.662]
 [-0.579]
 [-0.396]] [[0.787]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.788]
 [0.917]]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.544]
 [0.553]
 [0.537]
 [0.52 ]
 [0.527]
 [0.518]] [[2.589]
 [2.553]
 [2.504]
 [2.563]
 [2.499]
 [2.607]
 [2.767]] [[-0.041]
 [-0.044]
 [-0.043]
 [-0.053]
 [-0.11 ]
 [-0.06 ]
 [-0.023]]
rdn probs:  [0.16778106584153823, 0.16206465871914177, 0.16243310960475593, 0.16280331085087185, 0.17308647052779275, 0.1718313844558995]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.009]
 [-0.01 ]
 [ 0.059]
 [ 0.044]
 [-0.016]
 [-0.012]] [[3.554]
 [3.987]
 [3.75 ]
 [3.544]
 [3.567]
 [3.517]
 [3.634]] [[-1.118]
 [-0.989]
 [-1.07 ]
 [-1.001]
 [-1.024]
 [-1.16 ]
 [-1.113]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16726444521317385, 0.16232642949088377, 0.16269547550731447, 0.16306627471147173, 0.173366043997581, 0.17128133107957522]
first move QE:  0.8573270171287111
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.589]
 [0.606]
 [0.573]
 [0.547]
 [0.773]
 [0.592]] [[ 0.031]
 [ 2.856]
 [ 0.249]
 [-0.25 ]
 [-0.652]
 [ 0.883]
 [ 0.145]] [[0.574]
 [0.589]
 [0.606]
 [0.573]
 [0.547]
 [0.773]
 [0.592]]
start point for exploration sampling:  11106
siam score:  -0.63929653
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.358]
 [0.386]
 [0.358]
 [0.358]
 [0.487]
 [0.306]] [[1.587]
 [2.624]
 [1.918]
 [2.624]
 [2.624]
 [1.765]
 [1.861]] [[1.153]
 [1.7  ]
 [1.287]
 [1.7  ]
 [1.7  ]
 [1.386]
 [1.088]]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.833]
 [0.654]
 [0.695]
 [0.654]
 [0.661]
 [0.687]] [[1.973]
 [1.919]
 [1.973]
 [1.353]
 [1.973]
 [1.561]
 [1.635]] [[1.894]
 [1.953]
 [1.894]
 [1.685]
 [1.894]
 [1.747]
 [1.785]]
using explorer policy with actor:  1
from probs:  [0.1678692053420072, 0.16254470250597455, 0.16328371610203865, 0.16291333576547698, 0.1719006146321462, 0.17148842565235642]
line 256 mcts: sample exp_bonus -1.7259978608990996
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.399]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[0.788]
 [0.534]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]] [[0.395]
 [0.399]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.156]
 [0.148]
 [0.138]
 [0.225]
 [0.16 ]
 [0.163]] [[2.126]
 [2.693]
 [2.153]
 [2.506]
 [2.101]
 [2.146]
 [2.315]] [[-0.632]
 [-0.196]
 [-0.571]
 [-0.355]
 [-0.451]
 [-0.552]
 [-0.434]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]] [[1.823]
 [1.823]
 [1.823]
 [1.823]
 [1.823]
 [1.823]
 [1.823]] [[0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16781074373916618, 0.16250112922414017, 0.16360924613291253, 0.16323812738883986, 0.171010440540139, 0.17183031297480222]
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.033]
 [0.047]
 [0.045]
 [0.039]
 [0.052]
 [0.041]] [[-0.494]
 [ 0.074]
 [-0.662]
 [-0.361]
 [-0.277]
 [ 0.165]
 [ 0.493]] [[0.045]
 [0.033]
 [0.047]
 [0.045]
 [0.039]
 [0.052]
 [0.041]]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.549]
 [0.561]] [[2.56 ]
 [2.56 ]
 [2.56 ]
 [2.56 ]
 [2.56 ]
 [2.593]
 [2.545]] [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.549]
 [0.561]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16741257423574793, 0.16177806298280084, 0.16398380056333842, 0.1636118322085433, 0.1714019386960645, 0.17181179131350494]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.629]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[-0.274]
 [ 1.111]
 [-0.274]
 [-0.274]
 [-0.274]
 [-0.274]
 [-0.274]] [[0.654]
 [1.442]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1674731366117648, 0.16147483180457559, 0.1640431225625116, 0.16367101964621492, 0.17146394424558023, 0.17187394512935278]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16765568383533558, 0.1609315832337022, 0.16422193103995172, 0.16347867113193626, 0.1716508414853273, 0.17206128927374684]
siam score:  -0.6037419
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.49 ]
 [0.492]
 [0.494]
 [0.49 ]
 [0.492]
 [0.492]] [[1.386]
 [1.286]
 [1.617]
 [1.35 ]
 [1.286]
 [1.532]
 [1.652]] [[0.482]
 [0.49 ]
 [0.492]
 [0.494]
 [0.49 ]
 [0.492]
 [0.492]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.60468996
UNIT TEST: sample policy line 217 mcts : [0.163 0.122 0.082 0.122 0.163 0.245 0.102]
line 256 mcts: sample exp_bonus 3.686355610460503
siam score:  -0.60410607
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.2695594765345035
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.6  ]
 [0.623]
 [0.619]
 [0.631]
 [0.732]
 [0.7  ]] [[4.232]
 [3.632]
 [4.025]
 [4.173]
 [4.177]
 [4.045]
 [3.872]] [[1.142]
 [0.788]
 [1.024]
 [1.104]
 [1.114]
 [1.108]
 [0.989]]
1388 1728
siam score:  -0.6056405
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.1675945305181465, 0.16090521448904216, 0.16492532081774952, 0.16343944284905104, 0.17156774566300542, 0.17156774566300542]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16798896984361392, 0.1609277147255757, 0.1649384935751271, 0.16382410303690348, 0.17156445111027935, 0.17075626770850041]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16798896984361392, 0.1609277147255757, 0.1649384935751271, 0.16382410303690348, 0.17156445111027935, 0.17075626770850041]
siam score:  -0.60335773
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7905758852574545
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.462]
 [1.462]
 [1.462]
 [1.462]
 [1.462]
 [1.462]
 [1.462]] [[0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]] [[2.795]
 [2.795]
 [2.795]
 [2.795]
 [2.795]
 [2.795]
 [2.795]]
line 256 mcts: sample exp_bonus 2.234633954388334
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
from probs:  [0.16759903049798033, 0.16129694120612076, 0.16419997488157556, 0.16419997488157556, 0.17195808211770472, 0.17074599641504298]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.558]
 [0.556]
 [0.574]
 [0.574]
 [0.551]
 [0.567]] [[3.014]
 [3.223]
 [2.283]
 [2.391]
 [2.391]
 [2.411]
 [3.071]] [[0.647]
 [0.558]
 [0.556]
 [0.574]
 [0.574]
 [0.551]
 [0.567]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.612]
 [0.614]
 [0.614]
 [0.662]
 [0.614]
 [0.65 ]] [[2.547]
 [2.559]
 [2.547]
 [2.547]
 [2.768]
 [2.547]
 [2.899]] [[0.614]
 [0.612]
 [0.614]
 [0.614]
 [0.662]
 [0.614]
 [0.65 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16739726435012992, 0.16076370630657685, 0.1643798086307881, 0.1643798086307881, 0.17214641263746838, 0.17093299944424867]
line 256 mcts: sample exp_bonus 4.864339891587489
start point for exploration sampling:  11106
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.292]
 [0.303]
 [0.304]
 [0.296]
 [0.308]
 [0.308]] [[-1.687]
 [ 1.31 ]
 [-1.985]
 [-1.642]
 [-1.997]
 [-1.75 ]
 [-1.062]] [[0.317]
 [0.292]
 [0.303]
 [0.304]
 [0.296]
 [0.308]
 [0.308]]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.573]
 [0.516]
 [0.736]
 [0.545]
 [0.036]
 [0.401]] [[-1.905]
 [-1.697]
 [-0.955]
 [-3.114]
 [-0.804]
 [ 2.238]
 [-0.771]] [[0.529]
 [0.618]
 [0.895]
 [0.11 ]
 [0.969]
 [1.995]
 [0.924]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.554]
 [0.553]
 [0.553]
 [0.509]
 [0.621]
 [0.48 ]] [[ 2.341]
 [ 1.92 ]
 [ 1.441]
 [ 1.441]
 [ 1.184]
 [-0.046]
 [ 2.487]] [[0.794]
 [1.167]
 [1.004]
 [1.004]
 [0.831]
 [0.644]
 [1.209]]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.335]
 [0.335]
 [0.335]
 [0.332]
 [0.329]
 [0.338]] [[-1.045]
 [-0.089]
 [-1.171]
 [-0.976]
 [-0.99 ]
 [-0.946]
 [-0.876]] [[0.332]
 [0.335]
 [0.335]
 [0.335]
 [0.332]
 [0.329]
 [0.338]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.593]
 [0.78 ]] [[3.679]
 [3.679]
 [3.679]
 [3.679]
 [3.679]
 [3.6  ]
 [3.515]] [[0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.593]
 [0.78 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1406 1750
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16809044115315422, 0.16107544653232092, 0.1643207010531737, 0.1632238903605112, 0.17245111348502357, 0.1708384074158164]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.214]
 [0.314]
 [0.289]
 [0.292]
 [0.282]
 [0.324]] [[2.685]
 [2.859]
 [2.673]
 [2.829]
 [2.865]
 [2.855]
 [2.734]] [[-0.619]
 [-0.619]
 [-0.481]
 [-0.478]
 [-0.46 ]
 [-0.484]
 [-0.441]]
Printing some Q and Qe and total Qs values:  [[1.074]
 [1.076]
 [1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]] [[1.188]
 [1.251]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]] [[2.373]
 [2.418]
 [2.373]
 [2.373]
 [2.373]
 [2.373]
 [2.373]]
using explorer policy with actor:  1
first move QE:  0.8522368398507599
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.829]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[0.38 ]
 [1.843]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]] [[1.313]
 [1.851]
 [1.313]
 [1.313]
 [1.313]
 [1.313]
 [1.313]]
siam score:  -0.61201656
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16854472680090773, 0.16117500467272622, 0.1647739715395188, 0.16331443670855164, 0.17169344506297263, 0.170498415215323]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16822436366015459, 0.16123710606575625, 0.1648374596293034, 0.1633773624335554, 0.17175959925440068, 0.17056410895682955]
from probs:  [0.16829078059838715, 0.16130076435330373, 0.1649025393783743, 0.16344186571924638, 0.1718274119448239, 0.17023663800586442]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16821546742262894, 0.16124512769462943, 0.16410527684227835, 0.16374197845169938, 0.17214292225720415, 0.1705492273315597]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.949]
 [0.517]
 [0.733]
 [0.924]
 [0.649]
 [0.517]
 [0.576]] [[2.362]
 [3.931]
 [2.779]
 [2.445]
 [3.252]
 [3.931]
 [3.594]] [[0.753]
 [0.414]
 [0.46 ]
 [0.731]
 [0.45 ]
 [0.414]
 [0.417]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16827440087784223, 0.16095127408259655, 0.1641627703125949, 0.1637993446422256, 0.17220323167675142, 0.17060897840798925]
1415 1767
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.398]
 [0.351]
 [0.428]
 [0.428]
 [0.496]
 [0.329]] [[4.209]
 [3.301]
 [3.159]
 [3.332]
 [3.332]
 [2.763]
 [3.163]] [[2.024]
 [1.446]
 [1.324]
 [1.499]
 [1.499]
 [1.342]
 [1.298]]
1417 1770
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16750824615372298, 0.16096416779375455, 0.16453244097357272, 0.16380563141344384, 0.17259100828624238, 0.1705985053792636]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.008]
 [ 0.025]
 [-0.015]
 [-0.006]
 [-0.006]
 [-0.005]] [[3.46 ]
 [3.453]
 [4.803]
 [3.489]
 [3.46 ]
 [3.46 ]
 [3.54 ]] [[-1.041]
 [-1.048]
 [-0.53 ]
 [-1.048]
 [-1.041]
 [-1.041]
 [-1.012]]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.245]
 [0.258]
 [0.267]
 [0.256]
 [0.263]
 [0.309]] [[ 0.124]
 [ 0.987]
 [ 0.014]
 [-0.24 ]
 [-0.516]
 [-0.04 ]
 [-0.195]] [[0.258]
 [0.245]
 [0.258]
 [0.267]
 [0.256]
 [0.263]
 [0.309]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16681848453144552, 0.1610402189789484, 0.16496679787074223, 0.16387621827885215, 0.17264320761948307, 0.1706550727205286]
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.563]
 [0.061]
 [0.055]
 [0.072]
 [0.101]
 [0.14 ]] [[1.661]
 [1.287]
 [1.299]
 [1.33 ]
 [1.365]
 [1.51 ]
 [1.593]] [[0.787]
 [1.341]
 [0.483]
 [0.499]
 [0.559]
 [0.733]
 [0.873]]
1419 1779
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16733053198803385, 0.1615345301146011, 0.16474219647399077, 0.16401793745400625, 0.17236965861682302, 0.17000514535254502]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.686]] [[ 0.109]
 [ 0.109]
 [ 0.109]
 [ 0.109]
 [ 0.109]
 [ 0.109]
 [-0.146]] [[0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.686]]
siam score:  -0.64150536
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.096]
 [0.096]
 [0.099]
 [0.096]
 [0.096]
 [0.096]] [[5.857]
 [5.857]
 [5.857]
 [5.777]
 [5.857]
 [5.857]
 [5.857]] [[1.266]
 [1.266]
 [1.266]
 [1.209]
 [1.266]
 [1.266]
 [1.266]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16733053198803385, 0.1615345301146011, 0.16474219647399077, 0.16401793745400625, 0.17236965861682302, 0.17000514535254502]
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[-0.521]
 [-1.096]
 [-1.096]
 [-1.096]
 [-1.096]
 [-1.096]
 [-1.096]] [[0.821]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16733053198803385, 0.1615345301146011, 0.16474219647399077, 0.16401793745400625, 0.17236965861682302, 0.17000514535254502]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.001]
 [ 0.05 ]
 [ 0.043]
 [ 0.051]
 [ 0.033]
 [ 0.079]] [[4.854]
 [4.204]
 [5.057]
 [5.065]
 [4.827]
 [4.625]
 [4.713]] [[1.06 ]
 [0.675]
 [1.22 ]
 [1.219]
 [1.083]
 [0.949]
 [1.035]]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.488]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.476]] [[2.552]
 [2.896]
 [2.552]
 [2.552]
 [2.552]
 [2.552]
 [2.952]] [[0.474]
 [0.488]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.476]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1426 1782
siam score:  -0.6350414
Printing some Q and Qe and total Qs values:  [[ 0.024]
 [ 0.128]
 [-0.007]
 [-0.002]
 [-0.01 ]
 [-0.011]
 [-0.007]] [[1.986]
 [2.391]
 [1.962]
 [1.947]
 [2.014]
 [2.192]
 [2.219]] [[-0.463]
 [ 0.027]
 [-0.53 ]
 [-0.535]
 [-0.493]
 [-0.353]
 [-0.325]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16739128866354777, 0.16159318229788086, 0.16443891956011047, 0.16407749134704633, 0.17243224496787685, 0.1700668731635378]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.646]
 [0.497]
 [0.497]
 [0.742]
 [0.497]
 [0.833]] [[2.212]
 [0.807]
 [1.636]
 [1.636]
 [2.513]
 [1.636]
 [2.342]] [[1.879]
 [1.421]
 [1.635]
 [1.635]
 [1.987]
 [1.635]
 [1.962]]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[ 1.776]
 [-0.321]
 [-0.321]
 [-0.321]
 [-0.321]
 [-0.321]
 [-0.321]] [[1.834]
 [0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1676996776633295, 0.16225446090199097, 0.16474893634801088, 0.16295908759098654, 0.1727372685594819, 0.16960056893620026]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.467]
 [0.425]
 [0.425]
 [0.461]
 [0.446]
 [0.444]] [[-0.084]
 [-0.374]
 [ 0.525]
 [ 0.525]
 [-0.346]
 [-0.129]
 [-0.277]] [[1.293]
 [0.529]
 [0.744]
 [0.744]
 [0.525]
 [0.568]
 [0.514]]
1431 1793
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[2.986]
 [2.986]
 [2.986]
 [2.986]
 [2.986]
 [2.986]
 [2.986]] [[1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]]
from probs:  [0.1675764881804757, 0.1624985062237416, 0.16463494387601907, 0.16320419273473574, 0.17299708096802568, 0.16908878801700225]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.652]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.652]] [[2.796]
 [2.825]
 [2.796]
 [2.796]
 [2.796]
 [2.796]
 [2.783]] [[0.64 ]
 [0.652]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.652]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.63948417
1435 1797
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.082 0.143 0.49  0.02  0.02  0.224 0.02 ]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.167329365841603, 0.1626325556206049, 0.16512688345668153, 0.1629840619941507, 0.172714972248857, 0.16921216083810278]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16750521119343348, 0.16210444734094231, 0.16530041423397254, 0.16280346512485333, 0.17289647729974114, 0.16938998480705714]
from probs:  [0.16719564178994395, 0.16216472709012641, 0.16536188242731023, 0.16286400480902, 0.17296077014584993, 0.1694529737377494]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.564]
 [0.669]
 [0.611]
 [0.611]
 [0.625]
 [0.642]] [[1.658]
 [2.265]
 [1.912]
 [2.233]
 [2.233]
 [2.201]
 [2.224]] [[0.745]
 [0.678]
 [0.771]
 [0.762]
 [0.762]
 [0.779]
 [0.82 ]]
first move QE:  0.8469430180334547
1444 1802
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.186]
 [1.254]
 [0.701]
 [0.368]
 [0.627]
 [0.223]] [[1.877]
 [1.982]
 [1.948]
 [1.216]
 [1.731]
 [1.597]
 [1.667]] [[1.671]
 [1.459]
 [2.238]
 [1.557]
 [1.501]
 [1.643]
 [1.37 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.722]
 [0.738]
 [0.74 ]
 [0.748]
 [0.776]
 [0.737]] [[3.16 ]
 [3.06 ]
 [3.352]
 [3.019]
 [3.285]
 [3.112]
 [3.198]] [[1.782]
 [1.667]
 [1.895]
 [1.656]
 [1.858]
 [1.763]
 [1.783]]
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.466]
 [0.381]
 [0.381]
 [0.381]
 [0.498]
 [0.381]] [[4.181]
 [3.41 ]
 [4.181]
 [4.181]
 [4.181]
 [3.063]
 [4.181]] [[1.052]
 [0.966]
 [1.052]
 [1.052]
 [1.052]
 [0.913]
 [1.052]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.326]
 [0.392]
 [0.394]
 [0.391]
 [0.401]
 [0.4  ]] [[3.576]
 [3.464]
 [3.261]
 [3.368]
 [3.364]
 [3.553]
 [3.539]] [[-0.033]
 [-0.203]
 [-0.139]
 [-0.1  ]
 [-0.106]
 [-0.023]
 [-0.03 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[7.013]
 [2.571]
 [2.571]
 [2.571]
 [2.571]
 [2.571]
 [2.571]] [[ 1.802]
 [-0.14 ]
 [-0.14 ]
 [-0.14 ]
 [-0.14 ]
 [-0.14 ]
 [-0.14 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16724095289053, 0.16191104565066577, 0.1654240306190419, 0.16260112421238243, 0.1733458117227301, 0.16947703490464983]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.608]
 [0.662]
 [0.797]
 [0.777]
 [0.697]
 [0.947]] [[2.83 ]
 [1.973]
 [3.128]
 [2.335]
 [2.305]
 [1.962]
 [2.46 ]] [[1.733]
 [1.057]
 [1.875]
 [1.43 ]
 [1.396]
 [1.11 ]
 [1.616]]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.004]
 [ 0.019]
 [-0.002]
 [ 0.003]
 [ 0.025]
 [-0.007]] [[4.511]
 [3.977]
 [3.594]
 [3.856]
 [2.81 ]
 [4.095]
 [4.178]] [[ 0.508]
 [ 0.15 ]
 [-0.059]
 [ 0.075]
 [-0.611]
 [ 0.287]
 [ 0.278]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1669984414085992, 0.16203155084848833, 0.16554715041889398, 0.1627221430135435, 0.17347482745015877, 0.16922588686031617]
using another actor
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.485]
 [0.46 ]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[0.452]
 [1.088]
 [0.505]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[0.031]
 [0.503]
 [0.078]
 [0.13 ]
 [0.13 ]
 [0.13 ]
 [0.13 ]]
first move QE:  0.8467579106687975
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.472]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[1.235]
 [2.363]
 [1.235]
 [1.235]
 [1.235]
 [1.235]
 [1.235]] [[0.298]
 [0.695]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]]
using explorer policy with actor:  1
from probs:  [0.16663649959140897, 0.16170320083310252, 0.16591256556565165, 0.16308132247159857, 0.17306698939129353, 0.1695994221469447]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16675428439613382, 0.16181749860087974, 0.1656700743289473, 0.1628495219171257, 0.1731893195026422, 0.16971930125427118]
siam score:  -0.6299667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16675428439613382, 0.16181749860087974, 0.1656700743289473, 0.1628495219171257, 0.1731893195026422, 0.16971930125427118]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.22 ]
 [0.228]
 [0.229]
 [0.229]
 [0.225]
 [0.227]] [[2.22 ]
 [2.276]
 [2.47 ]
 [2.196]
 [2.267]
 [2.274]
 [2.171]] [[-0.975]
 [-0.993]
 [-0.912]
 [-1.003]
 [-0.978]
 [-0.984]
 [-1.015]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.098]
 [0.056]
 [0.052]
 [0.098]
 [0.038]
 [0.054]] [[3.04 ]
 [2.969]
 [3.194]
 [3.233]
 [2.317]
 [3.223]
 [3.516]] [[-0.146]
 [-0.326]
 [-0.335]
 [-0.33 ]
 [-0.543]
 [-0.36 ]
 [-0.232]]
Printing some Q and Qe and total Qs values:  [[1.462]
 [1.462]
 [1.462]
 [1.462]
 [1.462]
 [1.462]
 [1.462]] [[0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]] [[2.759]
 [2.759]
 [2.759]
 [2.759]
 [2.759]
 [2.759]
 [2.759]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16656816084944745, 0.16164821850470892, 0.16548770914458874, 0.16302267858831843, 0.1733734704027994, 0.16989976251013716]
from probs:  [0.16632307302804955, 0.16142163659562653, 0.16560390063954505, 0.1631371393469746, 0.1734951986134079, 0.1700190517763964]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.64 ]
 [0.653]
 [0.733]
 [0.698]
 [0.721]
 [0.81 ]] [[2.218]
 [2.   ]
 [2.321]
 [2.151]
 [2.223]
 [2.158]
 [2.345]] [[0.853]
 [0.614]
 [0.963]
 [0.952]
 [0.953]
 [0.936]
 [1.3  ]]
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.712]
 [0.758]
 [0.752]
 [0.746]
 [0.757]
 [0.753]] [[2.701]
 [3.03 ]
 [2.656]
 [2.7  ]
 [2.638]
 [2.451]
 [2.863]] [[1.427]
 [1.684]
 [1.402]
 [1.435]
 [1.361]
 [1.195]
 [1.599]]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.252]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[-0.702]
 [ 0.176]
 [-0.702]
 [-0.702]
 [-0.702]
 [-0.702]
 [-0.702]] [[0.239]
 [0.252]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
in main func line 156:  1463
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16662368664125352, 0.16137419328214228, 0.16518921834357278, 0.1634319946789139, 0.17380877518206544, 0.16957213187205222]
siam score:  -0.6369767
first move QE:  0.8473789351035875
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.8559159854607348
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16686917326119352, 0.16127373469488349, 0.1654325915618656, 0.1633254988920088, 0.17327703915471696, 0.16982196243533174]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.7  ]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]] [[2.582]
 [3.198]
 [2.582]
 [2.582]
 [2.582]
 [2.582]
 [2.582]] [[1.996]
 [2.15 ]
 [1.996]
 [1.996]
 [1.996]
 [1.996]
 [1.996]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.36 ]
 [0.329]
 [0.333]
 [0.372]
 [0.38 ]
 [0.36 ]] [[1.624]
 [2.007]
 [1.832]
 [1.944]
 [1.76 ]
 [1.808]
 [1.955]] [[-0.105]
 [ 0.265]
 [ 0.027]
 [ 0.148]
 [ 0.042]
 [ 0.105]
 [ 0.213]]
start point for exploration sampling:  11106
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.03478437921374
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5339254056403098
line 256 mcts: sample exp_bonus 1.8644295239643254
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1671581563704829, 0.1612426803379268, 0.16608148043736135, 0.16259487635568856, 0.1739374147164434, 0.16898539178209707]
siam score:  -0.630706
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16685780143601472, 0.16130083067590126, 0.16614137583353372, 0.16265351434777098, 0.17400014326592528, 0.16904633444085393]
from probs:  [0.16685780143601472, 0.16130083067590126, 0.16614137583353372, 0.16265351434777098, 0.17400014326592528, 0.16904633444085393]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
start point for exploration sampling:  11106
siam score:  -0.6286961
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.630523
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.773]
 [0.831]
 [0.918]
 [0.877]
 [0.785]
 [0.778]] [[1.678]
 [1.614]
 [1.464]
 [1.148]
 [1.205]
 [1.1  ]
 [1.411]] [[2.155]
 [2.095]
 [2.106]
 [2.082]
 [2.054]
 [1.89 ]
 [2.015]]
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.681]
 [0.879]
 [0.893]
 [0.681]
 [0.898]
 [0.78 ]] [[3.06 ]
 [3.06 ]
 [2.475]
 [2.472]
 [3.06 ]
 [2.438]
 [2.925]] [[1.721]
 [1.721]
 [1.726]
 [1.754]
 [1.721]
 [1.739]
 [1.829]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16629912075822675, 0.16079822224098297, 0.16665603163455578, 0.1621374932852139, 0.17453914315479221, 0.1695699889262283]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16641363342204574, 0.16057750913590027, 0.16641363342204574, 0.1622491402751461, 0.1746593298529071, 0.16968675389195492]
line 256 mcts: sample exp_bonus 1.803954070407614
using another actor
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.378]
 [0.316]
 [0.287]
 [0.309]
 [0.337]
 [0.339]] [[1.931]
 [2.344]
 [2.115]
 [1.532]
 [1.48 ]
 [1.505]
 [1.744]] [[0.485]
 [1.091]
 [0.73 ]
 [0.026]
 [0.005]
 [0.081]
 [0.352]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16622342673422458, 0.16042010332486045, 0.16657851453737108, 0.16174716836728698, 0.17481221716526085, 0.17021856987099607]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.57 ]
 [0.596]
 [0.563]
 [0.584]
 [0.581]
 [0.646]] [[ 0.601]
 [ 0.952]
 [-0.115]
 [ 0.34 ]
 [ 0.193]
 [ 0.296]
 [-0.735]] [[0.573]
 [0.57 ]
 [0.596]
 [0.563]
 [0.584]
 [0.581]
 [0.646]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.6211715
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16630626785787736, 0.16118411702236338, 0.16701507922813705, 0.1618479359955866, 0.17446668594112355, 0.16917991395491214]
1493 1869
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.036]
 [-0.036]
 [-0.035]
 [-0.036]
 [-0.033]
 [-0.034]] [[4.078]
 [4.139]
 [4.139]
 [4.112]
 [4.139]
 [4.324]
 [4.077]] [[-0.667]
 [-0.648]
 [-0.648]
 [-0.656]
 [-0.648]
 [-0.581]
 [-0.665]]
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]] [[3.91]
 [3.91]
 [3.91]
 [3.91]
 [3.91]
 [3.91]
 [3.91]] [[-0.65]
 [-0.65]
 [-0.65]
 [-0.65]
 [-0.65]
 [-0.65]
 [-0.65]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16624311249558885, 0.16081753356154335, 0.16694837760647227, 0.16247283209139426, 0.1751403031251949, 0.1683778411198064]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1664055818488392, 0.1599974007419964, 0.16711153621514324, 0.16263161675052615, 0.1753114677006751, 0.16854239674281993]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8524220427498981
1500 1883
1500 1884
from probs:  [0.16678363741020866, 0.16036089760600256, 0.1671366338223634, 0.16300109826961903, 0.17415335465242396, 0.16856437823938233]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [1.177]
 [0.994]] [[1.533]
 [1.533]
 [1.533]
 [1.533]
 [1.533]
 [1.455]
 [1.533]] [[0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]
 [1.082]
 [0.769]]
line 256 mcts: sample exp_bonus 3.9763518963026336
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.489]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[-1.867]
 [ 2.049]
 [-1.867]
 [-1.867]
 [-1.867]
 [-1.867]
 [-1.867]] [[0.178]
 [1.099]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.424]
 [0.605]
 [0.535]
 [0.583]
 [0.627]
 [0.751]] [[2.1  ]
 [2.68 ]
 [2.119]
 [2.144]
 [2.3  ]
 [2.299]
 [2.269]] [[0.755]
 [0.712]
 [0.7  ]
 [0.578]
 [0.776]
 [0.865]
 [1.093]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1668434382983056, 0.1604325926162038, 0.16719574804707224, 0.16306804157120028, 0.17419810724721216, 0.1682620722200058]
first move QE:  0.855335890839699
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.241]
 [0.203]
 [0.201]
 [0.201]
 [0.203]
 [0.207]] [[0.44 ]
 [0.653]
 [0.206]
 [0.529]
 [0.292]
 [0.404]
 [0.582]] [[-0.487]
 [-0.352]
 [-0.577]
 [-0.474]
 [-0.552]
 [-0.511]
 [-0.444]]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.623]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[2.006]
 [2.996]
 [2.05 ]
 [2.05 ]
 [2.05 ]
 [2.05 ]
 [2.05 ]] [[1.621]
 [2.259]
 [1.774]
 [1.774]
 [1.774]
 [1.774]
 [1.774]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16672382186405144, 0.16033169922574245, 0.1674278688329276, 0.16329443178984834, 0.17443994953100847, 0.16778222875642165]
Starting evaluation
line 256 mcts: sample exp_bonus 0.839029950448673
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.56 ]
 [1.103]
 [0.551]
 [0.513]
 [0.562]
 [0.651]] [[ 0.182]
 [ 0.351]
 [-0.371]
 [ 0.199]
 [ 0.367]
 [ 0.446]
 [ 0.511]] [[0.453]
 [0.511]
 [1.115]
 [0.392]
 [0.427]
 [0.578]
 [0.8  ]]
siam score:  -0.63331866
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.315]
 [0.707]
 [0.388]
 [0.385]
 [0.388]
 [0.422]] [[-0.824]
 [ 1.234]
 [-0.583]
 [-0.774]
 [-0.648]
 [-0.774]
 [-0.48 ]] [[0.413]
 [0.315]
 [0.707]
 [0.388]
 [0.385]
 [0.388]
 [0.422]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16648598785941204, 0.1601170308770336, 0.16754045093596479, 0.16340423448084185, 0.17455724670804834, 0.16789504913869932]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.669]
 [0.648]
 [0.648]
 [0.648]
 [0.663]
 [0.913]] [[2.48 ]
 [1.738]
 [2.48 ]
 [2.48 ]
 [2.48 ]
 [2.437]
 [2.42 ]] [[1.609]
 [0.907]
 [1.609]
 [1.609]
 [1.609]
 [1.589]
 [1.94 ]]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.269]
 [0.827]
 [0.354]
 [0.354]
 [0.354]
 [0.414]] [[-0.764]
 [ 1.261]
 [ 0.005]
 [-1.219]
 [-1.219]
 [-1.219]
 [-0.496]] [[0.543]
 [0.269]
 [0.827]
 [0.354]
 [0.354]
 [0.354]
 [0.414]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.166718623316831, 0.1600191942186896, 0.16777455982369038, 0.16329702648129954, 0.17441603633579902, 0.16777455982369038]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.28 ]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]] [[0.271]
 [0.706]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]] [[0.261]
 [0.28 ]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.322]
 [0.325]
 [0.322]
 [0.322]
 [0.321]
 [0.323]] [[-1.891]
 [-1.336]
 [-1.484]
 [-1.336]
 [-1.336]
 [-1.036]
 [-1.018]] [[0.327]
 [0.322]
 [0.325]
 [0.322]
 [0.322]
 [0.321]
 [0.323]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8774659096934831
siam score:  -0.64385176
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.546]
 [0.95 ]] [[1.215]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.11 ]
 [0.105]] [[0.546]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.546]
 [0.95 ]]
using explorer policy with actor:  0
siam score:  -0.6428939
siam score:  -0.6434299
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]] [[-0.631]
 [-0.631]
 [-0.631]
 [-0.631]
 [-0.631]
 [-0.631]
 [-0.631]] [[0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]]
siam score:  -0.6430588
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.343]
 [0.565]
 [0.406]
 [0.404]
 [0.404]
 [0.485]] [[-0.721]
 [ 1.202]
 [-0.245]
 [-0.557]
 [-0.533]
 [-0.665]
 [-0.552]] [[0.605]
 [0.343]
 [0.565]
 [0.406]
 [0.404]
 [0.404]
 [0.485]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.934746527908894
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[2.43]
 [2.43]
 [2.43]
 [2.43]
 [2.43]
 [2.43]
 [2.43]] [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]]
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.373]
 [0.416]
 [0.373]
 [0.412]
 [0.423]
 [0.551]] [[-1.817]
 [-1.753]
 [-1.635]
 [-1.753]
 [-1.569]
 [-1.394]
 [-1.539]] [[0.406]
 [0.373]
 [0.416]
 [0.373]
 [0.412]
 [0.423]
 [0.551]]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.608]
 [0.608]
 [0.644]
 [0.608]
 [0.677]
 [0.608]] [[1.582]
 [1.582]
 [1.582]
 [1.943]
 [1.582]
 [1.703]
 [1.582]] [[0.608]
 [0.608]
 [0.608]
 [0.644]
 [0.608]
 [0.677]
 [0.608]]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.557]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[1.809]
 [1.578]
 [1.809]
 [1.809]
 [1.809]
 [1.809]
 [1.809]] [[0.266]
 [0.557]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6154881889854258
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.312]
 [0.392]
 [0.393]
 [0.392]
 [0.504]
 [0.385]] [[3.374]
 [2.697]
 [2.417]
 [1.914]
 [2.043]
 [2.542]
 [2.851]] [[0.382]
 [0.312]
 [0.392]
 [0.393]
 [0.392]
 [0.504]
 [0.385]]
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.292]
 [0.286]
 [0.286]
 [0.27 ]
 [0.221]
 [0.293]] [[2.248]
 [1.555]
 [1.925]
 [2.32 ]
 [2.413]
 [2.64 ]
 [2.336]] [[0.719]
 [0.254]
 [0.506]
 [0.781]
 [0.834]
 [0.958]
 [0.797]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1663781778215852, 0.1603621550148091, 0.16777985518625746, 0.16364701248441263, 0.17440567410504076, 0.16742712538789487]
line 256 mcts: sample exp_bonus -2.6283755091899152
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.352]
 [0.377]
 [0.493]
 [0.493]
 [0.321]
 [0.366]] [[1.974]
 [2.67 ]
 [1.59 ]
 [1.841]
 [1.841]
 [1.732]
 [2.076]] [[1.733]
 [2.108]
 [1.561]
 [1.783]
 [1.783]
 [1.591]
 [1.807]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.519]
 [0.574]
 [0.674]
 [0.597]
 [0.544]
 [0.599]] [[1.924]
 [1.924]
 [1.722]
 [1.661]
 [1.583]
 [1.996]
 [2.238]] [[0.519]
 [0.519]
 [0.574]
 [0.674]
 [0.597]
 [0.544]
 [0.599]]
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.35 ]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]] [[3.403]
 [3.308]
 [3.403]
 [3.403]
 [3.403]
 [3.403]
 [3.403]] [[-0.238]
 [-0.139]
 [-0.238]
 [-0.238]
 [-0.238]
 [-0.238]
 [-0.238]]
line 256 mcts: sample exp_bonus -1.6401035601399871
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.394]
 [0.394]
 [0.398]
 [0.394]
 [0.397]
 [0.405]] [[2.64 ]
 [2.64 ]
 [2.64 ]
 [2.11 ]
 [2.64 ]
 [2.421]
 [2.487]] [[0.394]
 [0.394]
 [0.394]
 [0.398]
 [0.394]
 [0.397]
 [0.405]]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.553]
 [0.523]
 [0.529]
 [0.525]
 [0.595]
 [0.526]] [[1.958]
 [1.904]
 [1.795]
 [1.573]
 [1.518]
 [1.899]
 [2.236]] [[0.516]
 [0.553]
 [0.523]
 [0.529]
 [0.525]
 [0.595]
 [0.526]]
using explorer policy with actor:  0
siam score:  -0.64395326
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16677597702560273, 0.16042389242678523, 0.16818100570802144, 0.16270438326931702, 0.17443933140671256, 0.16747541016356116]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.36 ]
 [0.413]
 [0.417]
 [0.409]
 [0.413]
 [0.425]] [[3.623]
 [3.072]
 [2.929]
 [2.906]
 [3.249]
 [2.409]
 [2.326]] [[0.42 ]
 [0.36 ]
 [0.413]
 [0.417]
 [0.409]
 [0.413]
 [0.425]]
Printing some Q and Qe and total Qs values:  [[0.7  ]
 [0.672]
 [0.661]
 [0.66 ]
 [0.661]
 [0.696]
 [0.697]] [[1.611]
 [2.238]
 [1.696]
 [1.714]
 [1.838]
 [1.587]
 [1.409]] [[0.7  ]
 [0.672]
 [0.661]
 [0.66 ]
 [0.661]
 [0.696]
 [0.697]]
siam score:  -0.6419842
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16690492246246322, 0.1605618324511409, 0.16866239813218967, 0.16317009969879773, 0.17379582479294522, 0.16690492246246322]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.543]
 [0.545]] [[0.688]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [1.15 ]
 [0.889]] [[0.704]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.845]
 [0.762]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.2209256104940907
1520 1912
rdn probs:  [0.16666915002562702, 0.16034884842133787, 0.1687749402888692, 0.1632789770486343, 0.17391179229465692, 0.16701629192087464]
using explorer policy with actor:  1
siam score:  -0.6435092
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.086]
 [0.111]
 [0.105]
 [0.117]
 [0.129]
 [0.136]] [[1.608]
 [1.452]
 [1.666]
 [1.362]
 [1.812]
 [1.575]
 [1.356]] [[0.375]
 [0.185]
 [0.514]
 [0.103]
 [0.72 ]
 [0.431]
 [0.157]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.7966819679133124
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]] [[-2.372]
 [-2.372]
 [-2.372]
 [-2.372]
 [-2.372]
 [-2.372]
 [-2.372]] [[0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16699651855219305, 0.1597095389303216, 0.16910644497054025, 0.16293759230672844, 0.17425338668802365, 0.16699651855219305]
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.223]
 [0.219]
 [0.245]
 [0.219]
 [0.219]
 [0.245]] [[-0.822]
 [-0.203]
 [-0.751]
 [ 0.   ]
 [-0.689]
 [-0.638]
 [ 0.   ]] [[0.22 ]
 [0.223]
 [0.219]
 [0.245]
 [0.219]
 [0.219]
 [0.245]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16699651855219305, 0.1597095389303216, 0.16910644497054025, 0.16293759230672844, 0.17425338668802365, 0.16699651855219305]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [ 0.547]] [[2.833]
 [2.833]
 [2.833]
 [2.833]
 [2.833]
 [2.833]
 [6.824]] [[-0.248]
 [-0.248]
 [-0.248]
 [-0.248]
 [-0.248]
 [-0.248]
 [ 1.534]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16675989343957695, 0.15982970217120285, 0.16921211496530933, 0.1630530469471908, 0.17472961339820717, 0.166415629078513]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.475]
 [0.532]
 [0.533]
 [0.533]
 [0.541]
 [0.536]] [[-1.998]
 [ 1.088]
 [-1.776]
 [-1.765]
 [-1.697]
 [-1.602]
 [-1.763]] [[0.202]
 [1.45 ]
 [0.292]
 [0.297]
 [0.325]
 [0.368]
 [0.299]]
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.612]] [[0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.463]] [[0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.897]]
first move QE:  0.8618541623212801
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.699]
 [0.574]
 [0.768]
 [0.619]
 [0.59 ]
 [0.647]] [[1.708]
 [1.684]
 [1.778]
 [1.501]
 [1.26 ]
 [1.592]
 [1.505]] [[0.991]
 [1.142]
 [0.925]
 [1.219]
 [0.842]
 [0.894]
 [0.978]]
from probs:  [0.16681847630817018, 0.15990083540816147, 0.16962173173096806, 0.1627906408848869, 0.17439344960139244, 0.166474866066421]
Printing some Q and Qe and total Qs values:  [[0.829]
 [1.214]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[0.43 ]
 [2.426]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]] [[1.708]
 [3.034]
 [1.708]
 [1.708]
 [1.708]
 [1.708]
 [1.708]]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.44 ]
 [0.546]
 [0.546]
 [0.547]
 [0.558]
 [1.349]] [[1.475]
 [2.386]
 [1.138]
 [1.305]
 [1.66 ]
 [1.225]
 [4.964]] [[0.444]
 [0.611]
 [0.495]
 [0.527]
 [0.596]
 [0.526]
 [2.123]]
Printing some Q and Qe and total Qs values:  [[1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.363]] [[0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [1.45 ]] [[1.474]
 [1.474]
 [1.474]
 [1.474]
 [1.474]
 [1.474]
 [2.388]]
line 256 mcts: sample exp_bonus 3.923103737575707
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.166420088730388, 0.1598765168528807, 0.17026425743996537, 0.1627532192065773, 0.17392387463845987, 0.1667620431317287]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.37 ]
 [0.452]
 [0.461]
 [0.461]
 [0.463]
 [0.478]] [[0.843]
 [1.21 ]
 [0.84 ]
 [1.108]
 [1.108]
 [0.849]
 [0.935]] [[0.246]
 [0.196]
 [0.236]
 [0.344]
 [0.344]
 [0.262]
 [0.32 ]]
line 256 mcts: sample exp_bonus 1.1356657173972966
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[1.987]
 [1.987]
 [1.987]
 [1.987]
 [1.987]
 [1.987]
 [1.987]] [[2.072]
 [2.072]
 [2.072]
 [2.072]
 [2.072]
 [2.072]
 [2.072]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.63820374
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]] [[3.75 ]
 [3.412]
 [3.412]
 [3.412]
 [3.412]
 [3.412]
 [3.412]] [[1.1  ]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]]
1534 1952
first move QE:  0.8547675801436082
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16591757355637832, 0.16070051729976678, 0.1704254183907973, 0.1632653880535402, 0.17444500407447205, 0.16524609862504536]
from probs:  [0.16591757355637832, 0.16070051729976678, 0.1704254183907973, 0.1632653880535402, 0.17444500407447205, 0.16524609862504536]
line 256 mcts: sample exp_bonus -0.3966281521912588
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.261]
 [0.211]
 [0.211]
 [0.208]
 [0.211]
 [0.211]] [[-2.668]
 [ 0.396]
 [-2.145]
 [-2.145]
 [-2.787]
 [-2.145]
 [-2.145]] [[0.211]
 [0.261]
 [0.211]
 [0.211]
 [0.208]
 [0.211]
 [0.211]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16604135999526037, 0.16082041143958967, 0.17055256800602728, 0.16338719577135244, 0.17382908069241876, 0.16536938409535146]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.61 ]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.594]] [[2.27 ]
 [2.661]
 [2.38 ]
 [2.38 ]
 [2.38 ]
 [2.38 ]
 [2.279]] [[0.647]
 [0.61 ]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.594]]
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]] [[2.025]
 [2.025]
 [2.025]
 [2.025]
 [2.025]
 [2.025]
 [2.025]] [[0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]]
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.254]
 [0.274]
 [0.275]
 [0.273]
 [0.272]
 [0.273]] [[1.129]
 [1.339]
 [1.243]
 [1.143]
 [0.945]
 [0.917]
 [1.241]] [[0.271]
 [0.254]
 [0.274]
 [0.275]
 [0.273]
 [0.272]
 [0.273]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16615701863048765, 0.16093243334367624, 0.17067136899182173, 0.16317524200672548, 0.17357936237174568, 0.16548457465554317]
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.747]] [[1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.528]] [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.747]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16639448283267047, 0.16116243080152834, 0.17055841943824104, 0.1634084447867133, 0.17308971127717807, 0.16538651086366873]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.165]
 [0.159]
 [0.159]
 [0.155]
 [0.161]
 [0.16 ]] [[-0.814]
 [-0.822]
 [-1.077]
 [-1.784]
 [-1.675]
 [-1.098]
 [-0.918]] [[0.16 ]
 [0.165]
 [0.159]
 [0.159]
 [0.155]
 [0.161]
 [0.16 ]]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.603]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[0.584]
 [1.615]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[1.068]
 [1.503]
 [1.068]
 [1.068]
 [1.068]
 [1.068]
 [1.068]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1662236756422586, 0.1613241300413748, 0.17072954596342074, 0.16357239752169683, 0.1732633775255684, 0.1648868733056806]
line 256 mcts: sample exp_bonus 2.8225672043710284
from probs:  [0.16627774847301222, 0.16137660904201354, 0.17078508456108352, 0.16330030626868053, 0.17331974038273984, 0.16494051127247025]
1541 1964
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16655632727016484, 0.16133043936512023, 0.1703600068227904, 0.1629262596473272, 0.17361011720851496, 0.16521684968608236]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.501]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]] [[1.433]
 [2.57 ]
 [1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]] [[0.496]
 [1.155]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16685102248281788, 0.1613000905158248, 0.17030751705968736, 0.1632145320265305, 0.17281766301121573, 0.1655091749039237]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.15 ]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[0.986]
 [2.435]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [1.09 ]] [[0.735]
 [1.968]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]]
Printing some Q and Qe and total Qs values:  [[0.954]
 [0.688]
 [0.73 ]
 [0.696]
 [0.779]
 [0.808]
 [0.709]] [[1.642]
 [2.243]
 [1.924]
 [2.058]
 [1.882]
 [1.775]
 [2.135]] [[0.691]
 [0.559]
 [0.43 ]
 [0.451]
 [0.5  ]
 [0.487]
 [0.528]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1670735881928897, 0.16120032613844265, 0.170534693452741, 0.16310944030520907, 0.17268438212622228, 0.16539756978449532]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16734110133553984, 0.16083143004374578, 0.17080774841175553, 0.16272795902009732, 0.17296087910129082, 0.16533088208757074]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.02  0.02  0.    0.898 0.02 ]
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.036]
 [-0.04 ]
 [-0.039]
 [-0.032]
 [-0.036]
 [-0.042]] [[2.261]
 [2.378]
 [2.441]
 [2.212]
 [2.283]
 [2.464]
 [2.246]] [[-1.17 ]
 [-1.106]
 [-1.095]
 [-1.168]
 [-1.131]
 [-1.079]
 [-1.163]]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.079]
 [-0.048]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.047]] [[1.629]
 [1.464]
 [1.618]
 [1.606]
 [1.74 ]
 [1.687]
 [1.72 ]] [[-0.971]
 [-1.101]
 [-0.988]
 [-0.992]
 [-0.949]
 [-0.967]
 [-0.95 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.348]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.321]] [[ 0.003]
 [ 0.453]
 [ 0.003]
 [ 0.003]
 [ 0.003]
 [ 0.003]
 [-0.441]] [[0.33 ]
 [0.348]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.321]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.62433046
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.311]
 [0.279]
 [0.279]
 [0.282]
 [0.279]
 [0.286]] [[1.797]
 [2.368]
 [1.362]
 [1.362]
 [1.63 ]
 [1.362]
 [1.904]] [[0.281]
 [0.311]
 [0.279]
 [0.279]
 [0.282]
 [0.279]
 [0.286]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16795602482356595, 0.1599053696340719, 0.17071506226657193, 0.1633458262214179, 0.17213049036911185, 0.16594722668526046]
siam score:  -0.6261943
in main func line 156:  1552
using another actor
using explorer policy with actor:  0
siam score:  -0.6315872
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.371]
 [0.568]
 [0.497]
 [0.43 ]
 [0.211]
 [0.426]] [[1.858]
 [2.77 ]
 [1.858]
 [2.337]
 [1.886]
 [2.18 ]
 [2.554]] [[1.055]
 [1.268]
 [1.055]
 [1.231]
 [0.797]
 [0.554]
 [1.233]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1680476750270777, 0.1600263790484522, 0.1707958706514462, 0.16281994648703665, 0.17291961184706717, 0.1653905169389202]
from probs:  [0.1680476750270777, 0.1600263790484522, 0.1707958706514462, 0.16281994648703665, 0.17291961184706717, 0.1653905169389202]
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.316]
 [0.306]
 [0.306]
 [0.216]
 [0.288]
 [0.166]] [[1.005]
 [1.835]
 [1.612]
 [1.631]
 [1.916]
 [1.907]
 [1.333]] [[0.265]
 [0.316]
 [0.306]
 [0.306]
 [0.216]
 [0.288]
 [0.166]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16748768948003118, 0.16013409250356664, 0.1709108330311272, 0.16292954028714604, 0.17303600371295733, 0.1655018409851717]
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.511]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.495]] [[2.415]
 [2.657]
 [2.415]
 [2.415]
 [2.415]
 [2.415]
 [2.689]] [[0.485]
 [0.511]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.495]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.013]
 [0.02 ]
 [0.023]
 [0.321]
 [0.017]
 [0.321]] [[1.78 ]
 [1.14 ]
 [1.043]
 [0.864]
 [1.645]
 [1.581]
 [1.645]] [[ 0.807]
 [-0.032]
 [-0.148]
 [-0.382]
 [ 1.256]
 [ 0.563]
 [ 1.256]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19730633831024194, 0.009020686756565546, 0.2017318557612899, 0.1919594841079063, 0.2042345982158967, 0.19574703684809971]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.0134368947080268
siam score:  -0.6207635
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.452]
 [0.451]
 [0.462]
 [0.462]
 [0.462]
 [0.387]] [[2.037]
 [1.65 ]
 [1.665]
 [2.037]
 [2.037]
 [2.037]
 [1.604]] [[2.168]
 [1.92 ]
 [1.927]
 [2.168]
 [2.168]
 [2.168]
 [1.831]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.364]
 [0.347]
 [0.355]
 [0.386]
 [0.39 ]
 [0.361]] [[0.703]
 [1.845]
 [0.879]
 [0.818]
 [1.211]
 [1.151]
 [1.369]] [[0.615]
 [0.364]
 [0.347]
 [0.355]
 [0.386]
 [0.39 ]
 [0.361]]
Printing some Q and Qe and total Qs values:  [[0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]] [[0.845]
 [0.835]
 [0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.845]] [[0.019]
 [0.015]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.455]
 [0.467]
 [0.468]
 [0.47 ]
 [0.47 ]
 [0.469]] [[2.786]
 [2.619]
 [2.535]
 [2.451]
 [2.463]
 [2.641]
 [2.827]] [[0.464]
 [0.455]
 [0.467]
 [0.468]
 [0.47 ]
 [0.47 ]
 [0.469]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.4  ]
 [0.468]
 [0.463]
 [0.584]
 [0.715]
 [0.473]] [[1.14 ]
 [2.079]
 [1.608]
 [1.523]
 [1.417]
 [1.302]
 [1.868]] [[0.506]
 [0.4  ]
 [0.468]
 [0.463]
 [0.584]
 [0.715]
 [0.473]]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.798]
 [0.556]] [[1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.634]
 [1.377]] [[0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.798]
 [0.556]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[2.463]
 [2.463]
 [2.463]
 [2.463]
 [2.463]
 [2.463]
 [2.463]] [[0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.452]
 [0.387]
 [0.42 ]
 [0.387]
 [0.387]
 [0.429]] [[2.633]
 [2.26 ]
 [1.725]
 [1.769]
 [1.725]
 [1.725]
 [2.251]] [[0.851]
 [0.669]
 [0.183]
 [0.279]
 [0.183]
 [0.183]
 [0.617]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19776458226855526, 0.009077656566791622, 0.201770674109357, 0.19169015407694914, 0.20425703562603528, 0.19543989735231174]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.282]
 [0.408]
 [0.402]
 [0.437]
 [0.429]
 [0.4  ]] [[2.542]
 [2.157]
 [2.639]
 [2.709]
 [2.724]
 [2.77 ]
 [2.802]] [[1.653]
 [0.586]
 [1.321]
 [1.388]
 [1.461]
 [1.498]
 [1.486]]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.252]
 [0.384]
 [0.329]
 [0.406]
 [0.393]
 [0.398]] [[2.376]
 [1.87 ]
 [2.52 ]
 [2.499]
 [2.52 ]
 [2.499]
 [2.555]] [[ 0.911]
 [-0.117]
 [ 1.014]
 [ 0.875]
 [ 1.059]
 [ 1.003]
 [ 1.088]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24182013673328953, 0.011143891320273802, 0.24620006544366324, 0.011143891320273802, 0.25023536457024503, 0.23945665061225457]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24182013673328953, 0.011143891320273802, 0.24620006544366324, 0.011143891320273802, 0.25023536457024503, 0.23945665061225457]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.368]
 [0.944]] [[1.7  ]
 [1.7  ]
 [1.7  ]
 [1.7  ]
 [1.7  ]
 [1.211]
 [0.834]] [[0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.354]
 [1.38 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24181515210904153, 0.011165668050392034, 0.24618535285962645, 0.011165668050392034, 0.25021138600411014, 0.23945677292643777]
using another actor
from probs:  [0.24181515210904153, 0.011165668050392034, 0.24618535285962645, 0.011165668050392034, 0.25021138600411014, 0.23945677292643777]
siam score:  -0.6526547
rdn beta is 0 so we're just using the maxi policy
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24205830458548513, 0.011176895468582835, 0.24593866440257597, 0.011176895468582835, 0.24995168609375532, 0.23969755398101786]
using another actor
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.226]
 [0.137]
 [0.138]
 [0.157]
 [0.19 ]
 [0.466]] [[-0.603]
 [ 1.204]
 [-1.115]
 [-1.008]
 [-1.009]
 [-0.999]
 [-5.234]] [[0.236]
 [0.226]
 [0.137]
 [0.138]
 [0.157]
 [0.19 ]
 [0.466]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]] [[1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]]
first move QE:  0.8396618983432099
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[-1.735]
 [-1.735]
 [-1.735]
 [-1.735]
 [-1.735]
 [-1.735]
 [-1.735]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.466]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.486]] [[1.402]
 [0.753]
 [0.927]
 [0.927]
 [0.927]
 [0.927]
 [0.772]] [[1.643]
 [1.417]
 [1.514]
 [1.514]
 [1.514]
 [1.514]
 [1.439]]
first move QE:  0.8385442850253472
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2431249549525829, 0.011226147402636461, 0.24457130926162624, 0.011226147402636461, 0.25003257900726217, 0.23981886197325578]
siam score:  -0.65408623
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.363]
 [0.363]
 [0.39 ]
 [0.535]
 [0.792]
 [0.387]] [[1.331]
 [1.331]
 [1.331]
 [1.21 ]
 [0.983]
 [1.058]
 [1.248]] [[-0.442]
 [-0.442]
 [-0.442]
 [-0.43 ]
 [-0.214]
 [ 0.325]
 [-0.422]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24263794875036204, 0.011247810183889253, 0.24504325164606341, 0.011247810183889253, 0.2500070614435618, 0.23981611779223433]
first move QE:  0.8369862062594327
from probs:  [0.24276074827259253, 0.011253502721778871, 0.24516726849661347, 0.011253502721778871, 0.2496274886067268, 0.23993748918050942]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24299592555071822, 0.011264404682488419, 0.24540477711899503, 0.011264404682488419, 0.24936486798472873, 0.23970561998058107]
first move QE:  0.8340973513080377
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.56 ]
 [0.486]
 [0.486]
 [0.148]
 [0.486]
 [0.465]] [[0.877]
 [0.869]
 [0.868]
 [0.868]
 [1.268]
 [0.868]
 [1.013]] [[1.669]
 [1.817]
 [1.766]
 [1.766]
 [1.716]
 [1.766]
 [1.819]]
using another actor
from probs:  [0.24299168776630964, 0.011286309090468469, 0.24588198355020788, 0.011286309090468469, 0.24884529733285718, 0.23970841316968844]
from probs:  [0.2432339620385447, 0.011297562077540778, 0.2461271395848358, 0.011297562077540778, 0.24809636036057833, 0.23994741386095964]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.176]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]] [[0.163]
 [0.126]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]] [[0.167]
 [0.176]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24335451864434324, 0.011303161607008012, 0.24624913016663852, 0.011303161607008012, 0.24772368645466963, 0.24006634152033243]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[1.844]
 [1.844]
 [1.844]
 [1.844]
 [1.844]
 [1.844]
 [1.844]] [[0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]]
using another actor
using explorer policy with actor:  1
from probs:  [0.24335451864434324, 0.011303161607008012, 0.24624913016663852, 0.011303161607008012, 0.24772368645466963, 0.24006634152033243]
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.023]
 [0.23 ]
 [0.119]
 [0.119]
 [0.119]
 [0.033]] [[1.223]
 [1.773]
 [0.818]
 [1.175]
 [1.175]
 [1.175]
 [1.347]] [[0.105]
 [0.023]
 [0.23 ]
 [0.119]
 [0.119]
 [0.119]
 [0.033]]
siam score:  -0.66711485
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.24274800737290472, 0.011319135546241714, 0.24659713621825463, 0.011319135546241714, 0.24807377638983627, 0.23994280892652092]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6677473
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.496]
 [0.554]
 [0.449]
 [0.439]
 [0.445]
 [0.448]] [[-3.164]
 [-3.397]
 [-0.342]
 [-3.395]
 [-2.946]
 [-3.416]
 [-3.152]] [[0.149]
 [0.081]
 [1.158]
 [0.065]
 [0.217]
 [0.057]
 [0.149]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24404420622314973, 0.011379576209131553, 0.24693445007425552, 0.011379576209131553, 0.24596308976965708, 0.24029910151467457]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]] [[2.455]
 [2.425]
 [2.425]
 [2.425]
 [2.425]
 [2.425]
 [2.425]] [[0.475]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24391603077888788, 0.01139577173360773, 0.24679851332903593, 0.01139577173360773, 0.2463131468515196, 0.24018076557334117]
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.095]
 [0.116]
 [0.116]
 [0.116]
 [0.139]
 [0.105]] [[-2.511]
 [-0.01 ]
 [-2.137]
 [-2.137]
 [-2.137]
 [-1.914]
 [-1.811]] [[0.132]
 [0.095]
 [0.116]
 [0.116]
 [0.116]
 [0.139]
 [0.105]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]] [[-0.14]
 [-0.14]
 [-0.14]
 [-0.14]
 [-0.14]
 [-0.14]
 [-0.14]] [[0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8274346782130685
using explorer policy with actor:  1
from probs:  [0.31585490671127625, 0.01478546646169225, 0.32020921679753056, 0.01478546646169225, 0.31957947710611645, 0.01478546646169225]
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.543]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[0.495]
 [2.01 ]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]] [[1.845]
 [2.163]
 [1.845]
 [1.845]
 [1.845]
 [1.845]
 [1.845]]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.618]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[0.546]
 [2.368]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[1.352]
 [1.963]
 [1.352]
 [1.352]
 [1.352]
 [1.352]
 [1.352]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.31585490671127625, 0.01478546646169225, 0.32020921679753056, 0.01478546646169225, 0.31957947710611645, 0.01478546646169225]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
from probs:  [0.315634072166393, 0.014803809535317866, 0.3206064732016174, 0.014803809535317866, 0.3193480260260359, 0.014803809535317866]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.3154152121211707, 0.014822199799210482, 0.3203734473759659, 0.014822199799210482, 0.3197447411052319, 0.014822199799210482]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.127]
 [0.703]
 [0.541]
 [0.344]
 [0.579]
 [0.331]] [[ 0.945]
 [ 1.37 ]
 [ 1.689]
 [-0.757]
 [ 1.066]
 [-0.65 ]
 [ 1.204]] [[1.277]
 [1.238]
 [1.956]
 [0.604]
 [1.301]
 [0.694]
 [1.357]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.31598742999406637, 0.01487779154873138, 0.3196895976798697, 0.01487779154873138, 0.3196895976798697, 0.01487779154873138]
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.434]
 [0.493]
 [0.493]
 [0.493]
 [1.012]
 [0.493]] [[0.53 ]
 [0.564]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [3.935]
 [0.53 ]] [[0.656]
 [0.551]
 [0.656]
 [0.656]
 [0.656]
 [2.779]
 [0.656]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.31557109031419356, 0.014886847252152969, 0.3198841839646737, 0.014886847252152969, 0.3198841839646737, 0.014886847252152969]
Printing some Q and Qe and total Qs values:  [[1.125]
 [1.431]
 [1.125]
 [1.125]
 [1.125]
 [1.125]
 [1.125]] [[0.614]
 [0.928]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[1.981]
 [2.698]
 [1.981]
 [1.981]
 [1.981]
 [1.981]
 [1.981]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.31635794971082026, 0.014923966798290228, 0.3200565261882746, 0.014923966798290228, 0.3188136237060344, 0.014923966798290228]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.406]
 [0.474]
 [0.469]
 [0.469]
 [0.475]
 [0.469]] [[-0.298]
 [ 0.895]
 [-0.495]
 [ 0.665]
 [ 0.665]
 [-0.345]
 [ 0.665]] [[0.386]
 [0.646]
 [0.317]
 [0.695]
 [0.695]
 [0.369]
 [0.695]]
1618 2141
deleting a thread, now have 3 threads
Frames:  107339 train batches done:  12577 episodes:  3759
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3171407174620681, 0.014960893323895398, 0.3202241850553148, 0.014960893323895398, 0.31775241751093086, 0.014960893323895398]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 0.36699789529451227
from probs:  [0.3167244430918576, 0.014970013557895942, 0.32041939529023267, 0.014970013557895942, 0.3179461209442218, 0.014970013557895942]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1623 2151
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.31731265733629943, 0.014997815564991706, 0.3197705015783426, 0.014997815564991706, 0.31792339439038286, 0.014997815564991706]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]] [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.31731265733629943, 0.014997815564991706, 0.3197705015783426, 0.014997815564991706, 0.31792339439038286, 0.014997815564991706]
UNIT TEST: sample policy line 217 mcts : [0.265 0.306 0.02  0.224 0.102 0.02  0.061]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.597]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]] [[0.35 ]
 [1.263]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[0.265]
 [0.85 ]
 [0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.265]]
siam score:  -0.6608577
1631 2170
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.467]
 [0.546]
 [0.543]
 [0.541]
 [0.551]
 [0.548]] [[-1.848]
 [ 0.512]
 [-1.577]
 [-1.578]
 [-1.333]
 [-1.373]
 [-1.348]] [[0.584]
 [1.796]
 [0.734]
 [0.73 ]
 [0.863]
 [0.852]
 [0.862]]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[8.755]
 [2.534]
 [2.534]
 [2.534]
 [2.534]
 [2.534]
 [2.534]] [[1.755]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.204]
 [1.154]
 [1.189]
 [1.208]
 [1.158]
 [1.202]
 [1.232]] [[0.821]
 [0.848]
 [0.824]
 [0.819]
 [1.168]
 [0.827]
 [0.872]] [[2.15 ]
 [2.088]
 [2.129]
 [2.156]
 [2.256]
 [2.15 ]
 [2.218]]
siam score:  -0.6582604
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.021667170583218126, 0.021667170583218126, 0.4575391497738255, 0.021667170583218126, 0.455792167893302, 0.021667170583218126]
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.376]
 [0.459]
 [0.64 ]
 [0.481]
 [0.463]
 [0.481]] [[1.146]
 [1.491]
 [0.886]
 [1.808]
 [1.103]
 [0.908]
 [1.036]] [[0.392]
 [0.339]
 [0.303]
 [0.971]
 [0.418]
 [0.318]
 [0.395]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.021667170583218126, 0.021667170583218126, 0.4575391497738255, 0.021667170583218126, 0.455792167893302, 0.021667170583218126]
Printing some Q and Qe and total Qs values:  [[1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]] [[0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]] [[1.85]
 [1.85]
 [1.85]
 [1.85]
 [1.85]
 [1.85]
 [1.85]]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.163]
 [0.135]
 [0.139]
 [0.133]
 [0.136]
 [0.135]] [[-2.525]
 [-2.063]
 [-2.44 ]
 [-2.437]
 [-2.435]
 [-2.382]
 [-2.276]] [[0.134]
 [0.163]
 [0.135]
 [0.139]
 [0.133]
 [0.136]
 [0.135]]
siam score:  -0.65975744
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.02172398383820691, 0.02172398383820691, 0.456116769011789, 0.02172398383820691, 0.4569872956353833, 0.02172398383820691]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.847]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]] [[1.368]
 [0.992]
 [1.545]
 [1.545]
 [1.545]
 [1.545]
 [1.545]] [[0.735]
 [0.847]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
siam score:  -0.6595328
Printing some Q and Qe and total Qs values:  [[ 0.038]
 [ 0.021]
 [ 0.082]
 [ 0.026]
 [ 0.073]
 [ 0.076]
 [-0.011]] [[1.126]
 [0.92 ]
 [0.815]
 [0.767]
 [0.84 ]
 [0.778]
 [0.947]] [[1.216]
 [1.012]
 [0.984]
 [0.879]
 [0.997]
 [0.944]
 [1.001]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.8090692428372483
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.584]
 [0.59 ]
 [0.584]
 [0.584]
 [0.616]
 [0.584]] [[2.582]
 [2.232]
 [1.409]
 [2.232]
 [2.232]
 [1.279]
 [2.232]] [[2.079]
 [1.662]
 [1.147]
 [1.662]
 [1.662]
 [1.114]
 [1.662]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.8081889171202191
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.353]
 [0.362]
 [0.339]
 [0.376]
 [0.354]
 [0.405]] [[1.167]
 [0.407]
 [0.357]
 [0.067]
 [0.242]
 [0.098]
 [0.502]] [[1.287]
 [0.719]
 [0.719]
 [0.578]
 [0.708]
 [0.618]
 [0.854]]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[3.423]
 [3.423]
 [3.423]
 [3.423]
 [3.423]
 [3.423]
 [3.484]] [[1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.13 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.547]] [[2.906]
 [2.906]
 [2.906]
 [2.906]
 [2.906]
 [2.906]
 [2.62 ]] [[0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.935]]
using explorer policy with actor:  1
siam score:  -0.6421894
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.    0.796 0.02  0.    0.163 0.02  0.   ]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.76 ]
 [0.34 ]
 [0.34 ]
 [0.404]
 [0.34 ]
 [0.714]
 [0.466]] [[0.316]
 [1.191]
 [1.191]
 [0.96 ]
 [1.191]
 [0.745]
 [1.112]] [[0.76 ]
 [0.34 ]
 [0.34 ]
 [0.404]
 [0.34 ]
 [0.714]
 [0.466]]
1644 2212
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.63563067
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.248]
 [0.631]
 [0.338]
 [0.33 ]
 [0.361]
 [0.342]] [[0.511]
 [2.278]
 [0.713]
 [0.664]
 [0.855]
 [0.865]
 [1.139]] [[0.357]
 [0.248]
 [0.631]
 [0.338]
 [0.33 ]
 [0.361]
 [0.342]]
siam score:  -0.6355049
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]] [[-2.632]
 [-3.246]
 [-3.246]
 [-3.246]
 [-3.246]
 [-3.246]
 [-3.246]] [[0.372]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6401212
Printing some Q and Qe and total Qs values:  [[1.117]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]] [[1.198]
 [1.311]
 [1.311]
 [1.311]
 [1.311]
 [1.311]
 [1.311]] [[1.062]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.898]] [[1.859]
 [1.859]
 [1.859]
 [1.859]
 [1.859]
 [1.859]
 [1.86 ]] [[1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.551]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]] [[1.2  ]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]] [[ 0.199]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.4575418062359304
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.642502
Printing some Q and Qe and total Qs values:  [[1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]] [[0.803]
 [0.793]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]] [[1.99 ]
 [1.984]
 [1.99 ]
 [1.99 ]
 [1.99 ]
 [1.99 ]
 [1.99 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.653]
 [0.671]
 [0.656]
 [0.67 ]
 [0.702]
 [0.69 ]] [[1.183]
 [1.916]
 [1.132]
 [1.07 ]
 [0.999]
 [0.985]
 [1.137]] [[0.354]
 [0.779]
 [0.292]
 [0.222]
 [0.203]
 [0.257]
 [0.335]]
line 256 mcts: sample exp_bonus 1.2550876143295813
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.378]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.353]] [[-1.989]
 [-1.43 ]
 [-3.191]
 [-3.191]
 [-3.191]
 [-3.191]
 [-2.447]] [[0.342]
 [0.378]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.353]]
siam score:  -0.64113706
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.458]
 [0.448]
 [0.465]
 [0.418]
 [0.436]
 [0.434]] [[0.486]
 [1.134]
 [0.858]
 [0.456]
 [0.578]
 [0.304]
 [0.608]] [[0.447]
 [0.458]
 [0.448]
 [0.465]
 [0.418]
 [0.436]
 [0.434]]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]] [[0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4269470226965277
Starting evaluation
1656 2243
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.386]
 [0.419]
 [0.425]
 [0.419]
 [0.419]
 [0.38 ]] [[1.776]
 [1.644]
 [0.837]
 [0.745]
 [0.837]
 [0.837]
 [1.765]] [[0.362]
 [0.386]
 [0.419]
 [0.425]
 [0.419]
 [0.419]
 [0.38 ]]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.479]
 [0.402]] [[-0.712]
 [-0.712]
 [-0.712]
 [-0.712]
 [-0.712]
 [-1.195]
 [-0.712]] [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.479]
 [0.402]]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.601]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]] [[2.025]
 [2.443]
 [2.025]
 [2.025]
 [2.025]
 [2.025]
 [2.025]] [[1.701]
 [1.824]
 [1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.701]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.038]
 [0.051]
 [0.05 ]
 [0.05 ]
 [0.052]
 [0.052]] [[-2.674]
 [-1.665]
 [-2.894]
 [-3.014]
 [-2.556]
 [-2.83 ]
 [-2.977]] [[0.052]
 [0.038]
 [0.051]
 [0.05 ]
 [0.05 ]
 [0.052]
 [0.052]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.365]] [[1.598]
 [1.598]
 [1.598]
 [1.598]
 [1.598]
 [1.598]
 [1.598]]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.172]
 [0.183]
 [0.181]
 [0.182]
 [0.178]
 [0.175]] [[ 0.213]
 [-0.03 ]
 [-0.147]
 [ 0.055]
 [ 0.065]
 [ 0.012]
 [ 0.011]] [[0.178]
 [0.172]
 [0.183]
 [0.181]
 [0.182]
 [0.178]
 [0.175]]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.845]] [[1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.846]] [[0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.845]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.499]
 [0.502]
 [0.501]
 [0.499]
 [0.808]] [[0.606]
 [0.606]
 [0.606]
 [0.516]
 [0.446]
 [0.606]
 [2.686]] [[0.499]
 [0.499]
 [0.499]
 [0.502]
 [0.501]
 [0.499]
 [0.808]]
Printing some Q and Qe and total Qs values:  [[1.313]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.592]
 [0.475]] [[0.799]
 [1.26 ]
 [1.26 ]
 [1.26 ]
 [1.26 ]
 [1.358]
 [1.26 ]] [[2.244]
 [1.709]
 [1.709]
 [1.709]
 [1.709]
 [1.856]
 [1.709]]
first move QE:  0.7961654859713411
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]] [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]]
line 256 mcts: sample exp_bonus 1.0357863665905167
line 256 mcts: sample exp_bonus -1.3770071416638765
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[2.785]
 [2.785]
 [2.785]
 [2.785]
 [2.785]
 [2.785]
 [2.785]] [[0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]]
line 256 mcts: sample exp_bonus -2.5809505972088873
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.005]
 [0.039]
 [0.038]
 [0.04 ]
 [0.047]
 [0.046]] [[-2.285]
 [-0.643]
 [-2.639]
 [-2.511]
 [-2.358]
 [-2.098]
 [-2.229]] [[0.045]
 [0.005]
 [0.039]
 [0.038]
 [0.04 ]
 [0.047]
 [0.046]]
siam score:  -0.62996215
using explorer policy with actor:  1
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.39 ]
 [0.573]
 [0.921]
 [0.861]
 [0.663]
 [0.517]] [[2.609]
 [2.882]
 [2.858]
 [1.328]
 [1.654]
 [3.397]
 [2.878]] [[1.001]
 [0.986]
 [1.242]
 [0.21 ]
 [0.452]
 [1.932]
 [1.176]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8075749077322523
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.448]
 [0.374]
 [0.374]
 [0.374]
 [0.495]
 [0.374]] [[3.01 ]
 [2.33 ]
 [3.01 ]
 [3.01 ]
 [3.01 ]
 [2.286]
 [3.01 ]] [[0.358]
 [0.054]
 [0.358]
 [0.358]
 [0.358]
 [0.118]
 [0.358]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.42 ]
 [0.399]
 [0.433]
 [0.421]
 [0.426]
 [0.447]] [[1.745]
 [1.667]
 [1.65 ]
 [2.002]
 [1.681]
 [1.434]
 [1.63 ]] [[0.478]
 [0.459]
 [0.392]
 [0.931]
 [0.479]
 [0.159]
 [0.462]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.7926036727625348
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[1.318]
 [1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]] [[0.622]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1676 2284
siam score:  -0.61638963
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.5679351858145743
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.979]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.483]] [[-0.318]
 [ 1.176]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.351]] [[0.068]
 [1.532]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.029]]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.445]
 [0.452]
 [0.445]
 [0.445]
 [0.472]
 [0.445]] [[ 2.237]
 [-0.022]
 [ 0.425]
 [-0.022]
 [-0.022]
 [ 0.642]
 [-0.022]] [[0.971]
 [0.169]
 [0.332]
 [0.169]
 [0.169]
 [0.445]
 [0.169]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.909]] [[-0.468]
 [-0.468]
 [-0.468]
 [-0.468]
 [-0.468]
 [-0.468]
 [-1.589]] [[0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.909]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0129649907063385
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.088]
 [0.125]
 [0.135]
 [0.125]
 [0.137]
 [0.133]] [[-2.776]
 [-1.348]
 [-2.158]
 [-2.711]
 [-2.099]
 [-2.268]
 [-2.116]] [[0.136]
 [0.088]
 [0.125]
 [0.135]
 [0.125]
 [0.137]
 [0.133]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]] [[0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]] [[1.73]
 [1.73]
 [1.73]
 [1.73]
 [1.73]
 [1.73]
 [1.73]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]] [[0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]] [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]]
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.012]
 [0.032]
 [0.032]
 [0.032]
 [0.03 ]
 [0.027]] [[-3.502]
 [-0.249]
 [-1.627]
 [-1.627]
 [-1.627]
 [-1.092]
 [-0.971]] [[0.052]
 [0.012]
 [0.032]
 [0.032]
 [0.032]
 [0.03 ]
 [0.027]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.216]
 [1.223]
 [1.235]
 [1.235]
 [1.235]
 [1.215]
 [1.216]] [[0.796]
 [0.79 ]
 [0.786]
 [0.786]
 [0.786]
 [0.783]
 [0.788]] [[2.283]
 [2.292]
 [2.312]
 [2.312]
 [2.312]
 [2.272]
 [2.277]]
siam score:  -0.61526793
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.37 ]
 [0.363]
 [0.365]
 [0.367]
 [0.359]
 [0.364]] [[-2.447]
 [-1.743]
 [-3.621]
 [-2.806]
 [-2.865]
 [-3.058]
 [-2.931]] [[0.37 ]
 [0.37 ]
 [0.363]
 [0.365]
 [0.367]
 [0.359]
 [0.364]]
siam score:  -0.6121179
Printing some Q and Qe and total Qs values:  [[0.006]
 [1.47 ]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[2.178]
 [0.764]
 [2.178]
 [2.178]
 [2.178]
 [2.178]
 [2.178]] [[0.744]
 [2.257]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.373]
 [0.344]
 [0.364]
 [0.402]
 [0.38 ]
 [0.399]] [[1.68 ]
 [1.841]
 [1.001]
 [0.761]
 [0.896]
 [1.089]
 [1.095]] [[0.65 ]
 [0.755]
 [0.138]
 [0.016]
 [0.183]
 [0.268]
 [0.309]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[1.889]
 [1.889]
 [1.889]
 [1.889]
 [1.889]
 [1.889]
 [1.889]] [[1.937]
 [1.937]
 [1.937]
 [1.937]
 [1.937]
 [1.937]
 [1.937]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.40747237581770496
Printing some Q and Qe and total Qs values:  [[0.94 ]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.978]] [[0.707]
 [0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.916]] [[2.405]
 [2.453]
 [2.453]
 [2.453]
 [2.453]
 [2.453]
 [2.562]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.7789513580370651
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
first move QE:  0.7741138540069773
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.5993051
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]] [[2.346]
 [2.346]
 [2.346]
 [2.346]
 [2.346]
 [2.346]
 [2.346]] [[2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.083]]
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.903]
 [0.801]
 [0.872]
 [0.801]
 [0.801]
 [0.984]] [[1.669]
 [1.777]
 [2.297]
 [1.799]
 [2.297]
 [2.297]
 [2.054]] [[0.769]
 [0.984]
 [1.072]
 [0.954]
 [1.072]
 [1.072]
 [1.197]]
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.404]
 [0.613]] [[1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.476]
 [0.787]
 [1.476]] [[1.088]
 [1.088]
 [1.088]
 [1.088]
 [1.088]
 [0.211]
 [1.088]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 1.8242281272997278
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.726]
 [0.465]] [[1.863]
 [1.863]
 [1.863]
 [1.863]
 [1.863]
 [1.446]
 [1.776]] [[-0.42 ]
 [-0.42 ]
 [-0.42 ]
 [-0.42 ]
 [-0.42 ]
 [ 0.004]
 [-0.408]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.174]
 [0.216]
 [0.208]
 [0.209]
 [0.213]
 [0.217]] [[0.643]
 [0.637]
 [0.591]
 [0.492]
 [0.522]
 [0.456]
 [0.334]] [[0.219]
 [0.174]
 [0.216]
 [0.208]
 [0.209]
 [0.213]
 [0.217]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.518]
 [0.532]
 [0.543]
 [0.449]
 [0.495]
 [0.512]] [[1.58 ]
 [1.771]
 [1.409]
 [1.732]
 [1.226]
 [1.121]
 [2.122]] [[0.47 ]
 [0.518]
 [0.532]
 [0.543]
 [0.449]
 [0.495]
 [0.512]]
siam score:  -0.5991331
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.318]
 [0.287]] [[0.115]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.856]
 [0.947]] [[ 0.034]
 [-0.269]
 [-0.269]
 [-0.269]
 [-0.269]
 [-0.236]
 [-0.269]]
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]] [[-6.125]
 [-5.953]
 [-5.953]
 [-5.953]
 [-5.953]
 [-5.953]
 [-5.953]] [[0.126]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.1  ]
 [0.098]
 [0.102]
 [0.168]
 [0.217]
 [0.149]] [[0.919]
 [1.409]
 [1.079]
 [1.36 ]
 [1.376]
 [1.479]
 [1.447]] [[-0.632]
 [-0.502]
 [-0.615]
 [-0.512]
 [-0.375]
 [-0.243]
 [-0.39 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.462]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[0.463]
 [1.175]
 [0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]] [[0.538]
 [0.835]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6086638
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.217]
 [0.251]
 [0.275]
 [0.29 ]
 [0.245]
 [0.233]] [[3.648]
 [3.502]
 [3.444]
 [3.584]
 [3.408]
 [3.677]
 [3.685]] [[-0.558]
 [-0.562]
 [-0.513]
 [-0.417]
 [-0.447]
 [-0.447]
 [-0.469]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.817]
 [1.205]] [[1.068]
 [1.068]
 [1.068]
 [1.068]
 [1.068]
 [1.18 ]
 [1.157]] [[1.168]
 [1.168]
 [1.168]
 [1.168]
 [1.168]
 [1.208]
 [1.977]]
1711 2377
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.633]
 [0.608]
 [0.597]
 [0.608]
 [0.608]
 [0.668]] [[2.611]
 [2.141]
 [2.611]
 [1.211]
 [2.611]
 [2.611]
 [2.257]] [[1.102]
 [0.994]
 [1.102]
 [0.611]
 [1.102]
 [1.102]
 [1.104]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]] [[-1.704]
 [-1.704]
 [-1.704]
 [-1.704]
 [-1.704]
 [-1.704]
 [-1.704]] [[0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.47 ]
 [0.513]
 [0.47 ]
 [0.47 ]
 [0.546]
 [0.447]] [[1.344]
 [1.344]
 [1.142]
 [1.344]
 [1.344]
 [1.107]
 [1.24 ]] [[1.285]
 [1.285]
 [1.303]
 [1.285]
 [1.285]
 [1.358]
 [1.204]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.524]
 [0.378]
 [0.378]
 [0.378]
 [0.513]
 [0.378]] [[1.306]
 [1.281]
 [1.394]
 [1.394]
 [1.394]
 [1.268]
 [1.394]] [[-0.269]
 [-0.003]
 [-0.259]
 [-0.259]
 [-0.259]
 [-0.029]
 [-0.259]]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.387]
 [0.419]
 [0.387]
 [0.387]
 [0.387]
 [0.387]] [[1.656]
 [1.656]
 [1.891]
 [1.656]
 [1.656]
 [1.656]
 [1.656]] [[1.838]
 [1.838]
 [2.131]
 [1.838]
 [1.838]
 [1.838]
 [1.838]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.555]
 [1.041]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[0.138]
 [1.435]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[0.714]
 [2.12 ]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.263]
 [0.449]
 [0.456]
 [0.457]
 [0.518]
 [0.513]] [[-0.069]
 [ 1.053]
 [-0.071]
 [-0.048]
 [-0.03 ]
 [-0.051]
 [-0.096]] [[0.339]
 [0.209]
 [0.206]
 [0.228]
 [0.236]
 [0.351]
 [0.325]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.6086435
Printing some Q and Qe and total Qs values:  [[0.663]
 [1.032]
 [0.589]
 [0.586]
 [0.711]
 [0.588]
 [1.032]] [[3.316]
 [2.182]
 [3.427]
 [3.441]
 [2.919]
 [3.688]
 [2.182]] [[0.254]
 [0.612]
 [0.143]
 [0.141]
 [0.217]
 [0.228]
 [0.612]]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.289]
 [0.251]
 [0.241]
 [0.239]
 [0.255]
 [0.244]] [[ 0.486]
 [ 0.358]
 [-0.003]
 [-0.122]
 [-0.026]
 [ 0.459]
 [ 0.195]] [[0.249]
 [0.289]
 [0.251]
 [0.241]
 [0.239]
 [0.255]
 [0.244]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.095]
 [0.329]
 [0.533]
 [0.158]
 [0.578]
 [0.478]] [[1.081]
 [1.508]
 [1.009]
 [0.064]
 [1.406]
 [0.968]
 [1.161]] [[1.041]
 [1.428]
 [1.194]
 [0.443]
 [1.4  ]
 [1.47 ]
 [1.548]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 3.105479041751534
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.424]
 [0.47 ]
 [0.47 ]] [[3.744]
 [3.744]
 [3.744]
 [3.744]
 [7.075]
 [3.744]
 [3.744]] [[-0.541]
 [-0.541]
 [-0.541]
 [-0.541]
 [ 0.479]
 [-0.541]
 [-0.541]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 6.164089479787539
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]] [[4.143]
 [4.143]
 [4.143]
 [4.143]
 [4.143]
 [4.143]
 [4.143]] [[0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]]
line 256 mcts: sample exp_bonus 0.955938274583275
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]] [[2.791]
 [2.791]
 [2.791]
 [2.791]
 [2.791]
 [2.791]
 [2.791]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.976]
 [1.319]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]] [[0.335]
 [0.306]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]] [[1.76 ]
 [2.195]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.816375400107057
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.074]
 [-0.072]
 [-0.054]
 [-0.074]
 [-0.074]
 [-0.073]] [[3.663]
 [4.336]
 [4.04 ]
 [3.656]
 [4.336]
 [4.075]
 [4.115]] [[-0.216]
 [-0.03 ]
 [-0.124]
 [-0.216]
 [-0.03 ]
 [-0.116]
 [-0.1  ]]
UNIT TEST: sample policy line 217 mcts : [0.061 0.    0.224 0.245 0.143 0.184 0.143]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.073]
 [0.103]
 [0.103]
 [0.103]
 [0.105]
 [0.103]] [[-3.784]
 [-1.216]
 [-3.162]
 [-3.162]
 [-3.162]
 [-2.995]
 [-3.162]] [[0.103]
 [0.073]
 [0.103]
 [0.103]
 [0.103]
 [0.105]
 [0.103]]
first move QE:  0.7586449466775709
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.157]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.506]
 [0.497]] [[1.958]
 [4.186]
 [4.186]
 [4.186]
 [4.186]
 [4.064]
 [4.186]] [[1.337]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.738]
 [0.76 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.1579040097143318
start point for exploration sampling:  11106
siam score:  -0.60347813
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]] [[3.064]
 [3.064]
 [3.064]
 [3.064]
 [3.064]
 [3.064]
 [3.064]] [[0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.275]
 [0.287]
 [0.413]
 [0.275]
 [0.286]
 [0.275]] [[2.725]
 [2.725]
 [2.571]
 [2.517]
 [2.725]
 [2.676]
 [2.725]] [[-0.634]
 [-0.634]
 [-0.661]
 [-0.426]
 [-0.634]
 [-0.628]
 [-0.634]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.52 ]
 [0.637]
 [0.5  ]
 [0.494]
 [0.481]
 [0.494]] [[-2.509]
 [-1.645]
 [ 0.   ]
 [-1.819]
 [-2.329]
 [-2.422]
 [-2.469]] [[0.263]
 [1.024]
 [2.633]
 [0.841]
 [0.395]
 [0.293]
 [0.275]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.27 ]
 [0.259]
 [0.308]
 [0.299]
 [0.293]
 [0.272]] [[-0.688]
 [-0.553]
 [-0.9  ]
 [-0.064]
 [-0.568]
 [-0.688]
 [-0.872]] [[0.281]
 [0.27 ]
 [0.259]
 [0.308]
 [0.299]
 [0.293]
 [0.272]]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.285]
 [0.244]
 [0.274]
 [0.31 ]
 [0.289]
 [0.401]] [[-0.278]
 [ 0.339]
 [-1.092]
 [-0.303]
 [-0.319]
 [-0.331]
 [-0.295]] [[0.282]
 [0.285]
 [0.244]
 [0.274]
 [0.31 ]
 [0.289]
 [0.401]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]] [[0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]] [[1.84]
 [1.84]
 [1.84]
 [1.84]
 [1.84]
 [1.84]
 [1.84]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.041 0.02  0.061 0.02  0.102 0.061 0.694]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.247]
 [1.319]
 [0.453]
 [0.453]
 [0.453]
 [1.239]
 [0.453]] [[0.818]
 [0.787]
 [1.912]
 [1.912]
 [1.912]
 [0.783]
 [1.912]] [[1.414]
 [1.459]
 [1.356]
 [1.356]
 [1.356]
 [1.385]
 [1.356]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.829]
 [0.845]
 [0.564]
 [0.599]
 [0.609]
 [0.606]] [[2.242]
 [3.128]
 [2.935]
 [1.915]
 [1.857]
 [2.237]
 [2.161]] [[0.713]
 [1.606]
 [1.49 ]
 [0.492]
 [0.488]
 [0.762]
 [0.705]]
Printing some Q and Qe and total Qs values:  [[0.11]
 [0.11]
 [0.11]
 [0.11]
 [0.11]
 [0.11]
 [0.11]] [[1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]] [[1.735]
 [1.735]
 [1.735]
 [1.735]
 [1.735]
 [1.735]
 [1.735]]
siam score:  -0.59763694
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]] [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]]
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.362]
 [0.362]
 [0.315]
 [0.316]
 [0.349]
 [0.314]] [[-0.478]
 [ 0.443]
 [ 0.066]
 [-0.196]
 [-0.247]
 [ 0.017]
 [-0.154]] [[0.318]
 [0.362]
 [0.362]
 [0.315]
 [0.316]
 [0.349]
 [0.314]]
first move QE:  0.7572891092183804
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1744 2442
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1745 2452
1745 2453
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.434]
 [0.249]
 [0.241]
 [0.236]
 [0.224]
 [0.238]] [[-3.193]
 [-0.188]
 [-2.666]
 [-3.178]
 [-3.223]
 [-3.086]
 [-3.093]] [[-0.026]
 [ 1.025]
 [ 0.137]
 [-0.027]
 [-0.044]
 [-0.009]
 [-0.003]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.474]
 [0.3  ]
 [0.523]
 [0.529]
 [0.42 ]
 [0.46 ]] [[1.886]
 [1.947]
 [1.676]
 [1.895]
 [1.735]
 [1.81 ]
 [1.917]] [[1.266]
 [1.835]
 [1.327]
 [1.873]
 [1.752]
 [1.634]
 [1.789]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6323442
first move QE:  0.755367346445535
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.502]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]] [[-1.88 ]
 [-0.604]
 [-1.88 ]
 [-1.88 ]
 [-1.88 ]
 [-1.88 ]
 [-1.88 ]] [[0.37 ]
 [0.932]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[2.548]
 [2.548]
 [2.548]
 [2.548]
 [2.548]
 [2.548]
 [2.548]] [[0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[1.849]
 [2.004]
 [2.004]
 [2.004]
 [2.004]
 [2.004]
 [2.004]] [[-0.469]
 [-0.415]
 [-0.415]
 [-0.415]
 [-0.415]
 [-0.415]
 [-0.415]]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.547]
 [0.765]] [[1.979]
 [1.833]
 [1.833]
 [1.833]
 [1.833]
 [2.163]
 [3.264]] [[0.531]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.547]
 [0.765]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6367316
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
first move QE:  0.752496133269949
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1763 2480
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.497]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[ 0.959]
 [ 0.222]
 [-0.383]
 [-0.383]
 [-0.383]
 [-0.383]
 [-0.383]] [[1.193]
 [1.027]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
siam score:  -0.6356286
siam score:  -0.6381918
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.935]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[0.075]
 [1.113]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[1.202]
 [2.126]
 [1.202]
 [1.202]
 [1.202]
 [1.202]
 [1.202]]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.516]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[-0.018]
 [ 1.43 ]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[1.065]
 [1.582]
 [1.065]
 [1.065]
 [1.065]
 [1.065]
 [1.065]]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]] [[-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.861]
 [0.674]
 [0.857]
 [0.828]
 [0.746]
 [1.069]
 [0.811]] [[0.673]
 [1.038]
 [0.616]
 [1.262]
 [1.07 ]
 [1.029]
 [0.947]] [[1.02 ]
 [0.768]
 [0.994]
 [1.152]
 [0.924]
 [1.556]
 [1.013]]
first move QE:  0.74340267486211
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6248886
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.24789524682420344
siam score:  -0.6233768
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.62456137
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1778 2507
siam score:  -0.61444837
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.7378296099816903
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6191434
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.489]
 [0.479]
 [0.428]
 [0.428]
 [0.428]
 [0.25 ]] [[2.625]
 [3.349]
 [1.858]
 [2.114]
 [2.114]
 [2.114]
 [2.323]] [[0.583]
 [1.577]
 [0.572]
 [0.673]
 [0.673]
 [0.673]
 [0.576]]
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]] [[-3.307]
 [-3.307]
 [-3.307]
 [-3.307]
 [-3.307]
 [-3.307]
 [-3.307]] [[0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.58 ]
 [0.49 ]
 [0.49 ]] [[ 1.226]
 [ 0.578]
 [ 0.578]
 [ 0.578]
 [-0.044]
 [ 0.578]
 [ 0.578]] [[1.924]
 [1.462]
 [1.462]
 [1.462]
 [1.29 ]
 [1.462]
 [1.462]]
siam score:  -0.6236148
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
1781 2528
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.421]
 [0.342]
 [0.339]
 [0.341]
 [0.385]
 [0.536]] [[-0.051]
 [ 1.107]
 [ 0.12 ]
 [-0.127]
 [-0.284]
 [-0.171]
 [-1.446]] [[0.349]
 [0.421]
 [0.342]
 [0.339]
 [0.341]
 [0.385]
 [0.536]]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.336]
 [0.319]
 [0.315]
 [0.319]
 [0.315]
 [0.315]] [[-3.955]
 [-2.947]
 [-3.923]
 [-4.158]
 [-4.062]
 [-4.057]
 [-3.786]] [[0.315]
 [0.336]
 [0.319]
 [0.315]
 [0.319]
 [0.315]
 [0.315]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.839]
 [0.729]] [[1.569]
 [1.569]
 [1.569]
 [1.569]
 [1.569]
 [0.799]
 [1.569]] [[1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.297]
 [1.336]]
Printing some Q and Qe and total Qs values:  [[1.103]
 [0.45 ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[ 0.486]
 [ 0.117]
 [-0.355]
 [-0.355]
 [-0.355]
 [-0.355]
 [-0.355]] [[2.166]
 [1.495]
 [1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.583]
 [0.5  ]
 [0.492]
 [0.491]
 [0.489]
 [0.496]] [[0.999]
 [2.423]
 [1.191]
 [1.094]
 [0.957]
 [1.076]
 [0.903]] [[0.227]
 [1.665]
 [0.402]
 [0.3  ]
 [0.175]
 [0.281]
 [0.136]]
first move QE:  0.7338676174075697
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.485]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]] [[0.714]
 [1.803]
 [1.803]
 [1.803]
 [1.803]
 [1.803]
 [1.803]] [[2.63 ]
 [1.831]
 [1.831]
 [1.831]
 [1.831]
 [1.831]
 [1.831]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.425]
 [0.427]
 [0.419]
 [0.396]
 [0.427]
 [0.428]] [[3.081]
 [3.348]
 [2.254]
 [2.777]
 [2.768]
 [2.254]
 [3.518]] [[0.819]
 [0.89 ]
 [0.528]
 [0.685]
 [0.636]
 [0.528]
 [0.951]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.865]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[0.788]
 [0.84 ]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]] [[0.582]
 [0.772]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]]
1786 2549
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 1.0579572496394176
1787 2550
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.02  0.102 0.02  0.    0.    0.    0.857]
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.319]
 [0.313]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]] [[-3.192]
 [-2.622]
 [-3.269]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.31 ]
 [0.319]
 [0.313]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.009]
 [ 0.006]
 [ 0.036]
 [ 0.006]
 [ 0.014]
 [ 0.006]] [[ 0.517]
 [ 0.618]
 [ 0.305]
 [-0.252]
 [ 0.305]
 [-1.799]
 [ 0.305]] [[-0.014]
 [-0.009]
 [ 0.006]
 [ 0.036]
 [ 0.006]
 [ 0.014]
 [ 0.006]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.3881520864889376
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.715]
 [0.675]
 [0.775]
 [0.675]
 [0.675]
 [0.617]] [[2.187]
 [2.457]
 [2.187]
 [2.895]
 [2.187]
 [2.187]
 [2.601]] [[1.882]
 [2.052]
 [1.882]
 [2.318]
 [1.882]
 [1.882]
 [1.904]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1794 2565
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.097]
 [0.102]] [[-2.435]
 [-2.305]
 [-2.305]
 [-2.305]
 [-2.305]
 [-2.099]
 [-2.025]] [[0.103]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.097]
 [0.102]]
siam score:  -0.6042048
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6049656
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.459]
 [0.445]
 [0.445]
 [0.445]
 [0.448]
 [0.441]] [[2.086]
 [2.244]
 [1.813]
 [1.813]
 [1.813]
 [2.499]
 [2.501]] [[0.448]
 [0.459]
 [0.445]
 [0.445]
 [0.445]
 [0.448]
 [0.441]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.60248876
Starting evaluation
first move QE:  0.7270325956951474
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.24 ]
 [0.317]
 [0.317]
 [0.051]
 [0.293]
 [0.281]] [[ 0.615]
 [ 0.054]
 [ 0.   ]
 [ 0.   ]
 [ 0.287]
 [-3.563]
 [-0.681]] [[0.182]
 [0.24 ]
 [0.317]
 [0.317]
 [0.051]
 [0.293]
 [0.281]]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1799 2579
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.303]
 [0.477]
 [0.468]
 [0.484]
 [0.592]
 [0.577]] [[-3.728]
 [-0.504]
 [-3.475]
 [-3.618]
 [-3.604]
 [-3.482]
 [-3.27 ]] [[0.506]
 [0.303]
 [0.477]
 [0.468]
 [0.484]
 [0.592]
 [0.577]]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.629]] [[3.863]
 [3.863]
 [3.863]
 [3.863]
 [3.863]
 [3.863]
 [4.63 ]] [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.629]]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.748]] [[4.359]
 [4.359]
 [4.359]
 [4.359]
 [4.359]
 [4.359]
 [7.038]] [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.748]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus -0.6292410468793697
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.303]
 [0.477]
 [0.427]
 [0.484]
 [0.521]
 [0.577]] [[-2.294]
 [-0.509]
 [-3.499]
 [-2.147]
 [-3.633]
 [-2.636]
 [-3.359]] [[0.464]
 [0.303]
 [0.477]
 [0.427]
 [0.484]
 [0.521]
 [0.577]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.511]
 [0.527]
 [0.527]
 [0.546]
 [0.513]
 [0.527]] [[0.577]
 [0.803]
 [0.598]
 [0.042]
 [0.409]
 [0.319]
 [0.666]] [[0.569]
 [0.511]
 [0.527]
 [0.527]
 [0.546]
 [0.513]
 [0.527]]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]] [[0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.548]
 [0.539]
 [0.516]
 [0.567]
 [0.563]
 [0.679]] [[-0.384]
 [ 1.603]
 [-0.276]
 [-0.201]
 [-0.272]
 [-0.252]
 [-0.227]] [[0.56 ]
 [0.548]
 [0.539]
 [0.516]
 [0.567]
 [0.563]
 [0.679]]
line 256 mcts: sample exp_bonus -0.5494197732095651
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.13710340502017082
using explorer policy with actor:  0
siam score:  -0.5933665
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -0.7630308197724318
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.5974317
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[-2.58 ]
 [-2.636]
 [-2.636]
 [-2.636]
 [-2.636]
 [-2.636]
 [-2.636]] [[0.148]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.611]
 [0.563]
 [0.575]
 [0.563]
 [0.591]
 [0.563]] [[2.041]
 [2.003]
 [2.44 ]
 [1.921]
 [2.44 ]
 [1.756]
 [2.44 ]] [[0.593]
 [0.611]
 [0.563]
 [0.575]
 [0.563]
 [0.591]
 [0.563]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.755]
 [0.743]
 [0.743]
 [0.743]
 [0.748]
 [0.743]] [[1.217]
 [1.344]
 [1.402]
 [1.402]
 [1.402]
 [1.273]
 [1.402]] [[0.793]
 [0.755]
 [0.743]
 [0.743]
 [0.743]
 [0.748]
 [0.743]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.672]] [[-3.844]
 [-3.844]
 [-3.844]
 [-3.844]
 [-3.844]
 [-3.844]
 [-1.677]] [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.672]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.498]
 [0.497]
 [0.498]
 [0.503]
 [0.539]
 [0.545]] [[0.733]
 [2.082]
 [1.234]
 [1.107]
 [0.897]
 [1.065]
 [0.912]] [[0.523]
 [0.498]
 [0.497]
 [0.498]
 [0.503]
 [0.539]
 [0.545]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6169752
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.531]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.656]] [[0.375]
 [1.661]
 [1.28 ]
 [1.28 ]
 [1.28 ]
 [1.28 ]
 [0.462]] [[1.574]
 [1.897]
 [2.022]
 [2.022]
 [2.022]
 [2.022]
 [1.747]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[2.691]
 [2.691]
 [2.691]
 [2.691]
 [2.691]
 [2.691]
 [2.691]] [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]] [[1.492]
 [1.492]
 [1.492]
 [1.492]
 [1.492]
 [1.492]
 [1.492]] [[1.653]
 [1.653]
 [1.653]
 [1.653]
 [1.653]
 [1.653]
 [1.653]]
line 256 mcts: sample exp_bonus 0.23850689305202186
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [1.071]] [[1.642]
 [1.642]
 [1.642]
 [1.642]
 [1.642]
 [1.642]
 [1.743]] [[1.527]
 [1.527]
 [1.527]
 [1.527]
 [1.527]
 [1.527]
 [2.041]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.576]
 [0.576]
 [0.579]
 [0.576]
 [0.768]
 [0.576]] [[3.306]
 [3.306]
 [3.306]
 [3.549]
 [3.306]
 [3.547]
 [3.306]] [[1.055]
 [1.055]
 [1.055]
 [1.142]
 [1.055]
 [1.52 ]
 [1.055]]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.581]] [[-0.455]
 [-0.455]
 [-0.455]
 [-0.455]
 [-0.455]
 [-0.455]
 [-2.312]] [[0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.581]]
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.267]
 [0.251]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.663]
 [-0.904]] [[0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.267]
 [0.251]]
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[0.579]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]] [[0.668]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]]
Printing some Q and Qe and total Qs values:  [[1.194]
 [0.645]
 [0.303]
 [0.645]
 [0.645]
 [0.713]
 [0.646]] [[1.614]
 [2.557]
 [2.316]
 [2.557]
 [2.557]
 [1.453]
 [2.   ]] [[2.34 ]
 [2.052]
 [1.535]
 [2.052]
 [2.052]
 [1.685]
 [1.826]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[-0.046]
 [ 0.59 ]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]] [[2.197]
 [0.707]
 [2.197]
 [2.197]
 [2.197]
 [2.197]
 [2.197]] [[-0.804]
 [-0.523]
 [-0.804]
 [-0.804]
 [-0.804]
 [-0.804]
 [-0.804]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.381]
 [0.563]
 [0.536]
 [0.522]
 [0.867]
 [0.491]] [[1.1  ]
 [1.384]
 [0.984]
 [0.881]
 [0.757]
 [0.309]
 [0.881]] [[0.232]
 [0.208]
 [0.439]
 [0.351]
 [0.281]
 [0.823]
 [0.262]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.025]
 [-0.004]
 [-0.   ]
 [-0.003]
 [-0.005]
 [-0.006]] [[1.004]
 [0.804]
 [0.924]
 [0.753]
 [0.753]
 [0.733]
 [0.75 ]] [[ 0.115]
 [-0.026]
 [ 0.035]
 [-0.128]
 [-0.134]
 [-0.157]
 [-0.142]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6028584
using explorer policy with actor:  1
siam score:  -0.6054035
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.942]
 [0.671]
 [0.676]
 [0.674]
 [0.674]
 [0.679]] [[1.03 ]
 [2.028]
 [0.704]
 [0.587]
 [0.692]
 [0.642]
 [0.837]] [[1.019]
 [1.872]
 [0.888]
 [0.859]
 [0.889]
 [0.874]
 [0.949]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.096]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.547]
 [0.42 ]] [[1.568]
 [2.658]
 [1.568]
 [1.568]
 [1.568]
 [1.428]
 [1.568]] [[1.688]
 [2.004]
 [1.688]
 [1.688]
 [1.688]
 [1.693]
 [1.688]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]] [[1.657]
 [1.516]
 [1.516]
 [1.516]
 [1.516]
 [1.516]
 [1.516]] [[1.006]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
siam score:  -0.6013948
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[1.603]
 [1.231]
 [1.231]
 [1.231]
 [1.231]
 [1.231]
 [1.231]] [[0.585]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]]
siam score:  -0.6010964
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.796]
 [0.514]
 [0.512]
 [0.502]
 [0.505]
 [0.505]] [[ 0.321]
 [ 2.465]
 [-0.313]
 [-0.28 ]
 [ 0.06 ]
 [-0.255]
 [ 0.303]] [[ 0.127]
 [ 1.438]
 [-0.053]
 [-0.046]
 [ 0.048]
 [-0.051]
 [ 0.135]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.498]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.556]] [[1.112]
 [0.666]
 [1.112]
 [1.112]
 [1.112]
 [1.112]
 [0.69 ]] [[2.543]
 [2.304]
 [2.543]
 [2.543]
 [2.543]
 [2.543]
 [2.427]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.92 ]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]] [[1.848]
 [1.493]
 [1.848]
 [1.848]
 [1.848]
 [1.848]
 [1.848]] [[1.68 ]
 [2.075]
 [1.68 ]
 [1.68 ]
 [1.68 ]
 [1.68 ]
 [1.68 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.759]] [[2.124]
 [2.124]
 [2.124]
 [2.124]
 [2.124]
 [2.124]
 [1.894]] [[1.366]
 [1.366]
 [1.366]
 [1.366]
 [1.366]
 [1.366]
 [1.623]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]] [[0.719]
 [0.757]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]]
siam score:  -0.59227705
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.36 ]
 [0.354]
 [0.352]
 [0.354]
 [0.352]
 [0.367]] [[-0.297]
 [ 0.47 ]
 [-0.29 ]
 [-0.441]
 [-0.174]
 [-0.327]
 [-0.42 ]] [[0.358]
 [0.36 ]
 [0.354]
 [0.352]
 [0.354]
 [0.352]
 [0.367]]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.59998
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]] [[1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.525]
 [0.528]
 [0.523]
 [0.525]
 [0.513]
 [0.519]] [[1.528]
 [2.027]
 [2.278]
 [1.385]
 [2.027]
 [2.423]
 [2.243]] [[0.396]
 [0.525]
 [0.528]
 [0.523]
 [0.525]
 [0.513]
 [0.519]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5873904
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.501]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]] [[0.706]
 [1.721]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]] [[1.546]
 [2.039]
 [1.546]
 [1.546]
 [1.546]
 [1.546]
 [1.546]]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.683]
 [0.502]
 [0.502]
 [0.502]
 [0.553]
 [0.55 ]] [[2.121]
 [2.377]
 [1.253]
 [1.253]
 [1.253]
 [1.472]
 [1.876]] [[0.934]
 [1.352]
 [0.616]
 [0.616]
 [0.616]
 [0.79 ]
 [0.919]]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.603]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.566]] [[1.349]
 [2.004]
 [1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.53 ]] [[0.706]
 [1.091]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.861]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.355]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]] [[-0.208]
 [-0.426]
 [-0.208]
 [-0.208]
 [-0.208]
 [-0.208]
 [-0.208]] [[0.34 ]
 [0.355]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus -3.970118509525705
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.825]] [[1.825]
 [1.825]
 [1.825]
 [1.825]
 [1.825]
 [1.825]
 [0.204]] [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.825]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.705]] [[1.306]
 [1.306]
 [1.306]
 [1.306]
 [1.306]
 [1.306]
 [0.053]] [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.705]]
first move QE:  0.7295135672618038
siam score:  -0.6073378
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.819]
 [0.819]
 [0.819]
 [0.821]
 [0.819]
 [0.819]
 [0.819]] [[1.013]
 [1.013]
 [1.013]
 [1.023]
 [1.013]
 [1.013]
 [1.013]] [[0.752]
 [0.752]
 [0.752]
 [0.76 ]
 [0.752]
 [0.752]
 [0.752]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6115409
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.267]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]] [[1.898]
 [1.845]
 [1.898]
 [1.898]
 [1.898]
 [1.898]
 [1.898]] [[1.027]
 [0.519]
 [1.027]
 [1.027]
 [1.027]
 [1.027]
 [1.027]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]] [[4.234]
 [4.234]
 [4.234]
 [4.234]
 [4.234]
 [4.234]
 [4.234]] [[1.227]
 [1.227]
 [1.227]
 [1.227]
 [1.227]
 [1.227]
 [1.227]]
siam score:  -0.61705965
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.084]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.072]
 [-0.072]] [[3.949]
 [3.122]
 [4.316]
 [4.316]
 [4.316]
 [5.875]
 [5.218]] [[0.582]
 [0.186]
 [0.764]
 [0.764]
 [0.764]
 [1.499]
 [1.188]]
siam score:  -0.61862487
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.905]
 [0.464]] [[1.75 ]
 [1.75 ]
 [1.75 ]
 [1.75 ]
 [1.75 ]
 [1.649]
 [1.75 ]] [[0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [1.756]
 [0.908]]
first move QE:  0.7267685322755468
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.939]
 [1.219]
 [0.716]
 [0.716]
 [0.847]
 [0.716]
 [0.832]] [[1.628]
 [1.01 ]
 [2.249]
 [2.249]
 [1.672]
 [2.249]
 [1.991]] [[2.303]
 [2.536]
 [2.148]
 [2.148]
 [2.193]
 [2.148]
 [2.244]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6635433666753122
line 256 mcts: sample exp_bonus 5.894567110905013
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
siam score:  -0.6036364
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.581]
 [0.536]
 [0.535]
 [0.534]
 [0.543]
 [0.579]] [[2.651]
 [3.937]
 [4.399]
 [3.839]
 [4.227]
 [4.347]
 [4.837]] [[0.653]
 [1.169]
 [1.347]
 [1.079]
 [1.263]
 [1.328]
 [1.596]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.6049707
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.543]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[3.754]
 [2.685]
 [3.03 ]
 [3.03 ]
 [3.03 ]
 [3.03 ]
 [3.03 ]] [[ 0.516]
 [ 0.07 ]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.61673355
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.322]
 [0.316]] [[3.721]
 [3.711]
 [3.711]
 [3.711]
 [3.711]
 [3.719]
 [3.607]] [[-0.146]
 [-0.19 ]
 [-0.19 ]
 [-0.19 ]
 [-0.19 ]
 [-0.242]
 [-0.292]]
siam score:  -0.6157825
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]] [[1.652]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]] [[1.972]
 [1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[1.225]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]] [[2.058]
 [1.611]
 [1.611]
 [1.611]
 [1.611]
 [1.611]
 [1.611]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.245]
 [0.47 ]
 [0.461]
 [0.459]
 [0.485]
 [0.544]] [[-2.012]
 [ 0.936]
 [-1.75 ]
 [-1.452]
 [-1.57 ]
 [-1.372]
 [-2.131]] [[0.461]
 [0.245]
 [0.47 ]
 [0.461]
 [0.459]
 [0.485]
 [0.544]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.638]
 [0.575]
 [0.575]
 [0.561]
 [0.574]
 [0.575]] [[1.13 ]
 [2.016]
 [0.712]
 [0.712]
 [0.425]
 [0.945]
 [0.712]] [[0.575]
 [0.638]
 [0.575]
 [0.575]
 [0.561]
 [0.574]
 [0.575]]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[2.364]
 [2.364]
 [2.364]
 [2.364]
 [2.364]
 [2.364]
 [2.364]] [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.381]
 [0.6  ]
 [0.6  ]
 [0.14 ]
 [0.472]
 [0.341]] [[ 2.054]
 [ 0.078]
 [ 0.099]
 [ 0.099]
 [ 0.37 ]
 [-1.95 ]
 [ 0.414]] [[1.472]
 [0.623]
 [0.923]
 [0.923]
 [0.362]
 [0.288]
 [0.644]]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.82 ]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[0.442]
 [0.545]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[0.499]
 [1.016]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7381850094599813
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.348]
 [0.354]
 [0.357]
 [0.352]
 [0.356]
 [0.355]] [[-0.69 ]
 [ 0.502]
 [-0.747]
 [-1.064]
 [-0.862]
 [-0.681]
 [-0.607]] [[0.348]
 [0.348]
 [0.354]
 [0.357]
 [0.352]
 [0.356]
 [0.355]]
line 256 mcts: sample exp_bonus 1.6389598380688997
siam score:  -0.6147312
Printing some Q and Qe and total Qs values:  [[1.063]
 [0.594]
 [0.594]
 [0.61 ]
 [0.601]
 [0.593]
 [0.604]] [[0.582]
 [1.506]
 [1.156]
 [0.723]
 [0.883]
 [0.872]
 [1.119]] [[1.13 ]
 [0.5  ]
 [0.384]
 [0.271]
 [0.306]
 [0.287]
 [0.39 ]]
line 256 mcts: sample exp_bonus -0.09717570113820996
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.61089766
Printing some Q and Qe and total Qs values:  [[1.466]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[0.69]
 [1.52]
 [1.52]
 [1.52]
 [1.52]
 [1.52]
 [1.52]] [[2.785]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.413]
 [0.409]
 [0.398]
 [0.413]
 [0.408]
 [0.397]] [[ 1.073]
 [ 1.448]
 [ 0.804]
 [-0.879]
 [ 0.606]
 [ 0.197]
 [ 0.132]] [[0.411]
 [0.413]
 [0.409]
 [0.398]
 [0.413]
 [0.408]
 [0.397]]
Printing some Q and Qe and total Qs values:  [[0.999]
 [0.999]
 [0.999]
 [1.247]
 [1.03 ]
 [0.999]
 [0.999]] [[0.445]
 [0.445]
 [0.445]
 [0.436]
 [0.438]
 [0.445]
 [0.445]] [[1.231]
 [1.231]
 [1.231]
 [1.721]
 [1.288]
 [1.231]
 [1.231]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.593]] [[2.943]
 [2.943]
 [2.943]
 [2.943]
 [2.943]
 [2.943]
 [2.768]] [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.593]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.4942448878422985
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.60951734
Printing some Q and Qe and total Qs values:  [[0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.972]] [[2.416]
 [2.416]
 [2.416]
 [2.416]
 [2.416]
 [2.416]
 [2.592]] [[0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.972]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.154]
 [0.154]
 [0.154]
 [0.158]
 [0.154]
 [0.163]] [[2.635]
 [2.855]
 [2.855]
 [2.855]
 [2.814]
 [2.855]
 [2.669]] [[-0.03 ]
 [ 0.197]
 [ 0.197]
 [ 0.197]
 [ 0.163]
 [ 0.197]
 [ 0.029]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.05 ]
 [0.066]
 [0.066]
 [0.068]
 [0.06 ]
 [0.065]] [[ 0.48 ]
 [ 0.498]
 [-0.422]
 [-0.49 ]
 [ 0.6  ]
 [ 0.824]
 [ 0.34 ]] [[0.062]
 [0.05 ]
 [0.066]
 [0.066]
 [0.068]
 [0.06 ]
 [0.065]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6094525
siam score:  -0.61207825
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[2.379]
 [2.379]
 [2.379]
 [2.379]
 [2.379]
 [2.379]
 [2.379]] [[1.481]
 [1.481]
 [1.481]
 [1.481]
 [1.481]
 [1.481]
 [1.481]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[2.156]
 [2.156]
 [2.156]
 [2.156]
 [2.156]
 [2.156]
 [2.156]] [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.998]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]] [[2.595]
 [1.574]
 [1.574]
 [1.574]
 [1.574]
 [1.574]
 [1.574]] [[1.401]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.113]
 [1.113]
 [1.113]
 [1.113]
 [1.113]
 [1.113]
 [1.113]] [[0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]] [[2.511]
 [2.511]
 [2.511]
 [2.511]
 [2.511]
 [2.511]
 [2.511]]
using explorer policy with actor:  1
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[2.504]
 [2.504]
 [2.504]
 [2.504]
 [2.504]
 [2.504]
 [2.504]] [[0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1898 2779
Printing some Q and Qe and total Qs values:  [[0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.915]
 [0.971]] [[1.077]
 [1.077]
 [1.077]
 [1.077]
 [1.077]
 [0.773]
 [1.077]] [[1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [0.751]
 [1.041]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]] [[1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[1.15]
 [1.15]
 [1.15]
 [1.15]
 [1.15]
 [1.15]
 [1.15]] [[1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.16 ]
 [0.202]
 [0.165]
 [0.162]
 [0.165]
 [0.197]] [[2.833]
 [2.069]
 [2.526]
 [2.078]
 [2.408]
 [2.248]
 [2.913]] [[-0.307]
 [-0.579]
 [-0.344]
 [-0.566]
 [-0.462]
 [-0.511]
 [-0.225]]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.403]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]] [[-0.338]
 [-0.257]
 [-0.338]
 [-0.338]
 [-0.338]
 [-0.338]
 [-0.338]] [[0.38 ]
 [0.403]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.519]] [[3.153]
 [3.153]
 [3.153]
 [3.153]
 [3.153]
 [3.153]
 [2.181]] [[-0.54 ]
 [-0.54 ]
 [-0.54 ]
 [-0.54 ]
 [-0.54 ]
 [-0.54 ]
 [-0.034]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.443]] [[2.994]
 [2.994]
 [2.994]
 [2.994]
 [2.994]
 [2.994]
 [2.612]] [[-0.699]
 [-0.699]
 [-0.699]
 [-0.699]
 [-0.699]
 [-0.699]
 [-0.047]]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [ 0.206]] [[3.155]
 [3.155]
 [3.155]
 [3.155]
 [3.155]
 [3.155]
 [2.633]] [[-0.701]
 [-0.701]
 [-0.701]
 [-0.701]
 [-0.701]
 [-0.701]
 [-0.459]]
first move QE:  0.7238610195255277
using explorer policy with actor:  1
siam score:  -0.62056893
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6171114
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.185]
 [0.626]
 [0.587]
 [0.166]
 [0.587]
 [0.35 ]] [[ 0.964]
 [ 1.114]
 [-0.808]
 [ 0.964]
 [ 0.718]
 [ 0.964]
 [ 0.779]] [[2.145]
 [1.971]
 [1.241]
 [2.145]
 [1.751]
 [2.145]
 [1.899]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.697]
 [0.723]
 [0.748]
 [0.723]
 [0.723]
 [0.632]] [[1.99 ]
 [2.154]
 [1.99 ]
 [1.509]
 [1.99 ]
 [1.99 ]
 [2.222]] [[1.477]
 [1.58 ]
 [1.477]
 [1.127]
 [1.477]
 [1.477]
 [1.572]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.234]
 [0.078]
 [0.095]
 [0.102]
 [0.127]
 [0.159]] [[4.47 ]
 [1.864]
 [4.17 ]
 [4.22 ]
 [4.277]
 [4.38 ]
 [4.382]] [[ 1.229]
 [-0.025]
 [ 1.003]
 [ 1.037]
 [ 1.069]
 [ 1.133]
 [ 1.153]]
Printing some Q and Qe and total Qs values:  [[ 0.066]
 [ 0.06 ]
 [ 0.046]
 [-0.003]
 [ 0.06 ]
 [ 0.06 ]
 [ 0.032]] [[3.82 ]
 [3.908]
 [3.845]
 [4.239]
 [3.908]
 [3.908]
 [4.207]] [[0.643]
 [0.708]
 [0.643]
 [0.913]
 [0.708]
 [0.708]
 [0.921]]
1912 2817
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]] [[2.703]
 [2.335]
 [2.335]
 [2.335]
 [2.335]
 [2.335]
 [2.335]] [[ 0.001]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.664]
 [0.664]
 [0.655]
 [0.664]
 [0.41 ]
 [0.664]] [[0.845]
 [0.845]
 [0.845]
 [1.815]
 [0.845]
 [1.517]
 [0.845]] [[ 0.243]
 [ 0.243]
 [ 0.243]
 [ 0.548]
 [ 0.243]
 [-0.041]
 [ 0.243]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6160332
siam score:  -0.6149236
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.4101974507787853
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.421]
 [0.409]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[1.42 ]
 [2.885]
 [1.627]
 [1.062]
 [1.062]
 [1.062]
 [1.062]] [[0.411]
 [0.421]
 [0.409]
 [0.395]
 [0.395]
 [0.395]
 [0.395]]
siam score:  -0.6084853
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[3.155]
 [3.155]
 [3.155]
 [3.155]
 [3.155]
 [3.155]
 [3.155]] [[1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.02  0.837 0.02  0.02  0.02  0.061 0.02 ]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 3.415264649533842
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.517]
 [0.567]
 [0.579]
 [0.574]
 [0.619]
 [0.577]] [[2.29 ]
 [2.34 ]
 [1.91 ]
 [2.098]
 [1.959]
 [2.016]
 [2.2  ]] [[0.399]
 [0.298]
 [0.255]
 [0.34 ]
 [0.285]
 [0.393]
 [0.37 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.498]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[1.719]
 [1.59 ]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]] [[1.569]
 [1.489]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.607]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[1.931]
 [2.554]
 [1.931]
 [1.931]
 [1.931]
 [1.931]
 [1.931]] [[1.213]
 [1.518]
 [1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]]
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.774]
 [0.248]] [[3.105]
 [2.365]
 [2.365]
 [2.365]
 [2.365]
 [2.949]
 [2.365]] [[ 0.005]
 [-0.351]
 [-0.351]
 [-0.351]
 [-0.351]
 [ 0.896]
 [-0.351]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.408691196417412
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]] [[1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.661]] [[0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]] [[0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]] [[0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]] [[0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.413]
 [0.485]
 [0.54 ]
 [0.528]
 [0.523]
 [0.511]] [[ 0.183]
 [ 1.186]
 [ 0.199]
 [-0.121]
 [ 0.029]
 [ 0.326]
 [-0.034]] [[0.512]
 [0.413]
 [0.485]
 [0.54 ]
 [0.528]
 [0.523]
 [0.511]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.0803060147691497
line 256 mcts: sample exp_bonus 0.5774304492583484
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.768]] [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.892]] [[0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.768]]
using explorer policy with actor:  1
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.255230500837931
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.767]] [[-0.313]
 [-0.313]
 [-0.313]
 [-0.313]
 [-0.313]
 [-0.313]
 [ 0.859]] [[0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.767]]
first move QE:  0.7190417879760865
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.77 ]
 [0.77 ]
 [0.926]
 [0.77 ]
 [0.77 ]
 [0.77 ]] [[0.238]
 [0.238]
 [0.238]
 [0.309]
 [0.238]
 [0.238]
 [0.238]] [[0.77 ]
 [0.77 ]
 [0.77 ]
 [0.926]
 [0.77 ]
 [0.77 ]
 [0.77 ]]
line 256 mcts: sample exp_bonus 1.2525597271778017
siam score:  -0.62177694
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.844]] [[1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [2.733]] [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.844]]
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.589]
 [0.575]
 [0.584]
 [0.587]
 [0.596]
 [0.648]] [[-1.15 ]
 [ 1.401]
 [-1.209]
 [-0.853]
 [-1.286]
 [-1.097]
 [-0.922]] [[0.614]
 [0.589]
 [0.575]
 [0.584]
 [0.587]
 [0.596]
 [0.648]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.577]
 [0.563]
 [0.558]
 [0.558]
 [0.562]
 [0.566]] [[-1.379]
 [ 1.095]
 [-1.101]
 [-1.154]
 [-1.154]
 [-1.439]
 [-1.32 ]] [[0.561]
 [0.577]
 [0.563]
 [0.558]
 [0.558]
 [0.562]
 [0.566]]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.554]] [[1.421]
 [1.421]
 [1.421]
 [1.421]
 [1.421]
 [1.421]
 [1.584]] [[0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.554]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.522]
 [0.556]
 [0.608]
 [0.556]
 [0.558]
 [0.556]] [[0.21 ]
 [1.101]
 [0.327]
 [0.618]
 [0.327]
 [0.264]
 [0.327]] [[0.567]
 [0.522]
 [0.556]
 [0.608]
 [0.556]
 [0.558]
 [0.556]]
1943 2844
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.31 ]
 [0.349]
 [0.371]
 [0.355]
 [0.309]
 [0.362]] [[ 0.543]
 [ 1.606]
 [-0.009]
 [ 0.265]
 [ 0.208]
 [ 0.6  ]
 [ 0.65 ]] [[0.381]
 [0.31 ]
 [0.349]
 [0.371]
 [0.355]
 [0.309]
 [0.362]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5816630147012694
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.62289506
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.381]
 [0.469]
 [0.691]
 [0.492]
 [0.46 ]
 [0.461]] [[-0.229]
 [ 1.065]
 [ 0.015]
 [-1.443]
 [ 0.188]
 [-0.017]
 [ 0.383]] [[0.477]
 [0.381]
 [0.469]
 [0.691]
 [0.492]
 [0.46 ]
 [0.461]]
siam score:  -0.6234505
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.242]
 [0.399]
 [0.381]
 [0.392]
 [0.409]
 [0.353]] [[0.424]
 [1.435]
 [0.566]
 [0.65 ]
 [0.782]
 [0.716]
 [0.613]] [[0.386]
 [0.242]
 [0.399]
 [0.381]
 [0.392]
 [0.409]
 [0.353]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.012]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]] [[3.694]
 [3.522]
 [3.522]
 [3.522]
 [3.522]
 [3.522]
 [3.522]] [[1.814]
 [1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7227135135456096
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.554]
 [0.556]
 [0.564]
 [0.576]
 [0.559]
 [0.561]] [[0.885]
 [2.291]
 [0.843]
 [0.804]
 [1.581]
 [0.971]
 [1.036]] [[1.89 ]
 [2.267]
 [1.884]
 [1.886]
 [2.113]
 [1.923]
 [1.943]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0381772100280373
siam score:  -0.630783
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]] [[1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]] [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]]
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.988]
 [0.994]
 [0.994]
 [0.994]
 [0.884]
 [1.003]] [[1.061]
 [1.088]
 [1.169]
 [1.169]
 [1.169]
 [0.988]
 [1.015]] [[1.93 ]
 [2.101]
 [2.139]
 [2.139]
 [2.139]
 [1.86 ]
 [2.107]]
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.771]
 [0.771]
 [0.867]
 [0.771]
 [0.771]
 [0.771]] [[0.636]
 [0.636]
 [0.636]
 [0.886]
 [0.636]
 [0.636]
 [0.636]] [[0.771]
 [0.771]
 [0.771]
 [0.867]
 [0.771]
 [0.771]
 [0.771]]
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.767]
 [0.767]
 [0.91 ]
 [0.767]
 [0.767]
 [0.767]] [[0.609]
 [0.609]
 [0.609]
 [0.867]
 [0.609]
 [0.609]
 [0.609]] [[0.767]
 [0.767]
 [0.767]
 [0.91 ]
 [0.767]
 [0.767]
 [0.767]]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
1948 2852
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.704]] [[0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.621]] [[-0.322]
 [-0.322]
 [-0.322]
 [-0.322]
 [-0.322]
 [-0.322]
 [ 0.606]]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.64 ]] [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [1.081]] [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [1.184]]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[2.808]
 [2.808]
 [2.808]
 [2.808]
 [2.808]
 [2.808]
 [2.808]] [[1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[2.575]
 [2.575]
 [2.575]
 [2.575]
 [2.575]
 [2.575]
 [2.575]] [[1.199]
 [1.199]
 [1.199]
 [1.199]
 [1.199]
 [1.199]
 [1.199]]
siam score:  -0.6393796
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.52586304637286
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.202]
 [0.217]
 [0.179]
 [0.181]
 [0.177]
 [0.184]] [[-1.637]
 [-0.704]
 [-1.672]
 [-2.099]
 [-2.006]
 [-1.925]
 [-1.721]] [[0.186]
 [0.202]
 [0.217]
 [0.179]
 [0.181]
 [0.177]
 [0.184]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.311]
 [0.39 ]
 [0.392]
 [0.388]
 [0.389]
 [0.394]] [[4.148]
 [3.852]
 [3.323]
 [3.499]
 [3.181]
 [3.052]
 [3.136]] [[1.814]
 [1.307]
 [0.884]
 [1.069]
 [0.735]
 [0.603]
 [0.698]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.005]
 [0.007]
 [0.019]
 [0.013]
 [0.01 ]
 [0.008]] [[1.688]
 [2.146]
 [2.068]
 [1.332]
 [1.814]
 [2.146]
 [2.165]] [[0.993]
 [1.329]
 [1.274]
 [0.746]
 [1.093]
 [1.334]
 [1.346]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.197]
 [0.231]
 [0.197]
 [0.204]
 [0.231]
 [0.231]] [[ 0.   ]
 [-1.314]
 [ 0.   ]
 [-1.796]
 [-1.752]
 [ 0.   ]
 [ 0.   ]] [[0.231]
 [0.197]
 [0.231]
 [0.197]
 [0.204]
 [0.231]
 [0.231]]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.43 ]
 [0.462]
 [0.468]
 [0.455]
 [0.458]
 [0.453]] [[2.531]
 [1.647]
 [1.244]
 [1.369]
 [1.404]
 [1.414]
 [1.658]] [[0.902]
 [0.581]
 [0.448]
 [0.498]
 [0.504]
 [0.509]
 [0.598]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[1.925]
 [1.925]
 [1.925]
 [1.925]
 [1.925]
 [1.925]
 [1.925]] [[1.895]
 [1.895]
 [1.895]
 [1.895]
 [1.895]
 [1.895]
 [1.895]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6140381
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.857]
 [1.206]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[1.391]
 [1.515]
 [1.611]
 [1.611]
 [1.611]
 [1.611]
 [1.611]] [[2.436]
 [3.095]
 [2.253]
 [2.253]
 [2.253]
 [2.253]
 [2.253]]
1967 2873
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -0.8487136402434713
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.63 ]
 [0.528]
 [0.604]
 [0.579]
 [0.545]
 [0.605]] [[2.6  ]
 [2.201]
 [2.5  ]
 [2.207]
 [2.384]
 [2.31 ]
 [2.74 ]] [[0.409]
 [0.441]
 [0.339]
 [0.393]
 [0.402]
 [0.309]
 [0.572]]
Printing some Q and Qe and total Qs values:  [[1.292]
 [1.237]
 [1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.093]] [[0.888]
 [0.906]
 [1.622]
 [1.622]
 [1.622]
 [1.622]
 [1.622]] [[2.103]
 [2.   ]
 [1.949]
 [1.949]
 [1.949]
 [1.949]
 [1.949]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.273802599134695
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.019132710402668875
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
siam score:  -0.6244446
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.62326264
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.997187914837347
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.8445319106936013
siam score:  -0.61843574
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.573]
 [0.549]
 [0.567]
 [0.573]
 [0.555]
 [0.551]] [[1.262]
 [0.927]
 [1.053]
 [0.985]
 [0.927]
 [0.915]
 [1.211]] [[0.546]
 [0.346]
 [0.381]
 [0.373]
 [0.346]
 [0.301]
 [0.49 ]]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.573]
 [0.515]
 [0.515]
 [0.515]
 [0.526]
 [0.515]] [[1.491]
 [1.398]
 [1.491]
 [1.491]
 [1.491]
 [1.369]
 [1.491]] [[0.133]
 [0.187]
 [0.133]
 [0.133]
 [0.133]
 [0.075]
 [0.133]]
1975 2881
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.701]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.558]] [[2.236]
 [2.13 ]
 [2.236]
 [2.236]
 [2.236]
 [2.236]
 [2.288]] [[1.002]
 [1.089]
 [1.002]
 [1.002]
 [1.002]
 [1.002]
 [0.855]]
siam score:  -0.6218752
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -1.026821078337178
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.426]
 [0.58 ]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[-0.755]
 [-0.755]
 [-0.829]
 [-0.755]
 [-0.755]
 [-0.755]
 [-0.755]] [[0.426]
 [0.426]
 [0.58 ]
 [0.426]
 [0.426]
 [0.426]
 [0.426]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.6019],
        [-0.4313],
        [-0.0000],
        [-0.0000],
        [-0.5570],
        [-0.3547],
        [-0.6017],
        [-0.0000],
        [-0.5294],
        [-0.5877]], dtype=torch.float64)
-0.024259925299500003 -0.6261246560903779
-0.024259925299500003 -0.45554045467025467
-0.965448 -0.965448
-0.857637 -0.857637
-0.0727797758985 -0.629754299461035
-0.024259925299500003 -0.3789951429907672
-0.024259925299500003 -0.625911888301606
-0.965595015 -0.965595015
-0.0727797758985 -0.6022257317588129
-0.0727797758985 -0.6604754195308676
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.585]
 [0.573]
 [0.573]
 [0.559]
 [0.585]
 [0.573]] [[1.427]
 [2.163]
 [1.   ]
 [1.   ]
 [1.075]
 [1.298]
 [1.   ]] [[0.57 ]
 [0.585]
 [0.573]
 [0.573]
 [0.559]
 [0.585]
 [0.573]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1982 2889
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.613]
 [0.575]
 [0.584]
 [0.613]
 [0.577]
 [0.577]] [[2.59 ]
 [2.23 ]
 [2.65 ]
 [2.635]
 [2.23 ]
 [2.624]
 [2.718]] [[0.551]
 [0.253]
 [0.597]
 [0.599]
 [0.253]
 [0.575]
 [0.668]]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.241]
 [0.38 ]
 [0.389]
 [0.373]
 [0.372]
 [0.371]] [[1.461]
 [1.474]
 [1.258]
 [1.295]
 [1.384]
 [1.248]
 [1.254]] [[0.324]
 [0.093]
 [0.156]
 [0.211]
 [0.267]
 [0.13 ]
 [0.134]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[2.692]
 [2.692]
 [2.692]
 [2.692]
 [2.692]
 [2.692]
 [2.692]] [[1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]] [[2.547]
 [2.547]
 [2.547]
 [2.547]
 [2.547]
 [2.547]
 [2.547]] [[2.283]
 [2.283]
 [2.283]
 [2.283]
 [2.283]
 [2.283]
 [2.283]]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.007]
 [0.04 ]
 [0.037]
 [0.039]
 [0.048]
 [0.041]] [[-2.703]
 [-1.337]
 [-3.11 ]
 [-2.868]
 [-2.952]
 [-2.709]
 [-2.573]] [[0.043]
 [0.007]
 [0.04 ]
 [0.037]
 [0.039]
 [0.048]
 [0.041]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.59 ]
 [0.568]
 [0.565]
 [0.547]
 [0.584]
 [0.56 ]] [[-3.221]
 [ 0.75 ]
 [-2.721]
 [-2.906]
 [-2.316]
 [-2.462]
 [-2.536]] [[0.259]
 [1.577]
 [0.431]
 [0.369]
 [0.553]
 [0.523]
 [0.487]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.62029845
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.282]
 [0.439]
 [0.413]
 [0.417]
 [0.431]
 [0.403]] [[2.171]
 [1.978]
 [1.55 ]
 [1.407]
 [1.58 ]
 [1.364]
 [1.238]] [[0.586]
 [0.295]
 [0.466]
 [0.367]
 [0.431]
 [0.388]
 [0.291]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
start point for exploration sampling:  11106
using explorer policy with actor:  1
1997 2903
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.709]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]] [[1.864]
 [2.175]
 [1.627]
 [1.627]
 [1.627]
 [1.627]
 [1.627]] [[0.23 ]
 [1.429]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[2.229]
 [2.291]
 [2.291]
 [2.291]
 [2.291]
 [2.291]
 [2.291]] [[0.863]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.248]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[-1.596]
 [-0.374]
 [-1.329]
 [-1.329]
 [-1.329]
 [-1.329]
 [-1.329]] [[0.234]
 [0.248]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6129206
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.105]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]] [[2.23 ]
 [1.471]
 [1.471]
 [1.471]
 [1.471]
 [1.471]
 [1.471]] [[2.865]
 [2.099]
 [2.099]
 [2.099]
 [2.099]
 [2.099]
 [2.099]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6167406
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.9823732225403048
line 256 mcts: sample exp_bonus 1.8244763757713958
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.645]
 [0.414]
 [0.698]
 [0.698]
 [0.696]
 [0.691]] [[3.07 ]
 [5.219]
 [3.137]
 [3.193]
 [3.403]
 [3.122]
 [3.688]] [[1.246]
 [2.156]
 [0.939]
 [1.338]
 [1.429]
 [1.304]
 [1.545]]
siam score:  -0.6199562
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.272]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]] [[-0.065]
 [ 1.278]
 [ 1.435]
 [ 1.435]
 [ 1.435]
 [ 1.435]
 [ 1.435]] [[0.255]
 [0.272]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.532]
 [0.875]
 [0.561]
 [0.507]
 [0.504]] [[1.821]
 [2.654]
 [1.787]
 [1.203]
 [1.7  ]
 [1.887]
 [2.207]] [[0.517]
 [0.517]
 [0.532]
 [0.875]
 [0.561]
 [0.507]
 [0.504]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.323]
 [0.283]
 [0.283]
 [0.262]
 [0.269]
 [0.265]] [[-0.603]
 [ 0.522]
 [ 0.   ]
 [ 0.   ]
 [-1.627]
 [-1.62 ]
 [-1.366]] [[0.287]
 [0.323]
 [0.283]
 [0.283]
 [0.262]
 [0.269]
 [0.265]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.62261736
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.61995757
siam score:  -0.61971533
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7172781208822495
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.575]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.539]] [[ 0.   ]
 [ 0.214]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-1.397]] [[0.561]
 [0.575]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.539]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.489]
 [0.599]
 [0.612]
 [0.602]
 [0.607]
 [0.615]] [[-2.014]
 [ 1.701]
 [-0.883]
 [-1.733]
 [-1.676]
 [-1.241]
 [-1.036]] [[0.363]
 [1.888]
 [0.845]
 [0.488]
 [0.506]
 [0.696]
 [0.79 ]]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]] [[3.706]
 [3.706]
 [3.706]
 [3.706]
 [3.706]
 [3.706]
 [3.706]] [[0.92]
 [0.92]
 [0.92]
 [0.92]
 [0.92]
 [0.92]
 [0.92]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2025 2931
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.010943539194831153
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.834]
 [0.564]] [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [2.296]
 [0.607]] [[0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [1.465]
 [0.361]]
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.932]
 [0.92 ]] [[3.029]
 [3.029]
 [3.029]
 [3.029]
 [3.029]
 [3.049]
 [3.029]] [[1.969]
 [1.969]
 [1.969]
 [1.969]
 [1.969]
 [1.999]
 [1.969]]
siam score:  -0.62397146
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.454]
 [0.383]
 [0.378]
 [0.364]
 [0.362]
 [0.378]] [[-2.893]
 [-0.062]
 [-2.872]
 [-3.304]
 [-3.066]
 [-2.871]
 [-2.165]] [[ 0.063]
 [ 0.903]
 [ 0.07 ]
 [-0.057]
 [ 0.008]
 [ 0.064]
 [ 0.272]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2031 2939
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.758918533423607
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [1.224]
 [1.224]] [[2.037]
 [2.037]
 [2.037]
 [2.037]
 [2.037]
 [0.689]
 [0.706]] [[1.308]
 [1.308]
 [1.308]
 [1.308]
 [1.308]
 [1.925]
 [1.947]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.81 ]
 [0.718]] [[3.045]
 [3.045]
 [3.045]
 [3.045]
 [3.045]
 [3.657]
 [3.045]] [[1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.704]
 [1.112]]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.415]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[2.96 ]
 [3.854]
 [2.96 ]
 [2.96 ]
 [2.96 ]
 [2.96 ]
 [2.96 ]] [[0.219]
 [0.846]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]]
line 256 mcts: sample exp_bonus 5.392581923144959
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7376124367428005
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.257]
 [0.286]
 [0.292]
 [0.286]
 [0.293]
 [0.143]] [[4.582]
 [3.627]
 [4.582]
 [3.972]
 [4.582]
 [4.239]
 [4.45 ]] [[0.835]
 [0.192]
 [0.835]
 [0.47 ]
 [0.835]
 [0.637]
 [0.488]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.6099046
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.583]
 [0.556]
 [0.556]
 [0.583]
 [0.583]
 [0.555]] [[2.241]
 [1.716]
 [2.21 ]
 [2.06 ]
 [1.716]
 [1.716]
 [2.149]] [[0.741]
 [0.272]
 [0.712]
 [0.561]
 [0.272]
 [0.272]
 [0.649]]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.61196756
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.051]
 [0.031]
 [0.03 ]
 [0.027]
 [0.038]
 [0.046]] [[-4.813]
 [-3.72 ]
 [-4.48 ]
 [-4.351]
 [-4.601]
 [-4.39 ]
 [-4.331]] [[0.029]
 [0.051]
 [0.031]
 [0.03 ]
 [0.027]
 [0.038]
 [0.046]]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[-4.862]
 [-4.47 ]
 [-4.47 ]
 [-4.47 ]
 [-4.47 ]
 [-4.47 ]
 [-4.47 ]] [[0.035]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2040 2958
siam score:  -0.6159418
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.371]
 [0.347]
 [0.349]
 [0.332]
 [0.306]
 [0.301]] [[2.628]
 [2.737]
 [2.373]
 [2.903]
 [2.743]
 [2.732]
 [2.645]] [[-0.527]
 [-0.392]
 [-0.564]
 [-0.383]
 [-0.469]
 [-0.525]
 [-0.564]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.426]
 [0.483]
 [0.522]
 [0.588]
 [0.538]
 [0.687]] [[4.086]
 [5.033]
 [4.015]
 [2.957]
 [2.814]
 [3.092]
 [2.56 ]] [[0.601]
 [1.073]
 [0.576]
 [0.022]
 [0.055]
 [0.13 ]
 [0.081]]
siam score:  -0.6171123
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 9.697200837980908
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 5.758059078978199
UNIT TEST: sample policy line 217 mcts : [0.061 0.347 0.041 0.184 0.082 0.184 0.102]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
in main func line 156:  2049
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.061]
 [0.13 ]
 [0.267]
 [0.206]
 [0.137]
 [0.15 ]] [[3.837]
 [4.79 ]
 [3.412]
 [4.089]
 [3.471]
 [3.759]
 [5.251]] [[ 0.314]
 [ 0.69 ]
 [-0.09 ]
 [ 0.635]
 [ 0.102]
 [ 0.157]
 [ 1.175]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.738]] [[-5.824]
 [-5.824]
 [-5.824]
 [-5.824]
 [-5.824]
 [-5.824]
 [-3.976]] [[0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.738]]
start point for exploration sampling:  11106
siam score:  -0.6411157
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.899]
 [0.572]
 [0.572]
 [0.572]
 [0.647]] [[-3.575]
 [-3.575]
 [-3.25 ]
 [-3.575]
 [-3.575]
 [-3.575]
 [-3.948]] [[0.572]
 [0.572]
 [0.899]
 [0.572]
 [0.572]
 [0.572]
 [0.647]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2054 2975
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.708]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.562]] [[-0.275]
 [ 1.252]
 [-0.275]
 [-0.275]
 [-0.275]
 [-0.275]
 [ 1.07 ]] [[1.611]
 [2.357]
 [1.611]
 [1.611]
 [1.611]
 [1.611]
 [2.004]]
siam score:  -0.6481509
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.245 0.245 0.082 0.041 0.184 0.061 0.143]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.059]
 [0.108]
 [0.091]
 [0.099]
 [0.101]
 [0.092]] [[1.337]
 [1.355]
 [1.117]
 [1.196]
 [1.089]
 [1.111]
 [1.072]] [[-0.338]
 [-0.385]
 [-0.444]
 [-0.428]
 [-0.482]
 [-0.462]
 [-0.507]]
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.087]
 [0.087]
 [0.098]
 [0.087]
 [0.087]
 [0.094]] [[1.569]
 [1.653]
 [1.653]
 [1.37 ]
 [1.653]
 [1.653]
 [1.493]] [[0.188]
 [0.222]
 [0.222]
 [0.056]
 [0.222]
 [0.222]
 [0.129]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.019]
 [0.044]
 [0.043]
 [0.047]
 [0.053]
 [0.043]] [[-3.379]
 [-1.68 ]
 [-3.631]
 [-2.954]
 [-3.245]
 [-3.141]
 [-2.794]] [[0.039]
 [0.019]
 [0.044]
 [0.043]
 [0.047]
 [0.053]
 [0.043]]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.774]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]] [[0.425]
 [2.183]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[1.455]
 [2.075]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]]
first move QE:  0.704279921822548
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.253]
 [1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]] [[1.232]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]] [[2.135]
 [1.918]
 [1.918]
 [1.918]
 [1.918]
 [1.918]
 [1.918]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[1.329]
 [1.067]
 [0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.989]] [[2.068]
 [1.095]
 [1.418]
 [1.418]
 [1.418]
 [1.418]
 [1.228]] [[2.768]
 [2.299]
 [2.167]
 [2.167]
 [2.167]
 [2.167]
 [2.238]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.215]
 [0.477]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[-0.852]
 [ 0.922]
 [-1.595]
 [ 1.184]
 [ 1.184]
 [ 1.184]
 [ 1.184]] [[0.655]
 [2.081]
 [0.157]
 [2.275]
 [2.275]
 [2.275]
 [2.275]]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.437]
 [0.401]
 [0.411]
 [0.411]
 [0.407]
 [0.407]] [[-1.127]
 [-0.531]
 [-1.249]
 [-1.298]
 [-0.889]
 [-0.837]
 [-1.081]] [[0.402]
 [0.437]
 [0.401]
 [0.411]
 [0.411]
 [0.407]
 [0.407]]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.783]
 [0.828]
 [0.876]
 [0.897]
 [0.729]
 [0.883]] [[1.616]
 [1.615]
 [1.193]
 [0.844]
 [1.086]
 [1.311]
 [1.268]] [[1.478]
 [1.422]
 [1.373]
 [1.352]
 [1.474]
 [1.215]
 [1.507]]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.277]
 [0.267]
 [0.266]
 [0.263]
 [0.261]
 [0.265]] [[-1.526]
 [-1.408]
 [-2.153]
 [-2.026]
 [-2.169]
 [-1.911]
 [-1.705]] [[0.262]
 [0.277]
 [0.267]
 [0.266]
 [0.263]
 [0.261]
 [0.265]]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.38 ]
 [0.387]
 [0.393]
 [0.374]
 [0.427]
 [0.426]] [[1.363]
 [1.701]
 [1.184]
 [1.307]
 [1.099]
 [1.589]
 [1.63 ]] [[0.408]
 [0.38 ]
 [0.387]
 [0.393]
 [0.374]
 [0.427]
 [0.426]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.118]
 [0.781]
 [0.819]
 [0.781]
 [0.781]
 [0.784]
 [0.781]] [[4.328]
 [3.284]
 [4.194]
 [3.284]
 [3.284]
 [3.25 ]
 [3.284]] [[1.633]
 [0.609]
 [0.99 ]
 [0.609]
 [0.609]
 [0.605]
 [0.609]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.779]
 [0.791]
 [0.769]
 [0.77 ]
 [0.756]
 [0.774]] [[2.62 ]
 [3.817]
 [2.894]
 [2.485]
 [2.615]
 [3.195]
 [3.427]] [[0.374]
 [0.742]
 [0.458]
 [0.278]
 [0.323]
 [0.488]
 [0.602]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.9755466982999925
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.275]
 [0.275]
 [0.324]
 [0.275]
 [0.275]
 [0.275]] [[4.651]
 [4.651]
 [4.651]
 [7.547]
 [4.651]
 [4.651]
 [4.651]] [[0.477]
 [0.477]
 [0.477]
 [1.463]
 [0.477]
 [0.477]
 [0.477]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 4.488286541731857
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.422]] [[1.354]
 [1.633]
 [1.633]
 [1.633]
 [1.633]
 [1.633]
 [1.426]] [[1.599]
 [1.736]
 [1.736]
 [1.736]
 [1.736]
 [1.736]
 [1.663]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6590832
start point for exploration sampling:  11106
2072 3006
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.7477018911103714
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.548]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[0.586]
 [1.682]
 [1.992]
 [1.992]
 [1.992]
 [1.992]
 [1.992]] [[1.634]
 [1.923]
 [2.112]
 [2.112]
 [2.112]
 [2.112]
 [2.112]]
2072 3007
siam score:  -0.65950364
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]] [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]] [[0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6553426
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.484]
 [0.484]
 [0.488]
 [0.484]
 [0.484]
 [0.487]] [[0.397]
 [0.493]
 [0.493]
 [0.352]
 [0.493]
 [0.493]
 [0.329]] [[ 0.013]
 [ 0.041]
 [ 0.041]
 [ 0.001]
 [ 0.041]
 [ 0.041]
 [-0.009]]
2074 3010
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6547791
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.087]
 [0.565]
 [0.602]
 [0.247]
 [0.591]
 [0.228]] [[ 1.562]
 [ 1.93 ]
 [ 0.88 ]
 [-0.846]
 [ 1.702]
 [-0.149]
 [ 1.696]] [[1.704]
 [1.67 ]
 [1.609]
 [0.532]
 [1.73 ]
 [0.972]
 [1.701]]
using explorer policy with actor:  1
siam score:  -0.6534682
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.791]] [[2.291]
 [2.291]
 [2.291]
 [2.291]
 [2.291]
 [2.291]
 [1.914]] [[1.848]
 [1.848]
 [1.848]
 [1.848]
 [1.848]
 [1.848]
 [1.646]]
siam score:  -0.65540147
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64487296
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.25 ]
 [0.243]] [[-2.657]
 [-2.304]
 [-2.304]
 [-2.304]
 [-2.304]
 [-2.25 ]
 [-2.304]] [[0.245]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.25 ]
 [0.243]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6257108
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.252]
 [0.644]
 [0.355]
 [0.355]
 [0.377]
 [0.418]] [[-4.451]
 [ 0.802]
 [-3.13 ]
 [-3.982]
 [-4.067]
 [-4.154]
 [-4.512]] [[0.356]
 [0.252]
 [0.644]
 [0.355]
 [0.355]
 [0.377]
 [0.418]]
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.04 ]
 [-0.039]
 [-0.039]] [[3.316]
 [3.316]
 [3.316]
 [3.316]
 [3.132]
 [3.316]
 [3.316]] [[0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.233]
 [0.419]
 [0.419]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5398],
        [-0.0000],
        [-0.5370],
        [-0.0000],
        [-0.5595],
        [-0.0000],
        [-0.0000],
        [-0.4177],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.024259925299500003 -0.5640827642535295
-0.9320711845499999 -0.9320711845499999
-0.024259925299500003 -0.561220960425099
-0.8037314999999999 -0.8037314999999999
-0.024259925299500003 -0.5837370128599974
-0.004949999999999235 -0.004949999999999235
-0.94167714465 -0.94167714465
-0.024259925299500003 -0.4419914581266754
-0.6972142418999998 -0.6972142418999998
-0.84561754365 -0.84561754365
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.071]
 [-0.067]
 [-0.086]
 [-0.086]
 [-0.087]
 [-0.07 ]] [[1.352]
 [1.907]
 [2.316]
 [1.819]
 [1.819]
 [1.553]
 [2.093]] [[-0.063]
 [ 0.356]
 [ 0.655]
 [ 0.271]
 [ 0.271]
 [ 0.079]
 [ 0.492]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.492]
 [1.492]
 [1.492]
 [1.492]
 [1.492]
 [1.492]
 [1.492]] [[0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]] [[2.015]
 [2.015]
 [2.015]
 [2.015]
 [2.015]
 [2.015]
 [2.015]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.752]
 [0.644]
 [0.611]
 [0.615]
 [0.641]
 [0.689]] [[1.544]
 [1.598]
 [1.313]
 [1.04 ]
 [1.12 ]
 [1.081]
 [1.252]] [[0.331]
 [0.655]
 [0.248]
 [0.001]
 [0.062]
 [0.087]
 [0.297]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.347]
 [0.34 ]
 [0.338]
 [0.338]
 [0.333]
 [0.342]] [[-4.082]
 [-3.712]
 [-3.953]
 [-4.034]
 [-4.034]
 [-4.037]
 [-3.95 ]] [[0.339]
 [0.347]
 [0.34 ]
 [0.338]
 [0.338]
 [0.333]
 [0.342]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
siam score:  -0.61050504
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6096779
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -2.042916834844107
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]] [[2.046]
 [1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.781]] [[1.118]
 [0.891]
 [0.891]
 [0.891]
 [0.891]
 [0.891]
 [0.891]]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.516]] [[-1.474]
 [-1.474]
 [-1.474]
 [-1.474]
 [-1.474]
 [-1.474]
 [-1.392]] [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.516]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.2657308895748833
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.384]
 [0.654]
 [0.384]
 [0.384]
 [0.385]
 [0.385]] [[-2.423]
 [-2.317]
 [-3.128]
 [-2.317]
 [-2.317]
 [-2.16 ]
 [-2.203]] [[0.382]
 [0.384]
 [0.654]
 [0.384]
 [0.384]
 [0.385]
 [0.385]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.452]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.551]] [[0.973]
 [1.394]
 [3.144]
 [3.144]
 [3.144]
 [3.144]
 [1.265]] [[1.485]
 [1.587]
 [2.064]
 [2.064]
 [2.064]
 [2.064]
 [1.592]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.625]
 [0.625]
 [0.672]
 [0.625]
 [0.625]
 [0.625]] [[-1.695]
 [-1.695]
 [-1.695]
 [-1.681]
 [-1.695]
 [-1.695]
 [-1.695]] [[0.625]
 [0.625]
 [0.625]
 [0.672]
 [0.625]
 [0.625]
 [0.625]]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.608058
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.478148052108968
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -4.2211699439058075
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.363]
 [0.375]
 [0.351]
 [0.375]
 [0.351]
 [0.375]] [[-4.493]
 [-3.873]
 [ 0.   ]
 [-4.242]
 [ 0.   ]
 [-4.56 ]
 [ 0.   ]] [[0.351]
 [0.363]
 [0.375]
 [0.351]
 [0.375]
 [0.351]
 [0.375]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  11106
2101 3069
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -0.1902790942209316
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2108 3075
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.055]
 [0.087]
 [0.085]
 [0.087]
 [0.087]
 [0.083]] [[-2.095]
 [-0.321]
 [-1.899]
 [-1.711]
 [-1.859]
 [-1.988]
 [-1.581]] [[0.082]
 [0.055]
 [0.087]
 [0.085]
 [0.087]
 [0.087]
 [0.083]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.62 ]] [[0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.748]] [[0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.62 ]]
siam score:  -0.6113599
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6131796
line 256 mcts: sample exp_bonus 0.9791124936966061
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.678]] [[-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [ 0.495]] [[0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.678]]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.47 ]
 [0.603]
 [0.603]] [[1.233]
 [1.233]
 [1.233]
 [1.233]
 [2.381]
 [1.233]
 [1.233]] [[2.594]
 [2.594]
 [2.594]
 [2.594]
 [2.694]
 [2.594]
 [2.594]]
siam score:  -0.6168276
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.46813264684261774
Printing some Q and Qe and total Qs values:  [[0.966]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]] [[6.513]
 [2.586]
 [2.586]
 [2.586]
 [2.586]
 [2.586]
 [2.586]] [[2.405]
 [1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]]
2116 3091
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.315]
 [0.317]
 [0.314]
 [0.311]
 [0.306]
 [0.313]] [[-1.263]
 [-0.873]
 [-1.627]
 [-1.98 ]
 [-1.769]
 [-1.74 ]
 [-1.069]] [[0.305]
 [0.315]
 [0.317]
 [0.314]
 [0.311]
 [0.306]
 [0.313]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.331]
 [0.561]
 [0.561]
 [0.554]
 [0.562]
 [0.557]] [[-3.653]
 [ 0.206]
 [-2.817]
 [-3.395]
 [-3.415]
 [-2.866]
 [-2.847]] [[0.248]
 [1.601]
 [0.564]
 [0.346]
 [0.335]
 [0.546]
 [0.551]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.285]
 [0.265]
 [0.266]
 [0.285]
 [0.281]
 [0.285]] [[1.015]
 [0.862]
 [0.827]
 [0.664]
 [0.862]
 [0.517]
 [0.862]] [[-0.176]
 [-0.221]
 [-0.273]
 [-0.325]
 [-0.221]
 [-0.344]
 [-0.221]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.6868804237299008
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[2.72]
 [2.72]
 [2.72]
 [2.72]
 [2.72]
 [2.72]
 [2.72]] [[1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.472]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.1927598765725755
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.508]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[0.395]
 [1.543]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[0.488]
 [0.508]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.775]] [[-0.754]
 [-0.754]
 [-0.754]
 [-0.754]
 [-0.754]
 [-0.754]
 [-1.565]] [[0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.775]]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.551]
 [0.565]
 [0.559]
 [0.546]
 [0.585]
 [0.649]] [[-0.175]
 [ 1.7  ]
 [-0.245]
 [-0.477]
 [-0.397]
 [-0.366]
 [-0.891]] [[0.642]
 [0.551]
 [0.565]
 [0.559]
 [0.546]
 [0.585]
 [0.649]]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.884]] [[-0.94 ]
 [-0.94 ]
 [-0.94 ]
 [-0.94 ]
 [-0.94 ]
 [-0.94 ]
 [-1.331]] [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.884]]
line 256 mcts: sample exp_bonus 3.1812281708489616
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.794]] [[-0.588]
 [-0.588]
 [-0.588]
 [-0.588]
 [-0.588]
 [-0.588]
 [-2.184]] [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.794]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.56 ]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.662]] [[2.052]
 [2.339]
 [3.119]
 [3.119]
 [3.119]
 [3.119]
 [2.268]] [[1.786]
 [1.805]
 [2.212]
 [2.212]
 [2.212]
 [2.212]
 [1.909]]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.018]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.103]] [[-1.589]
 [ 0.991]
 [-1.589]
 [-1.589]
 [-1.589]
 [-1.589]
 [ 0.932]] [[0.837]
 [1.72 ]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [1.731]]
siam score:  -0.63091844
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.466]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[2.127]
 [2.339]
 [2.127]
 [2.127]
 [2.127]
 [2.127]
 [2.127]] [[1.53]
 [1.42]
 [1.53]
 [1.53]
 [1.53]
 [1.53]
 [1.53]]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6245018158063691
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.376]
 [0.358]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[-4.275]
 [-2.093]
 [-4.21 ]
 [-4.298]
 [-4.298]
 [-4.298]
 [-4.298]] [[0.354]
 [0.376]
 [0.358]
 [0.356]
 [0.356]
 [0.356]
 [0.356]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.395]
 [0.372]
 [0.371]
 [0.374]
 [0.371]
 [0.372]] [[-3.176]
 [-1.039]
 [-3.142]
 [-2.821]
 [-3.2  ]
 [-2.821]
 [-2.676]] [[0.368]
 [0.395]
 [0.372]
 [0.371]
 [0.374]
 [0.371]
 [0.372]]
line 256 mcts: sample exp_bonus 0.7164928794448068
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]] [[1.686]
 [1.686]
 [1.686]
 [1.686]
 [1.686]
 [1.686]
 [1.686]] [[0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.443]
 [0.398]
 [0.397]
 [0.397]
 [0.398]
 [0.401]] [[-0.601]
 [ 1.053]
 [-0.423]
 [-0.451]
 [-0.463]
 [-0.423]
 [-0.341]] [[0.412]
 [0.443]
 [0.398]
 [0.397]
 [0.397]
 [0.398]
 [0.401]]
line 256 mcts: sample exp_bonus 0.6831463109203184
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6388408
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[1.062]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.866]] [[1.293]
 [1.268]
 [1.268]
 [1.268]
 [1.268]
 [1.268]
 [1.369]] [[1.984]
 [1.545]
 [1.545]
 [1.545]
 [1.545]
 [1.545]
 [1.642]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.479]
 [0.478]
 [0.499]
 [0.477]
 [0.485]
 [0.508]] [[0.643]
 [0.698]
 [0.612]
 [0.5  ]
 [0.556]
 [0.622]
 [0.728]] [[0.472]
 [0.504]
 [0.444]
 [0.412]
 [0.405]
 [0.466]
 [0.581]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.544]] [[0.844]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.107]] [[0.875]
 [1.628]
 [1.628]
 [1.628]
 [1.628]
 [1.628]
 [1.162]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.299]
 [0.132]
 [0.475]
 [0.145]
 [0.16 ]
 [0.224]] [[2.512]
 [2.419]
 [1.822]
 [2.089]
 [1.89 ]
 [1.786]
 [2.03 ]] [[1.765]
 [1.621]
 [0.706]
 [1.55 ]
 [0.8  ]
 [0.712]
 [1.08 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.665]
 [0.756]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[-0.837]
 [-0.837]
 [-0.1  ]
 [-0.837]
 [-0.837]
 [-0.837]
 [-0.837]] [[0.665]
 [0.665]
 [0.756]
 [0.665]
 [0.665]
 [0.665]
 [0.665]]
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.359]
 [0.478]
 [0.424]
 [0.43 ]
 [0.441]
 [0.451]] [[-3.881]
 [ 1.003]
 [-3.61 ]
 [-3.014]
 [-2.919]
 [-3.248]
 [-2.753]] [[0.431]
 [0.359]
 [0.478]
 [0.424]
 [0.43 ]
 [0.441]
 [0.451]]
siam score:  -0.6433182
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.325]
 [0.326]
 [0.375]
 [0.399]
 [0.399]
 [0.147]] [[3.633]
 [3.999]
 [3.38 ]
 [3.895]
 [3.53 ]
 [3.53 ]
 [4.244]] [[1.207]
 [1.38 ]
 [0.976]
 [1.353]
 [1.132]
 [1.132]
 [1.402]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 1.4636174926088203
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.6527029
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.68144896415505
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.425]
 [0.572]
 [0.271]
 [0.358]
 [0.566]
 [0.58 ]] [[ 1.217]
 [-0.482]
 [-2.907]
 [ 0.484]
 [-0.514]
 [-2.79 ]
 [-2.203]] [[1.556]
 [0.995]
 [0.089]
 [1.318]
 [0.955]
 [0.133]
 [0.372]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.582]
 [0.552]
 [0.571]
 [0.36 ]
 [0.36 ]
 [0.36 ]] [[ 0.987]
 [ 1.375]
 [-2.555]
 [-1.018]
 [ 0.987]
 [ 0.987]
 [ 0.987]] [[1.428]
 [1.691]
 [0.097]
 [0.724]
 [1.428]
 [1.428]
 [1.428]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.102 0.265 0.061 0.061 0.204 0.082 0.224]
siam score:  -0.64407027
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64352167
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.472]
 [0.463]
 [0.46 ]
 [0.461]
 [0.45 ]
 [0.449]] [[-1.857]
 [-0.146]
 [-1.523]
 [-1.747]
 [-1.758]
 [-1.503]
 [-1.429]] [[0.454]
 [0.472]
 [0.463]
 [0.46 ]
 [0.461]
 [0.45 ]
 [0.449]]
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.137]
 [0.162]
 [0.162]
 [0.164]
 [0.164]
 [0.169]] [[-1.836]
 [-0.321]
 [-1.643]
 [-1.647]
 [-1.811]
 [-1.938]
 [-1.635]] [[0.16 ]
 [0.137]
 [0.162]
 [0.162]
 [0.164]
 [0.164]
 [0.169]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.328]
 [0.314]
 [0.314]
 [0.315]
 [0.309]
 [0.31 ]] [[-2.607]
 [-2.63 ]
 [-3.122]
 [-3.139]
 [-3.179]
 [-3.204]
 [-3.14 ]] [[0.309]
 [0.328]
 [0.314]
 [0.314]
 [0.315]
 [0.309]
 [0.31 ]]
line 256 mcts: sample exp_bonus 1.6068131493300237
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.562]] [[1.608]
 [1.608]
 [1.608]
 [1.608]
 [1.608]
 [1.608]
 [1.475]] [[0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.562]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6425093
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]] [[1.09 ]
 [2.092]
 [2.092]
 [2.092]
 [2.092]
 [2.092]
 [2.092]] [[0.999]
 [1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.64040697
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [1.094]
 [0.885]] [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.498]] [[1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]
 [2.086]
 [1.636]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[1.697]
 [1.697]
 [1.697]
 [1.697]
 [1.697]
 [1.697]
 [1.697]] [[1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.909]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[4.709]
 [4.709]
 [4.709]
 [4.709]
 [4.709]
 [4.709]
 [4.709]] [[1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[1.698]
 [2.585]
 [2.585]
 [2.585]
 [2.585]
 [2.585]
 [2.585]] [[-0.543]
 [-0.282]
 [-0.282]
 [-0.282]
 [-0.282]
 [-0.282]
 [-0.282]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.653505
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6513737
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]] [[2.243]
 [2.243]
 [2.243]
 [2.243]
 [2.243]
 [2.243]
 [2.243]] [[1.16]
 [1.16]
 [1.16]
 [1.16]
 [1.16]
 [1.16]
 [1.16]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]] [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.497]
 [0.622]
 [0.62 ]
 [0.632]
 [0.622]
 [0.622]] [[0.826]
 [1.035]
 [0.238]
 [0.64 ]
 [0.55 ]
 [0.238]
 [0.238]] [[1.656]
 [1.553]
 [1.537]
 [1.666]
 [1.66 ]
 [1.537]
 [1.537]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.284]
 [0.617]
 [0.617]
 [0.614]
 [0.611]
 [0.606]] [[0.267]
 [1.363]
 [0.2  ]
 [0.283]
 [0.333]
 [0.38 ]
 [0.295]] [[0.683]
 [0.395]
 [0.671]
 [0.699]
 [0.71 ]
 [0.72 ]
 [0.681]]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.347]
 [0.356]
 [0.354]
 [0.35 ]
 [0.336]
 [0.345]] [[ 0.636]
 [ 0.085]
 [ 0.32 ]
 [-0.471]
 [-0.525]
 [-0.6  ]
 [-0.42 ]] [[0.333]
 [0.347]
 [0.356]
 [0.354]
 [0.35 ]
 [0.336]
 [0.345]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]] [[2.929]
 [2.929]
 [2.929]
 [2.929]
 [2.929]
 [2.929]
 [2.929]] [[1.735]
 [1.735]
 [1.735]
 [1.735]
 [1.735]
 [1.735]
 [1.735]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.666786
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[2.138]
 [2.138]
 [2.138]
 [2.138]
 [2.138]
 [2.138]
 [2.138]] [[0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -3.1652499513607
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6666066
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.459]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[-1.097]
 [ 0.907]
 [-1.097]
 [-1.097]
 [-1.097]
 [-1.097]
 [-1.097]] [[0.445]
 [0.459]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2170 3191
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.6770155536577998
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.434]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[-2.623]
 [-0.999]
 [-2.928]
 [-2.928]
 [-2.928]
 [-2.928]
 [-2.928]] [[0.424]
 [0.434]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]]
siam score:  -0.6707913
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.406]
 [0.394]
 [0.406]
 [0.406]
 [0.4  ]
 [0.393]] [[1.033]
 [0.646]
 [0.547]
 [0.646]
 [0.646]
 [0.569]
 [0.445]] [[0.406]
 [0.406]
 [0.394]
 [0.406]
 [0.406]
 [0.4  ]
 [0.393]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.797]
 [0.68 ]] [[1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]
 [2.29 ]
 [1.206]] [[0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [1.243]
 [0.505]]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.409]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[-1.658]
 [-0.298]
 [-1.658]
 [-1.658]
 [-1.658]
 [-1.658]
 [-1.658]] [[0.407]
 [0.409]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[1.158]
 [0.879]
 [0.916]
 [0.742]
 [1.098]
 [1.485]
 [1.37 ]] [[ 0.82 ]
 [ 0.264]
 [ 0.339]
 [-0.01 ]
 [ 0.703]
 [ 1.477]
 [ 1.246]]
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.793]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]] [[0.754]
 [2.339]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[1.063]
 [1.654]
 [1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.6727135567592751
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.898]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]] [[2.62 ]
 [2.072]
 [2.072]
 [2.072]
 [2.072]
 [2.072]
 [2.072]] [[1.239]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]]
siam score:  -0.6395521
Printing some Q and Qe and total Qs values:  [[1.099]
 [1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]] [[0.699]
 [0.74 ]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[1.018]
 [0.967]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.437]
 [0.551]
 [0.568]
 [0.557]
 [0.528]
 [0.537]] [[2.527]
 [2.259]
 [2.494]
 [2.356]
 [2.522]
 [2.482]
 [2.578]] [[1.225]
 [0.835]
 [1.221]
 [1.162]
 [1.25 ]
 [1.166]
 [1.248]]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.545]
 [0.503]
 [0.512]
 [0.514]
 [0.519]
 [0.703]] [[-0.694]
 [ 1.485]
 [-0.177]
 [-0.491]
 [-0.39 ]
 [-0.307]
 [-0.097]] [[0.513]
 [0.545]
 [0.503]
 [0.512]
 [0.514]
 [0.519]
 [0.703]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.16 ]
 [-0.164]
 [-0.161]
 [-0.153]
 [-0.147]
 [-0.153]
 [-0.157]] [[1.287]
 [1.811]
 [1.991]
 [1.147]
 [1.402]
 [0.988]
 [1.409]] [[-0.443]
 [-0.276]
 [-0.209]
 [-0.477]
 [-0.379]
 [-0.529]
 [-0.397]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.63717467
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.487]
 [0.594]
 [0.596]
 [0.596]
 [0.599]
 [0.597]] [[2.338]
 [2.545]
 [2.478]
 [2.382]
 [2.501]
 [2.555]
 [2.748]] [[1.677]
 [1.67 ]
 [1.718]
 [1.688]
 [1.727]
 [1.747]
 [1.81 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.6714389780093768
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.896]] [[0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.935]] [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.896]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.806]] [[ 0.442]
 [ 0.442]
 [ 0.442]
 [ 0.442]
 [ 0.442]
 [ 0.442]
 [-1.378]] [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.806]]
line 256 mcts: sample exp_bonus 1.1801983360784454
first move QE:  0.6714389780093768
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.614]
 [0.613]
 [0.614]
 [0.614]
 [0.61 ]
 [0.605]] [[0.164]
 [0.164]
 [0.466]
 [0.164]
 [0.164]
 [0.218]
 [0.428]] [[0.614]
 [0.614]
 [0.613]
 [0.614]
 [0.614]
 [0.61 ]
 [0.605]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.306]
 [0.306]
 [0.306]
 [0.337]
 [0.328]
 [0.325]] [[6.313]
 [5.771]
 [5.771]
 [5.771]
 [5.98 ]
 [5.945]
 [6.294]] [[1.341]
 [0.964]
 [0.964]
 [0.964]
 [1.144]
 [1.109]
 [1.336]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 5.69956604137233
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[6.365]
 [6.365]
 [6.365]
 [6.365]
 [6.365]
 [6.365]
 [6.365]] [[1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]]
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.043]
 [-0.032]
 [-0.033]
 [-0.039]
 [-0.044]
 [-0.051]] [[2.126]
 [2.164]
 [2.158]
 [2.118]
 [2.134]
 [2.17 ]
 [2.156]] [[-0.563]
 [-0.527]
 [-0.51 ]
 [-0.553]
 [-0.55 ]
 [-0.523]
 [-0.551]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.082 0.347 0.245 0.02  0.02  0.122 0.163]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
2196 3228
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.6707205768449028
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.524]
 [0.415]
 [0.611]
 [0.525]
 [0.363]
 [0.559]] [[2.283]
 [1.858]
 [1.345]
 [0.115]
 [0.95 ]
 [1.287]
 [1.507]] [[0.865]
 [1.255]
 [0.864]
 [0.846]
 [0.953]
 [0.741]
 [1.206]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.62119675
in main func line 156:  2201
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.525]
 [0.555]
 [0.555]
 [0.56 ]
 [0.555]
 [0.555]] [[1.172]
 [1.14 ]
 [0.686]
 [0.686]
 [1.843]
 [0.686]
 [0.686]] [[1.701]
 [1.72 ]
 [1.629]
 [1.629]
 [2.026]
 [1.629]
 [1.629]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.796 0.    0.02  0.    0.163 0.    0.02 ]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.32 ]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.379]] [[7.198]
 [4.694]
 [7.198]
 [7.198]
 [7.198]
 [7.198]
 [6.813]] [[1.431]
 [0.108]
 [1.431]
 [1.431]
 [1.431]
 [1.431]
 [1.25 ]]
siam score:  -0.63083994
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]] [[1.949]
 [2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]] [[2.003]
 [2.062]
 [2.062]
 [2.062]
 [2.062]
 [2.062]
 [2.062]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6325777
first move QE:  0.6703134785192577
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.573]
 [0.57 ]] [[-0.648]
 [-0.323]
 [-0.323]
 [-0.323]
 [-0.323]
 [-1.02 ]
 [-0.534]] [[0.566]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.573]
 [0.57 ]]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.587]] [[-0.992]
 [-0.992]
 [-0.992]
 [-0.992]
 [-0.992]
 [-0.992]
 [-0.622]] [[0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.587]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.6701813965228406
in main func line 156:  2210
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.806]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[-3.623]
 [-1.408]
 [-3.623]
 [-3.623]
 [-3.623]
 [-3.623]
 [-3.623]] [[0.212]
 [0.916]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]] [[3.761]
 [3.761]
 [3.761]
 [3.761]
 [3.761]
 [3.761]
 [3.761]] [[2.1]
 [2.1]
 [2.1]
 [2.1]
 [2.1]
 [2.1]
 [2.1]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.029]] [[5.486]
 [5.486]
 [5.486]
 [5.486]
 [5.486]
 [5.486]
 [5.493]] [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.91 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.498986348576282
in main func line 156:  2212
line 256 mcts: sample exp_bonus 3.581664648833398
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.655]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[ 0.903]
 [ 1.203]
 [-0.278]
 [-0.278]
 [-0.278]
 [-0.278]
 [-0.278]] [[1.457]
 [1.761]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.754]
 [1.049]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[-0.019]
 [ 0.298]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[1.415]
 [2.11 ]
 [1.415]
 [1.415]
 [1.415]
 [1.415]
 [1.415]]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 1.1026209194367511
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.569]
 [0.569]
 [0.486]
 [0.569]
 [0.569]] [[ 1.652]
 [ 0.247]
 [ 0.836]
 [ 0.836]
 [-1.211]
 [ 0.836]
 [ 0.836]] [[1.757]
 [1.212]
 [1.484]
 [1.484]
 [0.627]
 [1.484]
 [1.484]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3098168498088476
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7147990810972575
siam score:  -0.65885335
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.537]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.576]] [[2.289]
 [2.539]
 [2.289]
 [2.289]
 [2.289]
 [2.289]
 [2.885]] [[1.452]
 [1.631]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.976]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.485]
 [0.387]
 [0.624]
 [0.589]
 [0.382]
 [0.525]] [[1.643]
 [1.735]
 [1.152]
 [1.154]
 [1.966]
 [1.32 ]
 [2.03 ]] [[0.872]
 [1.136]
 [0.746]
 [1.22 ]
 [1.42 ]
 [0.79 ]
 [1.313]]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.274]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[-4.121]
 [-3.978]
 [-4.234]
 [-4.234]
 [-4.234]
 [-4.234]
 [-4.234]] [[0.262]
 [0.274]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6597018
siam score:  -0.6594708
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.423]
 [0.472]
 [0.427]
 [0.475]
 [0.445]
 [0.431]] [[-0.593]
 [ 0.773]
 [-0.064]
 [-0.813]
 [-0.062]
 [-0.683]
 [-1.066]] [[0.41 ]
 [0.423]
 [0.472]
 [0.427]
 [0.475]
 [0.445]
 [0.431]]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.391]
 [0.599]
 [0.598]
 [0.542]
 [0.614]
 [0.599]] [[-2.111]
 [ 1.115]
 [-2.397]
 [-3.085]
 [-2.487]
 [-3.389]
 [-2.82 ]] [[0.652]
 [1.71 ]
 [0.574]
 [0.334]
 [0.519]
 [0.235]
 [0.426]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.454]
 [0.408]
 [0.396]
 [0.42 ]
 [0.387]
 [0.401]] [[-0.287]
 [-1.551]
 [-2.07 ]
 [-1.385]
 [-1.598]
 [-1.894]
 [-1.872]] [[0.837]
 [0.484]
 [0.24 ]
 [0.428]
 [0.407]
 [0.255]
 [0.287]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.512]
 [0.633]
 [0.595]
 [0.207]
 [0.633]
 [0.59 ]] [[ 1.128]
 [-0.178]
 [ 0.579]
 [-2.072]
 [ 0.951]
 [ 0.579]
 [-0.809]] [[1.708]
 [1.094]
 [1.618]
 [0.187]
 [1.366]
 [1.618]
 [0.844]]
siam score:  -0.64728665
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.329]
 [0.368]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[-2.456]
 [-0.802]
 [-3.054]
 [-2.241]
 [-2.451]
 [-2.578]
 [-2.196]] [[0.366]
 [0.329]
 [0.368]
 [0.366]
 [0.366]
 [0.366]
 [0.366]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.597]
 [0.597]
 [0.597]
 [0.566]
 [0.597]
 [0.597]] [[1.463]
 [1.013]
 [1.013]
 [1.013]
 [1.503]
 [1.013]
 [1.013]] [[0.255]
 [0.165]
 [0.165]
 [0.165]
 [0.267]
 [0.165]
 [0.165]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
start point for exploration sampling:  11106
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.63 ]
 [0.632]
 [0.632]
 [0.641]
 [0.642]
 [0.788]] [[1.96 ]
 [2.348]
 [2.607]
 [2.607]
 [2.102]
 [2.01 ]
 [0.772]] [[0.631]
 [0.63 ]
 [0.632]
 [0.632]
 [0.641]
 [0.642]
 [0.788]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.12221135370367167
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.7  ]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]] [[-2.483]
 [ 0.67 ]
 [-2.483]
 [-2.483]
 [-2.483]
 [-2.483]
 [-2.483]] [[0.566]
 [1.46 ]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Starting evaluation
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.524]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[-1.186]
 [ 0.004]
 [-1.186]
 [-1.186]
 [-1.186]
 [-1.186]
 [-1.186]] [[0.488]
 [0.524]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]]
line 256 mcts: sample exp_bonus 1.7947747984649878
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.426]
 [0.615]
 [0.691]
 [0.666]
 [0.667]
 [0.64 ]] [[1.592]
 [1.988]
 [1.366]
 [0.754]
 [1.245]
 [2.801]
 [1.818]] [[0.632]
 [0.426]
 [0.615]
 [0.691]
 [0.666]
 [0.667]
 [0.64 ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.74 ]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.741]] [[1.092]
 [0.79 ]
 [1.092]
 [1.092]
 [1.092]
 [1.092]
 [0.874]] [[0.714]
 [0.74 ]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.741]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.658]
 [0.666]] [[0.617]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.783]
 [0.57 ]] [[0.631]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.658]
 [0.666]]
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]] [[-4.846]
 [-4.846]
 [-4.846]
 [-4.846]
 [-4.846]
 [-4.846]
 [-4.846]] [[0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.016]
 [1.472]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[1.989]
 [0.655]
 [1.989]
 [1.989]
 [1.989]
 [1.989]
 [1.989]] [[1.642]
 [2.028]
 [1.642]
 [1.642]
 [1.642]
 [1.642]
 [1.642]]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]] [[0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]] [[0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.459]
 [0.45 ]
 [0.439]
 [0.452]
 [0.455]
 [0.461]] [[-4.428]
 [ 0.531]
 [-4.265]
 [-4.142]
 [-4.11 ]
 [-4.286]
 [-3.996]] [[-0.067]
 [ 1.188]
 [-0.026]
 [-0.   ]
 [ 0.014]
 [-0.029]
 [ 0.047]]
Printing some Q and Qe and total Qs values:  [[ 0.279]
 [ 0.29 ]
 [ 0.412]
 [ 0.465]
 [-0.012]
 [ 0.076]
 [ 0.23 ]] [[2.241]
 [2.046]
 [2.216]
 [1.981]
 [2.841]
 [2.363]
 [2.757]] [[1.161]
 [0.998]
 [1.321]
 [1.179]
 [1.311]
 [0.994]
 [1.565]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
2239 3319
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[0.661]
 [0.653]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[2.972]
 [2.961]
 [2.972]
 [2.972]
 [2.972]
 [2.972]
 [2.972]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.442]
 [0.422]
 [0.439]
 [0.421]
 [0.439]
 [0.413]] [[-4.965]
 [-4.693]
 [-4.835]
 [ 0.   ]
 [-4.969]
 [ 0.   ]
 [-4.99 ]] [[0.412]
 [0.442]
 [0.422]
 [0.439]
 [0.421]
 [0.439]
 [0.413]]
start point for exploration sampling:  11106
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.421]
 [0.46 ]
 [0.46 ]
 [0.456]
 [0.461]
 [0.465]] [[-1.673]
 [ 1.053]
 [-3.829]
 [-2.639]
 [-2.456]
 [-3.111]
 [-2.629]] [[0.452]
 [0.421]
 [0.46 ]
 [0.46 ]
 [0.456]
 [0.461]
 [0.465]]
line 256 mcts: sample exp_bonus 0.08753327769741766
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.513]] [[5.269]
 [5.269]
 [5.269]
 [5.269]
 [5.269]
 [5.269]
 [3.924]] [[0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.513]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2245 3323
Printing some Q and Qe and total Qs values:  [[0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]] [[3.047]
 [3.047]
 [3.047]
 [3.047]
 [3.047]
 [3.047]
 [3.047]] [[1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]]
siam score:  -0.62998086
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.6572963187736562
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.543]
 [0.493]
 [0.493]
 [0.491]
 [0.493]
 [0.493]] [[2.387]
 [2.837]
 [2.619]
 [2.619]
 [1.43 ]
 [2.619]
 [2.619]] [[0.533]
 [0.543]
 [0.493]
 [0.493]
 [0.491]
 [0.493]
 [0.493]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.872]
 [0.726]] [[0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.625]
 [0.296]] [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.872]
 [0.726]]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[-4.524]
 [-4.524]
 [-4.524]
 [-4.524]
 [-4.524]
 [-4.524]
 [-4.524]] [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]]
siam score:  -0.62506396
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.876748586871231
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.177]
 [0.895]
 [0.89 ]
 [0.891]
 [0.891]
 [0.889]
 [0.888]] [[0.538]
 [1.065]
 [0.786]
 [0.877]
 [0.952]
 [0.864]
 [0.948]] [[1.299]
 [1.085]
 [0.89 ]
 [0.954]
 [1.003]
 [0.941]
 [0.994]]
using explorer policy with actor:  1
start point for exploration sampling:  11106
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0098545479167662
siam score:  -0.6182159
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.61389273
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.194]
 [0.469]
 [0.719]
 [0.474]
 [0.944]
 [0.306]] [[1.137]
 [1.924]
 [1.187]
 [1.363]
 [1.368]
 [1.259]
 [1.882]] [[0.27 ]
 [0.083]
 [0.142]
 [0.759]
 [0.272]
 [1.139]
 [0.278]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.069]
 [ 0.305]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]] [[2.069]
 [0.634]
 [2.069]
 [2.069]
 [2.069]
 [2.069]
 [2.069]] [[-1.201]
 [-0.933]
 [-1.201]
 [-1.201]
 [-1.201]
 [-1.201]
 [-1.201]]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.551]
 [0.579]
 [0.581]
 [0.57 ]
 [0.57 ]
 [0.576]] [[-3.863]
 [-1.108]
 [-3.553]
 [-3.536]
 [-3.026]
 [-3.398]
 [-3.158]] [[0.571]
 [1.953]
 [0.733]
 [0.743]
 [0.993]
 [0.803]
 [0.932]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.345]
 [0.586]
 [0.583]
 [0.552]
 [0.568]
 [0.565]] [[-3.446]
 [ 0.795]
 [-3.366]
 [-3.13 ]
 [-2.911]
 [-3.06 ]
 [-2.583]] [[0.53 ]
 [1.768]
 [0.594]
 [0.672]
 [0.715]
 [0.681]
 [0.84 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.851]
 [0.851]
 [0.855]
 [0.851]
 [0.851]
 [0.845]] [[1.248]
 [1.521]
 [1.521]
 [1.05 ]
 [1.521]
 [1.521]
 [1.503]] [[0.319]
 [0.991]
 [0.991]
 [0.841]
 [0.991]
 [0.991]
 [0.972]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.433]
 [0.381]
 [0.42 ]
 [0.419]
 [0.409]
 [0.423]] [[1.864]
 [1.031]
 [1.145]
 [1.279]
 [1.564]
 [1.714]
 [1.855]] [[ 0.228]
 [-0.024]
 [-0.09 ]
 [ 0.032]
 [ 0.126]
 [ 0.156]
 [ 0.231]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.60720694
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[1.128]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]] [[0.487]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]]
2261 3382
siam score:  -0.60810363
siam score:  -0.6099572
line 256 mcts: sample exp_bonus 0.2536663313461022
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.022]
 [-0.004]
 [-0.004]
 [-0.005]
 [-0.004]
 [-0.004]] [[1.295]
 [2.88 ]
 [1.008]
 [1.315]
 [1.404]
 [1.688]
 [1.831]] [[-0.735]
 [-0.235]
 [-0.823]
 [-0.721]
 [-0.692]
 [-0.596]
 [-0.547]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6115849
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.388]
 [0.171]
 [0.525]
 [0.248]
 [0.248]
 [0.361]] [[1.632]
 [2.105]
 [1.924]
 [1.734]
 [1.939]
 [1.417]
 [2.094]] [[0.629]
 [1.409]
 [0.867]
 [1.324]
 [1.017]
 [0.557]
 [1.351]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.629]] [[-0.524]
 [-0.524]
 [-0.524]
 [-0.524]
 [-0.524]
 [-0.524]
 [-1.614]] [[0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.629]]
Printing some Q and Qe and total Qs values:  [[0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.891]] [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.825]] [[0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.891]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.517]
 [0.426]
 [0.509]
 [0.489]
 [0.422]
 [0.481]] [[1.773]
 [3.062]
 [1.655]
 [2.629]
 [2.215]
 [0.072]
 [2.452]] [[0.461]
 [0.517]
 [0.426]
 [0.509]
 [0.489]
 [0.422]
 [0.481]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.6398642544223638
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.318]
 [0.424]
 [0.426]
 [0.428]
 [0.423]
 [0.427]] [[1.167]
 [2.029]
 [1.047]
 [0.894]
 [1.065]
 [0.894]
 [1.027]] [[0.417]
 [0.318]
 [0.424]
 [0.426]
 [0.428]
 [0.423]
 [0.427]]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.373]
 [0.406]
 [0.455]
 [0.423]
 [0.453]
 [0.44 ]] [[0.984]
 [1.283]
 [0.874]
 [1.165]
 [1.054]
 [0.647]
 [0.762]] [[-0.125]
 [-0.149]
 [-0.22 ]
 [-0.024]
 [-0.125]
 [-0.202]
 [-0.189]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.1944971409528504
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.481]
 [0.495]
 [0.493]
 [0.491]
 [0.493]
 [0.495]] [[2.644]
 [3.022]
 [2.429]
 [2.785]
 [2.918]
 [2.908]
 [3.006]] [[0.11 ]
 [0.227]
 [0.058]
 [0.172]
 [0.213]
 [0.212]
 [0.249]]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[2.134]
 [2.134]
 [2.134]
 [2.134]
 [2.134]
 [2.134]
 [2.134]] [[0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]]
siam score:  -0.62486625
siam score:  -0.6263693
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.61240315
2280 3414
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.61155736
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.605]
 [0.592]
 [0.587]
 [0.605]
 [0.608]
 [0.596]] [[0.085]
 [1.746]
 [0.253]
 [0.067]
 [0.199]
 [0.083]
 [0.31 ]] [[0.559]
 [0.605]
 [0.592]
 [0.587]
 [0.605]
 [0.608]
 [0.596]]
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.808]
 [0.716]] [[-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.185]
 [-0.153]] [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.808]
 [0.716]]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.395]
 [0.964]
 [0.428]
 [0.408]
 [0.373]
 [0.401]] [[0.828]
 [1.386]
 [0.952]
 [1.031]
 [1.046]
 [1.139]
 [1.134]] [[-0.191]
 [ 0.008]
 [ 1.   ]
 [-0.045]
 [-0.08 ]
 [-0.119]
 [-0.064]]
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]] [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.187]
 [0.491]
 [0.492]
 [0.252]
 [0.492]
 [0.156]] [[1.118]
 [1.023]
 [0.857]
 [1.118]
 [1.039]
 [1.118]
 [1.534]] [[1.755]
 [1.588]
 [1.66 ]
 [1.755]
 [1.622]
 [1.755]
 [1.759]]
first move QE:  0.6327474690629733
2286 3424
using explorer policy with actor:  1
siam score:  -0.62784266
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.62159586
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.636]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.53 ]] [[1.482]
 [1.679]
 [0.879]
 [0.879]
 [0.879]
 [0.879]
 [1.749]] [[1.616]
 [1.833]
 [1.525]
 [1.525]
 [1.525]
 [1.525]
 [1.783]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.536]
 [0.624]
 [0.628]
 [0.607]
 [0.622]
 [0.625]] [[-1.409]
 [ 1.174]
 [-1.206]
 [-1.003]
 [ 0.789]
 [-1.056]
 [-1.175]] [[0.895]
 [1.579]
 [0.962]
 [1.036]
 [1.594]
 [1.006]
 [0.972]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.045]
 [0.068]
 [0.071]
 [0.07 ]
 [0.068]
 [0.07 ]] [[-1.077]
 [-0.18 ]
 [-1.036]
 [-0.964]
 [-0.889]
 [-1.033]
 [-1.166]] [[0.064]
 [0.045]
 [0.068]
 [0.071]
 [0.07 ]
 [0.068]
 [0.07 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.412]
 [0.508]
 [0.617]
 [0.508]
 [0.508]
 [0.442]] [[2.204]
 [2.222]
 [2.204]
 [2.967]
 [2.204]
 [2.204]
 [2.476]] [[0.727]
 [0.546]
 [0.727]
 [1.453]
 [0.727]
 [0.727]
 [0.777]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.0911252777567793
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.361]
 [0.361]
 [0.361]
 [0.36 ]
 [0.391]
 [0.353]] [[2.766]
 [3.775]
 [3.775]
 [3.775]
 [3.173]
 [2.535]
 [3.333]] [[-0.047]
 [ 0.631]
 [ 0.631]
 [ 0.631]
 [ 0.228]
 [-0.135]
 [ 0.322]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[1.463]
 [1.463]
 [1.463]
 [1.463]
 [1.463]
 [1.463]
 [1.463]] [[0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]] [[4.241]
 [4.241]
 [4.241]
 [4.241]
 [4.241]
 [4.241]
 [4.241]] [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6208564984010152
2301 3439
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.345]
 [0.415]
 [0.191]
 [0.208]
 [0.248]
 [0.415]] [[1.945]
 [1.945]
 [1.243]
 [1.411]
 [1.649]
 [1.918]
 [1.243]] [[-0.587]
 [-0.44 ]
 [-0.535]
 [-0.928]
 [-0.813]
 [-0.643]
 [-0.535]]
Printing some Q and Qe and total Qs values:  [[ 0.334]
 [ 0.015]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.703]
 [1.321]
 [1.152]
 [1.152]
 [1.152]
 [1.152]
 [1.152]] [[-0.796]
 [-1.228]
 [-1.32 ]
 [-1.32 ]
 [-1.32 ]
 [-1.32 ]
 [-1.32 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2305 3441
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.45 ]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[1.855]
 [2.432]
 [1.855]
 [1.855]
 [1.855]
 [1.855]
 [1.855]] [[0.272]
 [0.424]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]]
siam score:  -0.6293504
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2765377371189022
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.432]
 [0.433]
 [0.406]
 [0.404]
 [0.414]
 [0.402]] [[-0.747]
 [-0.553]
 [-0.927]
 [-1.545]
 [-1.219]
 [-0.747]
 [-0.964]] [[0.395]
 [0.432]
 [0.433]
 [0.406]
 [0.404]
 [0.414]
 [0.402]]
line 256 mcts: sample exp_bonus -0.09781347746139299
line 256 mcts: sample exp_bonus 2.5713554750365306
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.713]] [[1.816]
 [1.816]
 [1.816]
 [1.816]
 [1.816]
 [1.816]
 [2.333]] [[0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.713]]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.531]] [[1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.407]] [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.531]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.564]] [[1.409]
 [1.409]
 [1.409]
 [1.409]
 [1.409]
 [1.409]
 [2.232]] [[0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.564]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2322 3457
using explorer policy with actor:  1
siam score:  -0.60882264
first move QE:  0.6302385961978582
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.426]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]] [[0.623]
 [0.945]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[0.43 ]
 [0.426]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8791458855029615
2323 3460
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.794]
 [0.621]
 [0.631]
 [0.639]
 [0.633]
 [0.639]] [[2.022]
 [2.374]
 [2.532]
 [2.2  ]
 [2.522]
 [2.616]
 [2.737]] [[1.26 ]
 [1.698]
 [1.586]
 [1.399]
 [1.602]
 [1.65 ]
 [1.731]]
rdn beta is 0 so we're just using the maxi policy
first move QE:  0.6305069352039663
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.54 ]] [[ 0.112]
 [ 0.112]
 [ 0.112]
 [ 0.112]
 [ 0.112]
 [ 0.112]
 [-0.448]] [[0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.54 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3106101418470628
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.76 ]] [[1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]
 [0.508]] [[0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.76 ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.63 ]] [[2.161]
 [2.161]
 [2.161]
 [2.161]
 [2.161]
 [2.161]
 [1.944]] [[0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.63 ]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.1533206240893989
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.339]
 [0.322]
 [0.33 ]
 [0.331]
 [0.318]
 [0.331]] [[0.928]
 [0.854]
 [0.265]
 [0.792]
 [0.498]
 [0.145]
 [0.539]] [[0.329]
 [0.339]
 [0.322]
 [0.33 ]
 [0.331]
 [0.318]
 [0.331]]
siam score:  -0.62655234
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.656]] [[2.511]
 [2.511]
 [2.511]
 [2.511]
 [2.511]
 [2.511]
 [1.904]] [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.656]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.432]
 [0.575]
 [0.432]
 [0.432]
 [0.513]
 [0.432]] [[1.208]
 [1.208]
 [0.99 ]
 [1.208]
 [1.208]
 [1.351]
 [1.208]] [[1.21 ]
 [1.21 ]
 [1.423]
 [1.21 ]
 [1.21 ]
 [1.421]
 [1.21 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.808]] [[1.867]
 [1.867]
 [1.867]
 [1.867]
 [1.867]
 [1.867]
 [1.938]] [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.808]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.62042445
siam score:  -0.6193892
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.202]
 [0.37 ]
 [0.366]
 [0.363]
 [0.364]
 [0.364]] [[-0.622]
 [ 0.942]
 [-1.067]
 [-0.406]
 [-0.068]
 [-0.467]
 [-0.613]] [[0.354]
 [0.202]
 [0.37 ]
 [0.366]
 [0.363]
 [0.364]
 [0.364]]
Printing some Q and Qe and total Qs values:  [[0.91 ]
 [0.811]
 [0.877]
 [0.654]
 [0.654]
 [0.925]
 [0.654]] [[0.543]
 [0.858]
 [0.308]
 [1.189]
 [1.189]
 [0.433]
 [1.189]] [[0.91 ]
 [0.811]
 [0.877]
 [0.654]
 [0.654]
 [0.925]
 [0.654]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]] [[2.397]
 [2.408]
 [2.408]
 [2.408]
 [2.408]
 [2.408]
 [2.408]] [[1.942]
 [1.971]
 [1.971]
 [1.971]
 [1.971]
 [1.971]
 [1.971]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6286946
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.484]
 [0.677]
 [0.676]
 [0.677]
 [0.683]
 [0.685]] [[2.776]
 [2.72 ]
 [3.003]
 [2.808]
 [2.952]
 [3.19 ]
 [3.271]] [[1.882]
 [1.503]
 [2.012]
 [1.895]
 [1.981]
 [2.132]
 [2.183]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.547]
 [0.547]
 [0.511]
 [0.547]
 [0.547]
 [0.505]] [[1.358]
 [0.995]
 [0.995]
 [1.253]
 [0.995]
 [0.995]
 [1.224]] [[-0.014]
 [-0.16 ]
 [-0.16 ]
 [-0.061]
 [-0.16 ]
 [-0.16 ]
 [-0.091]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.587]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.747]] [[ 0.642]
 [ 1.863]
 [ 0.642]
 [ 0.642]
 [ 0.642]
 [ 0.642]
 [-2.023]] [[0.532]
 [0.587]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.747]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.54 ]
 [0.505]
 [0.496]
 [0.496]
 [0.515]
 [0.551]] [[1.272]
 [2.39 ]
 [0.744]
 [0.853]
 [0.833]
 [1.255]
 [0.363]] [[0.508]
 [0.54 ]
 [0.505]
 [0.496]
 [0.496]
 [0.515]
 [0.551]]
siam score:  -0.6378445
Printing some Q and Qe and total Qs values:  [[0.946]
 [0.946]
 [1.053]
 [0.945]
 [0.944]
 [0.941]
 [0.946]] [[0.831]
 [0.831]
 [0.659]
 [0.793]
 [0.811]
 [0.864]
 [0.831]] [[0.612]
 [0.612]
 [0.769]
 [0.598]
 [0.602]
 [0.614]
 [0.612]]
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.864]] [[0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [4.331]] [[0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.864]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  11106
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.29 ]
 [0.471]
 [0.471]
 [0.471]
 [0.493]
 [0.498]] [[4.286]
 [4.789]
 [3.932]
 [3.932]
 [3.932]
 [4.489]
 [4.484]] [[0.669]
 [0.468]
 [0.543]
 [0.543]
 [0.543]
 [0.773]
 [0.782]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.157]
 [0.131]
 [0.157]
 [0.157]
 [0.106]
 [0.157]] [[5.413]
 [3.642]
 [4.545]
 [3.642]
 [3.642]
 [4.273]
 [3.642]] [[-0.287]
 [-0.845]
 [-0.597]
 [-0.845]
 [-0.845]
 [-0.738]
 [-0.845]]
line 256 mcts: sample exp_bonus -1.8425431696029908
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.42 ]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[-0.077]
 [ 1.68 ]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]] [[0.692]
 [1.407]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.232]
 [0.726]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[3.258]
 [2.313]
 [2.513]
 [2.513]
 [2.513]
 [2.513]
 [2.513]] [[2.005]
 [0.881]
 [0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.912]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6457842
first move QE:  0.6313824890702626
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.739]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.633]] [[0.672]
 [2.693]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [1.53 ]] [[1.023]
 [1.617]
 [1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.244]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]] [[0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 6.146356043199456
line 256 mcts: sample exp_bonus -1.746075213701292
siam score:  -0.64356536
siam score:  -0.64407
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.551]
 [0.551]
 [0.551]
 [0.597]
 [0.551]
 [0.551]] [[1.878]
 [1.764]
 [1.764]
 [1.764]
 [3.145]
 [1.764]
 [1.764]] [[1.038]
 [0.934]
 [0.934]
 [0.934]
 [1.487]
 [0.934]
 [0.934]]
siam score:  -0.63935846
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.53 ]
 [0.506]
 [0.525]
 [0.508]
 [0.497]
 [0.504]] [[-1.13 ]
 [-0.378]
 [-1.143]
 [-1.079]
 [-1.099]
 [-0.626]
 [-0.708]] [[0.184]
 [0.902]
 [0.158]
 [0.245]
 [0.202]
 [0.625]
 [0.559]]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.52 ]
 [0.487]
 [0.516]
 [0.49 ]
 [0.487]
 [0.483]] [[-1.707]
 [-0.925]
 [-1.556]
 [-0.921]
 [-1.425]
 [-1.374]
 [-1.291]] [[0.074]
 [0.524]
 [0.158]
 [0.524]
 [0.231]
 [0.256]
 [0.297]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6466467
UNIT TEST: sample policy line 217 mcts : [0.041 0.51  0.02  0.286 0.061 0.061 0.02 ]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.122 0.    0.02  0.    0.816 0.02  0.02 ]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.033]
 [0.226]
 [0.19 ]
 [0.177]
 [0.208]
 [0.208]] [[4.438]
 [5.054]
 [4.784]
 [4.677]
 [4.71 ]
 [4.778]
 [4.749]] [[-0.339]
 [-0.471]
 [-0.174]
 [-0.282]
 [-0.297]
 [-0.212]
 [-0.223]]
line 256 mcts: sample exp_bonus 3.0053859830096883
siam score:  -0.64685065
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.023106883951459
siam score:  -0.64594823
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]] [[2.856]
 [2.856]
 [2.856]
 [2.856]
 [2.856]
 [2.856]
 [2.856]] [[0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.585]
 [0.621]
 [0.606]
 [0.6  ]
 [0.606]
 [0.608]] [[ 0.158]
 [ 0.565]
 [ 0.243]
 [-0.296]
 [ 0.044]
 [-0.063]
 [ 0.242]] [[2.554]
 [2.647]
 [2.61 ]
 [2.401]
 [2.502]
 [2.478]
 [2.584]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.228]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[0.736]
 [1.399]
 [1.177]
 [1.177]
 [1.177]
 [1.177]
 [1.177]] [[-1.089]
 [-0.025]
 [-0.654]
 [-0.654]
 [-0.654]
 [-0.654]
 [-0.654]]
using explorer policy with actor:  1
siam score:  -0.6373794
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.199]] [[7.284]
 [7.284]
 [7.284]
 [7.284]
 [7.284]
 [7.284]
 [7.135]] [[1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.428]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.296]
 [0.36 ]
 [0.296]
 [0.336]
 [0.296]
 [0.296]] [[2.114]
 [2.114]
 [2.665]
 [2.114]
 [1.935]
 [2.114]
 [2.114]] [[0.15 ]
 [0.15 ]
 [0.644]
 [0.15 ]
 [0.11 ]
 [0.15 ]
 [0.15 ]]
first move QE:  0.6307022750431316
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]] [[5.659]
 [5.659]
 [5.659]
 [5.659]
 [5.659]
 [5.659]
 [5.659]] [[0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]]
using explorer policy with actor:  1
siam score:  -0.63703066
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[3.104]
 [3.383]
 [3.383]
 [3.383]
 [3.383]
 [3.383]
 [3.383]] [[0.452]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.073]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]] [[3.095]
 [3.88 ]
 [3.095]
 [3.095]
 [3.095]
 [3.095]
 [3.095]] [[-0.09 ]
 [ 0.488]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]]
Starting evaluation
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.392]
 [0.358]
 [0.358]] [[0.622]
 [0.877]
 [0.877]
 [0.877]
 [1.249]
 [0.877]
 [0.877]] [[0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.392]
 [0.358]
 [0.358]]
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.397]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[-3.048]
 [-2.179]
 [-2.683]
 [-2.683]
 [-2.683]
 [-2.683]
 [-2.953]] [[0.383]
 [0.397]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.193]
 [0.422]
 [0.422]
 [0.417]
 [0.419]
 [0.425]] [[0.807]
 [0.73 ]
 [0.457]
 [0.457]
 [0.635]
 [0.858]
 [0.785]] [[-0.032]
 [-0.508]
 [-0.142]
 [-0.142]
 [-0.091]
 [-0.013]
 [-0.025]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.421]
 [0.414]
 [0.409]
 [0.409]
 [0.408]
 [0.405]] [[-2.532]
 [-2.249]
 [-2.895]
 [-2.991]
 [-2.74 ]
 [-2.585]
 [-2.766]] [[0.414]
 [0.421]
 [0.414]
 [0.409]
 [0.409]
 [0.408]
 [0.405]]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.543]
 [0.543]
 [0.543]
 [0.661]
 [0.543]
 [0.543]] [[1.894]
 [1.778]
 [1.778]
 [1.778]
 [2.696]
 [1.778]
 [1.778]] [[0.94 ]
 [0.822]
 [0.822]
 [0.822]
 [1.611]
 [0.822]
 [0.822]]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.6353266
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.2694287113597373
line 256 mcts: sample exp_bonus 0.642296751271917
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]] [[-2.542]
 [-2.542]
 [-2.542]
 [-2.542]
 [-2.542]
 [-2.542]
 [-2.542]] [[0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.64582795
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.605]
 [0.717]
 [0.71 ]
 [0.737]
 [0.696]
 [0.741]] [[-1.372]
 [ 1.2  ]
 [-1.84 ]
 [-1.707]
 [-0.779]
 [-0.565]
 [-1.222]] [[0.727]
 [0.605]
 [0.717]
 [0.71 ]
 [0.737]
 [0.696]
 [0.741]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2389 3538
siam score:  -0.63674206
Printing some Q and Qe and total Qs values:  [[1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]] [[0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.458]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.513]] [[1.09 ]
 [2.351]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [2.606]] [[0.383]
 [0.458]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.513]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -0.821258779014216
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.935]
 [0.966]
 [0.885]
 [0.785]
 [0.857]
 [0.834]
 [0.866]] [[2.141]
 [2.833]
 [2.617]
 [2.908]
 [2.649]
 [3.301]
 [3.247]] [[0.485]
 [1.05 ]
 [0.767]
 [0.835]
 [0.749]
 [1.204]
 [1.212]]
line 256 mcts: sample exp_bonus 0.6566229868941165
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
2395 3546
siam score:  -0.6272144
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.555]
 [0.576]
 [0.578]
 [0.577]
 [0.576]
 [0.576]] [[-0.094]
 [ 2.937]
 [ 0.076]
 [ 0.234]
 [ 0.23 ]
 [ 0.212]
 [ 0.858]] [[0.182]
 [1.839]
 [0.278]
 [0.367]
 [0.363]
 [0.353]
 [0.709]]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.216]
 [0.255]
 [0.249]
 [0.261]
 [0.333]
 [0.316]] [[6.891]
 [6.983]
 [6.736]
 [7.004]
 [7.177]
 [6.533]
 [7.008]] [[1.299]
 [1.313]
 [1.219]
 [1.362]
 [1.472]
 [1.193]
 [1.438]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]] [[7.098]
 [7.098]
 [7.098]
 [7.098]
 [7.098]
 [7.098]
 [6.631]] [[1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.233]
 [0.98 ]]
siam score:  -0.62480223
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.267]
 [0.287]
 [0.281]
 [0.267]
 [0.278]
 [0.266]] [[ 1.044]
 [ 1.044]
 [-1.823]
 [-0.263]
 [ 1.044]
 [-1.33 ]
 [ 2.227]] [[0.267]
 [0.267]
 [0.287]
 [0.281]
 [0.267]
 [0.278]
 [0.266]]
first move QE:  0.6257396757615661
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.63138527
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.874]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]] [[-0.99 ]
 [ 2.224]
 [-0.99 ]
 [-0.99 ]
 [-0.99 ]
 [-0.99 ]
 [-0.99 ]] [[0.689]
 [1.635]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]]
siam score:  -0.6310314
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.786]] [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [4.018]] [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.786]]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.712]] [[0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [1.654]] [[0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.712]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.62556183
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6253326733320183
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.624642674850049
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.63607293
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.528]
 [0.53 ]
 [0.51 ]
 [0.557]
 [0.672]
 [0.58 ]] [[1.167]
 [1.024]
 [1.106]
 [1.025]
 [1.294]
 [1.292]
 [1.056]] [[0.255]
 [0.079]
 [0.16 ]
 [0.045]
 [0.388]
 [0.601]
 [0.207]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
2418 3565
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.291]
 [0.316]
 [0.311]
 [0.306]
 [0.303]
 [0.31 ]] [[1.767]
 [2.113]
 [1.944]
 [2.048]
 [2.231]
 [2.135]
 [2.111]] [[0.146]
 [0.397]
 [0.333]
 [0.393]
 [0.505]
 [0.436]
 [0.433]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[1.695]
 [1.695]
 [1.695]
 [1.695]
 [1.695]
 [1.695]
 [1.695]] [[1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.63167405
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]] [[2.435]
 [2.435]
 [2.435]
 [2.435]
 [2.435]
 [2.435]
 [2.435]] [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.532]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[1.05 ]
 [3.111]
 [1.237]
 [1.237]
 [1.237]
 [1.237]
 [1.237]] [[0.449]
 [1.579]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
using explorer policy with actor:  1
2426 3567
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.409]
 [0.623]
 [0.623]
 [0.623]
 [1.014]
 [0.607]] [[ 0.591]
 [ 0.985]
 [ 0.591]
 [ 0.591]
 [ 0.591]
 [-0.218]
 [ 0.173]] [[0.512]
 [0.215]
 [0.512]
 [0.512]
 [0.512]
 [1.023]
 [0.338]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.6231960144931694
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
2429 3572
siam score:  -0.62664205
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]] [[2.289]
 [2.289]
 [2.289]
 [2.289]
 [2.289]
 [2.289]
 [2.289]] [[-0.79]
 [-0.79]
 [-0.79]
 [-0.79]
 [-0.79]
 [-0.79]
 [-0.79]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 2.4655303011423
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.632]
 [0.617]
 [0.617]
 [0.614]
 [0.617]
 [0.617]] [[2.155]
 [3.194]
 [2.789]
 [2.789]
 [1.459]
 [2.249]
 [2.789]] [[0.621]
 [0.632]
 [0.617]
 [0.617]
 [0.614]
 [0.617]
 [0.617]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]] [[3.625]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]] [[2.27]
 [1.64]
 [1.64]
 [1.64]
 [1.64]
 [1.64]
 [1.64]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
first move QE:  0.6224433093769651
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.624]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[1.614]
 [3.487]
 [1.475]
 [1.475]
 [1.475]
 [1.475]
 [1.475]] [[1.034]
 [1.874]
 [1.138]
 [1.138]
 [1.138]
 [1.138]
 [1.138]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.244]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]] [[-1.09 ]
 [-1.186]
 [-1.09 ]
 [-1.09 ]
 [-1.09 ]
 [-1.09 ]
 [-1.09 ]] [[0.223]
 [0.244]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.023]
 [-0.021]
 [ 0.024]
 [-0.021]
 [-0.01 ]
 [-0.012]] [[3.326]
 [3.777]
 [3.931]
 [3.549]
 [3.931]
 [3.541]
 [4.617]] [[-0.468]
 [-0.308]
 [-0.253]
 [-0.291]
 [-0.253]
 [-0.361]
 [-0.005]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.01 ]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.005]
 [-0.015]] [[3.946]
 [4.   ]
 [2.975]
 [2.975]
 [2.975]
 [3.932]
 [3.643]] [[-0.687]
 [-0.656]
 [-1.031]
 [-1.031]
 [-1.031]
 [-0.668]
 [-0.785]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0786721658432785
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.482]] [[1.637]
 [1.637]
 [1.637]
 [1.637]
 [1.637]
 [1.637]
 [1.874]] [[0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.482]]
actor:  1 policy actor:  1  step number:  107 total reward:  0.22999999999999943  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.672]
 [0.465]] [[-0.078]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.34 ]
 [-0.138]] [[0.45 ]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.672]
 [0.465]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[2.842]
 [1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]] [[1.222]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
2453 3590
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
from probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
from probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.477]
 [0.443]
 [0.443]
 [0.442]
 [0.439]
 [0.437]] [[-0.627]
 [-0.642]
 [-0.625]
 [-0.625]
 [-0.539]
 [-0.589]
 [-0.628]] [[0.437]
 [0.477]
 [0.443]
 [0.443]
 [0.442]
 [0.439]
 [0.437]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.468]
 [0.466]
 [0.465]
 [0.467]
 [0.467]
 [0.458]] [[-0.36 ]
 [ 0.104]
 [-0.42 ]
 [-0.228]
 [-0.182]
 [-0.19 ]
 [-0.558]] [[0.466]
 [0.468]
 [0.466]
 [0.465]
 [0.467]
 [0.467]
 [0.458]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.487]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]] [[-0.991]
 [-0.006]
 [-0.991]
 [-0.991]
 [-0.991]
 [-0.991]
 [-0.991]] [[0.44 ]
 [0.487]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.64790237
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
line 256 mcts: sample exp_bonus 2.5024190054249877
from probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
siam score:  -0.6534369
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.859]
 [0.697]
 [0.701]
 [0.691]
 [0.688]
 [0.682]] [[1.181]
 [3.119]
 [0.942]
 [1.125]
 [1.254]
 [1.553]
 [1.822]] [[0.341]
 [1.632]
 [0.208]
 [0.322]
 [0.395]
 [0.574]
 [0.735]]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.495]
 [0.478]
 [0.477]
 [0.477]
 [0.477]
 [0.513]] [[2.821]
 [3.294]
 [2.965]
 [2.821]
 [2.821]
 [2.821]
 [2.679]] [[0.477]
 [0.495]
 [0.478]
 [0.477]
 [0.477]
 [0.477]
 [0.513]]
siam score:  -0.65004706
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.813]
 [0.823]
 [0.826]
 [0.797]
 [0.821]
 [0.802]] [[3.34 ]
 [4.527]
 [3.754]
 [5.345]
 [3.709]
 [4.527]
 [4.911]] [[0.708]
 [1.545]
 [1.04 ]
 [2.097]
 [0.99 ]
 [1.551]
 [1.79 ]]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.11 ]
 [0.18 ]
 [0.125]
 [0.133]
 [0.217]
 [0.143]] [[5.231]
 [3.429]
 [5.051]
 [4.15 ]
 [4.317]
 [4.457]
 [4.647]] [[-0.245]
 [-0.893]
 [-0.21 ]
 [-0.622]
 [-0.551]
 [-0.335]
 [-0.419]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.576]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.67 ]] [[2.775]
 [2.553]
 [1.643]
 [1.643]
 [1.643]
 [1.643]
 [3.509]] [[1.939]
 [1.822]
 [1.467]
 [1.467]
 [1.467]
 [1.467]
 [2.255]]
from probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.082949674295707
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[0.762]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]] [[0.429]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [1.247]
 [0.539]] [[1.116]
 [1.116]
 [1.116]
 [1.116]
 [1.116]
 [0.667]
 [1.116]] [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [1.997]
 [0.731]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.513]
 [0.616]
 [0.624]
 [0.626]
 [0.614]
 [0.654]] [[2.566]
 [2.497]
 [2.279]
 [2.413]
 [2.676]
 [2.236]
 [2.816]] [[1.854]
 [1.777]
 [1.753]
 [1.808]
 [1.907]
 [1.736]
 [1.975]]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.431]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[2.827]
 [3.471]
 [2.827]
 [2.827]
 [2.827]
 [2.827]
 [2.827]] [[0.938]
 [1.297]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
siam score:  -0.6207189
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.607]
 [0.618]
 [0.614]
 [0.599]
 [0.61 ]
 [0.683]] [[2.741]
 [2.894]
 [2.871]
 [2.828]
 [2.949]
 [2.853]
 [2.518]] [[1.451]
 [1.507]
 [1.508]
 [1.455]
 [1.545]
 [1.472]
 [1.284]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[8.09]
 [8.09]
 [8.09]
 [8.09]
 [8.09]
 [8.09]
 [8.09]] [[1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.351]]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.03 ]] [[6.182]
 [6.182]
 [6.182]
 [6.182]
 [6.182]
 [6.182]
 [7.824]] [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [1.505]]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.029]] [[5.991]
 [5.991]
 [5.991]
 [5.991]
 [5.991]
 [5.991]
 [7.547]] [[0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [1.407]]
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.021]] [[4.811]
 [4.811]
 [4.811]
 [4.811]
 [4.811]
 [4.811]
 [6.994]] [[0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.892]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.514]
 [0.514]
 [0.529]
 [0.523]
 [0.522]
 [0.541]] [[1.184]
 [1.328]
 [1.174]
 [1.264]
 [1.346]
 [1.345]
 [1.088]] [[0.754]
 [0.858]
 [0.705]
 [0.824]
 [0.894]
 [0.89 ]
 [0.672]]
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.071]
 [0.082]
 [0.052]
 [0.054]
 [0.076]
 [0.132]] [[1.447]
 [2.251]
 [1.746]
 [1.499]
 [1.668]
 [1.934]
 [1.726]] [[-1.017]
 [-0.759]
 [-0.905]
 [-1.047]
 [-0.987]
 [-0.855]
 [-0.812]]
using explorer policy with actor:  1
using another actor
first move QE:  0.6261234887383503
line 256 mcts: sample exp_bonus 6.0689011325201
first move QE:  0.6260039197141323
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[2.002]
 [2.002]
 [2.002]
 [2.002]
 [2.002]
 [2.002]
 [2.002]] [[0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.762948776985388
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
line 256 mcts: sample exp_bonus 2.6678414993296165
first move QE:  0.6258655682846677
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 0.4727564064732841
using explorer policy with actor:  1
using explorer policy with actor:  0
siam score:  -0.6336763
siam score:  -0.6351788
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
using explorer policy with actor:  1
using explorer policy with actor:  1
2507 3647
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.663]
 [0.658]
 [0.658]
 [0.658]
 [0.651]
 [0.586]] [[ 1.475]
 [ 2.78 ]
 [ 1.359]
 [ 1.359]
 [ 1.359]
 [-2.725]
 [ 1.507]] [[1.222]
 [2.02 ]
 [1.6  ]
 [1.6  ]
 [1.6  ]
 [0.397]
 [1.58 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.627548
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.63 ]] [[1.707]
 [1.707]
 [1.707]
 [1.707]
 [1.707]
 [1.707]
 [2.984]] [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.63 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.303]
 [0.575]
 [0.159]
 [0.159]
 [0.159]
 [0.159]] [[2.868]
 [1.635]
 [0.399]
 [2.868]
 [2.868]
 [2.868]
 [2.868]] [[4.299]
 [2.238]
 [0.413]
 [4.299]
 [4.299]
 [4.299]
 [4.299]]
from probs:  [0.8620806846641407, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191, 0.02758386306717191]
actor:  1 policy actor:  1  step number:  104 total reward:  0.2849999999999995  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.536]
 [0.298]
 [0.717]
 [0.592]
 [0.761]
 [0.675]] [[ 1.069]
 [ 0.913]
 [ 1.276]
 [-0.147]
 [ 0.059]
 [ 0.389]
 [ 0.946]] [[0.412]
 [0.537]
 [0.181]
 [0.545]
 [0.363]
 [0.813]
 [0.826]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.1 ]
 [0.1 ]
 [0.1 ]
 [0.1 ]
 [0.11]
 [0.1 ]
 [0.1 ]] [[1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.083]
 [1.351]
 [1.351]] [[0.1 ]
 [0.1 ]
 [0.1 ]
 [0.1 ]
 [0.11]
 [0.1 ]
 [0.1 ]]
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.105]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]] [[-0.288]
 [ 0.566]
 [-0.288]
 [-0.288]
 [-0.288]
 [-0.288]
 [-0.288]] [[0.118]
 [0.105]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]]
using explorer policy with actor:  0
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
line 256 mcts: sample exp_bonus -0.5606728804186442
2519 3676
from probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.854]] [[1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.504]] [[1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.244]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.61513823
siam score:  -0.6145454
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.601]
 [0.573]
 [0.563]
 [0.573]
 [0.575]
 [0.608]] [[0.991]
 [1.376]
 [0.781]
 [0.739]
 [0.903]
 [0.819]
 [0.71 ]] [[0.569]
 [0.601]
 [0.573]
 [0.563]
 [0.573]
 [0.575]
 [0.608]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.45 ]
 [0.452]
 [0.448]
 [0.449]
 [0.688]
 [0.446]] [[0.901]
 [1.076]
 [0.905]
 [0.79 ]
 [0.823]
 [1.998]
 [1.208]] [[0.303]
 [0.355]
 [0.303]
 [0.256]
 [0.27 ]
 [1.14 ]
 [0.392]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[3.104]
 [3.104]
 [3.104]
 [3.104]
 [3.104]
 [3.104]
 [3.104]] [[0.75]
 [0.75]
 [0.75]
 [0.75]
 [0.75]
 [0.75]
 [0.75]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
siam score:  -0.611067
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.56 ]
 [0.553]
 [0.552]
 [0.546]
 [0.553]
 [0.54 ]] [[0.935]
 [1.143]
 [0.661]
 [0.717]
 [1.225]
 [0.661]
 [1.111]] [[0.186]
 [0.322]
 [0.148]
 [0.165]
 [0.322]
 [0.148]
 [0.272]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]] [[1.896]
 [1.896]
 [1.896]
 [1.896]
 [1.896]
 [1.896]
 [1.896]] [[-0.251]
 [-0.251]
 [-0.251]
 [-0.251]
 [-0.251]
 [-0.251]
 [-0.251]]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.526]
 [0.583]
 [0.612]
 [0.583]
 [0.583]
 [0.549]] [[2.341]
 [3.293]
 [2.341]
 [3.064]
 [2.341]
 [2.341]
 [3.091]] [[0.168]
 [0.372]
 [0.168]
 [0.467]
 [0.168]
 [0.168]
 [0.351]]
siam score:  -0.60827535
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.303]
 [0.503]
 [0.297]
 [0.293]
 [0.503]
 [0.503]] [[-2.5  ]
 [-2.117]
 [ 0.   ]
 [-2.597]
 [-2.572]
 [ 0.   ]
 [ 0.   ]] [[-0.155]
 [ 0.245]
 [ 2.697]
 [-0.251]
 [-0.232]
 [ 2.697]
 [ 2.697]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.302]
 [0.526]
 [0.302]
 [0.302]
 [0.728]
 [0.302]] [[2.465]
 [2.465]
 [1.955]
 [2.465]
 [2.465]
 [1.899]
 [2.465]] [[1.859]
 [1.859]
 [1.942]
 [1.859]
 [1.859]
 [2.224]
 [1.859]]
line 256 mcts: sample exp_bonus 1.062387736938129
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.161]] [[3.766]
 [3.927]
 [3.927]
 [3.927]
 [3.927]
 [3.927]
 [3.93 ]] [[1.049]
 [1.191]
 [1.191]
 [1.191]
 [1.191]
 [1.191]
 [1.197]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
siam score:  -0.61085576
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.025]
 [-0.024]] [[1.586]
 [1.559]
 [1.559]
 [1.559]
 [1.559]
 [1.458]
 [1.593]] [[-0.042]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.163]
 [-0.026]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.843]
 [0.786]] [[2.234]
 [2.234]
 [2.234]
 [2.234]
 [2.234]
 [3.442]
 [2.234]] [[1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.83 ]
 [1.017]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.6251734923266832
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.481]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[-1.335]
 [-0.717]
 [-1.335]
 [-1.335]
 [-1.335]
 [-1.335]
 [-1.335]] [[0.433]
 [0.481]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.5  ]
 [0.481]
 [0.481]
 [0.503]
 [0.481]
 [0.481]] [[-1.034]
 [ 0.324]
 [ 0.   ]
 [ 0.   ]
 [-0.69 ]
 [ 0.   ]
 [ 0.   ]] [[0.473]
 [0.5  ]
 [0.481]
 [0.481]
 [0.503]
 [0.481]
 [0.481]]
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.315]
 [0.456]
 [0.464]
 [0.462]
 [0.479]
 [0.474]] [[ 1.773]
 [ 1.392]
 [-0.273]
 [-0.159]
 [ 0.064]
 [ 0.037]
 [ 0.085]] [[0.661]
 [0.315]
 [0.456]
 [0.464]
 [0.462]
 [0.479]
 [0.474]]
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.839]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.798]] [[0.88 ]
 [0.876]
 [1.194]
 [1.194]
 [1.194]
 [1.194]
 [0.891]] [[0.786]
 [0.839]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.798]]
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.539]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.539]] [[1.074]
 [1.482]
 [1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.224]] [[0.521]
 [0.539]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.539]]
line 256 mcts: sample exp_bonus -0.2087371288016071
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.538]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]] [[0.717]
 [1.992]
 [1.982]
 [1.982]
 [1.982]
 [1.982]
 [1.982]] [[0.664]
 [0.538]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.462]
 [0.453]] [[-0.003]
 [-0.327]
 [-0.327]
 [-0.327]
 [-0.327]
 [-0.388]
 [-0.327]] [[0.435]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.462]
 [0.453]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]] [[1.879]
 [1.879]
 [1.879]
 [1.879]
 [1.879]
 [1.879]
 [1.879]] [[0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.244]
 [0.437]
 [0.439]
 [0.421]
 [0.44 ]
 [0.414]] [[-0.073]
 [ 1.249]
 [-0.586]
 [-0.32 ]
 [-0.143]
 [-0.127]
 [ 0.308]] [[0.398]
 [0.244]
 [0.437]
 [0.439]
 [0.421]
 [0.44 ]
 [0.414]]
UNIT TEST: sample policy line 217 mcts : [0.122 0.184 0.082 0.143 0.082 0.163 0.224]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.377]
 [0.585]
 [0.585]
 [0.585]
 [0.583]
 [0.585]] [[-2.468]
 [-0.92 ]
 [-2.468]
 [-2.468]
 [-2.468]
 [-2.26 ]
 [-2.468]] [[0.585]
 [0.377]
 [0.585]
 [0.585]
 [0.585]
 [0.583]
 [0.585]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.905876494193843
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.636]
 [0.562]] [[0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.434]
 [0.072]] [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.636]
 [0.562]]
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.738]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]] [[1.461]
 [1.161]
 [1.461]
 [1.461]
 [1.008]
 [1.461]
 [1.461]] [[0.71 ]
 [0.738]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.631]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[1.469]
 [1.67 ]
 [1.07 ]
 [1.07 ]
 [1.07 ]
 [1.07 ]
 [1.07 ]] [[0.674]
 [0.631]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.649]
 [0.621]] [[1.369]
 [1.369]
 [1.369]
 [1.369]
 [1.369]
 [2.001]
 [1.369]] [[0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.649]
 [0.621]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.927821223402431, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771, 0.014435755319513771]
actor:  1 policy actor:  1  step number:  76 total reward:  0.4249999999999996  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.5849370640808742, 0.009100900174950799, 0.3786593352193227, 0.009100900174950799, 0.009100900174950799, 0.009100900174950799]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7435528377342762
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.682]
 [0.667]] [[0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.498]
 [0.691]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.682]
 [0.667]]
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]] [[0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9881218338932105
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.5849370640808742, 0.009100900174950799, 0.3786593352193227, 0.009100900174950799, 0.009100900174950799, 0.009100900174950799]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.6514568977612414
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.575312981282593
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.356]
 [0.454]
 [0.472]
 [0.465]
 [0.508]
 [0.498]] [[-0.084]
 [ 0.398]
 [-0.228]
 [-0.239]
 [-0.178]
 [-0.031]
 [ 0.458]] [[0.453]
 [0.356]
 [0.454]
 [0.472]
 [0.465]
 [0.508]
 [0.498]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.5849370640808742, 0.009100900174950799, 0.3786593352193227, 0.009100900174950799, 0.009100900174950799, 0.009100900174950799]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.075]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]] [[3.673]
 [3.673]
 [3.673]
 [3.673]
 [3.673]
 [3.673]
 [3.673]] [[-0.401]
 [-0.401]
 [-0.401]
 [-0.401]
 [-0.401]
 [-0.401]
 [-0.401]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.5849370640808742, 0.009100900174950799, 0.3786593352193227, 0.009100900174950799, 0.009100900174950799, 0.009100900174950799]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.5849370640808742, 0.009100900174950799, 0.3786593352193227, 0.009100900174950799, 0.009100900174950799, 0.009100900174950799]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.5849370640808742, 0.009100900174950799, 0.3786593352193227, 0.009100900174950799, 0.009100900174950799, 0.009100900174950799]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.501]
 [0.441]
 [0.44 ]
 [0.433]
 [0.487]
 [0.47 ]] [[3.807]
 [3.978]
 [4.734]
 [3.998]
 [4.045]
 [3.779]
 [3.84 ]] [[0.774]
 [0.852]
 [1.099]
 [0.825]
 [0.839]
 [0.771]
 [0.784]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
rdn probs:  [0.5849370640808742, 0.009100900174950799, 0.3786593352193227, 0.009100900174950799, 0.009100900174950799, 0.009100900174950799]
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]] [[5.991]
 [5.991]
 [5.991]
 [5.991]
 [5.991]
 [5.991]
 [5.991]] [[8.406]
 [8.406]
 [8.406]
 [8.406]
 [8.406]
 [8.406]
 [8.406]]
siam score:  -0.6127691
actor:  1 policy actor:  1  step number:  53 total reward:  0.6099999999999998  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11106
UNIT TEST: sample policy line 217 mcts : [0.184 0.061 0.224 0.245 0.    0.184 0.102]
actor:  1 policy actor:  1  step number:  89 total reward:  0.22999999999999943  reward:  1.0 rdn_beta:  0.667
from probs:  [0.39744315351498727, 0.006183725886888647, 0.2572850474673436, 0.3267206213570033, 0.006183725886888647, 0.006183725886888647]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[1.045]
 [1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]] [[0.889]
 [1.541]
 [1.541]
 [1.541]
 [1.541]
 [1.541]
 [1.541]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.404]
 [0.404]
 [0.404]
 [0.631]
 [0.404]
 [0.404]] [[1.222]
 [1.234]
 [1.234]
 [1.234]
 [1.392]
 [1.234]
 [1.234]] [[1.346]
 [1.002]
 [1.002]
 [1.002]
 [1.509]
 [1.002]
 [1.002]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
from probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
siam score:  -0.61285925
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.163 0.245 0.082 0.082 0.122 0.061 0.245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.614]
 [0.633]
 [0.614]
 [0.614]
 [0.649]
 [0.676]] [[2.267]
 [2.282]
 [2.044]
 [2.282]
 [2.282]
 [1.943]
 [1.642]] [[0.628]
 [0.614]
 [0.633]
 [0.614]
 [0.614]
 [0.649]
 [0.676]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
first move QE:  0.6248243016799246
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
Printing some Q and Qe and total Qs values:  [[1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]] [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[1.968]
 [1.968]
 [1.968]
 [1.968]
 [1.968]
 [1.968]
 [1.968]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.485]
 [0.562]
 [0.56 ]
 [0.56 ]
 [0.571]
 [0.563]] [[2.641]
 [2.794]
 [2.716]
 [2.699]
 [2.739]
 [2.728]
 [2.716]] [[1.25 ]
 [1.254]
 [1.33 ]
 [1.308]
 [1.349]
 [1.36 ]
 [1.332]]
Printing some Q and Qe and total Qs values:  [[ 0.146]
 [ 0.037]
 [ 0.146]
 [-0.021]
 [ 0.146]
 [-0.059]
 [ 0.04 ]] [[4.07 ]
 [4.373]
 [4.07 ]
 [4.442]
 [4.07 ]
 [2.893]
 [4.751]] [[0.863]
 [0.747]
 [0.863]
 [0.652]
 [0.863]
 [0.06 ]
 [0.878]]
siam score:  -0.61517584
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
from probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.489725576966045
siam score:  -0.61203146
line 256 mcts: sample exp_bonus 2.8683806322413243
from probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.471]] [[0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.668]] [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.471]]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]] [[1.86]
 [1.86]
 [1.86]
 [1.86]
 [1.86]
 [1.86]
 [1.86]] [[0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.843]] [[1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [0.381]] [[0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.843]]
2570 3730
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.032856504795203
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.33480830264517003, 0.0052092047627488305, 0.21673834176457887, 0.4328257413020046, 0.0052092047627488305, 0.0052092047627488305]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.404]
 [0.565]
 [0.582]
 [0.59 ]
 [0.619]
 [0.552]] [[ 1.231]
 [ 1.262]
 [-0.454]
 [ 0.813]
 [ 1.97 ]
 [ 1.522]
 [ 1.131]] [[1.001]
 [0.742]
 [0.491]
 [0.949]
 [1.35 ]
 [1.259]
 [0.994]]
Printing some Q and Qe and total Qs values:  [[1.271]
 [1.271]
 [1.271]
 [1.271]
 [1.271]
 [1.271]
 [1.271]] [[1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]] [[2.764]
 [2.764]
 [2.764]
 [2.764]
 [2.764]
 [2.764]
 [2.764]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6068564
actor:  1 policy actor:  1  step number:  81 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.546]
 [0.571]
 [0.565]
 [0.604]
 [0.714]
 [0.574]] [[-0.069]
 [ 0.728]
 [-0.501]
 [-0.583]
 [-0.751]
 [-1.89 ]
 [-0.064]] [[0.558]
 [0.546]
 [0.571]
 [0.565]
 [0.604]
 [0.714]
 [0.574]]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.372]
 [0.33 ]
 [0.328]
 [0.33 ]
 [0.325]
 [0.349]] [[-0.587]
 [ 0.353]
 [-0.807]
 [-0.56 ]
 [-0.807]
 [-0.468]
 [-0.041]] [[0.317]
 [0.372]
 [0.33 ]
 [0.328]
 [0.33 ]
 [0.325]
 [0.349]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.501]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[2.691]
 [2.474]
 [2.691]
 [2.691]
 [2.691]
 [2.691]
 [2.691]] [[0.687]
 [0.472]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
using explorer policy with actor:  1
first move QE:  0.6231496085579047
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
2582 3744
siam score:  -0.6173079
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.675]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[ 0.819]
 [ 0.522]
 [-0.337]
 [-0.337]
 [-0.337]
 [-0.337]
 [-0.337]] [[1.591]
 [1.668]
 [1.33 ]
 [1.33 ]
 [1.33 ]
 [1.33 ]
 [1.33 ]]
2583 3751
2583 3754
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.506]] [[1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.579]] [[0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.506]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.377]
 [0.385]
 [0.388]
 [0.39 ]
 [0.39 ]
 [0.378]] [[1.558]
 [1.657]
 [1.385]
 [0.908]
 [0.968]
 [1.136]
 [1.708]] [[-0.191]
 [-0.139]
 [-0.212]
 [-0.366]
 [-0.342]
 [-0.286]
 [-0.12 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.284]
 [0.303]] [[1.478]
 [1.689]
 [1.689]
 [1.689]
 [1.689]
 [1.913]
 [1.689]] [[-0.644]
 [-0.538]
 [-0.538]
 [-0.538]
 [-0.538]
 [-0.501]
 [-0.538]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6099271
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.408]
 [0.397]
 [0.434]
 [0.397]
 [0.397]
 [0.408]] [[5.544]
 [5.214]
 [5.544]
 [6.044]
 [5.544]
 [5.544]
 [5.17 ]] [[1.07 ]
 [0.942]
 [1.07 ]
 [1.334]
 [1.07 ]
 [1.07 ]
 [0.922]]
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.127]
 [0.189]
 [0.188]
 [0.187]
 [0.193]
 [0.186]] [[1.659]
 [1.883]
 [1.581]
 [1.596]
 [1.767]
 [1.883]
 [1.8  ]] [[0.193]
 [0.127]
 [0.189]
 [0.188]
 [0.187]
 [0.193]
 [0.186]]
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.134]
 [0.19 ]
 [0.189]
 [0.189]
 [0.19 ]
 [0.188]] [[1.252]
 [1.686]
 [1.291]
 [1.317]
 [1.213]
 [1.683]
 [1.381]] [[0.195]
 [0.134]
 [0.19 ]
 [0.189]
 [0.189]
 [0.19 ]
 [0.188]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.013]] [[4.343]
 [4.343]
 [4.343]
 [4.343]
 [4.343]
 [4.343]
 [4.274]] [[-0.42 ]
 [-0.42 ]
 [-0.42 ]
 [-0.42 ]
 [-0.42 ]
 [-0.42 ]
 [-0.462]]
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.031]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[1.39 ]
 [2.769]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]] [[-1.115]
 [-0.221]
 [-1.115]
 [-1.115]
 [-1.115]
 [-1.115]
 [-1.115]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.884]
 [1.023]
 [0.919]
 [0.884]
 [0.884]
 [1.026]
 [0.895]] [[2.204]
 [1.954]
 [2.122]
 [2.204]
 [2.204]
 [2.338]
 [2.304]] [[0.872]
 [0.886]
 [0.865]
 [0.872]
 [0.872]
 [1.092]
 [0.935]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.61252904
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.49 ]
 [0.476]
 [0.475]
 [0.475]
 [0.477]
 [0.475]] [[ 0.   ]
 [-2.453]
 [-3.116]
 [ 0.   ]
 [ 0.   ]
 [-3.107]
 [ 0.   ]] [[0.475]
 [0.49 ]
 [0.476]
 [0.475]
 [0.475]
 [0.477]
 [0.475]]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.517]
 [0.538]
 [0.541]
 [0.535]
 [0.532]
 [0.52 ]] [[-2.155]
 [ 1.036]
 [-2.444]
 [-2.72 ]
 [-2.679]
 [-2.427]
 [-2.232]] [[0.536]
 [0.517]
 [0.538]
 [0.541]
 [0.535]
 [0.532]
 [0.52 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]] [[1.428]
 [1.428]
 [1.428]
 [1.428]
 [1.428]
 [1.428]
 [1.428]] [[2.577]
 [2.577]
 [2.577]
 [2.577]
 [2.577]
 [2.577]
 [2.577]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.61874735
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.58 ]
 [0.467]] [[0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.264]
 [0.551]] [[0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.58 ]
 [0.467]]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.726]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[-3.614]
 [ 1.04 ]
 [-3.614]
 [-3.614]
 [-3.614]
 [-3.614]
 [-3.614]] [[0.675]
 [0.726]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 0.6030797924478961
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.242]
 [0.242]
 [0.242]
 [0.678]
 [0.242]
 [0.242]] [[1.652]
 [1.335]
 [1.335]
 [1.335]
 [1.136]
 [1.335]
 [1.335]] [[0.274]
 [0.242]
 [0.242]
 [0.242]
 [0.678]
 [0.242]
 [0.242]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
siam score:  -0.6073304
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.257]] [[-0.358]
 [-0.358]
 [-0.358]
 [-0.358]
 [-0.358]
 [-0.358]
 [-0.218]] [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.257]]
from probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.477]
 [0.484]
 [0.477]
 [0.477]
 [0.45 ]] [[2.633]
 [2.633]
 [2.633]
 [2.4  ]
 [2.633]
 [2.633]
 [3.051]] [[0.287]
 [0.287]
 [0.287]
 [0.146]
 [0.287]
 [0.287]
 [0.511]]
from probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.463]
 [0.475]
 [0.482]
 [0.479]
 [0.473]
 [0.474]] [[-3.621]
 [-1.013]
 [-3.576]
 [-3.928]
 [-3.369]
 [-3.751]
 [-3.491]] [[0.472]
 [0.463]
 [0.475]
 [0.482]
 [0.479]
 [0.473]
 [0.474]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.563]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.59 ]] [[2.115]
 [1.926]
 [2.115]
 [2.115]
 [2.115]
 [2.115]
 [2.128]] [[0.462]
 [0.319]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.509]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[0.597]
 [0.61 ]
 [0.607]
 [0.607]
 [0.596]
 [0.602]
 [0.6  ]] [[2.864]
 [2.881]
 [2.877]
 [2.877]
 [2.861]
 [2.871]
 [2.867]]
start point for exploration sampling:  11106
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.383967500961064
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[2.143]
 [2.143]
 [2.143]
 [2.143]
 [2.143]
 [2.143]
 [2.143]] [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]]
using another actor
siam score:  -0.6216746
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2607 3820
2608 3821
siam score:  -0.62067777
2608 3824
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.021]
 [0.016]
 [0.019]
 [0.018]
 [0.014]
 [0.013]] [[0.926]
 [1.136]
 [1.11 ]
 [0.648]
 [0.729]
 [0.803]
 [0.742]] [[0.28 ]
 [0.516]
 [0.481]
 [0.024]
 [0.103]
 [0.169]
 [0.106]]
using another actor
from probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
siam score:  -0.61810327
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.397]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[0.974]
 [0.824]
 [0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]] [[0.299]
 [0.397]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.411]
 [0.419]
 [0.409]
 [0.413]
 [0.406]
 [0.41 ]] [[-4.435]
 [-3.652]
 [-4.887]
 [-5.113]
 [-4.892]
 [-4.675]
 [-4.75 ]] [[0.415]
 [0.411]
 [0.419]
 [0.409]
 [0.413]
 [0.406]
 [0.41 ]]
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [1.015]
 [0.721]] [[1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [0.943]
 [1.349]] [[0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.727]
 [0.274]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.633]
 [0.631]] [[0.353]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.346]
 [0.482]] [[0.605]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.633]
 [0.631]]
using explorer policy with actor:  0
from probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[2.25]
 [2.25]
 [2.25]
 [2.25]
 [2.25]
 [2.25]
 [2.25]] [[1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.059]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.534]
 [0.633]
 [0.62 ]
 [0.609]
 [0.618]
 [0.612]] [[2.318]
 [2.493]
 [1.959]
 [2.353]
 [2.196]
 [2.197]
 [2.36 ]] [[0.745]
 [0.702]
 [0.545]
 [0.781]
 [0.655]
 [0.674]
 [0.77 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.328]
 [0.329]
 [0.311]
 [0.312]
 [0.316]
 [0.31 ]] [[-1.123]
 [ 0.   ]
 [-1.439]
 [-1.2  ]
 [-1.118]
 [-0.6  ]
 [-0.813]] [[0.306]
 [0.328]
 [0.329]
 [0.311]
 [0.312]
 [0.316]
 [0.31 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.877]
 [0.749]
 [0.746]
 [0.742]
 [0.777]
 [0.879]] [[2.084]
 [2.187]
 [1.101]
 [1.615]
 [1.362]
 [1.444]
 [1.836]] [[0.813]
 [0.877]
 [0.749]
 [0.746]
 [0.742]
 [0.777]
 [0.879]]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.444]
 [0.484]
 [0.481]
 [0.484]
 [0.534]
 [0.585]] [[2.042]
 [2.638]
 [2.125]
 [2.172]
 [2.141]
 [2.048]
 [1.844]] [[0.487]
 [0.444]
 [0.484]
 [0.481]
 [0.484]
 [0.534]
 [0.585]]
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.217]
 [0.206]
 [0.206]
 [0.203]
 [0.201]
 [0.203]] [[-4.493]
 [-3.744]
 [-4.229]
 [-4.672]
 [-4.323]
 [-4.246]
 [-4.305]] [[0.203]
 [0.217]
 [0.206]
 [0.206]
 [0.203]
 [0.201]
 [0.203]]
line 256 mcts: sample exp_bonus 7.846857944730573
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.405]
 [0.46 ]
 [0.444]
 [0.463]
 [0.626]
 [0.48 ]] [[-0.65 ]
 [ 0.907]
 [-0.743]
 [-0.946]
 [-1.021]
 [-2.135]
 [-0.658]] [[0.454]
 [0.405]
 [0.46 ]
 [0.444]
 [0.463]
 [0.626]
 [0.48 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.2699621439410095
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.57 ]
 [0.594]
 [0.489]
 [0.489]
 [0.489]
 [0.47 ]] [[ 0.974]
 [ 0.712]
 [-3.619]
 [ 0.794]
 [ 0.794]
 [ 0.794]
 [ 2.113]] [[1.392]
 [1.403]
 [0.074]
 [1.403]
 [1.403]
 [1.403]
 [1.804]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
from probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.763]
 [0.862]
 [0.763]
 [0.763]
 [0.908]
 [0.826]] [[1.707]
 [1.707]
 [1.429]
 [1.707]
 [1.707]
 [1.562]
 [1.473]] [[1.535]
 [1.535]
 [1.454]
 [1.535]
 [1.535]
 [1.68 ]
 [1.427]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.684]
 [0.643]
 [0.643]
 [0.643]
 [0.646]
 [0.643]] [[0.765]
 [2.28 ]
 [1.052]
 [1.052]
 [1.052]
 [1.003]
 [1.973]] [[0.637]
 [0.684]
 [0.643]
 [0.643]
 [0.643]
 [0.646]
 [0.643]]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.11 ]
 [0.082]
 [0.076]
 [0.078]
 [0.049]
 [0.072]] [[2.042]
 [2.049]
 [1.934]
 [1.883]
 [2.009]
 [2.059]
 [2.038]] [[-0.003]
 [ 0.03 ]
 [-0.14 ]
 [-0.204]
 [-0.072]
 [-0.081]
 [-0.057]]
2627 3850
siam score:  -0.6210701
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.605]
 [0.566]
 [0.589]
 [0.581]
 [0.573]
 [0.586]] [[ 0.921]
 [ 2.467]
 [-0.387]
 [ 1.074]
 [ 1.742]
 [ 1.931]
 [ 2.616]] [[0.566]
 [0.605]
 [0.566]
 [0.589]
 [0.581]
 [0.573]
 [0.586]]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.225]
 [0.491]
 [0.565]
 [0.537]
 [0.557]
 [0.511]] [[-1.004]
 [ 1.09 ]
 [-0.913]
 [-0.839]
 [-0.561]
 [-1.131]
 [-0.845]] [[-0.411]
 [-0.203]
 [-0.339]
 [-0.166]
 [-0.13 ]
 [-0.281]
 [-0.276]]
siam score:  -0.6234106
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
from probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.558]
 [1.046]
 [1.046]
 [1.046]
 [0.547]
 [1.046]] [[ 1.192]
 [-0.731]
 [ 0.309]
 [ 0.309]
 [ 0.309]
 [-3.423]
 [ 0.309]] [[2.318]
 [1.543]
 [2.423]
 [2.423]
 [2.423]
 [0.361]
 [2.423]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.5963943
Printing some Q and Qe and total Qs values:  [[1.323]
 [1.464]
 [1.444]
 [1.323]
 [1.323]
 [1.323]
 [1.402]] [[0.761]
 [0.789]
 [0.885]
 [0.761]
 [0.761]
 [0.761]
 [0.951]] [[2.887]
 [3.084]
 [3.078]
 [2.887]
 [2.887]
 [2.887]
 [3.036]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.567]
 [0.567]
 [0.567]
 [1.11 ]
 [0.567]
 [0.624]] [[ 1.192]
 [ 0.762]
 [ 0.762]
 [ 0.762]
 [ 1.436]
 [ 0.762]
 [-0.173]] [[1.781]
 [1.748]
 [1.748]
 [1.748]
 [2.858]
 [1.748]
 [1.58 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [1.307]
 [0.504]] [[0.839]
 [2.998]
 [2.998]
 [2.998]
 [2.998]
 [0.934]
 [2.998]] [[0.488]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [1.586]
 [0.669]]
line 256 mcts: sample exp_bonus -0.9333089362261324
Printing some Q and Qe and total Qs values:  [[-0.012]
 [ 0.473]
 [-0.013]
 [ 0.473]
 [-0.014]
 [-0.012]
 [-0.011]] [[2.076]
 [2.302]
 [2.079]
 [2.302]
 [2.038]
 [2.022]
 [1.657]] [[-0.02 ]
 [ 1.1  ]
 [-0.018]
 [ 1.1  ]
 [-0.048]
 [-0.056]
 [-0.297]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.865]
 [0.781]] [[0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [1.52 ]
 [0.695]] [[0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [1.334]
 [0.89 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.54]
 [0.46]] [[1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.379]
 [3.507]
 [1.379]] [[0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [1.347]
 [0.476]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.555]
 [0.449]] [[1.972]
 [1.972]
 [1.972]
 [1.972]
 [1.972]
 [1.678]
 [1.972]] [[0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.678]
 [0.565]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.2916],
        [-0.3730],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.965448 -0.965448
-0.9702 -0.9702
-0.926783055 -0.926783055
-0.9608890648499999 -0.9608890648499999
-0.922945522455 -0.922945522455
-0.45539999999999964 -0.45539999999999964
-0.024259925299500003 -0.3158448904352777
-0.0727797758985 -0.445760295260792
-0.9277004727045 -0.9277004727045
-0.87615 -0.87615
2645 3879
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739730304795141, 0.18596448561456477, 0.17735659434132298, 0.35418052383165133, 0.0042626828664734245, 0.0042626828664734245]
siam score:  -0.6055629
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  151 total reward:  0.0899999999999993  reward:  1.0 rdn_beta:  0.167
using another actor
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.311]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.281]] [[-0.258]
 [ 1.641]
 [-0.258]
 [-0.258]
 [-0.258]
 [-0.258]
 [-0.634]] [[0.251]
 [0.311]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.281]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
siam score:  -0.60684276
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.363]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]] [[1.749]
 [2.466]
 [1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]] [[1.627]
 [1.677]
 [1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]]
siam score:  -0.60366285
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.521]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[-0.456]
 [ 0.105]
 [-0.456]
 [-0.456]
 [-0.456]
 [-0.456]
 [-0.456]] [[1.609]
 [1.985]
 [1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.609]]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.353]
 [0.353]
 [0.336]
 [0.336]
 [0.329]
 [0.334]] [[3.114]
 [2.972]
 [2.972]
 [3.01 ]
 [3.089]
 [3.176]
 [3.148]] [[1.241]
 [1.088]
 [1.088]
 [1.104]
 [1.205]
 [1.303]
 [1.276]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -2.0501241768797085
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[3.   ]
 [0.534]
 [3.   ]
 [3.   ]
 [3.   ]
 [3.   ]
 [3.   ]] [[0.   ]
 [0.525]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[6.575]
 [2.343]
 [6.575]
 [6.575]
 [6.575]
 [6.575]
 [6.575]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using another actor
2658 3899
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]] [[0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]]
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.495]
 [0.534]
 [0.537]
 [0.537]
 [0.54 ]
 [0.541]] [[1.783]
 [3.169]
 [1.984]
 [1.824]
 [1.824]
 [2.02 ]
 [2.188]] [[0.526]
 [0.495]
 [0.534]
 [0.537]
 [0.537]
 [0.54 ]
 [0.541]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.87 ]
 [0.721]] [[0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.421]
 [0.67 ]] [[0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.652]
 [0.438]]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.647]] [[3.168]
 [3.046]
 [3.046]
 [3.046]
 [3.046]
 [3.046]
 [3.656]] [[1.679]
 [1.626]
 [1.626]
 [1.626]
 [1.626]
 [1.626]
 [1.93 ]]
siam score:  -0.6119192
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.029]
 [-0.016]
 [-0.016]
 [-0.069]
 [-0.001]
 [-0.038]] [[1.829]
 [1.289]
 [1.734]
 [1.734]
 [1.277]
 [2.662]
 [1.975]] [[-0.56 ]
 [-0.696]
 [-0.52 ]
 [-0.52 ]
 [-0.779]
 [-0.181]
 [-0.483]]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.675]
 [0.748]
 [0.761]
 [0.746]
 [1.13 ]
 [0.835]] [[ 1.153]
 [ 1.46 ]
 [ 0.858]
 [ 0.749]
 [ 0.947]
 [-0.241]
 [ 0.82 ]] [[0.611]
 [0.594]
 [0.538]
 [0.528]
 [0.563]
 [0.936]
 [0.7  ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.472]
 [0.344]
 [0.545]
 [0.45 ]
 [0.425]
 [0.492]] [[1.705]
 [1.155]
 [1.333]
 [1.087]
 [1.34 ]
 [1.075]
 [1.235]] [[0.322]
 [0.384]
 [0.189]
 [0.509]
 [0.403]
 [0.263]
 [0.452]]
line 256 mcts: sample exp_bonus 0.8953347833240985
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.498]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]] [[0.12 ]
 [0.531]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]] [[0.573]
 [0.838]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.487]
 [0.518]
 [0.518]
 [0.518]
 [0.53 ]
 [0.518]] [[2.74 ]
 [2.659]
 [2.74 ]
 [2.74 ]
 [2.74 ]
 [2.425]
 [2.74 ]] [[1.615]
 [1.443]
 [1.615]
 [1.615]
 [1.615]
 [1.22 ]
 [1.615]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.296]
 [0.333]
 [0.335]
 [0.33 ]
 [0.333]
 [0.328]] [[ 0.091]
 [ 1.401]
 [-0.424]
 [-0.231]
 [ 0.088]
 [ 0.085]
 [ 0.298]] [[0.324]
 [0.296]
 [0.333]
 [0.335]
 [0.33 ]
 [0.333]
 [0.328]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.344]
 [0.375]
 [0.389]
 [0.388]
 [0.452]
 [0.385]] [[2.359]
 [3.108]
 [3.219]
 [2.699]
 [2.682]
 [3.736]
 [2.899]] [[0.458]
 [0.881]
 [0.988]
 [0.685]
 [0.673]
 [1.4  ]
 [0.803]]
2674 3919
using another actor
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]]
line 256 mcts: sample exp_bonus 1.4744263077443485
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.521]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.513]] [[0.403]
 [1.273]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [1.265]] [[0.248]
 [0.647]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.629]]
in main func line 156:  2675
line 256 mcts: sample exp_bonus -1.9051278042558053
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.472]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[0.26 ]
 [0.544]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]] [[0.443]
 [0.472]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
siam score:  -0.6138974
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[1.985]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[1.531]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]]
line 256 mcts: sample exp_bonus 0.935707268240164
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 3.2948952999361354
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.40505663322300667
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.064]] [[-2.417]
 [-2.259]
 [-2.259]
 [-2.259]
 [-2.259]
 [-2.259]
 [-1.789]] [[0.067]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.064]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6198017
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.121]
 [0.147]
 [0.137]
 [0.137]
 [0.176]
 [0.137]] [[0.502]
 [0.475]
 [0.267]
 [0.502]
 [0.502]
 [0.487]
 [0.502]] [[-0.078]
 [-0.161]
 [-0.526]
 [-0.078]
 [-0.078]
 [-0.03 ]
 [-0.078]]
siam score:  -0.62404543
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
2685 3937
siam score:  -0.6259748
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.524]
 [0.503]
 [0.503]
 [0.509]
 [0.503]
 [0.503]] [[1.1  ]
 [0.986]
 [1.1  ]
 [1.1  ]
 [1.431]
 [1.1  ]
 [1.1  ]] [[1.902]
 [1.871]
 [1.902]
 [1.902]
 [2.029]
 [1.902]
 [1.902]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
from probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.458]
 [0.662]
 [0.723]
 [0.719]
 [0.619]
 [0.669]] [[0.801]
 [1.16 ]
 [0.685]
 [0.7  ]
 [0.592]
 [0.633]
 [0.809]] [[0.666]
 [0.685]
 [0.618]
 [0.757]
 [0.639]
 [0.482]
 [0.756]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Starting evaluation
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.127027517389737
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.425]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.392]] [[-1.084]
 [-1.663]
 [-1.084]
 [-1.084]
 [-1.084]
 [-1.084]
 [-0.647]] [[0.393]
 [0.425]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.392]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.6317117
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 3.6381555230249343
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.526]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]] [[0.881]
 [1.559]
 [1.105]
 [1.105]
 [1.105]
 [1.105]
 [1.105]] [[0.028]
 [0.391]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.717]
 [0.403]] [[0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [2.558]
 [0.881]] [[0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [1.858]
 [0.67 ]]
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.741]
 [0.551]
 [0.557]
 [0.552]
 [0.534]
 [0.541]] [[0.791]
 [0.915]
 [0.575]
 [0.908]
 [0.823]
 [0.978]
 [0.982]] [[0.548]
 [0.741]
 [0.551]
 [0.557]
 [0.552]
 [0.534]
 [0.541]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.844969360951132
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.251]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[1.406]
 [1.846]
 [1.406]
 [1.406]
 [1.406]
 [1.406]
 [1.406]] [[-0.477]
 [-0.027]
 [-0.477]
 [-0.477]
 [-0.477]
 [-0.477]
 [-0.477]]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3331390476450935
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.327]
 [0.507]
 [0.505]
 [0.505]
 [0.503]
 [0.505]] [[-0.017]
 [ 0.876]
 [-0.244]
 [-0.017]
 [-0.017]
 [-0.198]
 [ 0.033]] [[0.505]
 [0.327]
 [0.507]
 [0.505]
 [0.505]
 [0.503]
 [0.505]]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.163]
 [0.162]
 [0.155]
 [0.157]
 [0.149]
 [0.146]] [[2.335]
 [1.903]
 [1.877]
 [1.93 ]
 [1.952]
 [2.056]
 [2.04 ]] [[0.159]
 [0.163]
 [0.162]
 [0.155]
 [0.157]
 [0.149]
 [0.146]]
line 256 mcts: sample exp_bonus 3.320112652508541
line 256 mcts: sample exp_bonus 2.836062923368267
siam score:  -0.62475014
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.017]
 [-0.017]
 [-0.021]
 [-0.021]
 [-0.013]
 [-0.01 ]] [[4.229]
 [3.826]
 [3.429]
 [3.575]
 [3.575]
 [3.367]
 [3.641]] [[0.995]
 [0.714]
 [0.424]
 [0.526]
 [0.526]
 [0.382]
 [0.586]]
using explorer policy with actor:  1
from probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [1.087]
 [0.558]
 [0.558]] [[1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.203]
 [1.852]
 [1.852]] [[2.5  ]
 [2.5  ]
 [2.5  ]
 [2.5  ]
 [3.231]
 [2.5  ]
 [2.5  ]]
siam score:  -0.6282717
using explorer policy with actor:  0
2700 3956
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
from probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.61976403
siam score:  -0.61980826
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.985]
 [0.868]
 [0.869]
 [0.869]
 [0.869]
 [0.844]] [[0.819]
 [0.736]
 [0.752]
 [0.819]
 [0.819]
 [0.819]
 [0.857]] [[0.869]
 [0.985]
 [0.868]
 [0.869]
 [0.869]
 [0.869]
 [0.844]]
line 256 mcts: sample exp_bonus 2.106180069449896
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]] [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]] [[3.403]
 [3.403]
 [3.403]
 [3.403]
 [3.403]
 [3.403]
 [3.403]]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[4.598]
 [4.598]
 [4.598]
 [4.598]
 [4.598]
 [4.598]
 [4.598]] [[7.126]
 [7.126]
 [7.126]
 [7.126]
 [7.126]
 [7.126]
 [7.126]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]] [[0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.61 ]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[-0.799]
 [-0.154]
 [-0.799]
 [-0.799]
 [-0.799]
 [-0.799]
 [-0.799]] [[0.727]
 [1.155]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[1.086]
 [1.33 ]
 [1.086]
 [1.086]
 [1.086]
 [1.086]
 [1.086]] [[0.548]
 [0.708]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[2.208]
 [2.749]
 [2.208]
 [2.208]
 [2.208]
 [2.208]
 [2.208]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.479]
 [0.439]
 [0.426]
 [0.457]
 [0.48 ]
 [0.463]] [[-1.525]
 [-0.861]
 [-1.137]
 [-1.053]
 [-0.912]
 [-0.767]
 [-0.569]] [[0.064]
 [0.384]
 [0.211]
 [0.214]
 [0.323]
 [0.418]
 [0.45 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
from probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.349]
 [0.349]
 [0.36 ]
 [0.349]
 [0.349]
 [0.363]] [[5.276]
 [5.518]
 [5.518]
 [5.338]
 [5.518]
 [5.518]
 [4.759]] [[1.356]
 [1.494]
 [1.494]
 [1.39 ]
 [1.494]
 [1.494]
 [1.015]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.325]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.494]] [[1.559]
 [2.215]
 [1.559]
 [1.559]
 [1.559]
 [1.559]
 [2.362]] [[0.545]
 [0.881]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [1.316]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.499]
 [0.498]
 [0.498]
 [0.49 ]
 [0.511]
 [0.498]] [[ 0.564]
 [ 1.538]
 [ 1.281]
 [ 1.281]
 [ 0.148]
 [-0.02 ]
 [ 1.281]] [[ 0.278]
 [ 0.933]
 [ 0.76 ]
 [ 0.76 ]
 [-0.011]
 [-0.081]
 [ 0.76 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.022]] [[4.416]
 [4.416]
 [4.416]
 [4.416]
 [4.416]
 [4.416]
 [4.333]] [[1.162]
 [1.162]
 [1.162]
 [1.162]
 [1.162]
 [1.162]
 [1.138]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.332]
 [1.173]] [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.542]
 [0.566]] [[2.039]
 [2.039]
 [2.039]
 [2.039]
 [2.039]
 [2.35 ]
 [2.039]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64364296
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.071]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[1.264]
 [1.361]
 [1.264]
 [1.264]
 [1.264]
 [1.264]
 [1.264]] [[0.089]
 [0.071]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.097]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[0.41]
 [1.85]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]] [[-0.008]
 [ 1.596]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
first move QE:  0.6051941947667723
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[3.23 ]
 [1.317]
 [1.317]
 [1.317]
 [1.317]
 [1.317]
 [1.317]] [[1.925]
 [1.37 ]
 [1.37 ]
 [1.37 ]
 [1.37 ]
 [1.37 ]
 [1.37 ]]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.367]
 [0.41 ]
 [0.4  ]
 [0.399]
 [0.408]
 [0.409]] [[0.458]
 [0.148]
 [0.05 ]
 [0.614]
 [1.014]
 [0.661]
 [1.2  ]] [[0.999]
 [0.663]
 [0.651]
 [1.193]
 [1.592]
 [1.256]
 [1.798]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
2725 3995
siam score:  -0.6469606
first move QE:  0.6058761559013408
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
2727 3998
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.443466838950523
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
siam score:  -0.6465999
2730 4000
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.124]
 [0.229]
 [0.239]
 [0.221]
 [0.243]
 [0.188]] [[1.742]
 [1.796]
 [1.611]
 [1.632]
 [1.636]
 [1.675]
 [2.166]] [[ 0.051]
 [-0.136]
 [-0.048]
 [-0.014]
 [-0.047]
 [ 0.021]
 [ 0.24 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.408]
 [0.408]
 [0.638]
 [0.565]
 [0.678]
 [0.582]] [[2.483]
 [2.027]
 [2.371]
 [1.738]
 [2.451]
 [5.141]
 [2.497]] [[ 0.371]
 [-0.032]
 [ 0.084]
 [ 0.332]
 [ 0.424]
 [ 1.548]
 [ 0.473]]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.534]
 [0.565]
 [0.565]
 [0.615]
 [0.567]
 [0.566]] [[-3.843]
 [-1.515]
 [-3.518]
 [-3.537]
 [ 1.386]
 [-3.669]
 [-3.52 ]] [[0.327]
 [1.55 ]
 [0.499]
 [0.489]
 [3.21 ]
 [0.418]
 [0.499]]
using explorer policy with actor:  1
siam score:  -0.62791204
using another actor
using another actor
from probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
siam score:  -0.6250352
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1078542442250543
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
line 256 mcts: sample exp_bonus 2.097181187652732
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.839]] [[-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
start point for exploration sampling:  11106
using explorer policy with actor:  1
using another actor
line 256 mcts: sample exp_bonus 1.2819922064615532
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.623]
 [0.609]] [[4.289]
 [4.168]
 [4.168]
 [4.168]
 [4.168]
 [4.191]
 [3.897]] [[1.631]
 [1.571]
 [1.571]
 [1.571]
 [1.571]
 [1.591]
 [1.416]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.538]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.524]] [[ 1.457]
 [ 1.283]
 [-0.275]
 [-0.275]
 [-0.275]
 [-0.275]
 [ 1.558]] [[2.005]
 [2.058]
 [1.443]
 [1.443]
 [1.443]
 [1.443]
 [2.147]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.6031697481312506
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [1.066]
 [0.505]] [[1.283]
 [1.556]
 [1.556]
 [1.556]
 [1.556]
 [0.761]
 [1.64 ]] [[0.855]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [1.621]
 [1.38 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.75 ]
 [0.75 ]
 [0.41 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]] [[2.323]
 [2.131]
 [2.131]
 [2.102]
 [2.131]
 [2.131]
 [2.131]] [[0.321]
 [0.889]
 [0.889]
 [0.189]
 [0.889]
 [0.889]
 [0.889]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2755 4049
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6436601
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.494]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.507]] [[1.941]
 [0.672]
 [1.941]
 [1.941]
 [1.941]
 [1.941]
 [0.776]] [[2.649]
 [0.382]
 [2.649]
 [2.649]
 [2.649]
 [2.649]
 [0.616]]
Printing some Q and Qe and total Qs values:  [[1.215]
 [1.215]
 [1.215]
 [1.215]
 [1.215]
 [1.215]
 [1.215]] [[0.568]
 [0.586]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[2.142]
 [2.154]
 [2.164]
 [2.164]
 [2.164]
 [2.164]
 [2.164]]
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.362]
 [0.276]
 [0.362]
 [0.362]
 [0.242]
 [0.319]] [[1.933]
 [1.907]
 [1.871]
 [1.907]
 [1.907]
 [2.071]
 [2.274]] [[0.35 ]
 [0.447]
 [0.239]
 [0.447]
 [0.447]
 [0.371]
 [0.727]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.659]
 [0.068]] [[1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.966]
 [1.663]] [[0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]
 [2.  ]
 [0.86]]
Printing some Q and Qe and total Qs values:  [[1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]] [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[2.706]
 [2.706]
 [2.706]
 [2.706]
 [2.706]
 [2.706]
 [2.706]]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.314]
 [0.314]
 [0.314]
 [0.295]
 [0.293]
 [0.314]] [[2.167]
 [1.798]
 [1.798]
 [1.798]
 [1.749]
 [1.713]
 [1.798]] [[ 0.352]
 [ 0.05 ]
 [ 0.05 ]
 [ 0.05 ]
 [-0.037]
 [-0.076]
 [ 0.05 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.406]
 [0.34 ]
 [0.427]
 [0.307]
 [0.307]
 [0.307]] [[2.274]
 [1.908]
 [1.503]
 [1.408]
 [2.274]
 [2.274]
 [2.274]] [[1.03 ]
 [0.984]
 [0.583]
 [0.693]
 [1.03 ]
 [1.03 ]
 [1.03 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.992]
 [0.724]] [[1.719]
 [1.719]
 [1.719]
 [1.719]
 [1.719]
 [7.124]
 [1.719]] [[0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [1.791]
 [0.246]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65644515
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]] [[0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]] [[0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]]
Printing some Q and Qe and total Qs values:  [[0.568]
 [1.009]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[0.801]
 [1.576]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]] [[1.142]
 [2.125]
 [1.142]
 [1.142]
 [1.142]
 [1.142]
 [1.142]]
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]] [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]]
using explorer policy with actor:  1
siam score:  -0.6626456
2768 4071
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.341]
 [0.421]
 [0.41 ]
 [0.391]
 [0.454]
 [0.425]] [[1.225]
 [1.151]
 [1.086]
 [1.1  ]
 [1.269]
 [1.83 ]
 [1.394]] [[0.49 ]
 [0.342]
 [0.479]
 [0.462]
 [0.482]
 [0.794]
 [0.591]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.379]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]] [[2.257]
 [2.251]
 [2.257]
 [2.257]
 [2.257]
 [2.257]
 [2.257]] [[1.953]
 [2.049]
 [1.953]
 [1.953]
 [1.953]
 [1.953]
 [1.953]]
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[0.593]
 [0.579]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.585]] [[2.932]
 [2.915]
 [2.942]
 [2.942]
 [2.942]
 [2.942]
 [2.922]]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.061]
 [0.114]
 [0.084]
 [0.084]
 [0.111]
 [0.084]] [[-0.651]
 [ 0.836]
 [-2.558]
 [-0.651]
 [-0.651]
 [-2.508]
 [-0.651]] [[0.084]
 [0.061]
 [0.114]
 [0.084]
 [0.084]
 [0.111]
 [0.084]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.66710466
2771 4086
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.357]
 [0.373]
 [0.357]
 [0.357]
 [0.383]] [[1.943]
 [1.943]
 [1.943]
 [1.11 ]
 [1.943]
 [1.943]
 [1.891]] [[0.357]
 [0.357]
 [0.357]
 [0.373]
 [0.357]
 [0.357]
 [0.383]]
rdn beta is 0 so we're just using the maxi policy
2774 4089
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
UNIT TEST: sample policy line 217 mcts : [0.102 0.041 0.061 0.041 0.082 0.388 0.286]
line 256 mcts: sample exp_bonus 1.8195229732922116
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6662307
siam score:  -0.6653692
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
siam score:  -0.66476166
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]] [[2.193]
 [2.193]
 [2.193]
 [2.193]
 [2.193]
 [2.193]
 [2.193]] [[0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.955]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]] [[-1.411]
 [ 0.866]
 [-1.411]
 [-1.411]
 [-1.411]
 [-1.411]
 [-1.411]] [[0.6  ]
 [0.955]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  1
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.224 0.327 0.061 0.082 0.082 0.122 0.102]
siam score:  -0.6567009
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.492]
 [0.493]
 [0.493]
 [0.488]
 [0.491]
 [0.492]] [[2.406]
 [2.592]
 [2.247]
 [2.26 ]
 [1.91 ]
 [2.395]
 [2.512]] [[0.496]
 [0.492]
 [0.493]
 [0.493]
 [0.488]
 [0.491]
 [0.492]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
siam score:  -0.6698736
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.41 ]
 [0.266]
 [0.395]
 [0.329]
 [0.372]
 [0.443]] [[1.908]
 [2.157]
 [1.598]
 [1.689]
 [1.459]
 [1.591]
 [2.06 ]] [[0.617]
 [0.825]
 [0.439]
 [0.58 ]
 [0.417]
 [0.514]
 [0.8  ]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.51  0.02  0.163 0.143 0.082 0.041]
using explorer policy with actor:  0
using another actor
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.405]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[1.846]
 [3.06 ]
 [1.846]
 [1.846]
 [1.846]
 [1.846]
 [1.846]] [[0.413]
 [0.405]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.656]
 [0.677]
 [0.632]
 [0.642]
 [0.63 ]
 [0.64 ]] [[-0.311]
 [ 0.522]
 [ 0.203]
 [-0.777]
 [ 0.029]
 [-0.553]
 [-0.303]] [[0.655]
 [0.656]
 [0.677]
 [0.632]
 [0.642]
 [0.63 ]
 [0.64 ]]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2806 4142
using explorer policy with actor:  1
from probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
siam score:  -0.6522772
siam score:  -0.6514568
Printing some Q and Qe and total Qs values:  [[0.83]
 [0.83]
 [0.83]
 [0.83]
 [0.83]
 [0.83]
 [0.83]] [[5.393]
 [5.393]
 [5.393]
 [5.393]
 [5.393]
 [5.393]
 [5.393]] [[7.053]
 [7.053]
 [7.053]
 [7.053]
 [7.053]
 [7.053]
 [7.053]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using another actor
from probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.63523966
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.557]
 [0.557]
 [0.557]
 [0.684]
 [0.557]
 [0.557]] [[1.893]
 [1.965]
 [1.965]
 [1.965]
 [2.123]
 [1.965]
 [1.965]] [[1.422]
 [1.455]
 [1.455]
 [1.455]
 [1.737]
 [1.455]
 [1.455]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.6472937
UNIT TEST: sample policy line 217 mcts : [0.102 0.102 0.122 0.122 0.102 0.306 0.143]
siam score:  -0.64289933
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34073740152557785, 0.1688634653492138, 0.16104714308238294, 0.3216106043891224, 0.003870692826851487, 0.003870692826851487]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7791905748109293
actor:  1 policy actor:  1  step number:  129 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]] [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]] [[0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]]
siam score:  -0.6422452
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
from probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6514223715017434
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.359]] [[4.186]
 [4.186]
 [4.186]
 [4.186]
 [4.186]
 [4.186]
 [5.532]] [[1.221]
 [1.221]
 [1.221]
 [1.221]
 [1.221]
 [1.221]
 [1.754]]
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.286]
 [0.317]
 [0.321]
 [0.324]
 [0.322]
 [0.397]] [[0.852]
 [1.014]
 [0.869]
 [0.928]
 [1.416]
 [1.116]
 [4.097]] [[-0.05 ]
 [-0.023]
 [-0.046]
 [-0.032]
 [ 0.077]
 [ 0.01 ]
 [ 0.694]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.273]] [[5.4  ]
 [5.4  ]
 [5.4  ]
 [5.4  ]
 [5.4  ]
 [5.4  ]
 [8.927]] [[1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.88 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
from probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.306]
 [0.279]
 [0.296]
 [0.279]
 [0.279]
 [0.279]] [[1.308]
 [2.03 ]
 [1.308]
 [1.127]
 [1.308]
 [1.308]
 [1.308]] [[0.279]
 [0.306]
 [0.279]
 [0.296]
 [0.279]
 [0.279]
 [0.279]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.636]
 [0.534]
 [0.534]
 [0.534]
 [0.521]
 [0.512]] [[-0.892]
 [-0.273]
 [-0.892]
 [-0.892]
 [-0.892]
 [-0.63 ]
 [-0.472]] [[0.99 ]
 [1.401]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [1.053]
 [1.087]]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.442]
 [0.289]] [[1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]
 [0.788]
 [2.685]] [[0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.64 ]
 [0.969]]
siam score:  -0.6421003
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.492]
 [0.482]] [[-1.522]
 [-1.522]
 [-1.522]
 [-1.522]
 [-1.522]
 [-1.294]
 [-1.522]] [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.492]
 [0.482]]
in main func line 156:  2839
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
from probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.325]
 [0.291]
 [0.325]
 [0.325]
 [0.332]
 [0.301]] [[-2.459]
 [-2.133]
 [-2.708]
 [-2.133]
 [-2.133]
 [-2.032]
 [-1.972]] [[0.29 ]
 [0.325]
 [0.291]
 [0.325]
 [0.325]
 [0.332]
 [0.301]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.427]
 [0.558]
 [0.511]
 [0.564]
 [0.581]
 [0.577]] [[1.099]
 [1.573]
 [0.854]
 [1.486]
 [1.771]
 [1.741]
 [1.65 ]] [[1.853]
 [1.916]
 [1.8  ]
 [1.974]
 [2.132]
 [2.139]
 [2.102]]
2839 4184
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
from probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.657]] [[1.255]
 [0.761]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [1.41 ]] [[1.986]
 [1.823]
 [1.568]
 [1.568]
 [1.568]
 [1.568]
 [2.188]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
from probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
first move QE:  0.6007221384677558
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.429]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[-3.636]
 [-3.116]
 [-3.636]
 [-3.636]
 [-3.636]
 [-3.636]
 [-3.636]] [[0.397]
 [0.429]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]]
UNIT TEST: sample policy line 217 mcts : [0.163 0.286 0.184 0.061 0.143 0.082 0.082]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.48 ]
 [0.455]
 [0.455]] [[1.089]
 [1.089]
 [1.089]
 [1.089]
 [1.773]
 [1.089]
 [1.089]] [[1.925]
 [1.925]
 [1.925]
 [1.925]
 [2.208]
 [1.925]
 [1.925]]
siam score:  -0.6432318
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.389]
 [0.324]
 [0.451]
 [0.409]
 [0.324]
 [0.404]] [[1.5  ]
 [1.533]
 [1.743]
 [0.266]
 [1.072]
 [1.503]
 [1.617]] [[0.635]
 [1.058]
 [1.068]
 [0.338]
 [0.792]
 [0.909]
 [1.144]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.23 ]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[-0.386]
 [ 0.539]
 [-0.386]
 [-0.386]
 [-0.386]
 [-0.525]
 [-0.386]] [[0.244]
 [0.23 ]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
siam score:  -0.638108
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 3.0629762231179076
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
start point for exploration sampling:  11106
2852 4194
in main func line 156:  2853
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.619]
 [0.625]
 [0.62 ]
 [0.619]
 [0.621]
 [0.615]] [[-1.874]
 [ 0.966]
 [-2.647]
 [-2.595]
 [-2.694]
 [-2.472]
 [-0.167]] [[0.611]
 [0.619]
 [0.625]
 [0.62 ]
 [0.619]
 [0.621]
 [0.615]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.02  0.    0.02  0.898 0.02 ]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.604]
 [0.454]
 [0.454]
 [0.454]
 [0.462]
 [0.467]] [[-1.686]
 [ 0.545]
 [-1.606]
 [-1.606]
 [-1.606]
 [-1.673]
 [-1.382]] [[0.055]
 [0.769]
 [0.075]
 [0.075]
 [0.075]
 [0.072]
 [0.144]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.308]
 [0.339]
 [0.326]
 [0.207]
 [0.366]
 [0.269]] [[ 1.534]
 [ 0.68 ]
 [-3.13 ]
 [-1.619]
 [ 0.732]
 [-1.894]
 [ 0.482]] [[0.176]
 [0.308]
 [0.339]
 [0.326]
 [0.207]
 [0.366]
 [0.269]]
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.289]
 [0.369]
 [0.32 ]
 [0.205]
 [0.367]
 [0.261]] [[ 1.612]
 [ 0.65 ]
 [-2.007]
 [-0.931]
 [ 0.732]
 [-2.223]
 [ 0.467]] [[0.177]
 [0.289]
 [0.369]
 [0.32 ]
 [0.205]
 [0.367]
 [0.261]]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.814]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.462]] [[-1.811]
 [ 0.511]
 [-1.811]
 [-1.811]
 [-1.811]
 [-1.811]
 [-1.673]] [[0.46 ]
 [0.814]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.462]]
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.374]
 [0.394]
 [0.394]
 [0.394]
 [0.397]
 [0.393]] [[-2.474]
 [-0.95 ]
 [-2.868]
 [-2.868]
 [-2.868]
 [-2.62 ]
 [-2.232]] [[0.393]
 [0.374]
 [0.394]
 [0.394]
 [0.394]
 [0.397]
 [0.393]]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.582]
 [0.444]
 [0.444]
 [0.638]] [[1.496]
 [1.496]
 [1.496]
 [1.02 ]
 [1.496]
 [1.496]
 [1.808]] [[0.444]
 [0.444]
 [0.444]
 [0.582]
 [0.444]
 [0.444]
 [0.638]]
2856 4202
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.464]
 [0.448]
 [0.508]
 [0.448]
 [0.448]
 [0.485]] [[1.767]
 [1.375]
 [1.767]
 [1.112]
 [1.767]
 [1.767]
 [1.369]] [[0.448]
 [0.464]
 [0.448]
 [0.508]
 [0.448]
 [0.448]
 [0.485]]
using explorer policy with actor:  0
2857 4203
using explorer policy with actor:  0
using explorer policy with actor:  0
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.75 ]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.645]] [[2.059]
 [2.053]
 [2.059]
 [2.059]
 [2.059]
 [2.059]
 [2.17 ]] [[0.656]
 [0.75 ]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.645]]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.679]
 [0.732]] [[-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [ 0.22 ]
 [ 0.367]] [[0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.679]
 [0.732]]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.547856380944977
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]] [[0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]] [[0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]]
from probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.047]
 [0.052]
 [0.047]
 [0.047]
 [0.049]
 [0.046]] [[-3.245]
 [-3.16 ]
 [-3.507]
 [-3.16 ]
 [-3.16 ]
 [-3.046]
 [-3.089]] [[0.044]
 [0.047]
 [0.052]
 [0.047]
 [0.047]
 [0.049]
 [0.046]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.531]
 [0.573]
 [0.552]
 [0.652]
 [0.606]
 [0.557]] [[2.367]
 [2.622]
 [2.106]
 [2.169]
 [1.815]
 [2.187]
 [2.13 ]] [[0.541]
 [0.531]
 [0.573]
 [0.552]
 [0.652]
 [0.606]
 [0.557]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.232]
 [0.353]
 [0.353]
 [0.362]
 [0.366]
 [0.368]] [[-0.957]
 [ 0.969]
 [ 0.   ]
 [ 0.   ]
 [ 0.063]
 [-0.372]
 [-0.568]] [[0.359]
 [0.232]
 [0.353]
 [0.353]
 [0.362]
 [0.366]
 [0.368]]
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.374]] [[1.313]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.575]] [[0.197]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.374]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0181725847803875
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.6346363
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.522]
 [0.5  ]
 [0.51 ]
 [0.514]
 [0.502]
 [0.498]] [[1.096]
 [1.266]
 [1.121]
 [1.263]
 [1.277]
 [1.132]
 [1.137]] [[0.317]
 [0.535]
 [0.345]
 [0.508]
 [0.529]
 [0.361]
 [0.356]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.253]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.364]] [[-0.66 ]
 [ 1.212]
 [ 0.   ]
 [ 0.   ]
 [-0.035]
 [ 0.   ]
 [-0.235]] [[0.351]
 [0.253]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.364]]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.691]
 [0.485]] [[1.278]
 [1.278]
 [1.278]
 [1.278]
 [1.278]
 [5.405]
 [1.278]] [[0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [1.473]
 [0.089]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
rdn probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
siam score:  -0.64472795
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
first move QE:  0.5983125200433119
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
Printing some Q and Qe and total Qs values:  [[1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]] [[0.587]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.591]] [[2.084]
 [2.106]
 [2.106]
 [2.106]
 [2.106]
 [2.106]
 [2.091]]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.472]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]] [[ 0.912]
 [ 0.541]
 [-0.544]
 [-0.544]
 [-0.544]
 [-0.544]
 [-0.544]] [[1.558]
 [1.513]
 [1.149]
 [1.149]
 [1.149]
 [1.149]
 [1.149]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
2865 4218
from probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2865 4221
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
Printing some Q and Qe and total Qs values:  [[1.261]
 [1.261]
 [1.261]
 [1.261]
 [1.261]
 [1.414]
 [1.261]] [[0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.811]
 [0.719]] [[1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.858]
 [1.461]]
start point for exploration sampling:  11106
line 256 mcts: sample exp_bonus 2.7172235388102126
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.495]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.466]] [[-2.511]
 [-2.157]
 [-2.511]
 [-2.511]
 [-2.511]
 [-2.511]
 [-2.515]] [[0.481]
 [0.495]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.466]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
siam score:  -0.65080595
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
2871 4224
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.761]] [[1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.263]] [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.761]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.551]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]] [[0.89 ]
 [0.821]
 [0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]] [[1.75 ]
 [1.79 ]
 [1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.583]]
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [1.017]] [[1.299]
 [1.299]
 [1.299]
 [1.299]
 [1.299]
 [1.299]
 [1.612]] [[2.041]
 [2.041]
 [2.041]
 [2.041]
 [2.041]
 [2.041]
 [2.353]]
line 256 mcts: sample exp_bonus 2.7074615782468623
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
from probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
line 256 mcts: sample exp_bonus 0.09907499328781288
using explorer policy with actor:  0
using explorer policy with actor:  0
using another actor
line 256 mcts: sample exp_bonus 1.1062029694660669
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.494]
 [0.436]
 [0.436]
 [0.436]
 [0.426]
 [0.443]] [[-0.28 ]
 [ 0.581]
 [-0.28 ]
 [-0.28 ]
 [-0.28 ]
 [-0.161]
 [-0.167]] [[0.436]
 [0.494]
 [0.436]
 [0.436]
 [0.436]
 [0.426]
 [0.443]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.088]
 [0.089]
 [0.083]
 [0.084]
 [0.095]
 [0.098]] [[-1.418]
 [-0.734]
 [-1.319]
 [-1.037]
 [-0.99 ]
 [-1.195]
 [-1.34 ]] [[0.093]
 [0.088]
 [0.089]
 [0.083]
 [0.084]
 [0.095]
 [0.098]]
siam score:  -0.6422113
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2878 4241
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6459051
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.257]
 [0.236]
 [0.238]
 [0.254]
 [0.245]
 [0.258]] [[-1.053]
 [-0.619]
 [-1.149]
 [-1.039]
 [-0.832]
 [-1.113]
 [-1.181]] [[0.241]
 [0.257]
 [0.236]
 [0.238]
 [0.254]
 [0.245]
 [0.258]]
siam score:  -0.6464731
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
start point for exploration sampling:  11106
start point for exploration sampling:  11106
line 256 mcts: sample exp_bonus 2.6591872585442857
first move QE:  0.5984544924430079
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
from probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]] [[1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]] [[0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.497]
 [0.478]
 [0.478]
 [0.47 ]
 [0.534]] [[1.979]
 [1.979]
 [1.659]
 [1.979]
 [1.979]
 [2.158]
 [2.256]] [[0.965]
 [0.965]
 [0.897]
 [0.965]
 [0.965]
 [1.008]
 [1.169]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
start point for exploration sampling:  11106
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 1.6361229705588918
Printing some Q and Qe and total Qs values:  [[1.485]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]] [[0.583]
 [1.511]
 [1.511]
 [1.511]
 [1.511]
 [1.511]
 [1.511]] [[2.781]
 [1.268]
 [1.268]
 [1.268]
 [1.268]
 [1.268]
 [1.268]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3105827367351254, 0.24241759672804178, 0.14679475225775915, 0.2931486277320608, 0.0035281432735065075, 0.0035281432735065075]
actor:  1 policy actor:  1  step number:  112 total reward:  0.09499999999999931  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.102 0.163 0.143 0.143 0.102 0.102 0.245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.214]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[-2.268]
 [-1.423]
 [-3.153]
 [-3.153]
 [-3.153]
 [-3.153]
 [-3.153]] [[0.214]
 [0.214]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]]
siam score:  -0.640286
Printing some Q and Qe and total Qs values:  [[1.052]
 [1.052]
 [1.052]
 [1.052]
 [1.052]
 [1.343]
 [1.052]] [[0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.945]
 [0.642]] [[1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.907]
 [1.225]]
siam score:  -0.6431351
rdn beta is 0 so we're just using the maxi policy
first move QE:  0.5961199430443983
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2863592664204861, 0.2235105720819942, 0.13534569890392376, 0.27028490659838805, 0.0812465853689322, 0.003252970626275746]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2863592664204861, 0.2235105720819942, 0.13534569890392376, 0.27028490659838805, 0.0812465853689322, 0.003252970626275746]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2863592664204861, 0.2235105720819942, 0.13534569890392376, 0.27028490659838805, 0.0812465853689322, 0.003252970626275746]
from probs:  [0.2863592664204861, 0.2235105720819942, 0.13534569890392376, 0.27028490659838805, 0.0812465853689322, 0.003252970626275746]
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]] [[-2.93]
 [-2.93]
 [-2.93]
 [-2.93]
 [-2.93]
 [-2.93]
 [-2.93]] [[0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.859]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.629]] [[1.127]
 [2.1  ]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.449]] [[1.48 ]
 [1.84 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.568]]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.183]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]] [[-3.038]
 [-3.077]
 [-2.953]
 [-2.953]
 [-2.953]
 [-2.953]
 [-2.953]] [[0.159]
 [0.183]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6374454
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.566]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[0.994]
 [1.401]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]] [[2.067]
 [2.278]
 [2.067]
 [2.067]
 [2.067]
 [2.067]
 [2.067]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64003843
siam score:  -0.6429309
Printing some Q and Qe and total Qs values:  [[ 0.002]
 [-0.005]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]] [[-0.409]
 [ 0.027]
 [-0.409]
 [-0.409]
 [-0.409]
 [-0.409]
 [-0.409]] [[ 0.002]
 [-0.005]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.426]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]] [[0.278]
 [1.975]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[0.94 ]
 [1.609]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]]
siam score:  -0.64356923
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2863592664204861, 0.2235105720819942, 0.13534569890392376, 0.27028490659838805, 0.0812465853689322, 0.003252970626275746]
from probs:  [0.2863592664204861, 0.2235105720819942, 0.13534569890392376, 0.27028490659838805, 0.0812465853689322, 0.003252970626275746]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.309]
 [0.174]
 [0.448]
 [0.456]
 [0.174]
 [0.072]] [[ 1.105]
 [ 1.171]
 [ 1.204]
 [-1.019]
 [ 1.072]
 [ 1.204]
 [ 2.548]] [[1.305]
 [1.292]
 [1.233]
 [0.099]
 [1.32 ]
 [1.233]
 [1.955]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2863592664204861, 0.2235105720819942, 0.13534569890392376, 0.27028490659838805, 0.0812465853689322, 0.003252970626275746]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.511]
 [0.497]
 [0.498]
 [0.506]
 [0.479]
 [0.487]] [[0.653]
 [0.976]
 [0.803]
 [0.868]
 [1.04 ]
 [0.897]
 [0.969]] [[0.331]
 [0.808]
 [0.548]
 [0.637]
 [0.882]
 [0.638]
 [0.75 ]]
siam score:  -0.63806987
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2863592664204861, 0.2235105720819942, 0.13534569890392376, 0.27028490659838805, 0.0812465853689322, 0.003252970626275746]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2863592664204861, 0.2235105720819942, 0.13534569890392376, 0.27028490659838805, 0.0812465853689322, 0.003252970626275746]
siam score:  -0.63767153
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2863592664204861, 0.2235105720819942, 0.13534569890392376, 0.27028490659838805, 0.0812465853689322, 0.003252970626275746]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.317]
 [0.519]
 [0.518]
 [0.517]
 [0.522]
 [0.519]] [[3.079]
 [3.067]
 [3.118]
 [3.148]
 [3.133]
 [3.185]
 [3.114]] [[ 0.017]
 [-0.394]
 [ 0.044]
 [ 0.061]
 [ 0.05 ]
 [ 0.094]
 [ 0.04 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  116 total reward:  0.2949999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.25819312982117587, 0.299885636063839, 0.12203317198237727, 0.2436998350023345, 0.07325521686762936, 0.0029330102626439736]
using another actor
from probs:  [0.25819312982117587, 0.299885636063839, 0.12203317198237727, 0.2436998350023345, 0.07325521686762936, 0.0029330102626439736]
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.25819312982117587, 0.299885636063839, 0.12203317198237727, 0.2436998350023345, 0.07325521686762936, 0.0029330102626439736]
siam score:  -0.6437931
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.25819312982117587, 0.299885636063839, 0.12203317198237727, 0.2436998350023345, 0.07325521686762936, 0.0029330102626439736]
line 256 mcts: sample exp_bonus 2.2609963793919525
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.25819312982117587, 0.299885636063839, 0.12203317198237727, 0.2436998350023345, 0.07325521686762936, 0.0029330102626439736]
siam score:  -0.6504471
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.25819312982117587, 0.299885636063839, 0.12203317198237727, 0.2436998350023345, 0.07325521686762936, 0.0029330102626439736]
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.421]
 [0.431]] [[1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.026]
 [1.095]] [[0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.793]
 [0.859]]
siam score:  -0.65069216
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.25819312982117587, 0.299885636063839, 0.12203317198237727, 0.2436998350023345, 0.07325521686762936, 0.0029330102626439736]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.004]
 [-0.005]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[1.737]
 [1.071]
 [1.586]
 [1.07 ]
 [1.925]
 [1.704]
 [1.717]] [[ 0.879]
 [-0.01 ]
 [ 0.676]
 [-0.011]
 [ 1.129]
 [ 0.834]
 [ 0.851]]
line 256 mcts: sample exp_bonus 0.47349425352270574
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.388]
 [0.355]
 [0.385]
 [0.401]
 [0.387]
 [0.376]] [[2.299]
 [3.434]
 [3.135]
 [2.561]
 [2.22 ]
 [2.086]
 [2.864]] [[0.211]
 [0.856]
 [0.61 ]
 [0.311]
 [0.13 ]
 [0.022]
 [0.481]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  57 total reward:  0.5299999999999997  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.505]
 [0.398]
 [0.508]
 [0.572]
 [0.572]
 [0.923]] [[3.044]
 [2.484]
 [1.831]
 [1.647]
 [3.044]
 [3.044]
 [1.24 ]] [[2.16 ]
 [1.884]
 [1.531]
 [1.583]
 [2.16 ]
 [2.16 ]
 [1.888]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.728]
 [0.549]] [[1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [3.934]
 [1.53 ]] [[1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.466]
 [2.34 ]
 [1.506]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22702294226971334, 0.3844064598614256, 0.10730080144709198, 0.21427934047351285, 0.06441153132700891, 0.0025789246212471785]
from probs:  [0.22702294226971334, 0.3844064598614256, 0.10730080144709198, 0.21427934047351285, 0.06441153132700891, 0.0025789246212471785]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.505]
 [0.705]
 [0.579]
 [0.578]
 [0.574]
 [0.577]] [[-5.057]
 [ 0.781]
 [ 0.485]
 [-4.406]
 [-4.286]
 [-3.947]
 [-4.164]] [[0.085]
 [1.727]
 [1.711]
 [0.271]
 [0.305]
 [0.4  ]
 [0.34 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22702294226971334, 0.3844064598614256, 0.10730080144709198, 0.21427934047351285, 0.06441153132700891, 0.0025789246212471785]
line 256 mcts: sample exp_bonus 0.7619279289368799
from probs:  [0.22702294226971334, 0.3844064598614256, 0.10730080144709198, 0.21427934047351285, 0.06441153132700891, 0.0025789246212471785]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22702294226971334, 0.3844064598614256, 0.10730080144709198, 0.21427934047351285, 0.06441153132700891, 0.0025789246212471785]
2917 4359
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.783]] [[0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.485]] [[0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.783]]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.844]] [[1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.146]
 [0.117]] [[0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.844]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22702294226971334, 0.3844064598614256, 0.10730080144709198, 0.21427934047351285, 0.06441153132700891, 0.0025789246212471785]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22702294226971334, 0.3844064598614256, 0.10730080144709198, 0.21427934047351285, 0.06441153132700891, 0.0025789246212471785]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22702294226971334, 0.3844064598614256, 0.10730080144709198, 0.21427934047351285, 0.06441153132700891, 0.0025789246212471785]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64944214
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22702294226971334, 0.3844064598614256, 0.10730080144709198, 0.21427934047351285, 0.06441153132700891, 0.0025789246212471785]
first move QE:  0.5941396013207629
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22702294226971334, 0.3844064598614256, 0.10730080144709198, 0.21427934047351285, 0.06441153132700891, 0.0025789246212471785]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6386417
line 256 mcts: sample exp_bonus -1.0091227656900346
actor:  1 policy actor:  1  step number:  90 total reward:  0.034999999999999254  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.474]
 [0.504]
 [0.503]
 [0.505]
 [0.504]
 [0.503]] [[2.937]
 [2.445]
 [2.934]
 [2.886]
 [2.981]
 [3.096]
 [3.08 ]] [[0.922]
 [0.374]
 [0.918]
 [0.866]
 [0.966]
 [1.081]
 [1.063]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.640889925264279
line 256 mcts: sample exp_bonus 1.8589308224810477
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[ 0.122]
 [-0.004]
 [ 0.133]
 [ 0.135]
 [ 0.14 ]
 [ 0.144]
 [ 0.133]] [[3.237]
 [2.98 ]
 [3.932]
 [3.409]
 [3.528]
 [3.465]
 [3.896]] [[0.788]
 [0.515]
 [1.223]
 [0.905]
 [0.982]
 [0.947]
 [1.202]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.691]
 [1.113]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]] [[-0.266]
 [-0.423]
 [-0.266]
 [-0.266]
 [-0.266]
 [-0.266]
 [-0.266]] [[1.032]
 [1.824]
 [1.032]
 [1.032]
 [1.032]
 [1.032]
 [1.032]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.6350331711363935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2926 4375
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.725]
 [0.728]
 [0.725]
 [0.753]
 [0.886]
 [0.729]] [[2.636]
 [2.636]
 [1.21 ]
 [1.055]
 [1.896]
 [3.615]
 [2.019]] [[1.374]
 [1.374]
 [0.637]
 [0.553]
 [1.019]
 [2.051]
 [1.057]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.404]
 [0.415]
 [0.455]
 [0.425]
 [0.385]
 [0.465]] [[ 1.451]
 [ 2.527]
 [ 0.575]
 [ 1.251]
 [ 0.64 ]
 [-0.048]
 [ 1.292]] [[1.002]
 [1.611]
 [0.448]
 [0.903]
 [0.5  ]
 [0.038]
 [0.94 ]]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.609]] [[3.431]
 [3.431]
 [3.431]
 [3.431]
 [3.431]
 [3.431]
 [2.269]] [[1.605]
 [1.605]
 [1.605]
 [1.605]
 [1.605]
 [1.605]
 [0.773]]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.176]
 [0.275]
 [0.332]
 [0.228]
 [0.352]
 [0.205]] [[1.034]
 [2.482]
 [1.34 ]
 [1.916]
 [2.158]
 [2.563]
 [2.555]] [[0.089]
 [0.176]
 [0.275]
 [0.332]
 [0.228]
 [0.352]
 [0.205]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
line 256 mcts: sample exp_bonus 0.37693171530355285
siam score:  -0.63845587
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.361]
 [0.507]
 [0.507]
 [0.507]
 [0.497]
 [0.501]] [[0.283]
 [0.905]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.27 ]
 [0.201]] [[0.744]
 [0.667]
 [0.657]
 [0.657]
 [0.657]
 [0.727]
 [0.713]]
2933 4386
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[-1.121]
 [-1.121]
 [-1.121]
 [-1.121]
 [-1.121]
 [-1.121]
 [-1.121]] [[0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [0.023]
 [0.023]
 [0.023]
 [1.477]
 [1.477]] [[0.586]
 [0.593]
 [2.028]
 [2.028]
 [2.028]
 [0.593]
 [0.586]] [[1.908]
 [1.916]
 [1.895]
 [1.895]
 [1.895]
 [1.916]
 [1.908]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.644108
siam score:  -0.64591026
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.673]] [[4.105]
 [4.105]
 [4.105]
 [4.105]
 [4.105]
 [4.105]
 [3.733]] [[1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.246]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
first move QE:  0.5931264926284214
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.099]
 [0.309]
 [0.36 ]
 [0.039]
 [0.36 ]
 [0.083]] [[ 1.009]
 [ 0.8  ]
 [ 0.949]
 [ 0.099]
 [ 1.615]
 [-0.229]
 [ 1.478]] [[ 0.76 ]
 [ 0.512]
 [ 0.861]
 [ 0.083]
 [ 1.247]
 [-0.237]
 [ 1.156]]
siam score:  -0.63602376
from probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
2945 4410
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5918903771109566
using explorer policy with actor:  1
siam score:  -0.6336387
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.325]
 [0.423]
 [0.325]
 [0.325]
 [0.325]
 [0.325]] [[1.917]
 [1.917]
 [1.851]
 [1.917]
 [1.917]
 [1.917]
 [1.917]] [[1.217]
 [1.217]
 [1.348]
 [1.217]
 [1.217]
 [1.217]
 [1.217]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
siam score:  -0.6371076
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.355]
 [0.254]
 [0.254]
 [0.254]
 [0.008]
 [0.115]] [[1.229]
 [1.635]
 [1.229]
 [1.229]
 [1.229]
 [1.348]
 [1.711]] [[1.097]
 [1.959]
 [1.097]
 [1.097]
 [1.097]
 [0.808]
 [1.614]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64722824
using explorer policy with actor:  0
using explorer policy with actor:  0
2951 4436
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]] [[0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]] [[2.942]
 [2.942]
 [2.942]
 [2.942]
 [2.942]
 [2.942]
 [2.942]]
Printing some Q and Qe and total Qs values:  [[1.316]
 [1.316]
 [1.316]
 [1.316]
 [1.316]
 [1.316]
 [1.316]] [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[2.265]
 [2.265]
 [2.265]
 [2.265]
 [2.265]
 [2.265]
 [2.265]]
using another actor
2960 4459
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]] [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[1.34]
 [1.34]
 [1.34]
 [1.34]
 [1.34]
 [1.34]
 [1.34]] [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]]
siam score:  -0.65894514
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
Printing some Q and Qe and total Qs values:  [[0.49]
 [0.52]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]] [[1.385]
 [1.538]
 [1.385]
 [1.385]
 [1.385]
 [1.385]
 [1.385]] [[0.49]
 [0.52]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.371]
 [0.583]
 [0.403]
 [0.384]
 [0.393]
 [0.462]] [[ 0.776]
 [ 1.536]
 [ 0.   ]
 [ 0.83 ]
 [ 0.901]
 [ 0.883]
 [-0.059]] [[0.384]
 [0.371]
 [0.583]
 [0.403]
 [0.384]
 [0.393]
 [0.462]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using another actor
siam score:  -0.65818274
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.369]
 [0.354]
 [0.353]
 [0.349]
 [0.356]
 [0.352]] [[1.287]
 [2.821]
 [1.596]
 [1.62 ]
 [1.654]
 [2.216]
 [1.934]] [[0.358]
 [0.369]
 [0.354]
 [0.353]
 [0.349]
 [0.356]
 [0.352]]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
siam score:  -0.6445755
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
rdn beta is 0 so we're just using the maxi policy
using another actor
from probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
Printing some Q and Qe and total Qs values:  [[0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]] [[-1.706]
 [-1.706]
 [-1.706]
 [-1.706]
 [-1.706]
 [-1.706]
 [-1.706]] [[0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]] [[-3.236]
 [-3.236]
 [-3.236]
 [-3.236]
 [-3.236]
 [-3.236]
 [-3.236]] [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]]
line 256 mcts: sample exp_bonus -1.57311530491218
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.84 ]
 [0.777]
 [0.805]
 [0.823]
 [0.757]
 [0.777]] [[-0.739]
 [ 0.149]
 [-0.97 ]
 [-0.289]
 [-0.478]
 [-1.305]
 [-1.271]] [[0.783]
 [0.84 ]
 [0.777]
 [0.805]
 [0.823]
 [0.757]
 [0.777]]
in main func line 156:  2971
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
Printing some Q and Qe and total Qs values:  [[1.222]
 [1.222]
 [1.222]
 [1.222]
 [1.222]
 [1.222]
 [1.222]] [[0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[1.67]
 [1.67]
 [1.67]
 [1.67]
 [1.67]
 [1.67]
 [1.67]]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.036]
 [0.035]
 [0.035]
 [0.03 ]
 [0.018]
 [0.031]] [[0.921]
 [1.016]
 [0.786]
 [0.657]
 [0.739]
 [0.76 ]
 [0.622]] [[-0.503]
 [-0.412]
 [-0.569]
 [-0.653]
 [-0.609]
 [-0.619]
 [-0.686]]
2972 4499
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[1.334]
 [1.655]
 [1.655]
 [1.655]
 [1.655]
 [1.655]
 [1.655]] [[0.942]
 [1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.169]]
from probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
line 256 mcts: sample exp_bonus 1.0169257930469786
siam score:  -0.6441615
in main func line 156:  2976
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.387]
 [0.353]
 [0.353]
 [0.353]
 [0.368]
 [0.38 ]] [[-3.453]
 [-2.487]
 [-3.453]
 [-3.453]
 [-3.453]
 [-3.487]
 [-3.46 ]] [[0.353]
 [0.387]
 [0.353]
 [0.353]
 [0.353]
 [0.368]
 [0.38 ]]
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.365]] [[2.185]
 [2.185]
 [2.185]
 [2.185]
 [2.185]
 [2.185]
 [2.012]] [[0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.365]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 0.5687278433045214
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.232]
 [0.514]
 [0.47 ]
 [0.474]
 [1.224]
 [0.365]] [[ 0.659]
 [ 1.092]
 [ 1.104]
 [-0.084]
 [ 0.401]
 [ 0.719]
 [ 1.319]] [[1.174]
 [1.318]
 [1.721]
 [0.29 ]
 [0.855]
 [2.26 ]
 [1.763]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.5830561109660265
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.355]] [[2.75]
 [2.75]
 [2.75]
 [2.75]
 [2.75]
 [2.75]
 [2.75]]
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [1.117]] [[1.855]
 [1.855]
 [1.855]
 [1.855]
 [1.855]
 [1.855]
 [1.645]] [[1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.529]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.498]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[1.59 ]
 [1.785]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.59 ]] [[-0.054]
 [ 0.064]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]]
from probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.2531791165839463
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.419]
 [0.538]
 [0.547]
 [0.543]
 [0.488]
 [0.495]] [[1.441]
 [1.745]
 [0.818]
 [1.033]
 [1.026]
 [1.23 ]
 [1.202]] [[0.665]
 [0.809]
 [0.429]
 [0.59 ]
 [0.579]
 [0.603]
 [0.599]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5346059587184445
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]] [[1.652]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]] [[1.596]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.347]
 [0.363]
 [0.374]
 [0.125]
 [0.2  ]
 [0.805]] [[1.376]
 [1.773]
 [1.948]
 [1.838]
 [1.562]
 [1.628]
 [1.186]] [[0.428]
 [1.15 ]
 [1.298]
 [1.248]
 [0.565]
 [0.759]
 [1.674]]
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.618]
 [0.588]
 [0.637]
 [0.588]
 [0.588]
 [0.585]] [[2.338]
 [2.186]
 [2.338]
 [2.19 ]
 [2.338]
 [2.338]
 [2.186]] [[2.118]
 [2.083]
 [2.118]
 [2.098]
 [2.118]
 [2.118]
 [2.057]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21513837684796405, 0.36428292664010664, 0.1016836449524454, 0.20306189780042275, 0.061039611950413755, 0.05479354180864732]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64676726
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.422]
 [0.453]
 [0.451]
 [0.449]
 [0.448]
 [0.453]] [[-1.526]
 [ 0.334]
 [-1.503]
 [-1.626]
 [-1.536]
 [-1.683]
 [-1.919]] [[0.447]
 [0.422]
 [0.453]
 [0.451]
 [0.449]
 [0.448]
 [0.453]]
siam score:  -0.6508474
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1624743019545645
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  95 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
Printing some Q and Qe and total Qs values:  [[3.   ]
 [0.558]
 [3.   ]
 [3.   ]
 [3.   ]
 [3.   ]
 [3.   ]] [[0.  ]
 [0.07]
 [0.  ]
 [0.  ]
 [0.  ]
 [0.  ]
 [0.  ]] [[7.126]
 [2.359]
 [7.126]
 [7.126]
 [7.126]
 [7.126]
 [7.126]]
siam score:  -0.65054244
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.401]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.391]] [[0.736]
 [0.798]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [1.022]] [[0.392]
 [0.401]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.391]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8771893065075873
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
line 256 mcts: sample exp_bonus 1.7245080808752815
Printing some Q and Qe and total Qs values:  [[1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]] [[1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]] [[0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.336]
 [0.394]
 [0.394]
 [0.394]
 [0.408]
 [0.461]] [[0.928]
 [1.469]
 [1.224]
 [1.224]
 [1.224]
 [0.956]
 [1.31 ]] [[-0.051]
 [ 0.492]
 [ 0.281]
 [ 0.281]
 [ 0.281]
 [-0.048]
 [ 0.528]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6486765
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.31 ]
 [0.226]
 [0.248]
 [0.207]
 [0.203]
 [0.195]] [[2.571]
 [3.119]
 [1.882]
 [1.321]
 [1.903]
 [2.768]
 [2.798]] [[-0.396]
 [ 0.066]
 [-0.515]
 [-0.659]
 [-0.546]
 [-0.265]
 [-0.272]]
line 256 mcts: sample exp_bonus 2.4669795282022466
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.735]
 [0.722]] [[2.31 ]
 [2.31 ]
 [2.31 ]
 [2.31 ]
 [2.31 ]
 [2.516]
 [2.31 ]] [[-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [ 0.058]
 [-0.036]]
Printing some Q and Qe and total Qs values:  [[1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]] [[0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[2.316]
 [2.316]
 [2.316]
 [2.316]
 [2.316]
 [2.316]
 [2.316]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.081]
 [0.163]
 [0.18 ]
 [0.156]
 [0.177]
 [0.175]] [[-3.4  ]
 [ 0.53 ]
 [-1.183]
 [-3.49 ]
 [-3.258]
 [-3.215]
 [-2.745]] [[0.132]
 [0.081]
 [0.163]
 [0.18 ]
 [0.156]
 [0.177]
 [0.175]]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.366]
 [0.385]
 [0.347]
 [0.351]
 [0.355]
 [0.366]] [[-2.895]
 [-3.105]
 [-2.595]
 [-3.301]
 [-3.04 ]
 [-2.789]
 [-3.125]] [[0.354]
 [0.366]
 [0.385]
 [0.347]
 [0.351]
 [0.355]
 [0.366]]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.548]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[-2.375]
 [-2.913]
 [-2.375]
 [-2.375]
 [-2.375]
 [-2.375]
 [-2.375]] [[0.382]
 [0.548]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]]
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.389]
 [0.368]
 [0.368]
 [0.368]
 [0.365]
 [0.375]] [[-1.268]
 [-1.593]
 [-1.849]
 [-1.849]
 [-1.849]
 [-2.182]
 [-1.648]] [[0.37 ]
 [0.389]
 [0.368]
 [0.368]
 [0.368]
 [0.365]
 [0.375]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
siam score:  -0.6485804
line 256 mcts: sample exp_bonus -0.47250552442323573
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -1.4795288104155775
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.375]
 [0.552]
 [0.571]
 [0.568]
 [0.612]
 [0.627]] [[1.238]
 [1.637]
 [1.26 ]
 [1.23 ]
 [1.271]
 [1.17 ]
 [1.175]] [[0.347]
 [0.395]
 [0.372]
 [0.381]
 [0.415]
 [0.401]
 [0.437]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.185]
 [0.241]
 [0.267]
 [0.267]
 [0.254]
 [0.267]] [[1.645]
 [1.272]
 [0.269]
 [1.645]
 [1.645]
 [0.968]
 [1.645]] [[-0.142]
 [-0.43 ]
 [-0.653]
 [-0.142]
 [-0.142]
 [-0.394]
 [-0.142]]
siam score:  -0.64819115
using explorer policy with actor:  1
3007 4560
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.441]
 [0.558]
 [0.558]
 [0.498]
 [0.558]
 [0.542]] [[1.371]
 [1.531]
 [1.402]
 [1.402]
 [1.445]
 [1.402]
 [1.632]] [[1.913]
 [1.992]
 [2.043]
 [2.043]
 [2.004]
 [2.043]
 [2.139]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
using explorer policy with actor:  1
using explorer policy with actor:  0
rdn probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.868]] [[1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.162]] [[0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.868]]
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.734]] [[-0.295]
 [-0.295]
 [-0.295]
 [-0.295]
 [-0.295]
 [-0.295]
 [-1.772]] [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.734]]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.457]
 [0.431]
 [0.505]
 [0.508]
 [0.28 ]
 [0.48 ]] [[2.29 ]
 [2.338]
 [3.441]
 [3.152]
 [2.125]
 [2.219]
 [2.97 ]] [[0.427]
 [0.457]
 [0.431]
 [0.505]
 [0.508]
 [0.28 ]
 [0.48 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]] [[0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]]
siam score:  -0.65233636
siam score:  -0.6508103
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.181]
 [0.015]
 [0.181]
 [0.009]
 [0.009]
 [0.033]] [[1.476]
 [1.476]
 [1.671]
 [1.476]
 [1.52 ]
 [1.831]
 [1.987]] [[1.731]
 [1.731]
 [1.752]
 [1.731]
 [1.641]
 [1.861]
 [1.989]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.5805364108067124
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
siam score:  -0.6455713
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.401]] [[0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.115]] [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.401]]
using explorer policy with actor:  1
first move QE:  0.5804052512141967
Printing some Q and Qe and total Qs values:  [[3.   ]
 [0.496]
 [3.   ]
 [3.   ]
 [3.   ]
 [3.   ]
 [3.   ]] [[ 0.   ]
 [-0.279]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[7.068]
 [2.128]
 [7.068]
 [7.068]
 [7.068]
 [7.068]
 [7.068]]
using explorer policy with actor:  1
siam score:  -0.64300543
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.908837357198047
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]] [[1.494]
 [1.494]
 [1.494]
 [1.494]
 [1.494]
 [1.494]
 [1.494]] [[1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
start point for exploration sampling:  11106
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.2579035368416756
from probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.613]
 [0.647]
 [0.597]
 [0.633]
 [0.598]
 [0.644]] [[-0.221]
 [ 0.293]
 [-0.018]
 [ 0.403]
 [-0.015]
 [ 0.502]
 [ 0.13 ]] [[0.149]
 [0.255]
 [0.22 ]
 [0.261]
 [0.192]
 [0.296]
 [0.264]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.606]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.585]] [[0.664]
 [1.239]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [1.186]] [[0.593]
 [0.606]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.585]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
3029 4599
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.835]] [[2.299]
 [2.299]
 [2.299]
 [2.299]
 [2.299]
 [2.244]
 [2.42 ]] [[0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.835]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
from probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19484085457172798, 0.3299141593069064, 0.18643652119344453, 0.183903747337237, 0.05528074688204018, 0.04962397070864384]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
3034 4606
line 256 mcts: sample exp_bonus 0.9928266114569095
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.927]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.942]] [[1.383]
 [1.645]
 [1.383]
 [1.383]
 [1.383]
 [1.383]
 [1.273]] [[0.526]
 [0.927]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.942]]
siam score:  -0.64923686
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  81 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.167
first move QE:  0.5788374766153203
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.281]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]] [[-1.251]
 [ 0.44 ]
 [-2.196]
 [-2.196]
 [-2.196]
 [-2.196]
 [-2.196]] [[0.293]
 [0.281]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]]
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1994624794981212
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64396566
using another actor
first move QE:  0.5785244629630211
from probs:  [0.24927972966517617, 0.30760781675084503, 0.17383106977711815, 0.17146954325801278, 0.05154307378763204, 0.04626876676121571]
siam score:  -0.64287513
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.354]
 [0.414]
 [0.394]
 [0.387]
 [0.389]
 [0.351]] [[2.074]
 [2.008]
 [2.012]
 [2.053]
 [2.066]
 [2.065]
 [1.757]] [[0.39 ]
 [0.354]
 [0.414]
 [0.394]
 [0.387]
 [0.389]
 [0.351]]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.485]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.672]] [[1.829]
 [1.88 ]
 [1.829]
 [1.829]
 [1.829]
 [1.829]
 [2.089]] [[0.435]
 [0.485]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.672]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24927972966517617, 0.30760781675084503, 0.17383106977711815, 0.17146954325801278, 0.05154307378763204, 0.04626876676121571]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24927972966517617, 0.30760781675084503, 0.17383106977711815, 0.17146954325801278, 0.05154307378763204, 0.04626876676121571]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[-2.717]
 [-2.717]
 [-2.717]
 [-2.717]
 [-2.717]
 [-2.717]
 [-2.717]] [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]]
first move QE:  0.5789668978202693
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.828]
 [0.863]
 [0.863]
 [0.863]
 [0.911]
 [1.031]] [[2.92 ]
 [3.011]
 [2.92 ]
 [2.92 ]
 [2.92 ]
 [2.379]
 [2.22 ]] [[0.678]
 [0.639]
 [0.678]
 [0.678]
 [0.678]
 [0.593]
 [0.78 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24927972966517617, 0.30760781675084503, 0.17383106977711815, 0.17146954325801278, 0.05154307378763204, 0.04626876676121571]
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.475]
 [0.614]
 [0.617]
 [0.616]
 [0.493]
 [0.619]] [[-1.354]
 [ 0.335]
 [-2.756]
 [-2.908]
 [-2.724]
 [ 0.453]
 [-2.429]] [[1.115]
 [1.63 ]
 [0.633]
 [0.582]
 [0.645]
 [1.679]
 [0.748]]
start point for exploration sampling:  11106
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0594242178585545
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.566]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.545]] [[0.195]
 [0.7  ]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.201]] [[0.988]
 [1.261]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.886]]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24927972966517617, 0.30760781675084503, 0.17383106977711815, 0.17146954325801278, 0.05154307378763204, 0.04626876676121571]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.27366978269386977
using explorer policy with actor:  1
siam score:  -0.64563173
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24927972966517617, 0.30760781675084503, 0.17383106977711815, 0.17146954325801278, 0.05154307378763204, 0.04626876676121571]
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]] [[-0.482]
 [-0.482]
 [-0.482]
 [-0.482]
 [-0.482]
 [-0.482]
 [-0.482]] [[-0.575]
 [-0.575]
 [-0.575]
 [-0.575]
 [-0.575]
 [-0.575]
 [-0.575]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24927972966517617, 0.30760781675084503, 0.17383106977711815, 0.17146954325801278, 0.05154307378763204, 0.04626876676121571]
3057 4651
using explorer policy with actor:  1
3058 4652
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.585]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]] [[0.557]
 [0.801]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[1.352]
 [1.612]
 [1.352]
 [1.352]
 [1.352]
 [1.352]
 [1.352]]
actor:  1 policy actor:  1  step number:  75 total reward:  0.48999999999999966  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.317]
 [0.458]
 [0.293]
 [0.458]
 [0.297]
 [0.298]] [[-1.307]
 [-0.331]
 [ 0.   ]
 [-1.518]
 [ 0.   ]
 [-1.542]
 [-1.314]] [[0.296]
 [0.317]
 [0.458]
 [0.293]
 [0.458]
 [0.297]
 [0.298]]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.338]
 [0.303]
 [0.303]
 [0.303]
 [0.306]
 [0.308]] [[-0.686]
 [ 0.245]
 [-0.775]
 [-0.775]
 [-0.834]
 [-0.798]
 [-0.66 ]] [[0.305]
 [0.338]
 [0.303]
 [0.303]
 [0.303]
 [0.306]
 [0.308]]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.374]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[0.021]
 [1.734]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]] [[0.347]
 [0.374]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6419823
from probs:  [0.3122878164764506, 0.28179023757049404, 0.15924139694181777, 0.15707807376662444, 0.047217054368597855, 0.042385420876015295]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3122878164764506, 0.28179023757049404, 0.15924139694181777, 0.15707807376662444, 0.047217054368597855, 0.042385420876015295]
actor:  1 policy actor:  1  step number:  101 total reward:  0.3099999999999995  reward:  1.0 rdn_beta:  0.333
from probs:  [0.3122878164764506, 0.28179023757049404, 0.15924139694181777, 0.15707807376662444, 0.047217054368597855, 0.042385420876015295]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.32554574202052156, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
line 256 mcts: sample exp_bonus -3.184463615057035
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.32554574202052156, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.29326230100962913, 0.32554574202052156, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.32554574202052156, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.238]
 [0.238]
 [0.234]
 [0.232]
 [0.229]
 [0.224]] [[-1.593]
 [-0.191]
 [-2.134]
 [-1.523]
 [-1.924]
 [-2.169]
 [-2.056]] [[0.229]
 [0.238]
 [0.238]
 [0.234]
 [0.232]
 [0.229]
 [0.224]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.32554574202052156, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.359]
 [0.351]] [[-0.544]
 [-0.544]
 [-0.544]
 [-0.544]
 [-0.544]
 [-0.756]
 [-0.395]] [[0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.359]
 [0.351]]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.32554574202052156, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
Printing some Q and Qe and total Qs values:  [[0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]] [[1.152]
 [1.152]
 [1.152]
 [1.152]
 [1.152]
 [1.152]
 [1.152]] [[0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.749]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]] [[2.812]
 [1.969]
 [2.812]
 [2.812]
 [2.812]
 [2.812]
 [2.812]] [[-0.247]
 [ 0.596]
 [-0.247]
 [-0.247]
 [-0.247]
 [-0.247]
 [-0.247]]
first move QE:  0.5729862948552396
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.32554574202052156, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.416]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.908]] [[0.984]
 [1.07 ]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [1.897]] [[-0.257]
 [-0.26 ]
 [-0.257]
 [-0.257]
 [-0.257]
 [-0.257]
 [ 1.275]]
siam score:  -0.6272046
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6352388
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 0.5580096457297957
Printing some Q and Qe and total Qs values:  [[1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.253]] [[0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.347]] [[1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]
 [2.059]]
Printing some Q and Qe and total Qs values:  [[1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.191]] [[0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.319]] [[1.775]
 [1.775]
 [1.775]
 [1.775]
 [1.775]
 [1.775]
 [1.889]]
Printing some Q and Qe and total Qs values:  [[1.202]
 [1.202]
 [1.202]
 [1.202]
 [1.202]
 [1.202]
 [1.412]] [[0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.531]] [[1.973]
 [1.973]
 [1.973]
 [1.973]
 [1.973]
 [1.973]
 [2.34 ]]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.318]
 [0.457]
 [0.453]
 [0.455]
 [0.462]
 [0.685]] [[0.279]
 [1.515]
 [0.601]
 [0.634]
 [0.856]
 [0.628]
 [0.798]] [[0.272]
 [0.807]
 [0.477]
 [0.491]
 [0.642]
 [0.504]
 [1.064]]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.876]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]] [[1.987]
 [0.556]
 [1.987]
 [1.987]
 [1.987]
 [1.987]
 [1.987]] [[0.057]
 [1.008]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]]
Printing some Q and Qe and total Qs values:  [[ 1.275]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]] [[0.545]
 [1.897]
 [1.897]
 [1.897]
 [1.897]
 [1.897]
 [1.897]] [[2.458]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.237]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.22 ]] [[0.61 ]
 [0.361]
 [0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.631]] [[0.55 ]
 [0.237]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.22 ]]
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.47 ]
 [0.499]
 [0.499]
 [0.156]
 [0.499]
 [0.442]] [[0.998]
 [0.219]
 [0.472]
 [0.472]
 [1.218]
 [0.472]
 [0.889]] [[1.801]
 [1.553]
 [1.676]
 [1.676]
 [1.819]
 [1.676]
 [1.826]]
in main func line 156:  3078
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.3255457420205216, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.3255457420205216, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.3255457420205216, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
using explorer policy with actor:  1
from probs:  [0.29326230100962913, 0.3255457420205216, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.282]
 [0.33 ]
 [0.332]
 [0.335]
 [0.336]
 [0.338]] [[-1.67 ]
 [ 0.128]
 [-1.762]
 [-1.745]
 [-1.54 ]
 [-1.471]
 [-1.474]] [[0.332]
 [0.282]
 [0.33 ]
 [0.332]
 [0.335]
 [0.336]
 [0.338]]
first move QE:  0.5705565260780403
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.568]] [[-1.626]
 [-1.626]
 [-1.626]
 [-1.626]
 [-1.626]
 [-1.626]
 [-1.549]] [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.568]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.3255457420205216, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
from probs:  [0.29326230100962913, 0.3255457420205216, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.769]
 [0.58 ]] [[1.868]
 [1.868]
 [1.868]
 [1.868]
 [1.868]
 [7.25 ]
 [1.868]] [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [2.012]
 [0.634]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.3255457420205216, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.567]
 [0.391]] [[1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]
 [3.267]
 [1.074]] [[0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [1.457]
 [0.534]]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.698]
 [0.684]] [[1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]
 [6.174]
 [1.944]] [[0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [2.006]
 [0.817]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
in main func line 156:  3084
from probs:  [0.29326230100962913, 0.3255457420205216, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[1.374]
 [1.374]
 [1.374]
 [1.374]
 [1.374]
 [1.374]
 [1.374]] [[-0.265]
 [-0.265]
 [-0.265]
 [-0.265]
 [-0.265]
 [-0.265]
 [-0.265]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.625]] [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [1.724]] [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.625]]
from probs:  [0.29326230100962913, 0.3255457420205216, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
siam score:  -0.6443035
Printing some Q and Qe and total Qs values:  [[1.165]
 [0.941]
 [0.941]
 [0.941]
 [0.941]
 [0.941]
 [1.262]] [[0.68 ]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.878]] [[1.977]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [2.422]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.3255457420205216, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29326230100962913, 0.3255457420205216, 0.149539930856274, 0.14750840385229785, 0.04434044903598076, 0.03980317322529667]
first move QE:  0.5692992033837131
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5657698488917176
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.5349999999999997  reward:  1.0 rdn_beta:  0.167
siam score:  -0.63646567
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3478173001463586, 0.3004159835538836, 0.13799653815142018, 0.13612183022488464, 0.04091768955628151, 0.03673065836717143]
from probs:  [0.3478173001463586, 0.3004159835538836, 0.13799653815142018, 0.13612183022488464, 0.04091768955628151, 0.03673065836717143]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7722584476917012
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.484]
 [0.509]
 [0.537]
 [0.537]
 [0.501]
 [0.507]] [[0.661]
 [0.785]
 [0.546]
 [0.669]
 [0.669]
 [0.814]
 [0.74 ]] [[0.735]
 [0.735]
 [0.705]
 [0.803]
 [0.803]
 [0.779]
 [0.766]]
siam score:  -0.64075285
siam score:  -0.6469989
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3478173001463586, 0.3004159835538836, 0.13799653815142018, 0.13612183022488464, 0.04091768955628151, 0.03673065836717143]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  0
siam score:  -0.6458094
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3478173001463586, 0.3004159835538836, 0.13799653815142018, 0.13612183022488464, 0.04091768955628151, 0.03673065836717143]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.323]
 [0.341]
 [0.343]
 [0.342]
 [0.333]
 [0.336]] [[-0.751]
 [-0.883]
 [-0.711]
 [-0.599]
 [-0.585]
 [-0.524]
 [-0.613]] [[0.339]
 [0.323]
 [0.341]
 [0.343]
 [0.342]
 [0.333]
 [0.336]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
3097 4751
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3478173001463586, 0.3004159835538836, 0.13799653815142018, 0.13612183022488464, 0.04091768955628151, 0.03673065836717143]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -0.28553378497774523
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.432]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[-0.872]
 [-0.433]
 [-0.872]
 [-0.872]
 [-0.872]
 [-0.872]
 [-0.872]] [[0.408]
 [0.432]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]]
first move QE:  0.5700846585828212
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3478173001463586, 0.3004159835538836, 0.13799653815142018, 0.13612183022488464, 0.04091768955628151, 0.03673065836717143]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6371392
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.374]
 [0.374]
 [0.511]
 [0.374]
 [0.374]
 [0.411]] [[2.071]
 [2.071]
 [2.071]
 [2.019]
 [2.071]
 [2.071]
 [2.506]] [[1.469]
 [1.469]
 [1.469]
 [1.613]
 [1.469]
 [1.469]
 [1.688]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3478173001463586, 0.3004159835538836, 0.13799653815142018, 0.13612183022488464, 0.04091768955628151, 0.03673065836717143]
actor:  1 policy actor:  1  step number:  63 total reward:  0.47999999999999965  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32452917086190275, 0.34725667553062123, 0.1287569712295755, 0.12700778449058694, 0.038178043069442176, 0.034271354817871476]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32452917086190275, 0.34725667553062123, 0.1287569712295755, 0.12700778449058694, 0.038178043069442176, 0.034271354817871476]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32452917086190275, 0.34725667553062123, 0.1287569712295755, 0.12700778449058694, 0.038178043069442176, 0.034271354817871476]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.462]
 [0.514]
 [0.515]
 [0.512]
 [0.515]
 [0.514]] [[0.928]
 [1.152]
 [0.888]
 [0.91 ]
 [0.955]
 [0.96 ]
 [1.145]] [[0.512]
 [0.462]
 [0.514]
 [0.515]
 [0.512]
 [0.515]
 [0.514]]
start point for exploration sampling:  11106
line 256 mcts: sample exp_bonus 1.5346136984021703
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.318]
 [0.413]
 [0.414]
 [0.417]
 [0.421]
 [0.413]] [[1.64 ]
 [1.982]
 [1.64 ]
 [1.763]
 [1.738]
 [1.701]
 [1.69 ]] [[0.486]
 [0.637]
 [0.486]
 [0.609]
 [0.591]
 [0.561]
 [0.535]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32452917086190275, 0.34725667553062123, 0.1287569712295755, 0.12700778449058694, 0.038178043069442176, 0.034271354817871476]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
3106 4769
siam score:  -0.64303774
first move QE:  0.5696683357744915
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.32452917086190275, 0.34725667553062123, 0.1287569712295755, 0.12700778449058694, 0.038178043069442176, 0.034271354817871476]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.611]
 [0.451]
 [0.491]
 [0.46 ]
 [0.417]
 [0.424]] [[0.954]
 [2.359]
 [1.702]
 [0.782]
 [1.665]
 [1.294]
 [1.585]] [[0.651]
 [1.385]
 [0.845]
 [0.618]
 [0.85 ]
 [0.64 ]
 [0.752]]
siam score:  -0.64840865
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.773]] [[-3.502]
 [-3.502]
 [-3.502]
 [-3.502]
 [-3.502]
 [-3.502]
 [-3.098]] [[0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.773]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32452917086190275, 0.34725667553062123, 0.1287569712295755, 0.12700778449058694, 0.038178043069442176, 0.034271354817871476]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]] [[0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]] [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]]
in main func line 156:  3110
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.475]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.536]] [[1.387]
 [1.521]
 [1.387]
 [1.387]
 [1.387]
 [1.387]
 [1.949]] [[0.42 ]
 [0.475]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.536]]
using explorer policy with actor:  1
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.48 ]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.387]] [[-0.24 ]
 [ 1.33 ]
 [-0.24 ]
 [-0.24 ]
 [-0.24 ]
 [-0.24 ]
 [ 0.529]] [[0.333]
 [0.48 ]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.387]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.326]
 [0.282]
 [0.315]
 [0.315]
 [0.259]
 [0.331]] [[1.358]
 [1.74 ]
 [1.421]
 [3.093]
 [3.093]
 [1.563]
 [1.978]] [[0.317]
 [0.714]
 [0.356]
 [1.954]
 [1.954]
 [0.455]
 [0.94 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using another actor
in main func line 156:  3113
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32452917086190275, 0.34725667553062123, 0.1287569712295755, 0.12700778449058694, 0.038178043069442176, 0.034271354817871476]
line 256 mcts: sample exp_bonus -2.283690643203877
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.354]
 [0.324]
 [0.333]
 [0.323]
 [0.326]
 [0.322]] [[-4.406]
 [-3.076]
 [-3.985]
 [-4.014]
 [-3.909]
 [-4.038]
 [-4.105]] [[0.323]
 [0.354]
 [0.324]
 [0.333]
 [0.323]
 [0.326]
 [0.322]]
using explorer policy with actor:  1
siam score:  -0.63610405
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.319]
 [0.404]
 [0.356]
 [0.362]
 [0.382]
 [0.395]] [[1.689]
 [1.54 ]
 [1.689]
 [1.838]
 [1.838]
 [1.788]
 [1.88 ]] [[-0.347]
 [-0.569]
 [-0.347]
 [-0.395]
 [-0.383]
 [-0.359]
 [-0.302]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32452917086190275, 0.34725667553062123, 0.1287569712295755, 0.12700778449058694, 0.038178043069442176, 0.034271354817871476]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32452917086190275, 0.34725667553062123, 0.1287569712295755, 0.12700778449058694, 0.038178043069442176, 0.034271354817871476]
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.104]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]] [[-1.994]
 [-2.007]
 [-1.994]
 [-1.994]
 [-1.994]
 [-1.994]
 [-1.994]] [[0.064]
 [0.104]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]]
from probs:  [0.32452917086190275, 0.34725667553062123, 0.1287569712295755, 0.12700778449058694, 0.038178043069442176, 0.034271354817871476]
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.104]
 [0.075]
 [0.054]
 [0.048]
 [0.06 ]
 [0.061]] [[-3.713]
 [-2.995]
 [-4.285]
 [-4.236]
 [-3.945]
 [-3.793]
 [-3.835]] [[0.06 ]
 [0.104]
 [0.075]
 [0.054]
 [0.048]
 [0.06 ]
 [0.061]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  56 total reward:  0.5949999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.455]
 [0.371]
 [0.492]
 [0.371]
 [0.371]
 [0.486]] [[2.15 ]
 [1.774]
 [2.15 ]
 [2.727]
 [2.15 ]
 [2.15 ]
 [2.266]] [[1.638]
 [1.594]
 [1.638]
 [1.973]
 [1.638]
 [1.638]
 [1.802]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.342]
 [0.344]
 [0.347]
 [0.336]
 [0.337]
 [0.4  ]] [[1.044]
 [2.994]
 [1.53 ]
 [1.133]
 [0.935]
 [1.148]
 [0.607]] [[0.334]
 [0.342]
 [0.344]
 [0.347]
 [0.336]
 [0.337]
 [0.4  ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.6227167038363595
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.30111303867548783, 0.394354826766089, 0.119466619147429, 0.11784364352157425, 0.035423338151057145, 0.031798533738362736]
line 256 mcts: sample exp_bonus 0.5167594982291634
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.145]
 [0.194]
 [0.194]
 [0.208]
 [0.229]
 [0.235]] [[0.239]
 [0.6  ]
 [0.887]
 [0.887]
 [0.369]
 [0.56 ]
 [0.503]] [[-0.074]
 [ 0.171]
 [ 0.557]
 [ 0.557]
 [ 0.067]
 [ 0.3  ]
 [ 0.254]]
start point for exploration sampling:  11106
actor:  1 policy actor:  1  step number:  69 total reward:  0.38999999999999957  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.5341776130763878
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]] [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65053296
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[-2.875]
 [-2.875]
 [-2.875]
 [-2.875]
 [-2.875]
 [-2.875]
 [-2.875]] [[0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]]
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.171]
 [0.011]
 [0.171]
 [0.01 ]
 [0.016]
 [0.015]] [[1.379]
 [1.379]
 [1.325]
 [1.379]
 [1.773]
 [1.976]
 [1.976]] [[1.818]
 [1.818]
 [1.712]
 [1.818]
 [1.905]
 [1.996]
 [1.996]]
Printing some Q and Qe and total Qs values:  [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[-1.065]
 [-1.065]
 [-1.065]
 [-1.065]
 [-1.065]
 [-1.065]
 [-1.065]] [[1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.245]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[0.889]
 [0.963]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]] [[0.203]
 [0.245]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
3132 4836
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.307]
 [0.335]
 [0.333]
 [0.299]
 [0.34 ]
 [0.208]] [[ 1.102]
 [ 0.281]
 [-1.58 ]
 [ 0.99 ]
 [ 1.424]
 [-0.882]
 [ 0.471]] [[0.26 ]
 [0.307]
 [0.335]
 [0.333]
 [0.299]
 [0.34 ]
 [0.208]]
siam score:  -0.65248865
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.167]
 [0.197]
 [0.167]
 [0.167]
 [0.256]
 [0.167]] [[2.64 ]
 [2.64 ]
 [2.896]
 [2.64 ]
 [2.64 ]
 [4.45 ]
 [2.64 ]] [[-0.319]
 [-0.319]
 [-0.173]
 [-0.319]
 [-0.319]
 [ 0.465]
 [-0.319]]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[-1.636]
 [-1.636]
 [-1.636]
 [-1.636]
 [-1.636]
 [-1.636]
 [-1.636]] [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.042]
 [0.039]] [[-1.375]
 [-1.375]
 [-1.375]
 [-1.375]
 [-1.375]
 [-1.122]
 [-1.162]] [[0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.042]
 [0.039]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.102 0.367 0.041 0.061 0.082 0.286 0.061]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.879]
 [0.753]] [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [4.086]
 [1.485]] [[1.299]
 [1.299]
 [1.299]
 [1.299]
 [1.299]
 [2.42 ]
 [1.299]]
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.825]
 [0.485]] [[1.33 ]
 [1.33 ]
 [1.33 ]
 [1.33 ]
 [1.33 ]
 [2.559]
 [1.33 ]] [[0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [1.831]
 [0.742]]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.827]
 [0.427]] [[2.002]
 [2.002]
 [2.002]
 [2.002]
 [2.002]
 [3.976]
 [2.002]] [[0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [2.082]
 [0.767]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6536412
3139 4844
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
3140 4847
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.686]
 [0.466]] [[1.686]
 [1.686]
 [1.686]
 [1.686]
 [1.686]
 [1.827]
 [1.686]] [[0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [1.447]
 [0.914]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]] [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
UNIT TEST: sample policy line 217 mcts : [0.061 0.367 0.061 0.082 0.143 0.082 0.204]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5293579080874729
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.372]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[-0.695]
 [ 0.582]
 [-0.695]
 [-0.695]
 [-0.695]
 [-0.695]
 [-0.695]] [[0.359]
 [0.372]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.489]
 [0.511]
 [0.515]
 [0.526]
 [0.508]
 [0.506]] [[-0.784]
 [ 0.355]
 [-0.673]
 [-0.787]
 [-0.425]
 [-0.492]
 [-0.091]] [[2.055]
 [2.383]
 [2.084]
 [2.055]
 [2.197]
 [2.139]
 [2.268]]
siam score:  -0.6503769
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.927]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[0.345]
 [1.283]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[0.809]
 [1.775]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
rdn beta is 0 so we're just using the maxi policy
UNIT TEST: sample policy line 217 mcts : [0.102 0.245 0.102 0.082 0.082 0.286 0.102]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.518]
 [0.515]
 [0.517]
 [0.516]
 [0.533]
 [0.515]] [[0.519]
 [0.172]
 [0.437]
 [0.324]
 [0.487]
 [0.645]
 [0.483]] [[0.63 ]
 [0.517]
 [0.598]
 [0.564]
 [0.618]
 [0.705]
 [0.614]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.392]
 [0.522]
 [0.522]
 [0.539]
 [0.522]] [[0.961]
 [0.961]
 [0.75 ]
 [0.961]
 [0.961]
 [0.657]
 [0.961]] [[1.249]
 [1.249]
 [0.919]
 [1.249]
 [1.249]
 [1.182]
 [1.249]]
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.818]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[-1.661]
 [-0.915]
 [-1.661]
 [-1.661]
 [-1.661]
 [-1.661]
 [-1.661]] [[0.401]
 [1.36 ]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.646]] [[0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.269]] [[0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.646]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64818794
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1635042212454294
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.469]
 [0.368]
 [0.424]
 [0.368]
 [0.368]
 [0.421]] [[1.927]
 [1.958]
 [1.927]
 [1.781]
 [1.927]
 [1.927]
 [1.708]] [[0.708]
 [0.921]
 [0.708]
 [0.773]
 [0.708]
 [0.708]
 [0.741]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.66 ]
 [0.235]] [[2.403]
 [2.403]
 [2.403]
 [2.403]
 [2.403]
 [2.736]
 [2.403]] [[0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [1.219]
 [0.148]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.02  0.02  0.02  0.776 0.122]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
in main func line 156:  3160
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.328]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]] [[-3.017]
 [-2.979]
 [-3.017]
 [-3.017]
 [-3.017]
 [-3.017]
 [-3.017]] [[0.32 ]
 [0.328]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.341]
 [0.348]
 [0.348]
 [0.323]
 [0.334]
 [0.348]] [[-2.75 ]
 [-3.879]
 [ 0.   ]
 [ 0.   ]
 [-2.953]
 [-2.963]
 [ 0.   ]] [[0.334]
 [0.341]
 [0.348]
 [0.348]
 [0.323]
 [0.334]
 [0.348]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.496]
 [0.377]
 [0.377]
 [0.377]
 [0.418]
 [0.397]] [[-2.569]
 [-1.851]
 [-2.577]
 [-2.577]
 [-2.577]
 [-2.624]
 [-2.535]] [[0.408]
 [0.496]
 [0.377]
 [0.377]
 [0.377]
 [0.418]
 [0.397]]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.818]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.681]] [[0.912]
 [0.727]
 [0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.982]] [[0.43 ]
 [0.818]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.681]]
Printing some Q and Qe and total Qs values:  [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]] [[-0.373]
 [-0.373]
 [-0.373]
 [-0.373]
 [-0.373]
 [-0.373]
 [-0.373]] [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
line 256 mcts: sample exp_bonus 0.5628282439566819
siam score:  -0.63725656
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.729]
 [0.596]
 [0.596]
 [0.518]
 [0.535]
 [0.573]] [[1.744]
 [2.988]
 [1.88 ]
 [1.232]
 [1.01 ]
 [1.236]
 [2.005]] [[ 0.373]
 [ 1.558]
 [ 0.597]
 [ 0.183]
 [-0.107]
 [ 0.069]
 [ 0.633]]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.52 ]
 [0.634]
 [0.646]
 [0.645]
 [0.64 ]
 [0.653]] [[1.688]
 [1.735]
 [1.738]
 [1.856]
 [1.766]
 [1.737]
 [1.718]] [[ 0.226]
 [-0.011]
 [ 0.219]
 [ 0.323]
 [ 0.26 ]
 [ 0.232]
 [ 0.244]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2854699668550606, 0.4258185952706591, 0.11326022930895736, 0.11172156861144224, 0.03358306638715834, 0.03014657356672245]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.425]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.668]
 [0.637]] [[-5.933]
 [ 0.862]
 [-5.933]
 [-5.933]
 [-5.933]
 [-6.486]
 [-5.861]] [[0.64 ]
 [0.425]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.668]
 [0.637]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  88 total reward:  0.5049999999999997  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[2.177]
 [2.177]
 [2.177]
 [2.177]
 [2.177]
 [2.177]
 [2.177]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.506]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[-2.066]
 [-2.153]
 [-2.066]
 [-2.066]
 [-2.066]
 [-2.066]
 [-2.066]] [[0.493]
 [0.506]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.5643635669532189
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.449]
 [0.656]
 [0.656]
 [0.656]
 [0.638]
 [0.656]] [[-5.848]
 [ 0.215]
 [-5.848]
 [-5.848]
 [-5.848]
 [-4.191]
 [-5.848]] [[0.656]
 [0.449]
 [0.656]
 [0.656]
 [0.656]
 [0.638]
 [0.656]]
using another actor
from probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
using another actor
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.048]
 [0.004]
 [0.004]
 [0.004]
 [0.049]
 [0.01 ]] [[ 0.558]
 [ 0.269]
 [ 0.039]
 [ 0.039]
 [ 0.039]
 [-1.436]
 [ 0.662]] [[0.011]
 [0.048]
 [0.004]
 [0.004]
 [0.004]
 [0.049]
 [0.01 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]] [[-1.498]
 [-1.498]
 [-1.498]
 [-1.498]
 [-1.498]
 [-1.498]
 [-1.498]] [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]]
using explorer policy with actor:  0
rdn probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
siam score:  -0.64846146
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.56 ]
 [0.528]
 [0.497]
 [0.314]
 [0.525]
 [0.525]] [[ 1.291]
 [ 0.688]
 [-0.95 ]
 [ 1.071]
 [ 0.603]
 [-1.246]
 [ 0.61 ]] [[0.751]
 [1.372]
 [0.761]
 [1.373]
 [0.851]
 [0.656]
 [1.275]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.802]] [[0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [1.564]] [[2.546]
 [2.546]
 [2.546]
 [2.546]
 [2.546]
 [2.546]
 [2.705]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.281]
 [0.281]
 [0.281]
 [1.424]
 [0.281]
 [1.426]] [[1.675]
 [1.675]
 [1.675]
 [1.675]
 [0.719]
 [1.675]
 [1.969]] [[1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.833]
 [1.576]
 [2.26 ]]
siam score:  -0.64220923
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.461]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.35 ]] [[-0.329]
 [-0.678]
 [-0.494]
 [-0.494]
 [-0.494]
 [-0.494]
 [ 0.752]] [[1.172]
 [1.13 ]
 [1.161]
 [1.161]
 [1.161]
 [1.161]
 [1.682]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.39 ]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.417]] [[0.905]
 [1.187]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [1.614]] [[0.399]
 [0.39 ]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.417]]
from probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6472072
siam score:  -0.6453598
siam score:  -0.6474395
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.27 ]] [[-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.073]] [[0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.27 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.337]
 [0.281]
 [0.311]
 [0.31 ]
 [0.326]
 [0.307]] [[-0.381]
 [ 0.012]
 [-1.363]
 [-0.309]
 [-0.188]
 [-0.047]
 [-0.791]] [[0.291]
 [0.337]
 [0.281]
 [0.311]
 [0.31 ]
 [0.326]
 [0.307]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64600813
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
siam score:  -0.6429297
from probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[1.874]
 [1.874]
 [1.874]
 [1.874]
 [1.874]
 [1.874]
 [1.874]] [[1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]]
3180 4943
siam score:  -0.63948095
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.523]
 [0.438]
 [0.438]
 [0.438]
 [0.454]
 [0.438]] [[0.715]
 [1.208]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.603]
 [0.67 ]] [[0.146]
 [0.485]
 [0.136]
 [0.136]
 [0.136]
 [0.146]
 [0.136]]
3182 4950
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.893]
 [0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.742]] [[1.491]
 [2.136]
 [1.491]
 [1.491]
 [1.491]
 [1.491]
 [1.979]] [[1.185]
 [1.636]
 [1.185]
 [1.185]
 [1.185]
 [1.185]
 [1.282]]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
from probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.242]
 [0.209]
 [0.208]
 [0.208]
 [0.211]
 [0.21 ]] [[-2.269]
 [-1.616]
 [-2.137]
 [-2.229]
 [-2.174]
 [-2.055]
 [-2.112]] [[0.206]
 [0.242]
 [0.209]
 [0.208]
 [0.208]
 [0.211]
 [0.21 ]]
siam score:  -0.6471641
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.469]
 [0.672]
 [0.672]
 [0.172]
 [0.516]
 [0.488]] [[ 2.019]
 [ 1.015]
 [ 1.086]
 [ 1.086]
 [ 0.678]
 [-2.791]
 [-1.697]] [[1.895]
 [1.483]
 [1.661]
 [1.661]
 [1.135]
 [0.096]
 [0.484]]
line 256 mcts: sample exp_bonus 1.0362668793265213
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
3186 4979
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
using another actor
siam score:  -0.65238816
siam score:  -0.65370864
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
3192 4993
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.488]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.364]] [[3.843]
 [3.349]
 [3.843]
 [3.843]
 [3.843]
 [3.843]
 [3.863]] [[1.345]
 [0.97 ]
 [1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.064]]
using another actor
from probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.807572843244219
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.614]
 [0.614]
 [0.628]] [[-0.263]
 [-0.263]
 [-0.263]
 [-0.263]
 [-0.507]
 [-0.752]
 [-0.507]] [[0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.257]
 [0.094]
 [0.283]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26908199209316175, 0.45878062792551116, 0.10675829917637548, 0.10530796837554196, 0.031655163250996377, 0.028415949178413294]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.385]
 [0.504]
 [0.507]
 [0.505]
 [0.496]
 [0.509]] [[-1.414]
 [ 0.289]
 [-1.461]
 [-1.3  ]
 [-1.021]
 [-0.972]
 [-1.141]] [[0.429]
 [1.487]
 [0.394]
 [0.506]
 [0.695]
 [0.72 ]
 [0.616]]
actor:  1 policy actor:  1  step number:  99 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.8206291956389532
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.579]] [[-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-1.912]] [[0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.579]]
siam score:  -0.6432039
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.475]] [[0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.222]] [[0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.475]]
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.383]] [[1.162]
 [1.162]
 [1.162]
 [1.162]
 [1.162]
 [1.162]
 [1.019]] [[0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.383]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2580300219186266, 0.48101006182972045, 0.10237342923690775, 0.1009826676871762, 0.03035499478785029, 0.027248824539718785]
UNIT TEST: sample policy line 217 mcts : [0.041 0.082 0.102 0.163 0.204 0.082 0.327]
siam score:  -0.63928163
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.143]
 [0.279]
 [0.357]
 [0.351]
 [0.365]
 [0.198]] [[ 2.056]
 [ 1.393]
 [ 0.911]
 [ 0.223]
 [ 1.082]
 [-0.096]
 [ 1.602]] [[ 1.628]
 [ 1.086]
 [ 0.786]
 [ 0.17 ]
 [ 1.086]
 [-0.161]
 [ 1.4  ]]
first move QE:  0.5579473528241983
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.213]
 [0.399]
 [0.4  ]
 [0.396]
 [0.458]
 [0.398]] [[0.127]
 [0.758]
 [0.194]
 [0.158]
 [0.295]
 [0.248]
 [0.188]] [[0.238]
 [0.287]
 [0.284]
 [0.262]
 [0.344]
 [0.438]
 [0.277]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
first move QE:  0.5578567986557879
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.395]] [[0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.888]] [[0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.395]]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.501]] [[1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]
 [0.729]] [[0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.501]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.07413733063956
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2580300219186266, 0.4810100618297205, 0.10237342923690775, 0.1009826676871762, 0.03035499478785029, 0.027248824539718785]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2580300219186266, 0.4810100618297205, 0.10237342923690775, 0.1009826676871762, 0.03035499478785029, 0.027248824539718785]
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.819]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]] [[0.407]
 [0.694]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[0.32 ]
 [0.819]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2580300219186266, 0.4810100618297205, 0.10237342923690775, 0.1009826676871762, 0.03035499478785029, 0.027248824539718785]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[1.028]
 [1.028]
 [1.028]
 [1.028]
 [1.028]
 [1.028]
 [1.028]] [[0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.374]
 [0.285]
 [0.354]
 [0.374]
 [0.798]
 [0.326]] [[1.932]
 [2.669]
 [1.237]
 [1.481]
 [2.669]
 [2.962]
 [1.862]] [[0.502]
 [0.718]
 [0.062]
 [0.282]
 [0.718]
 [1.665]
 [0.353]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2580300219186266, 0.4810100618297205, 0.10237342923690775, 0.1009826676871762, 0.03035499478785029, 0.027248824539718785]
Printing some Q and Qe and total Qs values:  [[0.974]
 [0.974]
 [0.974]
 [0.974]
 [1.027]
 [1.008]
 [1.022]] [[2.14 ]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [1.747]
 [1.67 ]
 [1.75 ]] [[2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.059]
 [1.994]
 [2.049]]
siam score:  -0.6302556
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.512]
 [0.512]
 [0.512]
 [0.502]
 [0.512]
 [0.512]] [[1.752]
 [1.008]
 [1.008]
 [1.008]
 [1.154]
 [1.008]
 [1.008]] [[1.209]
 [1.008]
 [1.008]
 [1.008]
 [1.087]
 [1.008]
 [1.008]]
first move QE:  0.5562053607823944
from probs:  [0.2580300219186266, 0.4810100618297205, 0.10237342923690775, 0.1009826676871762, 0.03035499478785029, 0.027248824539718785]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2580300219186266, 0.4810100618297205, 0.10237342923690775, 0.1009826676871762, 0.03035499478785029, 0.027248824539718785]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.453]
 [0.258]
 [0.493]
 [0.332]
 [0.3  ]
 [0.425]] [[2.155]
 [1.66 ]
 [1.955]
 [1.75 ]
 [2.128]
 [2.144]
 [1.7  ]] [[1.795]
 [1.715]
 [1.624]
 [1.88 ]
 [1.934]
 [1.887]
 [1.701]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2580300219186266, 0.4810100618297205, 0.10237342923690775, 0.1009826676871762, 0.03035499478785029, 0.027248824539718785]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2580300219186266, 0.4810100618297205, 0.10237342923690775, 0.1009826676871762, 0.03035499478785029, 0.027248824539718785]
actor:  1 policy actor:  1  step number:  49 total reward:  0.6399999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.267]
 [0.498]
 [0.499]
 [0.485]
 [0.486]
 [0.5  ]] [[-2.933]
 [ 0.067]
 [-2.965]
 [-3.467]
 [-3.192]
 [-3.003]
 [-3.141]] [[0.254]
 [1.257]
 [0.245]
 [0.056]
 [0.151]
 [0.223]
 [0.179]]
siam score:  -0.6371595
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.268]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.223]] [[1.263]
 [1.838]
 [1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.732]] [[0.258]
 [0.268]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.223]]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[1.147]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]] [[2.186]
 [1.989]
 [1.989]
 [1.989]
 [1.989]
 [1.989]
 [1.989]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
3211 5052
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.464]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.473]] [[0.45 ]
 [0.649]
 [0.393]
 [0.393]
 [1.391]
 [0.393]
 [0.307]] [[0.87 ]
 [0.945]
 [0.915]
 [0.915]
 [1.247]
 [0.915]
 [0.85 ]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5566521676466161
Printing some Q and Qe and total Qs values:  [[1.42]
 [1.42]
 [1.42]
 [1.42]
 [1.42]
 [1.42]
 [1.42]] [[0.557]
 [0.536]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[1.549]
 [1.526]
 [1.549]
 [1.549]
 [1.549]
 [1.549]
 [1.549]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.12031874295278
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
3220 5073
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.07708491449889499
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.48366640283251466
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
siam score:  -0.64170563
line 256 mcts: sample exp_bonus -2.10315235890035
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.421]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[-1.497]
 [-1.654]
 [-1.497]
 [-1.497]
 [-1.497]
 [-1.497]
 [-1.497]] [[0.404]
 [0.421]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.05 ]
 [0.04 ]
 [0.046]
 [0.029]
 [0.031]
 [0.046]] [[0.637]
 [0.897]
 [1.079]
 [0.592]
 [1.006]
 [1.056]
 [0.653]] [[0.178]
 [0.372]
 [0.534]
 [0.059]
 [0.439]
 [0.494]
 [0.119]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.337]] [[1.52]
 [1.52]
 [1.52]
 [1.52]
 [1.52]
 [1.52]
 [1.53]] [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.624]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
siam score:  -0.64892256
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
siam score:  -0.6476054
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.268]
 [0.268]
 [0.268]
 [0.697]
 [0.268]
 [0.268]] [[0.889]
 [0.639]
 [0.639]
 [0.639]
 [0.56 ]
 [0.639]
 [0.639]] [[0.24 ]
 [0.268]
 [0.268]
 [0.268]
 [0.697]
 [0.268]
 [0.268]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]] [[0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]]
Printing some Q and Qe and total Qs values:  [[0.525]
 [1.029]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.516]] [[1.596]
 [1.363]
 [1.596]
 [1.596]
 [1.596]
 [1.596]
 [1.614]] [[1.568]
 [1.954]
 [1.568]
 [1.568]
 [1.568]
 [1.568]
 [1.565]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.518]
 [0.488]
 [0.495]
 [0.488]
 [0.488]
 [0.474]] [[4.461]
 [4.408]
 [4.461]
 [4.928]
 [4.461]
 [4.461]
 [4.857]] [[1.203]
 [1.226]
 [1.203]
 [1.526]
 [1.203]
 [1.203]
 [1.437]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.436]
 [0.436]
 [0.436]
 [0.363]
 [0.436]
 [0.436]] [[ 0.734]
 [ 0.104]
 [ 0.104]
 [ 0.104]
 [-0.449]
 [ 0.104]
 [ 0.104]] [[1.755]
 [1.527]
 [1.527]
 [1.527]
 [1.174]
 [1.527]
 [1.527]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.588]] [[-1.767]
 [-1.767]
 [-1.767]
 [-1.767]
 [-1.767]
 [-1.767]
 [-2.325]] [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.588]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
3236 5140
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]] [[1.302]
 [1.302]
 [1.302]
 [1.302]
 [1.302]
 [1.302]
 [1.302]] [[2.093]
 [2.093]
 [2.093]
 [2.093]
 [2.093]
 [2.093]
 [2.093]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.30355568813458245, 0.4514963292417767, 0.09609201798510222, 0.09478659054317756, 0.028492478242002676, 0.02557689585335847]
siam score:  -0.63624376
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.413]
 [0.453]
 [0.443]
 [0.436]
 [0.437]
 [0.46 ]] [[-3.421]
 [-0.707]
 [-3.375]
 [-3.609]
 [-3.38 ]
 [-3.53 ]
 [-3.636]] [[0.086]
 [1.006]
 [0.111]
 [0.025]
 [0.1  ]
 [0.049]
 [0.024]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.30355568813458245, 0.4514963292417767, 0.09609201798510222, 0.09478659054317756, 0.028492478242002676, 0.02557689585335847]
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.43]
 [0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]] [[0.781]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[ 0.352]
 [-0.77 ]
 [-0.77 ]
 [-0.77 ]
 [-0.77 ]
 [-0.77 ]
 [-0.77 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.317]
 [0.334]
 [0.338]
 [0.335]
 [0.336]
 [0.337]] [[-0.361]
 [ 0.196]
 [-0.502]
 [-0.623]
 [-0.461]
 [-0.316]
 [-0.296]] [[0.331]
 [0.317]
 [0.334]
 [0.338]
 [0.335]
 [0.336]
 [0.337]]
Printing some Q and Qe and total Qs values:  [[0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]] [[-2.998]
 [-2.998]
 [-2.998]
 [-2.998]
 [-2.998]
 [-2.998]
 [-2.998]] [[0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.342]
 [0.345]
 [0.335]
 [0.335]
 [0.338]
 [0.343]] [[-2.675]
 [-2.919]
 [-2.824]
 [-3.114]
 [-2.563]
 [-2.623]
 [-2.713]] [[0.343]
 [0.342]
 [0.345]
 [0.335]
 [0.335]
 [0.338]
 [0.343]]
first move QE:  0.5502667318997532
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.374]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[-1.512]
 [-1.465]
 [-1.512]
 [-1.512]
 [-1.512]
 [-1.512]
 [-1.512]] [[0.386]
 [0.374]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
line 256 mcts: sample exp_bonus -0.5634676015550791
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.259]
 [0.242]
 [0.241]
 [0.238]
 [0.239]
 [0.248]] [[-2.456]
 [-1.657]
 [-3.031]
 [-2.674]
 [-2.265]
 [-2.608]
 [-2.161]] [[0.243]
 [0.259]
 [0.242]
 [0.241]
 [0.238]
 [0.239]
 [0.248]]
3246 5163
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.516]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[-3.642]
 [-3.079]
 [-3.642]
 [-3.642]
 [-3.642]
 [-3.642]
 [-3.642]] [[0.499]
 [0.516]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 0.5221599149592975
3247 5172
start point for exploration sampling:  11106
first move QE:  0.5483776667083259
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.911]
 [0.875]
 [0.875]
 [0.875]
 [1.047]
 [0.799]] [[1.862]
 [1.799]
 [1.983]
 [1.983]
 [1.983]
 [1.923]
 [2.156]] [[1.674]
 [1.883]
 [1.873]
 [1.873]
 [1.873]
 [2.197]
 [1.779]]
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.252]
 [0.266]
 [0.245]
 [0.266]
 [0.252]
 [0.266]] [[ 0.   ]
 [-3.826]
 [ 0.   ]
 [-3.917]
 [ 0.   ]
 [-3.926]
 [ 0.   ]] [[0.266]
 [0.252]
 [0.266]
 [0.245]
 [0.266]
 [0.252]
 [0.266]]
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.31 ]
 [0.288]
 [0.277]
 [0.294]
 [0.297]
 [0.295]] [[-3.914]
 [-1.696]
 [-3.489]
 [-3.721]
 [-2.903]
 [-3.198]
 [-3.948]] [[0.276]
 [0.31 ]
 [0.288]
 [0.277]
 [0.294]
 [0.297]
 [0.295]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.054]
 [0.1  ]
 [0.244]
 [0.236]
 [0.254]
 [0.243]] [[-3.284]
 [ 1.291]
 [ 0.825]
 [-3.31 ]
 [-3.208]
 [-3.353]
 [-3.355]] [[ 0.017]
 [ 1.778]
 [ 1.609]
 [-0.001]
 [ 0.036]
 [-0.015]
 [-0.02 ]]
3252 5196
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
line 256 mcts: sample exp_bonus -4.062842393450272
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.856]] [[1.671]
 [1.671]
 [1.671]
 [1.671]
 [1.671]
 [1.671]
 [1.417]] [[0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.856]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.63 ]
 [0.316]] [[0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.05 ]
 [0.326]] [[0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.63 ]
 [0.316]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
3261 5211
using explorer policy with actor:  1
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.288]
 [0.448]
 [0.288]
 [1.155]
 [0.556]
 [0.441]] [[1.796]
 [1.796]
 [1.312]
 [1.796]
 [1.839]
 [2.353]
 [1.491]] [[1.674]
 [1.674]
 [1.594]
 [1.674]
 [2.112]
 [1.987]
 [1.649]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]] [[0.547]
 [0.529]
 [0.547]
 [0.547]
 [0.533]
 [0.54 ]
 [0.54 ]] [[2.444]
 [2.421]
 [2.444]
 [2.444]
 [2.426]
 [2.435]
 [2.435]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
siam score:  -0.64715284
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
3263 5231
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.562]
 [0.494]
 [0.494]
 [0.494]
 [1.38 ]
 [0.494]] [[-0.036]
 [-0.521]
 [-0.036]
 [-0.036]
 [-0.036]
 [ 0.491]
 [-0.036]] [[0.698]
 [0.673]
 [0.698]
 [0.698]
 [0.698]
 [2.646]
 [0.698]]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.355]
 [0.533]
 [0.431]
 [0.162]
 [0.529]
 [0.936]] [[ 0.859]
 [ 0.52 ]
 [-1.393]
 [-0.128]
 [ 0.086]
 [-1.091]
 [ 0.518]] [[0.491]
 [0.66 ]
 [0.376]
 [0.595]
 [0.129]
 [0.469]
 [1.821]]
using explorer policy with actor:  0
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.337]] [[-1.16 ]
 [-1.16 ]
 [-1.16 ]
 [-1.16 ]
 [-1.16 ]
 [-1.16 ]
 [-3.014]] [[0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.337]]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.411]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]] [[1.925]
 [1.998]
 [1.925]
 [1.925]
 [1.925]
 [1.925]
 [1.925]] [[0.431]
 [0.779]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.357]
 [0.281]
 [0.28 ]
 [0.279]
 [0.281]
 [0.288]] [[-3.784]
 [-3.436]
 [-3.951]
 [-4.037]
 [-3.834]
 [-3.869]
 [-4.123]] [[0.288]
 [0.357]
 [0.281]
 [0.28 ]
 [0.279]
 [0.281]
 [0.288]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus -0.8369490687786065
using another actor
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.81 ]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[-2.994]
 [-0.179]
 [-2.994]
 [-2.994]
 [-2.994]
 [-2.994]
 [-2.994]] [[0.47 ]
 [1.498]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.451]
 [0.365]
 [0.415]
 [0.365]
 [0.365]
 [0.394]] [[1.621]
 [1.813]
 [1.621]
 [0.433]
 [1.621]
 [1.621]
 [1.224]] [[0.365]
 [0.451]
 [0.365]
 [0.415]
 [0.365]
 [0.365]
 [0.394]]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.403]
 [0.428]
 [0.403]
 [0.403]
 [0.396]
 [0.29 ]] [[0.353]
 [0.353]
 [0.683]
 [0.353]
 [0.353]
 [0.828]
 [1.637]] [[1.457]
 [1.457]
 [1.618]
 [1.457]
 [1.457]
 [1.638]
 [1.833]]
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]] [[-3.177]
 [-3.177]
 [-3.177]
 [-3.177]
 [-3.177]
 [-3.177]
 [-3.177]] [[0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]]
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.294]
 [0.696]
 [0.37 ]
 [0.252]
 [0.366]
 [0.324]] [[1.422]
 [1.385]
 [0.484]
 [1.266]
 [2.074]
 [0.962]
 [1.221]] [[0.26 ]
 [0.294]
 [0.696]
 [0.37 ]
 [0.252]
 [0.366]
 [0.324]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[-1.223]
 [-1.223]
 [-1.223]
 [-1.223]
 [-1.223]
 [-1.223]
 [-1.223]] [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.357]
 [0.411]
 [0.603]
 [0.287]
 [0.449]
 [0.412]] [[ 1.422]
 [ 0.724]
 [-1.62 ]
 [ 1.549]
 [ 0.322]
 [-1.468]
 [ 0.612]] [[0.708]
 [0.936]
 [0.301]
 [1.658]
 [0.679]
 [0.42 ]
 [1.003]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]]
siam score:  -0.6448019
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.277]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]] [[-1.569]
 [-0.814]
 [-1.569]
 [-1.569]
 [-1.569]
 [-1.569]
 [-1.569]] [[0.256]
 [0.277]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.676]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.39 ]] [[ 0.194]
 [ 1.978]
 [-1.076]
 [-1.076]
 [-1.076]
 [-1.076]
 [ 0.161]] [[0.395]
 [0.676]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.39 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
siam score:  -0.647234
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.312]
 [0.312]
 [0.312]
 [0.329]
 [0.312]
 [0.312]] [[1.545]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [1.62 ]
 [0.88 ]
 [0.88 ]] [[0.441]
 [0.016]
 [0.016]
 [0.016]
 [0.542]
 [0.016]
 [0.016]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.046]
 [1.297]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[1.468]
 [0.543]
 [1.468]
 [1.468]
 [1.468]
 [1.468]
 [1.468]] [[0.632]
 [1.592]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1005326132469886
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.452]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.547]
 [0.693]] [[0.654]
 [0.766]
 [0.681]
 [0.681]
 [0.681]
 [0.538]
 [0.736]] [[0.452]
 [0.452]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.547]
 [0.693]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.37 ]
 [0.351]
 [0.346]
 [0.332]
 [0.344]
 [0.346]] [[-1.546]
 [-1.268]
 [-1.391]
 [-1.725]
 [-1.704]
 [-1.755]
 [-1.657]] [[0.333]
 [0.37 ]
 [0.351]
 [0.346]
 [0.332]
 [0.344]
 [0.346]]
siam score:  -0.6403079
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.53 ]
 [0.421]
 [0.554]
 [0.557]
 [0.428]
 [0.412]] [[2.847]
 [3.507]
 [1.893]
 [3.672]
 [2.448]
 [2.965]
 [3.794]] [[0.625]
 [1.311]
 [0.062]
 [1.451]
 [0.596]
 [0.823]
 [1.388]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
3282 5303
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
using explorer policy with actor:  1
3284 5307
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.133]
 [0.153]
 [0.153]] [[0.828]
 [0.828]
 [0.828]
 [0.828]
 [1.388]
 [0.828]
 [0.828]] [[1.534]
 [1.534]
 [1.534]
 [1.534]
 [1.996]
 [1.534]
 [1.534]]
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.264]
 [0.323]
 [0.308]
 [0.237]
 [0.328]
 [0.319]] [[1.119]
 [1.313]
 [1.14 ]
 [1.171]
 [1.119]
 [1.336]
 [1.178]] [[-0.757]
 [-0.573]
 [-0.572]
 [-0.579]
 [-0.757]
 [-0.431]
 [-0.554]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7896022395446451
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
siam score:  -0.63151217
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.002]
 [1.129]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.893]
 [0.514]
 [0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.893]] [[-0.04]
 [ 1.71]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.508]
 [0.407]
 [0.566]
 [0.403]
 [0.396]
 [0.501]] [[1.953]
 [3.053]
 [1.86 ]
 [0.82 ]
 [1.804]
 [1.66 ]
 [2.427]] [[0.481]
 [0.508]
 [0.407]
 [0.566]
 [0.403]
 [0.396]
 [0.501]]
using another actor
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[1.118]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]] [[ 1.274]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]]
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.187]
 [0.188]
 [0.202]
 [0.201]
 [0.191]
 [0.192]] [[2.202]
 [2.063]
 [2.491]
 [2.335]
 [1.098]
 [2.405]
 [2.329]] [[0.715]
 [0.653]
 [0.925]
 [0.843]
 [0.061]
 [0.874]
 [0.827]]
line 256 mcts: sample exp_bonus 2.4029769366381606
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.396]
 [0.344]
 [0.386]
 [0.362]
 [0.359]
 [0.38 ]] [[1.019]
 [0.969]
 [0.702]
 [0.256]
 [0.339]
 [0.649]
 [0.447]] [[0.295]
 [0.396]
 [0.344]
 [0.386]
 [0.362]
 [0.359]
 [0.38 ]]
using another actor
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.568]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[1.837]
 [2.928]
 [1.837]
 [1.837]
 [1.837]
 [1.837]
 [1.837]] [[0.061]
 [0.515]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.761]
 [0.835]
 [0.752]
 [0.752]
 [0.83 ]
 [0.881]] [[1.127]
 [1.254]
 [1.176]
 [2.338]
 [2.338]
 [1.022]
 [0.967]] [[1.427]
 [1.36 ]
 [1.481]
 [1.703]
 [1.703]
 [1.42 ]
 [1.504]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]] [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
siam score:  -0.61449254
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
line 256 mcts: sample exp_bonus -0.9851183922701258
Starting evaluation
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.6090234
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.454]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.441]] [[-2.261]
 [-1.893]
 [-2.261]
 [-2.261]
 [-2.261]
 [-2.261]
 [-2.318]] [[0.433]
 [0.454]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.441]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.465]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[-1.73 ]
 [-1.062]
 [-1.73 ]
 [-1.73 ]
 [-1.73 ]
 [-1.73 ]
 [-1.73 ]] [[0.487]
 [0.465]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.52 ]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]] [[0.804]
 [1.645]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]] [[0.512]
 [0.52 ]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.4195],
        [-0.2609],
        [-0.3824],
        [-0.3797],
        [-0.0000],
        [-0.4160],
        [-0.0000],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.9405 -0.9405
-0.0632698753995 -0.4827833981049535
-0.024259925299500003 -0.2852086235315753
-0.024259925299500003 -0.40666753631800306
-0.034159925299499995 -0.41384885041957975
-0.96074352 -0.96074352
-0.0337698257985 -0.44981763112027817
-0.96074352 -0.96074352
-0.955892025 -0.955892025
-0.9405 -0.9405
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.514]
 [0.514]
 [0.523]
 [0.514]
 [0.514]
 [0.516]] [[1.302]
 [1.302]
 [1.302]
 [0.741]
 [1.302]
 [1.302]
 [1.697]] [[0.514]
 [0.514]
 [0.514]
 [0.523]
 [0.514]
 [0.514]
 [0.516]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.352]
 [0.341]
 [0.392]
 [0.308]
 [0.325]
 [0.382]] [[-0.569]
 [ 1.34 ]
 [ 0.855]
 [ 0.294]
 [-1.063]
 [ 0.225]
 [ 0.077]] [[0.345]
 [0.352]
 [0.341]
 [0.392]
 [0.308]
 [0.325]
 [0.382]]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.286]
 [0.291]
 [0.299]
 [0.297]
 [0.308]
 [0.29 ]] [[2.8  ]
 [2.779]
 [2.889]
 [2.753]
 [3.357]
 [3.222]
 [2.699]] [[0.294]
 [0.286]
 [0.291]
 [0.299]
 [0.297]
 [0.308]
 [0.29 ]]
siam score:  -0.60788625
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.41180599219394043
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.578]] [[-0.53 ]
 [-0.53 ]
 [-0.53 ]
 [-0.53 ]
 [-0.53 ]
 [-0.53 ]
 [-2.321]] [[0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.578]]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.515]] [[-1.829]
 [-1.829]
 [-1.829]
 [-1.829]
 [-1.829]
 [-1.829]
 [-2.164]] [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.515]]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.492]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.509]] [[ 0.201]
 [ 1.003]
 [ 0.201]
 [ 0.201]
 [ 0.201]
 [ 0.201]
 [-1.671]] [[0.466]
 [0.492]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.509]]
line 256 mcts: sample exp_bonus -2.7090487703430055
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.439]
 [0.464]
 [0.478]
 [0.464]
 [0.448]
 [0.49 ]] [[1.137]
 [1.668]
 [1.137]
 [1.062]
 [1.137]
 [1.564]
 [1.463]] [[0.464]
 [0.439]
 [0.464]
 [0.478]
 [0.464]
 [0.448]
 [0.49 ]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.51 ]
 [0.504]
 [0.504]] [[2.976]
 [2.976]
 [2.976]
 [2.976]
 [2.889]
 [2.976]
 [2.976]] [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.51 ]
 [0.504]
 [0.504]]
first move QE:  0.5364568944206232
siam score:  -0.61553484
line 256 mcts: sample exp_bonus 0.8898333060640011
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[2.364]
 [2.364]
 [2.364]
 [2.364]
 [2.364]
 [2.364]
 [2.364]] [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.7469418794190066
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
from probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.6028],
        [-0.0000],
        [-0.4058],
        [-0.0000],
        [-0.5019],
        [-0.5103],
        [-0.0000],
        [-0.2043],
        [-0.0000],
        [-0.4285]], dtype=torch.float64)
-0.024259925299500003 -0.6270643058114589
-0.94167714465 -0.94167714465
-0.024259925299500003 -0.43002617741684734
-0.9560860847999999 -0.9560860847999999
-0.024259925299500003 -0.5261717150203179
-0.024259925299500003 -0.5345824348407804
-0.97515 -0.97515
-0.024259925299500003 -0.22860649088115587
-0.93163455 -0.93163455
-0.024259925299500003 -0.45275822425454815
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.860196584945916
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.414]
 [0.439]
 [0.438]
 [0.434]
 [0.591]] [[ 1.681]
 [ 1.681]
 [ 1.681]
 [ 1.604]
 [ 1.407]
 [ 1.518]
 [-3.546]] [[0.414]
 [0.414]
 [0.414]
 [0.439]
 [0.438]
 [0.434]
 [0.591]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3035556881345824, 0.4514963292417767, 0.09609201798510221, 0.09478659054317755, 0.028492478242002673, 0.025576895853358466]
using explorer policy with actor:  1
siam score:  -0.6217029
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.532]] [[-0.292]
 [-0.292]
 [-0.292]
 [-0.292]
 [-0.292]
 [-0.292]
 [-0.193]] [[0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.532]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  49 total reward:  0.48999999999999966  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.472]
 [0.472]
 [0.472]
 [0.396]
 [0.472]
 [0.472]] [[1.414]
 [1.833]
 [1.833]
 [1.833]
 [1.012]
 [1.833]
 [1.833]] [[0.381]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.06 ]
 [0.76 ]
 [0.76 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.28892275997820366, 0.47793699605002526, 0.09145989396127548, 0.09021739476190307, 0.027118995868226293, 0.024343959380366254]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.28892275997820366, 0.47793699605002526, 0.09145989396127548, 0.09021739476190307, 0.027118995868226293, 0.024343959380366254]
using explorer policy with actor:  0
rdn probs:  [0.28892275997820366, 0.47793699605002526, 0.09145989396127548, 0.09021739476190307, 0.027118995868226293, 0.024343959380366254]
siam score:  -0.6263226
using another actor
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.267]] [[1.552]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.777]] [[0.194]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.267]]
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.739]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[-4.147]
 [ 1.267]
 [-4.147]
 [-4.147]
 [-4.147]
 [-4.147]
 [-4.147]] [[0.407]
 [0.739]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6252197
actor:  1 policy actor:  1  step number:  53 total reward:  0.6699999999999998  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  11106
actor:  1 policy actor:  1  step number:  84 total reward:  0.38499999999999956  reward:  1.0 rdn_beta:  0.333
from probs:  [0.27242566144904856, 0.5077460869476726, 0.08623765780978965, 0.08506610363294156, 0.02557053791052289, 0.022953952250024744]
line 256 mcts: sample exp_bonus 1.4760579428720246
using another actor
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.367]
 [0.347]
 [0.345]
 [0.345]
 [0.372]
 [0.347]] [[-2.833]
 [-2.17 ]
 [-2.694]
 [-2.602]
 [-2.46 ]
 [-2.759]
 [-2.941]] [[0.367]
 [0.367]
 [0.347]
 [0.345]
 [0.345]
 [0.372]
 [0.347]]
from probs:  [0.2621312730192825, 0.5263473191520063, 0.08297891947344808, 0.08185163584622017, 0.024604281471210864, 0.022086571037832105]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2621312730192825, 0.5263473191520063, 0.08297891947344808, 0.08185163584622017, 0.024604281471210864, 0.022086571037832105]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.517]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[1.314]
 [0.901]
 [1.314]
 [1.314]
 [1.314]
 [1.314]
 [1.314]] [[-0.026]
 [ 0.179]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.54 ]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[0.885]
 [1.115]
 [0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.885]] [[0.983]
 [1.194]
 [0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]] [[2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]] [[2.079]
 [2.079]
 [2.079]
 [2.079]
 [2.079]
 [2.079]
 [2.079]]
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.872]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]] [[3.228]
 [3.217]
 [3.228]
 [3.228]
 [3.228]
 [3.228]
 [3.228]] [[0.773]
 [1.18 ]
 [0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2621312730192825, 0.5263473191520064, 0.08297891947344808, 0.08185163584622017, 0.024604281471210864, 0.022086571037832105]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5649999999999997  reward:  1.0 rdn_beta:  0.167
first move QE:  0.5352004776496506
from probs:  [0.2621312730192825, 0.5263473191520064, 0.08297891947344808, 0.08185163584622017, 0.024604281471210864, 0.022086571037832105]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.333]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.455]] [[ 1.243]
 [ 0.578]
 [ 1.111]
 [ 1.111]
 [ 1.111]
 [ 1.111]
 [-1.714]] [[0.365]
 [0.333]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.455]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.05269814705482423
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29609407466193405, 0.502120476433453, 0.0791595455393744, 0.07808414879774905, 0.023471789605637408, 0.021069964961852105]
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.175]
 [0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]] [[ 0.054]
 [-0.023]
 [ 0.054]
 [ 0.054]
 [ 0.054]
 [ 0.054]
 [ 0.054]] [[0.141]
 [0.175]
 [0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]]
siam score:  -0.6055562
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29609407466193405, 0.502120476433453, 0.0791595455393744, 0.07808414879774905, 0.023471789605637408, 0.021069964961852105]
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.309]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.244]] [[3.132]
 [3.15 ]
 [3.132]
 [3.132]
 [3.132]
 [3.132]
 [2.643]] [[0.485]
 [0.63 ]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.163]]
first move QE:  0.534022101695755
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29609407466193405, 0.502120476433453, 0.0791595455393744, 0.07808414879774905, 0.023471789605637408, 0.021069964961852105]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
actor:  1 policy actor:  1  step number:  83 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.2880375309890407, 0.5156674821613892, 0.07700566138450737, 0.07595952554864138, 0.022833136170053843, 0.020496663746367442]
siam score:  -0.60439986
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2880375309890407, 0.5156674821613892, 0.07700566138450737, 0.07595952554864138, 0.022833136170053843, 0.020496663746367442]
3329 5441
using another actor
actor:  1 policy actor:  1  step number:  44 total reward:  0.6449999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.385]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.352]] [[1.4  ]
 [2.222]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.771]] [[0.358]
 [0.385]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.352]]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.412]
 [0.398]
 [0.398]
 [0.398]
 [0.707]
 [0.398]] [[2.758]
 [3.383]
 [2.758]
 [2.758]
 [2.758]
 [3.284]
 [2.758]] [[0.662]
 [0.898]
 [0.662]
 [0.662]
 [0.662]
 [1.454]
 [0.662]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32159863957077406, 0.49135949808333945, 0.07337570127337424, 0.07237887910207194, 0.021756807858353353, 0.01953047411208698]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32159863957077406, 0.49135949808333945, 0.07337570127337424, 0.07237887910207194, 0.021756807858353353, 0.01953047411208698]
actor:  1 policy actor:  1  step number:  56 total reward:  0.5349999999999997  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.543]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]] [[0.587]
 [1.363]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]] [[0.7  ]
 [1.437]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6197445
first move QE:  0.5337105620499915
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[ 0.103]
 [-0.005]
 [ 0.111]
 [ 0.114]
 [ 0.108]
 [ 0.11 ]
 [ 0.113]] [[2.341]
 [1.45 ]
 [2.591]
 [2.351]
 [2.616]
 [2.156]
 [2.343]] [[ 0.075]
 [-1.033]
 [ 0.34 ]
 [ 0.107]
 [ 0.36 ]
 [-0.096]
 [ 0.097]]
using explorer policy with actor:  1
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.399]
 [0.37 ]
 [0.371]
 [0.373]
 [0.364]
 [0.379]] [[-2.721]
 [-1.32 ]
 [-2.948]
 [-3.013]
 [-2.733]
 [-3.146]
 [-3.168]] [[0.379]
 [0.399]
 [0.37 ]
 [0.371]
 [0.373]
 [0.364]
 [0.379]]
from probs:  [0.34834579316602154, 0.4719867952376324, 0.07048273662649293, 0.06952521590309177, 0.020899007866381825, 0.018760451200379547]
3335 5456
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.66 ]
 [0.422]
 [0.575]
 [0.422]
 [0.422]
 [0.695]] [[ 0.316]
 [ 0.131]
 [ 0.316]
 [-0.583]
 [ 0.316]
 [ 0.316]
 [ 0.457]] [[0.422]
 [0.66 ]
 [0.422]
 [0.575]
 [0.422]
 [0.422]
 [0.695]]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.574]
 [0.323]
 [0.315]
 [0.315]
 [0.315]
 [0.312]] [[-3.307]
 [-2.502]
 [-3.77 ]
 [-3.894]
 [-3.732]
 [-3.833]
 [-3.843]] [[ 0.09 ]
 [ 0.573]
 [-0.071]
 [-0.122]
 [-0.063]
 [-0.1  ]
 [-0.105]]
3336 5456
3336 5459
3337 5460
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34834579316602154, 0.4719867952376324, 0.07048273662649293, 0.06952521590309177, 0.020899007866381825, 0.018760451200379547]
first move QE:  0.5317376834151812
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.488]
 [0.486]
 [0.484]
 [0.49 ]
 [0.485]
 [0.492]] [[2.199]
 [2.797]
 [2.062]
 [1.59 ]
 [2.138]
 [2.202]
 [2.143]] [[0.497]
 [0.488]
 [0.486]
 [0.484]
 [0.49 ]
 [0.485]
 [0.492]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34834579316602154, 0.4719867952376324, 0.07048273662649293, 0.06952521590309177, 0.020899007866381825, 0.018760451200379547]
start point for exploration sampling:  11106
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.472]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.496]] [[1.037]
 [1.079]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.384]] [[0.487]
 [0.472]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.496]]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.357]
 [0.351]
 [0.314]
 [0.316]
 [0.351]
 [0.377]] [[-3.19 ]
 [-0.943]
 [-3.19 ]
 [-2.931]
 [-2.776]
 [-3.19 ]
 [-2.639]] [[0.351]
 [0.357]
 [0.351]
 [0.314]
 [0.316]
 [0.351]
 [0.377]]
siam score:  -0.6135491
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34834579316602154, 0.47198679523763243, 0.07048273662649293, 0.06952521590309177, 0.020899007866381825, 0.018760451200379547]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34834579316602154, 0.47198679523763243, 0.07048273662649293, 0.06952521590309177, 0.020899007866381825, 0.018760451200379547]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.7918461218858224
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34834579316602154, 0.47198679523763243, 0.07048273662649293, 0.06952521590309177, 0.020899007866381825, 0.018760451200379547]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34834579316602154, 0.47198679523763243, 0.07048273662649293, 0.06952521590309177, 0.020899007866381825, 0.018760451200379547]
line 256 mcts: sample exp_bonus 3.2322333547751483
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.886]
 [0.818]
 [0.829]
 [0.793]
 [0.781]
 [0.754]
 [0.796]] [[0.534]
 [0.635]
 [0.641]
 [0.453]
 [0.732]
 [0.679]
 [0.858]] [[0.271]
 [0.169]
 [0.193]
 [0.058]
 [0.127]
 [0.056]
 [0.2  ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.34834579316602154, 0.47198679523763243, 0.07048273662649293, 0.06952521590309177, 0.020899007866381825, 0.018760451200379547]
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[0.891]
 [0.891]
 [0.891]
 [0.891]
 [0.891]
 [0.891]
 [0.891]] [[0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  85 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.167
from probs:  [0.366806535913061, 0.45861585906396113, 0.06848602785775706, 0.06755563278407058, 0.02030695888727054, 0.01822898549387974]
siam score:  -0.6044304
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.366806535913061, 0.45861585906396113, 0.06848602785775706, 0.06755563278407058, 0.02030695888727054, 0.01822898549387974]
UNIT TEST: sample policy line 217 mcts : [0.673 0.122 0.02  0.163 0.    0.    0.02 ]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.56 ]
 [0.493]] [[ 2.034]
 [ 1.284]
 [ 1.284]
 [ 1.284]
 [ 1.284]
 [-0.733]
 [ 1.284]] [[1.664]
 [1.199]
 [1.199]
 [1.199]
 [1.199]
 [0.659]
 [1.199]]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.409]
 [0.509]
 [0.47 ]
 [0.291]
 [0.557]
 [0.491]] [[ 1.005]
 [ 0.648]
 [-1.331]
 [ 0.815]
 [ 2.492]
 [-0.976]
 [-0.849]] [[0.693]
 [0.474]
 [0.014]
 [0.653]
 [0.854]
 [0.228]
 [0.138]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36680653591306106, 0.458615859063961, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
from probs:  [0.36680653591306106, 0.458615859063961, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.60411626
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36680653591306106, 0.458615859063961, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
using another actor
line 256 mcts: sample exp_bonus 0.5026662654490781
line 256 mcts: sample exp_bonus 0.27472483662894864
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.366806535913061, 0.45861585906396113, 0.06848602785775706, 0.06755563278407058, 0.020306958887270545, 0.01822898549387974]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]] [[1.092]
 [1.092]
 [1.092]
 [1.092]
 [1.092]
 [1.092]
 [1.092]]
using explorer policy with actor:  1
from probs:  [0.36680653591306106, 0.4586158590639611, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[-3.678]
 [-3.678]
 [-3.678]
 [-3.678]
 [-3.678]
 [-3.678]
 [-3.678]] [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]] [[1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]] [[2.026]
 [2.026]
 [2.026]
 [2.026]
 [2.026]
 [2.026]
 [2.026]]
siam score:  -0.6072068
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36680653591306106, 0.4586158590639611, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.212]
 [0.203]
 [0.203]
 [0.203]
 [0.199]
 [0.203]] [[ 0.   ]
 [-2.725]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-2.578]
 [ 0.   ]] [[0.203]
 [0.212]
 [0.203]
 [0.203]
 [0.203]
 [0.199]
 [0.203]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.259]
 [0.221]
 [0.207]
 [0.202]
 [0.215]
 [0.216]] [[-2.334]
 [-2.791]
 [-2.377]
 [-2.648]
 [-2.534]
 [-2.303]
 [-2.574]] [[0.213]
 [0.259]
 [0.221]
 [0.207]
 [0.202]
 [0.215]
 [0.216]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.574]] [[0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [4.573]] [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.574]]
line 256 mcts: sample exp_bonus 0.9940546798196537
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8898513306693855
line 256 mcts: sample exp_bonus 0.8894284328747282
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.61213785
using explorer policy with actor:  1
from probs:  [0.36680653591306106, 0.458615859063961, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36680653591306106, 0.458615859063961, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.62231046
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[1.711]
 [1.711]
 [1.711]
 [1.711]
 [1.711]
 [1.711]
 [1.711]] [[2.045]
 [2.045]
 [2.045]
 [2.045]
 [2.045]
 [2.045]
 [2.045]]
line 256 mcts: sample exp_bonus 0.5264912274140021
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.311]
 [0.277]
 [0.311]
 [0.299]
 [0.311]
 [0.311]] [[1.293]
 [1.293]
 [0.505]
 [1.293]
 [0.732]
 [1.293]
 [1.293]] [[2.218]
 [2.218]
 [1.733]
 [2.218]
 [1.881]
 [2.218]
 [2.218]]
using explorer policy with actor:  1
from probs:  [0.36680653591306106, 0.4586158590639611, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5998383854200587
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5265701563987513
using explorer policy with actor:  1
siam score:  -0.6171564
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.366806535913061, 0.45861585906396113, 0.06848602785775706, 0.06755563278407058, 0.020306958887270545, 0.01822898549387974]
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.36680653591306106, 0.45861585906396113, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36680653591306106, 0.45861585906396113, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
using explorer policy with actor:  1
3373 5534
using another actor
from probs:  [0.36680653591306106, 0.45861585906396113, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
siam score:  -0.6214819
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36680653591306106, 0.45861585906396113, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.249]
 [0.301]
 [0.301]
 [0.301]
 [0.603]
 [0.301]] [[-0.05 ]
 [ 1.481]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [ 1.864]
 [-0.05 ]] [[0.343]
 [0.749]
 [0.343]
 [0.343]
 [0.343]
 [1.586]
 [0.343]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.65 ]
 [0.409]
 [0.409]
 [0.409]
 [0.408]
 [0.406]] [[1.028]
 [0.783]
 [1.028]
 [1.028]
 [1.028]
 [0.772]
 [0.712]] [[-0.12 ]
 [ 0.281]
 [-0.12 ]
 [-0.12 ]
 [-0.12 ]
 [-0.207]
 [-0.23 ]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36680653591306106, 0.45861585906396113, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
3375 5534
siam score:  -0.6302822
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36680653591306106, 0.45861585906396113, 0.06848602785775705, 0.06755563278407056, 0.020306958887270538, 0.018228985493879735]
actor:  1 policy actor:  1  step number:  67 total reward:  0.5299999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797827, 0.4418006470948108, 0.06597497846294598, 0.06507869644939444, 0.019562401516674568, 0.017560617296391377]
first move QE:  0.5257576324911264
from probs:  [0.3900226591797827, 0.4418006470948108, 0.06597497846294598, 0.06507869644939444, 0.019562401516674568, 0.017560617296391377]
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
3381 5540
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.101]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[-0.662]
 [ 0.522]
 [-0.662]
 [-0.662]
 [-0.662]
 [-0.662]
 [-0.662]] [[0.132]
 [0.101]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.226511588105678
first move QE:  0.5257132676553679
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]] [[1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.636]] [[1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.372]
 [0.318]
 [0.39 ]
 [0.336]
 [0.321]
 [0.364]] [[1.844]
 [2.043]
 [1.427]
 [1.654]
 [1.844]
 [1.525]
 [1.805]] [[1.246]
 [1.479]
 [0.858]
 [1.175]
 [1.246]
 [0.947]
 [1.262]]
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[0.519]
 [0.51 ]
 [0.526]
 [0.526]
 [0.516]
 [0.526]
 [0.519]] [[2.923]
 [2.905]
 [2.936]
 [2.936]
 [2.916]
 [2.936]
 [2.923]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.5256013266127122
line 256 mcts: sample exp_bonus 3.6857328001628815
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.626]
 [0.555]
 [0.602]
 [0.621]
 [0.581]
 [0.598]] [[1.829]
 [2.738]
 [1.498]
 [1.347]
 [0.814]
 [1.298]
 [3.21 ]] [[0.539]
 [0.626]
 [0.555]
 [0.602]
 [0.621]
 [0.581]
 [0.598]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797827, 0.4418006470948108, 0.06597497846294598, 0.06507869644939444, 0.019562401516674568, 0.017560617296391377]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
from probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.05 ]
 [0.034]
 [0.034]
 [0.035]
 [0.037]
 [0.034]] [[-2.364]
 [-0.213]
 [-2.085]
 [-1.976]
 [-1.986]
 [-1.744]
 [-1.787]] [[0.032]
 [0.05 ]
 [0.034]
 [0.034]
 [0.035]
 [0.037]
 [0.034]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.318]
 [0.283]
 [0.317]
 [0.283]
 [0.283]
 [0.301]] [[1.239]
 [0.74 ]
 [1.239]
 [0.505]
 [1.239]
 [1.239]
 [0.704]] [[-0.02 ]
 [-0.117]
 [-0.02 ]
 [-0.197]
 [-0.02 ]
 [-0.02 ]
 [-0.163]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
using explorer policy with actor:  1
from probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.476]
 [0.418]
 [0.419]
 [0.426]
 [0.471]
 [0.459]] [[2.468]
 [3.014]
 [2.643]
 [2.576]
 [2.833]
 [2.798]
 [2.72 ]] [[0.472]
 [0.76 ]
 [0.519]
 [0.498]
 [0.599]
 [0.678]
 [0.626]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
siam score:  -0.61788726
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797827, 0.4418006470948108, 0.06597497846294598, 0.06507869644939444, 0.019562401516674568, 0.017560617296391377]
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.855]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[-0.746]
 [ 1.819]
 [-0.746]
 [-0.746]
 [-0.746]
 [-0.746]
 [-0.746]] [[0.214]
 [1.336]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6133857
siam score:  -0.6138075
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
siam score:  -0.6143673
in main func line 156:  3403
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6151633
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.124]
 [0.488]
 [0.477]
 [0.473]
 [0.483]
 [0.484]] [[-4.608]
 [ 0.74 ]
 [-3.582]
 [-3.515]
 [-3.316]
 [-3.332]
 [-4.101]] [[0.06 ]
 [1.786]
 [0.42 ]
 [0.44 ]
 [0.507]
 [0.505]
 [0.24 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.61239386
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
rdn beta is 0 so we're just using the maxi policy
3411 5601
siam score:  -0.61482185
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797827, 0.4418006470948108, 0.06597497846294598, 0.06507869644939444, 0.019562401516674568, 0.017560617296391377]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797827, 0.4418006470948108, 0.06597497846294598, 0.06507869644939444, 0.019562401516674568, 0.017560617296391377]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 1.318707110827591
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797827, 0.4418006470948108, 0.06597497846294598, 0.06507869644939444, 0.019562401516674568, 0.017560617296391377]
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.548]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.557]] [[2.878]
 [2.947]
 [2.862]
 [2.862]
 [2.862]
 [2.862]
 [2.914]] [[0.96 ]
 [0.996]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.991]]
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.831]] [[ 0.244]
 [ 0.244]
 [ 0.244]
 [ 0.244]
 [ 0.244]
 [ 0.244]
 [-0.536]] [[0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.831]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
first move QE:  0.5221897098972147
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -1.0986669654637828
first move QE:  0.521579509433604
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.933]
 [0.627]
 [1.025]
 [0.627]
 [0.627]
 [1.038]] [[3.361]
 [2.849]
 [3.361]
 [3.622]
 [3.361]
 [3.361]
 [2.986]] [[1.586]
 [1.862]
 [1.586]
 [2.139]
 [1.586]
 [1.586]
 [2.022]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
3420 5616
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3900226591797828, 0.4418006470948108, 0.06597497846294599, 0.06507869644939446, 0.01956240151667457, 0.01756061729639138]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  61 total reward:  0.5699999999999997  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.613839
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.406]
 [0.483]
 [0.462]
 [0.483]
 [0.467]
 [0.451]] [[0.818]
 [0.599]
 [0.9  ]
 [0.983]
 [0.9  ]
 [1.147]
 [1.183]] [[0.541]
 [0.275]
 [0.629]
 [0.643]
 [0.629]
 [0.762]
 [0.754]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]] [[1.07]
 [1.07]
 [1.07]
 [1.07]
 [1.07]
 [1.07]
 [1.07]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.41269693546943564, 0.42537788961384454, 0.06352253508552282, 0.06265956995876151, 0.018835221559003096, 0.0169078483134324]
Printing some Q and Qe and total Qs values:  [[1.474]
 [1.476]
 [0.037]
 [0.037]
 [1.475]
 [1.475]
 [1.475]] [[0.512]
 [0.509]
 [1.539]
 [1.539]
 [0.504]
 [0.516]
 [0.513]] [[1.91 ]
 [1.91 ]
 [1.789]
 [1.789]
 [1.908]
 [1.912]
 [1.911]]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[-1.814]
 [-1.814]
 [-1.814]
 [-1.814]
 [-1.814]
 [-1.814]
 [-1.814]] [[0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]]
siam score:  -0.6102423
from probs:  [0.41269693546943564, 0.42537788961384454, 0.06352253508552282, 0.06265956995876151, 0.018835221559003096, 0.0169078483134324]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.453]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]] [[1.214]
 [1.851]
 [1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]] [[0.54 ]
 [0.814]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]]
siam score:  -0.6091041
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.41269693546943564, 0.42537788961384454, 0.06352253508552282, 0.06265956995876151, 0.018835221559003096, 0.0169078483134324]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.41269693546943564, 0.42537788961384454, 0.06352253508552282, 0.06265956995876151, 0.018835221559003096, 0.0169078483134324]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.204]
 [0.544]
 [0.418]
 [0.45 ]
 [0.55 ]
 [0.267]] [[-0.41 ]
 [-0.363]
 [-2.388]
 [-1.096]
 [ 0.114]
 [-2.199]
 [-0.368]] [[1.311]
 [1.243]
 [0.124]
 [0.899]
 [1.724]
 [0.254]
 [1.282]]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.034]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]] [[1.582]
 [0.93 ]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[1.907]
 [1.451]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.339]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[3.821]
 [3.22 ]
 [3.821]
 [3.821]
 [3.821]
 [3.821]
 [3.821]] [[ 0.022]
 [-0.145]
 [ 0.022]
 [ 0.022]
 [ 0.022]
 [ 0.022]
 [ 0.022]]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.425]
 [0.502]
 [0.503]
 [0.495]
 [0.506]
 [0.494]] [[1.1  ]
 [0.577]
 [0.983]
 [1.071]
 [1.124]
 [0.987]
 [0.993]] [[0.975]
 [0.512]
 [0.937]
 [0.997]
 [1.018]
 [0.948]
 [0.928]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.41269693546943564, 0.42537788961384443, 0.06352253508552282, 0.06265956995876151, 0.018835221559003096, 0.0169078483134324]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.423629747891064
actor:  1 policy actor:  1  step number:  53 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
siam score:  -0.6061525
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 0.927758337412361
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43197089540501293, 0.41141794815082666, 0.06143786897327755, 0.06060322441264898, 0.018217092133786972, 0.01635297092444688]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43197089540501293, 0.41141794815082666, 0.06143786897327755, 0.06060322441264898, 0.018217092133786972, 0.01635297092444688]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.337]
 [0.362]
 [0.386]
 [0.368]
 [0.383]
 [0.344]] [[3.266]
 [3.84 ]
 [3.107]
 [3.095]
 [2.824]
 [3.377]
 [3.388]] [[ 0.053]
 [ 0.253]
 [ 0.057]
 [ 0.103]
 [-0.025]
 [ 0.191]
 [ 0.115]]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.331]
 [0.293]
 [0.29 ]
 [0.281]
 [0.293]
 [0.289]] [[-2.187]
 [-1.983]
 [-1.974]
 [-2.011]
 [-1.861]
 [-1.956]
 [-2.094]] [[-0.024]
 [ 0.137]
 [ 0.064]
 [ 0.045]
 [ 0.078]
 [ 0.07 ]
 [ 0.017]]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.022]
 [-0.004]
 [-0.002]
 [-0.007]
 [-0.008]
 [-0.004]] [[0.876]
 [1.22 ]
 [0.978]
 [0.849]
 [1.156]
 [1.161]
 [0.927]] [[-0.427]
 [-0.12 ]
 [-0.325]
 [-0.448]
 [-0.154]
 [-0.15 ]
 [-0.376]]
siam score:  -0.61191356
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.43197089540501293, 0.41141794815082666, 0.06143786897327755, 0.06060322441264898, 0.018217092133786972, 0.01635297092444688]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 2.547196618043692
3440 5637
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.7840682339851827
actor:  1 policy actor:  1  step number:  60 total reward:  0.5249999999999997  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.457]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[1.594]
 [2.85 ]
 [1.594]
 [1.594]
 [1.594]
 [1.594]
 [1.594]] [[-0.415]
 [ 0.109]
 [-0.415]
 [-0.415]
 [-0.415]
 [-0.415]
 [-0.415]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.035]
 [0.035]
 [0.028]
 [0.012]
 [0.054]
 [0.032]] [[1.039]
 [1.641]
 [1.478]
 [0.998]
 [1.489]
 [1.888]
 [1.242]] [[0.114]
 [1.099]
 [0.826]
 [0.012]
 [0.8  ]
 [1.548]
 [0.428]]
line 256 mcts: sample exp_bonus 3.089890441844047
Printing some Q and Qe and total Qs values:  [[0.781]
 [1.042]
 [0.765]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[2.506]
 [2.381]
 [1.888]
 [2.506]
 [2.506]
 [2.506]
 [2.506]] [[1.453]
 [1.933]
 [1.215]
 [1.453]
 [1.453]
 [1.453]
 [1.453]]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.558]
 [0.635]
 [0.659]
 [0.661]
 [0.653]
 [0.66 ]] [[ 0.319]
 [ 0.901]
 [ 0.439]
 [ 0.459]
 [ 0.738]
 [ 0.156]
 [-0.047]] [[0.506]
 [0.528]
 [0.527]
 [0.582]
 [0.68 ]
 [0.47 ]
 [0.415]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6217054
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.374]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[-2.973]
 [ 0.562]
 [ 0.374]
 [ 0.374]
 [ 0.374]
 [ 0.374]
 [ 0.374]] [[0.1  ]
 [1.554]
 [1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.4506067835972344, 0.3979201558370354, 0.05942221652221041, 0.05861495496464682, 0.017619426117298758, 0.015816462961574212]
from probs:  [0.4506067835972344, 0.3979201558370354, 0.05942221652221041, 0.05861495496464682, 0.017619426117298758, 0.015816462961574212]
from probs:  [0.4506067835972344, 0.3979201558370354, 0.05942221652221041, 0.05861495496464682, 0.017619426117298758, 0.015816462961574212]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4506067835972344, 0.3979201558370354, 0.05942221652221041, 0.05861495496464682, 0.017619426117298758, 0.015816462961574212]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.687]] [[-1.985]
 [-1.985]
 [-1.985]
 [-1.985]
 [-1.985]
 [-1.985]
 [-1.541]] [[0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.687]]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.681]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[1.388]
 [1.024]
 [1.388]
 [1.388]
 [1.388]
 [1.388]
 [1.388]] [[0.555]
 [0.681]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.689]] [[-2.066]
 [-2.066]
 [-2.066]
 [-2.066]
 [-2.066]
 [-2.066]
 [-2.016]] [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.689]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4506067835972344, 0.3979201558370354, 0.05942221652221041, 0.05861495496464682, 0.017619426117298758, 0.015816462961574212]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.575]
 [0.573]
 [0.573]
 [0.57 ]
 [0.573]
 [0.573]] [[-3.403]
 [-3.593]
 [-3.403]
 [-3.403]
 [-3.057]
 [-3.403]
 [-3.403]] [[0.573]
 [0.575]
 [0.573]
 [0.573]
 [0.57 ]
 [0.573]
 [0.573]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4506067835972344, 0.3979201558370354, 0.05942221652221041, 0.05861495496464682, 0.017619426117298758, 0.015816462961574212]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.387]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.328]] [[1.664]
 [1.429]
 [1.664]
 [1.664]
 [1.664]
 [1.664]
 [1.889]] [[0.315]
 [0.387]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.328]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4506067835972345, 0.39792015583703527, 0.05942221652221041, 0.05861495496464682, 0.017619426117298758, 0.015816462961574212]
3453 5662
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.4506067835972345, 0.39792015583703527, 0.05942221652221041, 0.05861495496464682, 0.017619426117298758, 0.015816462961574212]
start point for exploration sampling:  11106
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.35 ]
 [0.343]
 [0.351]
 [0.333]
 [0.345]
 [0.352]] [[ 0.057]
 [ 0.024]
 [ 0.104]
 [ 0.038]
 [ 0.088]
 [ 0.207]
 [-0.003]] [[-0.319]
 [-0.305]
 [-0.293]
 [-0.299]
 [-0.319]
 [-0.255]
 [-0.311]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4506067835972345, 0.3979201558370353, 0.05942221652221042, 0.058614954964646825, 0.01761942611729876, 0.015816462961574216]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.545]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[-2.797]
 [-1.563]
 [-2.797]
 [-2.797]
 [-2.797]
 [-2.797]
 [-2.797]] [[0.042]
 [0.691]
 [0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.042]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4506067835972345, 0.3979201558370353, 0.05942221652221042, 0.058614954964646825, 0.01761942611729876, 0.015816462961574216]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4506067835972345, 0.3979201558370353, 0.05942221652221042, 0.058614954964646825, 0.01761942611729876, 0.015816462961574216]
Printing some Q and Qe and total Qs values:  [[0.212]
 [1.   ]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]] [[1.575]
 [0.517]
 [1.575]
 [1.575]
 [1.575]
 [1.575]
 [1.575]] [[0.212]
 [1.   ]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.452]
 [0.44 ]
 [0.452]
 [0.438]
 [0.442]
 [0.443]] [[1.015]
 [1.075]
 [0.968]
 [1.075]
 [1.158]
 [1.127]
 [1.247]] [[0.535]
 [0.603]
 [0.471]
 [0.603]
 [0.657]
 [0.633]
 [0.756]]
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]] [[1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]] [[0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.546]
 [0.457]
 [0.425]
 [0.457]
 [0.411]
 [0.441]] [[2.028]
 [2.563]
 [2.028]
 [1.759]
 [2.028]
 [1.633]
 [2.25 ]] [[ 0.233]
 [ 0.769]
 [ 0.233]
 [-0.009]
 [ 0.233]
 [-0.122]
 [ 0.349]]
siam score:  -0.6102725
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.1925148168202657
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.44 ]
 [0.44 ]
 [0.499]
 [0.533]
 [0.44 ]
 [0.333]] [[1.25 ]
 [0.951]
 [0.951]
 [0.732]
 [0.635]
 [0.951]
 [0.686]] [[1.894]
 [1.947]
 [1.947]
 [1.926]
 [1.929]
 [1.947]
 [1.667]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4506067835972345, 0.39792015583703527, 0.05942221652221041, 0.05861495496464682, 0.017619426117298758, 0.015816462961574212]
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.44 ]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.444]] [[-3.716]
 [-3.676]
 [-4.063]
 [-4.018]
 [-3.848]
 [-3.799]
 [-3.94 ]] [[0.409]
 [0.44 ]
 [0.182]
 [0.213]
 [0.331]
 [0.364]
 [0.26 ]]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.28 ]
 [0.278]
 [0.279]
 [0.28 ]
 [0.282]
 [0.279]] [[-3.484]
 [-3.537]
 [-4.186]
 [-3.728]
 [-3.483]
 [-3.755]
 [-3.732]] [[0.279]
 [0.28 ]
 [0.278]
 [0.279]
 [0.28 ]
 [0.282]
 [0.279]]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.707]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[-2.269]
 [-0.056]
 [-2.269]
 [-2.269]
 [-2.269]
 [-2.269]
 [-2.269]] [[0.283]
 [0.707]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.482]
 [0.462]
 [0.462]
 [0.462]
 [0.475]
 [0.657]] [[ 0.142]
 [ 0.866]
 [ 0.142]
 [ 0.142]
 [ 0.142]
 [-0.21 ]
 [-1.049]] [[0.462]
 [0.482]
 [0.462]
 [0.462]
 [0.462]
 [0.475]
 [0.657]]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.437]
 [0.455]
 [0.393]
 [0.399]
 [0.525]
 [0.663]] [[-0.368]
 [ 0.867]
 [-0.102]
 [-0.356]
 [-0.321]
 [-0.053]
 [-0.802]] [[0.417]
 [0.437]
 [0.455]
 [0.393]
 [0.399]
 [0.525]
 [0.663]]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.399]
 [0.39 ]
 [0.39 ]
 [0.394]
 [0.406]
 [0.404]] [[1.292]
 [3.008]
 [1.64 ]
 [1.522]
 [1.424]
 [1.789]
 [1.638]] [[0.395]
 [0.399]
 [0.39 ]
 [0.39 ]
 [0.394]
 [0.406]
 [0.404]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4506067835972345, 0.39792015583703527, 0.05942221652221041, 0.05861495496464682, 0.017619426117298758, 0.015816462961574212]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4506067835972345, 0.39792015583703527, 0.05942221652221041, 0.05861495496464682, 0.017619426117298758, 0.015816462961574212]
3464 5699
siam score:  -0.5959558
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.488]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]] [[1.557]
 [2.166]
 [1.557]
 [1.557]
 [1.557]
 [1.557]
 [1.557]] [[1.249]
 [1.926]
 [1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4506067835972345, 0.3979201558370353, 0.05942221652221042, 0.058614954964646825, 0.01761942611729876, 0.015816462961574216]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4506067835972345, 0.3979201558370353, 0.05942221652221042, 0.058614954964646825, 0.01761942611729876, 0.015816462961574216]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.341]
 [0.345]
 [0.359]
 [0.351]
 [0.353]
 [0.338]] [[2.359]
 [3.059]
 [2.464]
 [2.655]
 [2.811]
 [3.293]
 [2.683]] [[0.348]
 [0.341]
 [0.345]
 [0.359]
 [0.351]
 [0.353]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.442]
 [0.515]
 [0.525]
 [0.517]
 [0.524]
 [0.53 ]] [[-0.366]
 [ 1.196]
 [-0.28 ]
 [-1.08 ]
 [-0.096]
 [-0.642]
 [-0.786]] [[0.523]
 [0.442]
 [0.515]
 [0.525]
 [0.517]
 [0.524]
 [0.53 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
actor:  0 policy actor:  1  step number:  95 total reward:  0.0899999999999993  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.938]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]] [[2.036]
 [1.667]
 [2.036]
 [2.036]
 [2.036]
 [2.036]
 [2.036]] [[1.654]
 [2.31 ]
 [1.654]
 [1.654]
 [1.654]
 [1.654]
 [1.654]]
siam score:  -0.6035321
siam score:  -0.6037022
line 256 mcts: sample exp_bonus -0.7745752318499518
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513, 0.05942512235855584, 0.058618181610622766, 0.01760553607598222, 0.01580079020529852]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513, 0.05942512235855584, 0.058618181610622766, 0.01760553607598222, 0.01580079020529852]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.806]] [[-1.853]
 [-1.853]
 [-1.853]
 [-1.853]
 [-1.853]
 [-1.853]
 [-1.334]] [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.806]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -1.1078453096499392
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.294]] [[-0.667]
 [-0.667]
 [-0.667]
 [-0.667]
 [-0.667]
 [-0.667]
 [-0.844]] [[0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.294]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513, 0.05942512235855584, 0.058618181610622766, 0.01760553607598222, 0.01580079020529852]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.469]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.494]] [[-1.908]
 [-1.523]
 [-1.908]
 [-1.908]
 [-1.908]
 [-1.908]
 [-2.053]] [[0.496]
 [0.469]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.494]]
3472 5720
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.533]
 [0.497]
 [0.477]
 [0.478]
 [0.49 ]
 [0.487]] [[-3.233]
 [-2.121]
 [-2.154]
 [-3.586]
 [-3.384]
 [-3.434]
 [-3.013]] [[0.484]
 [0.533]
 [0.497]
 [0.477]
 [0.478]
 [0.49 ]
 [0.487]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
Printing some Q and Qe and total Qs values:  [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]] [[0.502]
 [0.502]
 [0.514]
 [0.514]
 [0.502]
 [0.514]
 [0.508]] [[2.898]
 [2.898]
 [2.919]
 [2.919]
 [2.898]
 [2.919]
 [2.908]]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.529]] [[-1.369]
 [-1.369]
 [-1.369]
 [-1.369]
 [-1.369]
 [-1.369]
 [-0.736]] [[0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.529]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.488]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.404]] [[ 0.461]
 [ 0.088]
 [-0.297]
 [-0.297]
 [-0.297]
 [-0.297]
 [ 1.095]] [[1.405]
 [1.362]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.781]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
line 256 mcts: sample exp_bonus 0.9903348034272915
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.582]
 [0.562]
 [0.562]] [[1.931]
 [1.931]
 [1.931]
 [1.931]
 [1.874]
 [1.931]
 [1.931]] [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.582]
 [0.562]
 [0.562]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.465]
 [0.46 ]
 [0.46 ]
 [0.449]
 [0.46 ]
 [0.46 ]] [[0.615]
 [0.573]
 [0.605]
 [0.605]
 [1.063]
 [0.605]
 [0.605]] [[1.709]
 [1.801]
 [1.817]
 [1.817]
 [2.084]
 [1.817]
 [1.817]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.394]
 [0.488]
 [0.487]
 [0.487]
 [0.486]
 [0.493]] [[-1.741]
 [ 0.996]
 [-2.002]
 [-2.118]
 [-1.967]
 [-2.12 ]
 [-1.505]] [[0.304]
 [1.622]
 [0.172]
 [0.112]
 [0.189]
 [0.11 ]
 [0.428]]
3475 5740
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.343]
 [0.352]
 [0.352]
 [0.35 ]
 [0.352]
 [0.352]] [[1.901]
 [1.965]
 [1.901]
 [1.901]
 [1.917]
 [1.901]
 [1.901]] [[0.352]
 [0.343]
 [0.352]
 [0.352]
 [0.35 ]
 [0.352]
 [0.352]]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.93 ]] [[-0.884]
 [-0.884]
 [-0.884]
 [-0.884]
 [-0.884]
 [-0.884]
 [-1.414]] [[0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.93 ]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.623]
 [0.903]] [[-0.694]
 [-0.694]
 [-0.694]
 [-0.694]
 [-0.694]
 [-0.675]
 [-1.92 ]] [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.623]
 [0.903]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.407]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.617]] [[2.248]
 [2.545]
 [2.248]
 [2.248]
 [2.248]
 [2.248]
 [2.843]] [[0.3  ]
 [0.407]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.617]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.854]] [[-0.561]
 [-0.561]
 [-0.561]
 [-0.561]
 [-0.561]
 [-0.561]
 [-0.383]] [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.854]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
rdn probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
using another actor
siam score:  -0.60505795
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]] [[2.256]
 [2.256]
 [2.256]
 [2.256]
 [2.256]
 [2.256]
 [2.256]] [[0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]]
siam score:  -0.605597
from probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.432]
 [0.39 ]
 [0.416]
 [0.39 ]
 [0.39 ]
 [0.43 ]] [[1.374]
 [2.115]
 [1.192]
 [0.38 ]
 [1.192]
 [1.192]
 [2.044]] [[0.295]
 [0.432]
 [0.39 ]
 [0.416]
 [0.39 ]
 [0.39 ]
 [0.43 ]]
siam score:  -0.60665584
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45063728151441074, 0.39791308823513005, 0.059425122358555856, 0.05861818161062277, 0.017605536075982222, 0.015800790205298523]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.47 ]] [[1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.41 ]] [[1.868]
 [1.868]
 [1.868]
 [1.868]
 [1.868]
 [1.868]
 [1.954]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.60858667
actor:  1 policy actor:  1  step number:  68 total reward:  0.4449999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546376, 0.057724757280118576, 0.056940905990207555, 0.017101778783735644, 0.01534867314082377]
from probs:  [0.4377429387496509, 0.41514094605546376, 0.057724757280118576, 0.056940905990207555, 0.017101778783735644, 0.01534867314082377]
using another actor
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546376, 0.057724757280118576, 0.056940905990207555, 0.017101778783735644, 0.01534867314082377]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.314]
 [0.346]
 [0.346]
 [0.339]
 [0.333]
 [0.328]] [[0.655]
 [1.285]
 [0.408]
 [0.408]
 [0.564]
 [0.73 ]
 [0.719]] [[0.341]
 [0.314]
 [0.346]
 [0.346]
 [0.339]
 [0.333]
 [0.328]]
first move QE:  0.513292690479505
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.336]
 [0.286]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[-1.943]
 [ 0.904]
 [-1.933]
 [ 1.406]
 [ 1.406]
 [ 1.406]
 [ 1.406]] [[0.285]
 [0.336]
 [0.286]
 [0.252]
 [0.252]
 [0.252]
 [0.252]]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.275]
 [0.27 ]
 [0.275]
 [0.275]
 [0.282]
 [0.275]] [[-2.178]
 [-2.178]
 [-2.124]
 [-2.178]
 [-2.178]
 [-2.054]
 [-2.178]] [[0.275]
 [0.275]
 [0.27 ]
 [0.275]
 [0.275]
 [0.282]
 [0.275]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.6136872
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546376, 0.057724757280118576, 0.056940905990207555, 0.017101778783735644, 0.01534867314082377]
using another actor
from probs:  [0.4377429387496509, 0.41514094605546376, 0.057724757280118576, 0.056940905990207555, 0.017101778783735644, 0.01534867314082377]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
siam score:  -0.6102722
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
line 256 mcts: sample exp_bonus -4.7113749913582215
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.485]] [[-0.383]
 [-0.383]
 [-0.383]
 [-0.383]
 [-0.383]
 [-0.383]
 [-0.334]] [[0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.485]]
from probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.33 ]
 [0.318]
 [0.284]
 [0.263]
 [0.319]
 [0.313]] [[ 1.077]
 [-0.558]
 [-1.988]
 [ 0.272]
 [ 0.499]
 [-1.892]
 [ 0.364]] [[0.13 ]
 [0.33 ]
 [0.318]
 [0.284]
 [0.263]
 [0.319]
 [0.313]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.619]] [[ 0.47 ]
 [ 0.47 ]
 [ 0.47 ]
 [ 0.47 ]
 [ 0.47 ]
 [ 0.47 ]
 [-0.088]] [[0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.619]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.709]] [[-0.441]
 [-0.441]
 [-0.441]
 [-0.441]
 [-0.441]
 [-0.441]
 [ 0.562]] [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.709]]
from probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
using explorer policy with actor:  1
3508 5778
rdn beta is 0 so we're just using the maxi policy
first move QE:  0.5119759685937202
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4714],
        [-0.5552],
        [-0.0000],
        [-0.6071],
        [-0.4889],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.034159925299499995 -0.5055502069812591
-0.024259925299500003 -0.5794364749860751
-0.9464801247 -0.9464801247
-0.024259925299500003 -0.6313501282571915
-0.05336987539949999 -0.5423181759487113
-0.87443542395 -0.87443542395
-0.8264056234499999 -0.8264056234499999
-0.9801 -0.9801
-0.9560860847999999 -0.9560860847999999
-0.858862125 -0.858862125
siam score:  -0.6039567
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.429]
 [0.477]
 [0.477]
 [0.477]
 [0.477]] [[0.751]
 [0.751]
 [1.11 ]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[-0.423]
 [-0.423]
 [-0.281]
 [-0.423]
 [-0.423]
 [-0.423]
 [-0.423]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[2.489]
 [2.489]
 [2.489]
 [2.489]
 [2.489]
 [2.489]
 [2.489]] [[-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
line 256 mcts: sample exp_bonus 1.6475678768770454
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
3516 5805
siam score:  -0.6144562
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.598]
 [0.482]
 [0.522]
 [0.56 ]
 [0.577]
 [0.428]] [[3.517]
 [3.42 ]
 [3.289]
 [3.26 ]
 [3.339]
 [4.34 ]
 [3.86 ]] [[1.101]
 [1.145]
 [0.877]
 [0.914]
 [1.03 ]
 [1.797]
 [1.22 ]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
siam score:  -0.60733724
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.282]
 [0.274]
 [0.28 ]
 [0.274]
 [0.274]
 [0.291]] [[1.662]
 [1.632]
 [1.662]
 [1.557]
 [1.662]
 [1.662]
 [1.845]] [[0.274]
 [0.282]
 [0.274]
 [0.28 ]
 [0.274]
 [0.274]
 [0.291]]
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.48 ]
 [0.288]
 [0.532]
 [0.424]
 [0.248]
 [0.458]] [[1.9  ]
 [1.696]
 [1.542]
 [1.551]
 [1.473]
 [1.839]
 [2.036]] [[0.861]
 [0.994]
 [0.654]
 [0.942]
 [0.757]
 [0.836]
 [1.229]]
using another actor
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.655]
 [0.535]
 [0.535]
 [0.533]
 [0.559]
 [0.551]] [[1.082]
 [1.558]
 [1.082]
 [1.082]
 [1.299]
 [0.999]
 [1.418]] [[1.041]
 [1.44 ]
 [1.041]
 [1.041]
 [1.109]
 [1.06 ]
 [1.186]]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.278]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[3.007]
 [3.006]
 [3.007]
 [3.007]
 [3.007]
 [3.007]
 [3.007]] [[0.954]
 [0.932]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]]
siam score:  -0.60471463
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.231]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]] [[-1.877]
 [-1.462]
 [-1.877]
 [-1.877]
 [-1.877]
 [-1.877]
 [-1.877]] [[0.133]
 [0.231]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.26 ]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]] [[0.139]
 [1.748]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[0.241]
 [0.26 ]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]]
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.882]
 [0.761]
 [0.761]
 [0.761]
 [0.852]
 [0.761]] [[1.44 ]
 [1.849]
 [1.44 ]
 [1.44 ]
 [1.44 ]
 [2.447]
 [1.44 ]] [[0.997]
 [1.376]
 [0.997]
 [0.997]
 [0.997]
 [1.515]
 [0.997]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.567]
 [0.568]
 [0.57 ]
 [0.573]
 [0.559]
 [0.572]] [[4.027]
 [3.585]
 [3.744]
 [3.424]
 [3.157]
 [3.229]
 [3.596]] [[0.555]
 [0.567]
 [0.568]
 [0.57 ]
 [0.573]
 [0.559]
 [0.572]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4377429387496509, 0.41514094605546364, 0.05772475728011857, 0.05694090599020755, 0.01710177878373564, 0.015348673140823766]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.633]
 [0.553]
 [0.554]
 [0.563]
 [0.605]
 [0.664]] [[1.995]
 [1.996]
 [1.788]
 [1.842]
 [2.091]
 [1.808]
 [1.735]] [[0.567]
 [0.633]
 [0.553]
 [0.554]
 [0.563]
 [0.605]
 [0.664]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.5599999999999997  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
first move QE:  0.5116516273678375
3537 5840
from probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
3537 5843
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.669]
 [0.497]
 [0.525]
 [0.396]
 [0.463]
 [0.585]] [[0.686]
 [0.614]
 [0.3  ]
 [0.147]
 [0.811]
 [0.638]
 [0.997]] [[0.789]
 [0.822]
 [0.268]
 [0.221]
 [0.405]
 [0.424]
 [0.908]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
from probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.393]
 [0.341]
 [0.339]
 [0.335]
 [0.334]
 [0.346]] [[-3.065]
 [-2.78 ]
 [-3.299]
 [-3.444]
 [-3.255]
 [-3.222]
 [-3.   ]] [[0.34 ]
 [0.393]
 [0.341]
 [0.339]
 [0.335]
 [0.334]
 [0.346]]
siam score:  -0.59810317
using another actor
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.627]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]] [[4.281]
 [3.638]
 [4.281]
 [4.281]
 [4.281]
 [4.281]
 [4.281]] [[1.532]
 [1.462]
 [1.532]
 [1.532]
 [1.532]
 [1.532]
 [1.532]]
first move QE:  0.511541164647102
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.526]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.588]] [[0.632]
 [0.199]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.965]] [[1.646]
 [1.61 ]
 [1.645]
 [1.645]
 [1.645]
 [1.645]
 [2.012]]
using explorer policy with actor:  1
from probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.0497716240388244
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.207]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[-3.596]
 [-2.748]
 [-3.596]
 [-3.596]
 [-3.596]
 [-3.596]
 [-3.596]] [[0.181]
 [0.207]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
3549 5867
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.442]
 [0.497]
 [0.514]
 [0.526]
 [0.509]
 [0.53 ]] [[1.701]
 [1.693]
 [1.721]
 [1.621]
 [1.665]
 [1.567]
 [1.783]] [[0.21 ]
 [0.029]
 [0.158]
 [0.126]
 [0.178]
 [0.079]
 [0.266]]
siam score:  -0.60557353
from probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
first move QE:  0.5106066953067443
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.339]
 [0.296]
 [0.296]
 [0.296]
 [0.283]
 [0.336]] [[-0.964]
 [-0.543]
 [-1.345]
 [-1.345]
 [-1.345]
 [-0.823]
 [-0.815]] [[0.283]
 [0.339]
 [0.296]
 [0.296]
 [0.296]
 [0.283]
 [0.336]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
siam score:  -0.6068064
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
siam score:  -0.6075631
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using another actor
using another actor
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.088]
 [0.107]
 [0.11 ]
 [0.115]
 [0.123]
 [0.127]] [[-1.952]
 [-0.195]
 [-1.384]
 [-1.707]
 [-1.3  ]
 [-1.187]
 [-1.703]] [[0.082]
 [0.088]
 [0.107]
 [0.11 ]
 [0.115]
 [0.123]
 [0.127]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
siam score:  -0.6137248
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.009]
 [0.008]
 [0.259]
 [0.009]
 [0.014]
 [0.012]] [[0.539]
 [1.217]
 [0.977]
 [0.824]
 [1.154]
 [1.356]
 [1.194]] [[1.387]
 [1.835]
 [1.673]
 [1.739]
 [1.793]
 [1.932]
 [1.821]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.389]
 [0.438]
 [0.433]
 [0.435]
 [0.438]
 [0.438]] [[1.291]
 [0.337]
 [1.089]
 [1.28 ]
 [1.322]
 [1.315]
 [1.721]] [[1.053]
 [0.328]
 [0.927]
 [1.044]
 [1.076]
 [1.078]
 [1.349]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
siam score:  -0.6179043
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.40174010762122164, 0.055861389782123426, 0.0551028413793219, 0.016549729710772426, 0.014853214692567762]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.40174010762122164, 0.055861389782123426, 0.0551028413793219, 0.016549729710772426, 0.014853214692567762]
UNIT TEST: sample policy line 217 mcts : [0.02  0.796 0.02  0.02  0.02  0.02  0.102]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.40174010762122164, 0.055861389782123426, 0.0551028413793219, 0.016549729710772426, 0.014853214692567762]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[1.856]
 [1.856]
 [1.856]
 [1.856]
 [1.856]
 [1.856]
 [1.856]] [[0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.314]
 [0.289]
 [0.314]
 [0.286]
 [0.285]
 [0.293]] [[1.415]
 [1.461]
 [1.357]
 [1.461]
 [1.574]
 [1.472]
 [1.64 ]] [[0.066]
 [0.159]
 [0.04 ]
 [0.159]
 [0.179]
 [0.109]
 [0.236]]
from probs:  [0.45589271681399307, 0.40174010762122164, 0.055861389782123426, 0.0551028413793219, 0.016549729710772426, 0.014853214692567762]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.519]
 [0.541]
 [0.518]
 [0.533]
 [0.553]
 [0.569]] [[1.209]
 [1.282]
 [0.946]
 [1.227]
 [1.343]
 [1.124]
 [0.879]] [[0.515]
 [0.519]
 [0.541]
 [0.518]
 [0.533]
 [0.553]
 [0.569]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]] [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]]
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.084]
 [0.499]
 [0.331]
 [0.331]
 [0.429]
 [0.125]] [[1.48 ]
 [1.62 ]
 [0.593]
 [0.88 ]
 [0.88 ]
 [0.692]
 [1.05 ]] [[1.343]
 [1.206]
 [0.667]
 [0.712]
 [0.712]
 [0.659]
 [0.53 ]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.304]
 [0.3  ]
 [0.31 ]] [[1.478]
 [1.478]
 [1.478]
 [1.478]
 [1.709]
 [1.542]
 [1.59 ]] [[0.819]
 [0.819]
 [0.819]
 [0.819]
 [1.139]
 [0.909]
 [0.992]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.934]
 [0.544]] [[1.431]
 [1.431]
 [1.431]
 [1.431]
 [1.431]
 [1.034]
 [1.431]] [[0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [1.233]
 [0.587]]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.489]
 [0.28 ]] [[1.594]
 [1.594]
 [1.594]
 [1.594]
 [1.594]
 [3.947]
 [1.594]] [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [ 1.196]
 [-0.008]]
line 256 mcts: sample exp_bonus 1.6626045437463706
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.425]
 [0.221]] [[1.612]
 [1.612]
 [1.612]
 [1.612]
 [1.612]
 [3.029]
 [1.612]] [[-0.16 ]
 [-0.16 ]
 [-0.16 ]
 [-0.16 ]
 [-0.16 ]
 [ 0.721]
 [-0.16 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.4017401076212216, 0.05586138978212341, 0.055102841379321886, 0.016549729710772423, 0.014853214692567758]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.348]] [[1.726]
 [1.746]
 [1.746]
 [1.746]
 [1.746]
 [1.545]
 [1.662]] [[0.837]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.667]
 [0.781]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.40174010762122164, 0.055861389782123426, 0.0551028413793219, 0.016549729710772426, 0.014853214692567762]
using explorer policy with actor:  1
from probs:  [0.45589271681399307, 0.40174010762122164, 0.055861389782123426, 0.0551028413793219, 0.016549729710772426, 0.014853214692567762]
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.119]
 [0.132]
 [0.13 ]
 [0.13 ]
 [0.134]
 [0.135]] [[1.252]
 [0.732]
 [1.31 ]
 [1.479]
 [1.394]
 [1.216]
 [1.45 ]] [[ 0.138]
 [-0.407]
 [ 0.196]
 [ 0.361]
 [ 0.275]
 [ 0.107]
 [ 0.342]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
in main func line 156:  3576
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.41 ]
 [0.578]
 [0.494]
 [0.459]
 [0.546]
 [0.408]] [[ 1.37 ]
 [ 1.7  ]
 [-2.19 ]
 [ 1.343]
 [ 3.763]
 [-2.139]
 [ 2.117]] [[1.096]
 [1.455]
 [0.274]
 [1.419]
 [2.227]
 [0.258]
 [1.599]]
3579 5936
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.40174010762122164, 0.055861389782123426, 0.0551028413793219, 0.016549729710772426, 0.014853214692567762]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.40174010762122164, 0.055861389782123426, 0.0551028413793219, 0.016549729710772426, 0.014853214692567762]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.40174010762122164, 0.055861389782123426, 0.0551028413793219, 0.016549729710772426, 0.014853214692567762]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
from probs:  [0.45589271681399307, 0.40174010762122164, 0.055861389782123426, 0.0551028413793219, 0.016549729710772426, 0.014853214692567762]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.45589271681399307, 0.40174010762122164, 0.055861389782123426, 0.0551028413793219, 0.016549729710772426, 0.014853214692567762]
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.456]
 [0.277]
 [0.269]
 [0.304]
 [0.281]
 [0.269]] [[-2.574]
 [-1.66 ]
 [-2.707]
 [-2.875]
 [-2.49 ]
 [-2.555]
 [-2.393]] [[0.274]
 [0.456]
 [0.277]
 [0.269]
 [0.304]
 [0.281]
 [0.269]]
siam score:  -0.6130402
actor:  1 policy actor:  1  step number:  60 total reward:  0.5449999999999997  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.459]
 [0.489]
 [0.481]
 [0.48 ]
 [0.488]
 [0.496]] [[1.51 ]
 [1.285]
 [1.317]
 [1.362]
 [1.346]
 [1.44 ]
 [1.63 ]] [[1.194]
 [0.993]
 [1.075]
 [1.089]
 [1.077]
 [1.155]
 [1.297]]
actor:  1 policy actor:  1  step number:  71 total reward:  0.3299999999999995  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.61433846
siam score:  -0.613582
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
actor:  1 policy actor:  1  step number:  59 total reward:  0.46999999999999964  reward:  1.0 rdn_beta:  0.5
using another actor
Printing some Q and Qe and total Qs values:  [[ 0.356]
 [ 0.356]
 [ 0.356]
 [ 0.356]
 [-0.005]
 [ 0.356]
 [ 0.16 ]] [[0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.722]
 [0.83 ]
 [1.285]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.389]
 [1.667]
 [1.819]]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.422]
 [0.246]
 [0.266]
 [0.235]
 [0.338]
 [0.242]] [[1.03 ]
 [0.898]
 [0.843]
 [0.932]
 [1.216]
 [1.348]
 [0.966]] [[0.803]
 [0.997]
 [0.655]
 [0.781]
 [1.038]
 [1.344]
 [0.78 ]]
using another actor
3588 5961
Printing some Q and Qe and total Qs values:  [[0.89 ]
 [1.031]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.886]
 [0.881]] [[0.674]
 [0.629]
 [0.674]
 [0.674]
 [0.674]
 [0.329]
 [0.572]] [[1.19 ]
 [1.457]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.066]
 [1.137]]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.285]
 [0.37 ]
 [0.37 ]
 [0.348]
 [0.357]
 [0.34 ]] [[1.278]
 [0.886]
 [1.652]
 [1.652]
 [1.445]
 [1.403]
 [1.429]] [[1.114]
 [0.699]
 [1.379]
 [1.379]
 [1.198]
 [1.188]
 [1.171]]
siam score:  -0.5937228
line 256 mcts: sample exp_bonus -2.1012723443836476
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.092]
 [0.375]
 [0.374]
 [0.314]
 [0.368]
 [0.339]] [[-0.773]
 [ 0.362]
 [-1.427]
 [-1.148]
 [-0.676]
 [-1.415]
 [-0.643]] [[0.322]
 [0.248]
 [0.216]
 [0.308]
 [0.346]
 [0.205]
 [0.407]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.566]] [[-1.023]
 [-1.023]
 [-1.023]
 [-1.023]
 [-1.023]
 [-1.023]
 [-1.76 ]] [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.566]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.609]] [[0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]
 [1.813]] [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.609]]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.679]] [[1.403]
 [1.403]
 [1.403]
 [1.403]
 [1.403]
 [1.403]
 [1.194]] [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.679]]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.553]] [[-0.353]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.154]] [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.553]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.534]] [[-0.328]
 [-0.328]
 [-0.328]
 [-0.328]
 [-0.328]
 [-0.328]
 [-0.662]] [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.534]]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.371]
 [0.341]] [[2.947]
 [2.947]
 [2.947]
 [2.947]
 [2.947]
 [7.172]
 [2.947]] [[0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [1.99 ]
 [0.663]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.303]
 [0.302]] [[2.188]
 [2.188]
 [2.188]
 [2.188]
 [2.188]
 [2.422]
 [2.188]] [[1.288]
 [1.288]
 [1.288]
 [1.288]
 [1.288]
 [1.518]
 [1.288]]
from probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
siam score:  -0.5961407
in main func line 156:  3596
start point for exploration sampling:  11106
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
from probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.404]
 [0.401]
 [0.401]] [[2.522]
 [2.522]
 [2.522]
 [2.522]
 [2.483]
 [2.522]
 [2.522]] [[0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.404]
 [0.401]
 [0.401]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
from probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
line 256 mcts: sample exp_bonus 1.1953791942216885
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
3602 5979
siam score:  -0.6046099
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
using another actor
3608 5979
3608 5981
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.16 ]
 [0.272]
 [0.272]
 [0.197]
 [0.272]
 [0.15 ]] [[0.766]
 [1.052]
 [0.766]
 [0.766]
 [0.198]
 [0.766]
 [1.572]] [[1.375]
 [1.464]
 [1.375]
 [1.375]
 [1.049]
 [1.375]
 [1.722]]
from probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
from probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.231]
 [0.227]
 [0.227]
 [0.227]
 [0.235]
 [0.273]] [[-2.3  ]
 [ 0.42 ]
 [-2.244]
 [-2.617]
 [-2.133]
 [-2.071]
 [-1.963]] [[0.225]
 [0.231]
 [0.227]
 [0.227]
 [0.227]
 [0.235]
 [0.273]]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.286]
 [0.286]
 [0.294]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.499]
 [-0.674]
 [ 0.   ]] [[0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.286]
 [0.286]
 [0.294]]
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.194]
 [0.098]
 [0.493]
 [0.16 ]
 [0.515]
 [0.18 ]] [[0.911]
 [1.109]
 [2.041]
 [0.63 ]
 [0.675]
 [0.821]
 [1.35 ]] [[1.382]
 [1.498]
 [1.891]
 [1.44 ]
 [1.27 ]
 [1.545]
 [1.606]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.468]
 [0.468]
 [0.468]
 [0.482]
 [0.468]
 [0.468]] [[1.089]
 [0.679]
 [0.914]
 [0.914]
 [1.455]
 [0.914]
 [0.914]] [[1.229]
 [0.747]
 [1.062]
 [1.062]
 [1.811]
 [1.062]
 [1.062]]
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.335]
 [0.349]
 [0.351]
 [0.353]
 [0.383]
 [0.531]] [[-1.669]
 [ 0.356]
 [-1.695]
 [-1.603]
 [-1.645]
 [-1.274]
 [-2.599]] [[0.357]
 [0.335]
 [0.349]
 [0.351]
 [0.353]
 [0.383]
 [0.531]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7383662977384788
line 256 mcts: sample exp_bonus 0.7753600312066813
from probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.416]
 [0.377]
 [0.374]
 [0.37 ]
 [0.377]
 [0.374]] [[0.657]
 [0.865]
 [0.702]
 [0.529]
 [0.537]
 [0.541]
 [0.43 ]] [[0.692]
 [0.993]
 [0.754]
 [0.576]
 [0.576]
 [0.594]
 [0.477]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
in main func line 156:  3617
line 256 mcts: sample exp_bonus -2.6007224099480113
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.393]
 [0.404]
 [0.405]
 [0.404]
 [0.404]
 [0.611]] [[-0.642]
 [ 1.157]
 [-0.642]
 [-0.318]
 [-0.642]
 [-0.642]
 [-2.298]] [[0.404]
 [0.393]
 [0.404]
 [0.405]
 [0.404]
 [0.404]
 [0.611]]
from probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
siam score:  -0.58015513
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]] [[0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[-0.713]
 [-0.713]
 [-0.713]
 [-0.713]
 [-0.713]
 [-0.713]
 [-0.713]] [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.531]] [[-0.715]
 [-0.715]
 [-0.715]
 [-0.715]
 [-0.715]
 [-0.715]
 [-1.234]] [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.531]]
from probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.508]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[-2.037]
 [-0.492]
 [-2.037]
 [-2.037]
 [-2.037]
 [-2.037]
 [-2.037]] [[0.961]
 [1.456]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]]
from probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.661]] [[-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [ 0.611]] [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.661]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
using another actor
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.253]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[-0.109]
 [ 0.917]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]] [[0.425]
 [0.842]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]]
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.313]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]] [[-0.709]
 [ 1.454]
 [-0.709]
 [-0.709]
 [-0.709]
 [-0.709]
 [-0.709]] [[0.206]
 [1.014]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.3706872983130184, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[2.056]
 [2.056]
 [2.056]
 [2.056]
 [2.056]
 [2.056]
 [2.056]] [[-0.603]
 [-0.603]
 [-0.603]
 [-0.603]
 [-0.603]
 [-0.603]
 [-0.603]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.192]
 [0.216]
 [0.28 ]
 [0.216]
 [0.216]
 [0.295]] [[-0.492]
 [ 0.522]
 [ 0.   ]
 [-1.169]
 [ 0.   ]
 [ 0.   ]
 [-1.594]] [[0.23 ]
 [0.192]
 [0.216]
 [0.28 ]
 [0.216]
 [0.216]
 [0.295]]
3634 6015
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.316]
 [0.338]
 [0.294]
 [0.303]
 [0.291]
 [0.338]] [[-3.73 ]
 [-4.069]
 [ 0.   ]
 [-3.837]
 [-4.249]
 [-3.846]
 [ 0.   ]] [[0.294]
 [0.316]
 [0.338]
 [0.294]
 [0.303]
 [0.291]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.399]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[-3.475]
 [-3.776]
 [-3.475]
 [-3.475]
 [-3.475]
 [-3.475]
 [-3.475]] [[0.356]
 [0.399]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.435]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]] [[-2.749]
 [-2.775]
 [-2.749]
 [-2.749]
 [-2.749]
 [-2.749]
 [-2.749]] [[0.384]
 [0.435]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.332]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.294]] [[-3.328]
 [-3.715]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-2.99 ]] [[0.305]
 [0.332]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.294]]
line 256 mcts: sample exp_bonus 1.1065850700370312
using explorer policy with actor:  1
using explorer policy with actor:  0
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[1.467]
 [1.467]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [1.467]] [[0.496]
 [0.496]
 [1.614]
 [1.614]
 [1.614]
 [1.614]
 [0.496]] [[1.695]
 [1.695]
 [1.587]
 [1.587]
 [1.587]
 [1.587]
 [1.695]]
using another actor
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.423]
 [0.429]
 [0.416]
 [0.41 ]
 [0.427]
 [0.418]] [[1.026]
 [1.519]
 [0.801]
 [0.677]
 [0.891]
 [0.763]
 [0.806]] [[0.433]
 [0.423]
 [0.429]
 [0.416]
 [0.41 ]
 [0.427]
 [0.418]]
siam score:  -0.6002981
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.353]
 [0.355]
 [0.355]] [[0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.979]
 [0.841]
 [0.841]] [[0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.429]
 [0.34 ]
 [0.34 ]]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.564]
 [0.562]
 [0.55 ]
 [0.553]
 [0.575]
 [0.644]] [[-0.622]
 [ 1.729]
 [-0.88 ]
 [-0.885]
 [-0.925]
 [-1.395]
 [-4.245]] [[0.58 ]
 [0.564]
 [0.562]
 [0.55 ]
 [0.553]
 [0.575]
 [0.644]]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.475]
 [0.495]
 [0.494]
 [0.491]
 [0.49 ]
 [0.501]] [[-1.451]
 [-0.724]
 [-1.605]
 [-1.77 ]
 [-1.483]
 [-1.524]
 [-0.84 ]] [[0.624]
 [0.837]
 [0.583]
 [0.525]
 [0.616]
 [0.599]
 [0.851]]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.56 ]
 [0.58 ]
 [0.572]
 [0.576]
 [0.424]
 [0.667]] [[1.348]
 [2.414]
 [2.081]
 [1.813]
 [1.694]
 [1.105]
 [1.992]] [[0.626]
 [0.56 ]
 [0.58 ]
 [0.572]
 [0.576]
 [0.424]
 [0.667]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.3706872983130184, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
from probs:  [0.4715071604104165, 0.3706872983130184, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.609]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.616]
 [0.611]] [[0.86 ]
 [1.92 ]
 [0.725]
 [0.718]
 [0.704]
 [0.449]
 [0.662]] [[0.611]
 [0.609]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.616]
 [0.611]]
from probs:  [0.4715071604104165, 0.3706872983130184, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]] [[-3.312]
 [-3.312]
 [-3.312]
 [-3.312]
 [-3.312]
 [-3.312]
 [-3.312]] [[0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]]
rdn probs:  [0.4715071604104165, 0.3706872983130184, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.326]] [[1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.682]] [[0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.184]]
using explorer policy with actor:  1
3642 6026
using another actor
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.152]
 [0.213]
 [0.208]
 [0.203]
 [0.193]
 [0.209]] [[1.62 ]
 [1.014]
 [1.423]
 [1.541]
 [1.474]
 [1.641]
 [1.606]] [[ 0.4  ]
 [-0.53 ]
 [ 0.14 ]
 [ 0.286]
 [ 0.187]
 [ 0.39 ]
 [ 0.376]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.6694],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.2236],
        [-0.0000],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.9514752239519999 -0.9514752239519999
-0.024259925299500003 -0.693621249300538
-0.9320711845499999 -0.9320711845499999
-0.9507464999999999 -0.9507464999999999
-0.9368741646 -0.9368741646
-0.97515 -0.97515
-0.0727797758985 -0.2963740274567293
-0.9560860847999999 -0.9560860847999999
-0.9413375399999999 -0.9413375399999999
-0.945846 -0.945846
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.342]
 [0.342]
 [0.342]
 [0.318]
 [0.339]
 [0.349]] [[1.703]
 [1.891]
 [1.891]
 [1.891]
 [1.907]
 [2.044]
 [1.891]] [[1.458]
 [1.709]
 [1.709]
 [1.709]
 [1.683]
 [1.899]
 [1.723]]
using another actor
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.645]
 [0.415]
 [0.425]
 [0.415]
 [0.415]
 [0.461]] [[2.316]
 [2.409]
 [2.316]
 [1.815]
 [2.316]
 [2.316]
 [2.407]] [[1.172]
 [1.452]
 [1.172]
 [1.004]
 [1.172]
 [1.172]
 [1.255]]
first move QE:  0.5023645473694242
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.3706872983130184, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.649]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.574]] [[1.609]
 [1.108]
 [1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.349]] [[0.985]
 [1.288]
 [1.594]
 [1.594]
 [1.594]
 [1.594]
 [1.218]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.06677550083640156
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.645]
 [0.427]] [[-0.718]
 [-0.718]
 [-0.718]
 [-0.718]
 [-0.718]
 [ 3.64 ]
 [-0.718]] [[-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [ 1.158]
 [-0.067]]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.296]
 [0.309]
 [0.309]
 [0.308]
 [0.309]
 [0.312]] [[1.639]
 [1.579]
 [1.639]
 [1.639]
 [1.409]
 [1.639]
 [1.797]] [[0.423]
 [0.358]
 [0.423]
 [0.423]
 [0.268]
 [0.423]
 [0.535]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.173]
 [0.19 ]
 [0.182]
 [0.174]
 [0.185]
 [0.193]] [[1.176]
 [1.05 ]
 [0.958]
 [0.937]
 [1.22 ]
 [1.07 ]
 [1.51 ]] [[-0.063]
 [-0.175]
 [-0.203]
 [-0.231]
 [-0.059]
 [-0.137]
 [ 0.171]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.3706872983130184, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.435654822083384
from probs:  [0.4715071604104165, 0.3706872983130184, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.188]
 [0.178]
 [0.178]
 [0.192]
 [0.178]
 [0.178]] [[1.447]
 [1.814]
 [1.04 ]
 [1.04 ]
 [2.17 ]
 [1.04 ]
 [1.04 ]] [[0.141]
 [0.188]
 [0.178]
 [0.178]
 [0.192]
 [0.178]
 [0.178]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.641]
 [1.095]
 [0.641]
 [0.69 ]
 [0.641]
 [0.641]
 [0.641]] [[ 1.111]
 [ 0.555]
 [ 1.111]
 [-0.137]
 [ 1.111]
 [ 1.111]
 [ 1.111]] [[0.977]
 [1.699]
 [0.977]
 [0.657]
 [0.977]
 [0.977]
 [0.977]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4715071604104165, 0.37068729831301844, 0.07798628646008195, 0.05084362505206998, 0.015270505677469126, 0.013705124086944236]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.124]
 [0.064]
 [0.296]
 [0.232]
 [0.044]
 [0.214]] [[1.585]
 [1.708]
 [1.186]
 [1.042]
 [1.165]
 [1.438]
 [1.907]] [[-0.037]
 [ 0.285]
 [-0.183]
 [ 0.186]
 [ 0.14 ]
 [-0.055]
 [ 0.598]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.5499999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4866030326088718, 0.36009898441028015, 0.07575868577102102, 0.0493913274835953, 0.014834318874461771, 0.013313650851770135]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.474]
 [0.496]
 [0.495]
 [0.493]
 [0.49 ]
 [0.493]] [[-1.21 ]
 [ 1.097]
 [-0.931]
 [-1.547]
 [-1.437]
 [-1.019]
 [-1.162]] [[0.497]
 [1.637]
 [0.653]
 [0.341]
 [0.393]
 [0.598]
 [0.531]]
Printing some Q and Qe and total Qs values:  [[ 0.134]
 [ 0.345]
 [ 0.134]
 [ 0.411]
 [ 0.134]
 [-0.006]
 [ 0.388]] [[2.543]
 [1.905]
 [2.543]
 [1.675]
 [2.543]
 [1.385]
 [1.736]] [[1.814]
 [1.698]
 [1.814]
 [1.645]
 [1.814]
 [1.041]
 [1.653]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4866030326088718, 0.3600989844102802, 0.07575868577102102, 0.0493913274835953, 0.014834318874461771, 0.013313650851770135]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.592]
 [0.67 ]] [[0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.799]
 [1.224]] [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.592]
 [0.67 ]]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.592]
 [0.632]
 [0.708]] [[0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.748]
 [1.08 ]
 [1.719]] [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.592]
 [0.632]
 [0.708]]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.799]] [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.998]] [[0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.799]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4866030326088718, 0.36009898441028015, 0.07575868577102102, 0.0493913274835953, 0.014834318874461771, 0.013313650851770135]
actor:  1 policy actor:  1  step number:  59 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  0.5
in main func line 156:  3668
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.545]
 [0.598]
 [0.734]] [[-0.178]
 [-0.178]
 [-0.178]
 [-0.178]
 [-0.608]
 [-0.369]
 [-0.435]] [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.545]
 [0.598]
 [0.734]]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.843]] [[ 0.039]
 [ 0.039]
 [ 0.039]
 [ 0.039]
 [ 0.039]
 [ 0.039]
 [-0.928]] [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.843]]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[1.34]
 [1.34]
 [1.34]
 [1.34]
 [1.34]
 [1.34]
 [1.34]] [[0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.406]
 [0.245]
 [0.245]
 [0.245]
 [0.245]
 [0.245]] [[-1.306]
 [-0.044]
 [-1.306]
 [-1.306]
 [-1.306]
 [-1.306]
 [-1.306]] [[0.429]
 [1.006]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]] [[-2.075]
 [-2.075]
 [-2.075]
 [-2.075]
 [-2.075]
 [-2.075]
 [-2.075]] [[0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.495]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[4.336]
 [4.061]
 [4.336]
 [4.336]
 [4.336]
 [4.336]
 [4.336]] [[1.884]
 [1.776]
 [1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.044]
 [0.06 ]
 [0.048]
 [0.053]
 [0.063]
 [0.04 ]] [[ 0.56 ]
 [-0.484]
 [-0.376]
 [-0.159]
 [-0.243]
 [-0.482]
 [ 0.509]] [[0.047]
 [0.044]
 [0.06 ]
 [0.048]
 [0.053]
 [0.063]
 [0.04 ]]
siam score:  -0.595659
from probs:  [0.4739263269441135, 0.35071789031583234, 0.09983649523487526, 0.048104612689516955, 0.014447863629617719, 0.01296681118604428]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.373]
 [0.374]
 [0.374]] [[1.756]
 [1.756]
 [1.756]
 [1.756]
 [1.566]
 [1.467]
 [1.694]] [[-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.266]
 [-0.363]
 [-0.136]]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]] [[2.195]
 [2.195]
 [2.195]
 [2.195]
 [2.195]
 [2.195]
 [2.195]] [[0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]]
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]] [[3.962]
 [3.962]
 [3.962]
 [3.962]
 [3.962]
 [3.962]
 [3.962]] [[0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.347]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.412]] [[1.347]
 [2.935]
 [1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.428]] [[0.44 ]
 [0.347]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.412]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.42978472001798057
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4739263269441136, 0.35071789031583245, 0.09983649523487528, 0.04810461268951697, 0.014447863629617724, 0.012966811186044284]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.398]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[0.093]
 [1.099]
 [0.093]
 [0.093]
 [0.093]
 [0.093]
 [0.093]] [[0.41]
 [0.75]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.375]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[0.093]
 [1.181]
 [0.093]
 [0.093]
 [0.093]
 [0.093]
 [0.093]] [[0.41 ]
 [0.733]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  55 total reward:  0.5599999999999997  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.392]
 [0.393]
 [0.392]
 [0.394]
 [0.39 ]
 [0.392]] [[1.968]
 [2.009]
 [1.867]
 [2.009]
 [1.97 ]
 [1.845]
 [1.991]] [[0.961]
 [0.997]
 [0.856]
 [0.997]
 [0.962]
 [0.829]
 [0.979]]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.254]
 [0.488]
 [0.491]
 [0.492]
 [0.483]
 [0.493]] [[-1.433]
 [ 0.922]
 [-1.337]
 [-1.268]
 [-0.788]
 [-0.924]
 [-0.752]] [[0.274]
 [1.501]
 [0.33 ]
 [0.372]
 [0.65 ]
 [0.566]
 [0.671]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4883450341369873, 0.3411053610698969, 0.09710016139860385, 0.04678615415315325, 0.01405187438722144, 0.01261141485413738]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4883450341369873, 0.3411053610698969, 0.09710016139860385, 0.04678615415315325, 0.01405187438722144, 0.01261141485413738]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4883450341369873, 0.3411053610698969, 0.09710016139860385, 0.04678615415315325, 0.01405187438722144, 0.01261141485413738]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4883450341369873, 0.3411053610698969, 0.09710016139860385, 0.04678615415315325, 0.01405187438722144, 0.01261141485413738]
Printing some Q and Qe and total Qs values:  [[ 0.006]
 [-0.008]
 [ 0.018]
 [ 0.011]
 [ 0.007]
 [ 0.007]
 [ 0.007]] [[2.113]
 [1.492]
 [1.874]
 [2.058]
 [2.022]
 [1.944]
 [2.093]] [[ 0.267]
 [-0.383]
 [ 0.052]
 [ 0.221]
 [ 0.177]
 [ 0.099]
 [ 0.248]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.476]
 [0.5  ]
 [0.505]
 [0.505]
 [0.505]
 [0.563]] [[3.453]
 [3.506]
 [4.689]
 [3.453]
 [3.453]
 [3.453]
 [4.206]] [[1.735]
 [1.694]
 [2.137]
 [1.735]
 [1.735]
 [1.735]
 [2.102]]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.05 ]
 [-0.041]
 [-0.035]
 [-0.04 ]
 [-0.041]
 [-0.04 ]] [[2.278]
 [1.612]
 [1.844]
 [1.972]
 [2.144]
 [1.908]
 [2.087]] [[ 0.136]
 [-0.55 ]
 [-0.298]
 [-0.159]
 [ 0.002]
 [-0.234]
 [-0.055]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[3.698]
 [3.698]
 [3.698]
 [3.698]
 [3.698]
 [3.698]
 [3.698]] [[0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.286 0.041 0.041 0.02  0.224 0.082 0.306]
siam score:  -0.5959143
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4883450341369873, 0.34110536106989686, 0.09710016139860385, 0.04678615415315325, 0.01405187438722144, 0.01261141485413738]
actor:  1 policy actor:  1  step number:  75 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.482]
 [1.482]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [1.482]] [[0.497]
 [0.491]
 [1.966]
 [1.966]
 [1.966]
 [1.966]
 [0.499]] [[1.468]
 [1.464]
 [1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.47 ]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.427]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[-0.183]
 [-0.245]
 [-0.183]
 [-0.183]
 [-0.183]
 [-0.183]
 [-0.183]] [[0.402]
 [0.427]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]]
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.731]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[1.847]
 [0.955]
 [1.847]
 [1.847]
 [1.847]
 [1.847]
 [1.847]] [[0.646]
 [0.731]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
using another actor
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.13 ]
 [0.244]
 [0.243]
 [0.245]
 [0.235]
 [0.233]] [[1.712]
 [1.234]
 [1.709]
 [1.837]
 [1.795]
 [1.968]
 [1.803]] [[ 0.543]
 [-0.173]
 [ 0.531]
 [ 0.658]
 [ 0.618]
 [ 0.771]
 [ 0.602]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4987813829710964, 0.33414775335606245, 0.0951195861597906, 0.04583184576581037, 0.01376525494118368, 0.012354176806056594]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.36892328671649055
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.811]
 [0.68 ]
 [0.676]
 [0.68 ]
 [0.68 ]
 [0.666]] [[2.48 ]
 [2.21 ]
 [2.48 ]
 [3.029]
 [2.48 ]
 [2.48 ]
 [2.751]] [[0.68 ]
 [0.811]
 [0.68 ]
 [0.676]
 [0.68 ]
 [0.68 ]
 [0.666]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4987813829710964, 0.3341477533560625, 0.0951195861597906, 0.04583184576581037, 0.01376525494118368, 0.012354176806056594]
3703 6066
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.253]
 [0.305]
 [0.305]
 [0.426]
 [0.305]
 [0.305]] [[1.449]
 [1.814]
 [1.407]
 [1.407]
 [1.329]
 [1.407]
 [1.407]] [[1.948]
 [2.048]
 [1.856]
 [1.856]
 [1.894]
 [1.856]
 [1.856]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.36 ]
 [0.363]
 [0.356]
 [0.37 ]
 [0.367]
 [0.373]] [[-2.021]
 [ 0.128]
 [-2.243]
 [-2.527]
 [-1.568]
 [-1.433]
 [-1.325]] [[0.347]
 [0.36 ]
 [0.363]
 [0.356]
 [0.37 ]
 [0.367]
 [0.373]]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.36 ]
 [0.399]
 [0.388]
 [0.392]
 [0.375]
 [0.356]] [[-2.021]
 [ 0.128]
 [-0.769]
 [-0.738]
 [-0.611]
 [-1.099]
 [-1.877]] [[0.347]
 [0.36 ]
 [0.399]
 [0.388]
 [0.392]
 [0.375]
 [0.356]]
siam score:  -0.6099266
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]]
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [1.38 ]] [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.521]] [[1.428]
 [1.428]
 [1.428]
 [1.428]
 [1.428]
 [1.428]
 [2.567]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4987813829710964, 0.3341477533560625, 0.0951195861597906, 0.04583184576581037, 0.01376525494118368, 0.012354176806056594]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4987813829710964, 0.33414775335606245, 0.0951195861597906, 0.04583184576581037, 0.01376525494118368, 0.012354176806056594]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
siam score:  -0.6013941
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4987813829710964, 0.33414775335606245, 0.0951195861597906, 0.04583184576581037, 0.01376525494118368, 0.012354176806056594]
using explorer policy with actor:  1
first move QE:  0.5070977942483553
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4987813829710964, 0.3341477533560625, 0.0951195861597906, 0.04583184576581037, 0.01376525494118368, 0.012354176806056594]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
using another actor
from probs:  [0.4987813829710964, 0.3341477533560625, 0.0951195861597906, 0.04583184576581037, 0.01376525494118368, 0.012354176806056594]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4987813829710964, 0.33414775335606245, 0.0951195861597906, 0.04583184576581037, 0.01376525494118368, 0.012354176806056594]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.934650491605389
using another actor
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4987813829710964, 0.33414775335606245, 0.0951195861597906, 0.04583184576581037, 0.01376525494118368, 0.012354176806056594]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.3055106162498347
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.433]
 [0.475]
 [0.475]
 [0.475]
 [0.481]
 [0.486]] [[2.466]
 [3.315]
 [1.994]
 [1.994]
 [1.994]
 [1.864]
 [2.265]] [[0.482]
 [0.433]
 [0.475]
 [0.475]
 [0.475]
 [0.481]
 [0.486]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.249]
 [0.274]
 [0.291]
 [0.278]
 [0.268]
 [0.316]] [[2.407]
 [0.758]
 [2.705]
 [2.824]
 [2.818]
 [2.394]
 [3.099]] [[1.321]
 [0.421]
 [1.442]
 [1.521]
 [1.504]
 [1.278]
 [1.687]]
Printing some Q and Qe and total Qs values:  [[1.474]
 [1.474]
 [1.474]
 [1.474]
 [1.474]
 [1.474]
 [1.474]] [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[1.779]
 [1.779]
 [1.779]
 [1.779]
 [1.779]
 [1.779]
 [1.779]]
line 256 mcts: sample exp_bonus 0.5085721646876503
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
siam score:  -0.6124208
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.443]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[-0.467]
 [ 0.903]
 [-0.467]
 [-0.467]
 [-0.467]
 [-0.467]
 [-0.467]] [[0.526]
 [0.9  ]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]]
line 256 mcts: sample exp_bonus 0.48214243441084137
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.701]
 [0.139]
 [0.412]
 [0.175]
 [0.134]
 [0.27 ]] [[2.041]
 [4.304]
 [1.952]
 [2.309]
 [2.567]
 [2.448]
 [2.566]] [[0.454]
 [2.135]
 [0.29 ]
 [0.769]
 [0.655]
 [0.548]
 [0.755]]
from probs:  [0.4902801648777422, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
3727 6093
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.408]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]] [[4.045]
 [3.833]
 [4.045]
 [4.045]
 [4.045]
 [4.045]
 [4.045]] [[0.815]
 [0.704]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.515]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[2.149]
 [2.591]
 [2.149]
 [2.149]
 [2.149]
 [2.149]
 [2.149]] [[0.482]
 [0.515]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]]
first move QE:  0.5113331188835496
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.365]
 [0.364]
 [0.365]
 [0.365]
 [0.373]
 [0.366]] [[-2.301]
 [-0.481]
 [-2.4  ]
 [-2.478]
 [-2.175]
 [-2.247]
 [-2.036]] [[0.36 ]
 [0.365]
 [0.364]
 [0.365]
 [0.365]
 [0.373]
 [0.366]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
siam score:  -0.5996509
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[-1.518]
 [-1.518]
 [-1.518]
 [-1.518]
 [-1.518]
 [-1.518]
 [-1.518]] [[0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.6350],
        [-0.0000],
        [-0.7825],
        [-0.5892],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.9703485 -0.9703485
-0.7547264999999999 -0.7547264999999999
-0.965448 -0.965448
-0.5238178802999996 -0.5238178802999996
-0.024259925299500003 -0.6592157033448359
-0.8037314999999999 -0.8037314999999999
-0.024259925299500003 -0.8067928022624263
-0.0337698257985 -0.622971214639375
0.97515 0.97515
-0.9405 -0.9405
3731 6097
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.475]
 [0.444]
 [0.467]
 [0.474]
 [0.49 ]
 [0.68 ]] [[-0.106]
 [ 0.809]
 [-0.356]
 [-0.147]
 [ 0.217]
 [-0.175]
 [ 1.325]] [[0.473]
 [0.475]
 [0.444]
 [0.467]
 [0.474]
 [0.49 ]
 [0.68 ]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
Printing some Q and Qe and total Qs values:  [[1.382]
 [1.405]
 [1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.388]] [[0.542]
 [0.541]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.545]] [[2.099]
 [2.115]
 [2.099]
 [2.099]
 [2.099]
 [2.099]
 [2.105]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
rdn beta is 0 so we're just using the maxi policy
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.04202160976846896
line 256 mcts: sample exp_bonus -0.0047373331497307936
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8236569297780252
line 256 mcts: sample exp_bonus 1.029121168892561
Printing some Q and Qe and total Qs values:  [[0.966]
 [1.4  ]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]] [[0.75 ]
 [0.502]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]] [[1.471]
 [2.045]
 [1.471]
 [1.471]
 [1.471]
 [1.471]
 [1.471]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
line 256 mcts: sample exp_bonus -4.1904933774631985
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.383]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[-4.26 ]
 [-4.918]
 [-4.26 ]
 [-4.26 ]
 [-4.26 ]
 [-4.26 ]
 [-4.26 ]] [[0.337]
 [0.383]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]]
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.25 ]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[-0.911]
 [-0.075]
 [-0.911]
 [-0.911]
 [-0.911]
 [-0.911]
 [-0.911]] [[0.795]
 [1.158]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
siam score:  -0.59660935
3736 6119
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.555]] [[-0.656]
 [-0.656]
 [-0.656]
 [-0.656]
 [-0.656]
 [-0.656]
 [-0.16 ]] [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.555]]
from probs:  [0.4902801648777422, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.749]] [[ 0.436]
 [ 0.436]
 [ 0.436]
 [ 0.436]
 [ 0.436]
 [ 0.436]
 [-2.017]] [[0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.749]]
line 256 mcts: sample exp_bonus 2.7458404789310356
siam score:  -0.59910965
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.389]
 [0.519]
 [0.508]
 [0.514]
 [0.508]
 [0.496]] [[0.491]
 [1.139]
 [0.344]
 [0.386]
 [0.56 ]
 [0.418]
 [0.578]] [[0.447]
 [0.597]
 [0.327]
 [0.333]
 [0.461]
 [0.355]
 [0.436]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.775]] [[1.51]
 [1.51]
 [1.51]
 [1.51]
 [1.51]
 [1.51]
 [1.35]] [[1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.434]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.606]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[-2.514]
 [-0.243]
 [-2.514]
 [-2.514]
 [-2.514]
 [-2.514]
 [-2.514]] [[0.348]
 [0.606]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
from probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.375]
 [0.374]
 [0.363]
 [0.373]
 [0.364]
 [0.359]] [[-3.227]
 [-2.054]
 [-3.367]
 [-3.275]
 [-2.994]
 [-2.949]
 [-3.01 ]] [[0.36 ]
 [0.375]
 [0.374]
 [0.363]
 [0.373]
 [0.364]
 [0.359]]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.387]
 [0.381]
 [0.383]
 [0.388]
 [0.367]
 [0.388]] [[-2.547]
 [ 0.282]
 [-2.941]
 [-2.564]
 [ 0.   ]
 [-2.784]
 [ 0.   ]] [[0.394]
 [0.387]
 [0.381]
 [0.383]
 [0.388]
 [0.367]
 [0.388]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.516]
 [0.516]
 [0.516]
 [0.608]
 [0.516]
 [0.516]] [[0.526]
 [0.954]
 [0.954]
 [0.954]
 [0.955]
 [0.954]
 [0.954]] [[0.748]
 [0.918]
 [0.918]
 [0.918]
 [1.1  ]
 [0.918]
 [0.918]]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.486]
 [0.495]
 [0.51 ]
 [0.511]
 [0.509]
 [0.752]] [[0.967]
 [2.265]
 [0.841]
 [0.744]
 [0.676]
 [0.365]
 [0.092]] [[0.514]
 [0.486]
 [0.495]
 [0.51 ]
 [0.511]
 [0.509]
 [0.752]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.498]
 [0.508]
 [0.508]
 [0.517]
 [0.542]
 [0.794]] [[ 0.734]
 [ 1.395]
 [ 0.734]
 [ 0.734]
 [ 0.849]
 [ 0.806]
 [-0.068]] [[0.508]
 [0.498]
 [0.508]
 [0.508]
 [0.517]
 [0.542]
 [0.794]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.403]
 [0.38 ]
 [0.357]
 [0.369]
 [0.357]
 [0.374]] [[0.741]
 [2.034]
 [1.094]
 [0.456]
 [1.434]
 [0.742]
 [1.727]] [[0.353]
 [0.403]
 [0.38 ]
 [0.357]
 [0.369]
 [0.357]
 [0.374]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6392295454384957
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.483]
 [0.624]
 [0.483]
 [0.644]
 [0.539]
 [0.579]] [[1.913]
 [2.279]
 [1.965]
 [2.279]
 [2.039]
 [2.16 ]
 [2.147]] [[1.324]
 [1.451]
 [1.313]
 [1.451]
 [1.452]
 [1.403]
 [1.466]]
using explorer policy with actor:  1
siam score:  -0.6092215
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.214]
 [0.214]
 [0.203]
 [0.203]
 [0.214]
 [0.214]] [[1.313]
 [1.218]
 [1.218]
 [0.958]
 [1.144]
 [1.218]
 [1.218]] [[ 0.047]
 [-0.028]
 [-0.028]
 [-0.31 ]
 [-0.124]
 [-0.028]
 [-0.028]]
line 256 mcts: sample exp_bonus 2.194424139685735
siam score:  -0.61204356
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.198]
 [0.177]] [[1.537]
 [1.537]
 [1.537]
 [1.537]
 [1.537]
 [1.983]
 [1.799]] [[0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.818]
 [0.53 ]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777423, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
line 256 mcts: sample exp_bonus 3.462822360302649
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.208]] [[0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]
 [1.728]] [[-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [ 0.671]]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.11 ]
 [0.036]
 [0.036]
 [0.161]
 [0.1  ]
 [0.171]] [[1.581]
 [1.746]
 [1.634]
 [1.634]
 [1.341]
 [1.461]
 [1.549]] [[-0.5  ]
 [-0.522]
 [-0.783]
 [-0.783]
 [-0.826]
 [-0.826]
 [-0.596]]
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.016]
 [ 0.049]
 [ 0.049]
 [ 0.015]
 [ 0.049]
 [ 0.049]] [[1.624]
 [1.707]
 [1.349]
 [1.349]
 [1.54 ]
 [1.349]
 [1.349]] [[-0.844]
 [-0.767]
 [-0.996]
 [-0.996]
 [-0.874]
 [-0.996]
 [-0.996]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777422, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
line 256 mcts: sample exp_bonus 1.621738688333328
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.4902801648777422, 0.3398152661542162, 0.09673291878435938, 0.04660920419432552, 0.013998728779524518, 0.012563717209832315]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.058]
 [0.059]
 [0.059]
 [0.059]
 [0.058]
 [0.058]] [[0.501]
 [0.506]
 [0.511]
 [0.51 ]
 [0.51 ]
 [0.498]
 [0.502]] [[-0.706]
 [-0.701]
 [-0.695]
 [-0.696]
 [-0.696]
 [-0.709]
 [-0.705]]
actor:  1 policy actor:  1  step number:  133 total reward:  0.10999999999999932  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.276]
 [0.437]
 [0.404]
 [0.397]
 [0.439]
 [0.327]] [[ 1.927]
 [ 0.884]
 [-1.419]
 [ 0.338]
 [ 1.056]
 [-1.464]
 [ 0.763]] [[1.732]
 [1.226]
 [0.205]
 [1.136]
 [1.528]
 [0.182]
 [1.245]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.48360281524358856, 0.33518716674150495, 0.10903492597610913, 0.04597441214098439, 0.013808073694553084, 0.012392606203259975]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.48360281524358856, 0.33518716674150495, 0.10903492597610913, 0.04597441214098439, 0.013808073694553084, 0.012392606203259975]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.48360281524358856, 0.33518716674150495, 0.10903492597610913, 0.04597441214098439, 0.013808073694553084, 0.012392606203259975]
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.93 ]
 [0.498]
 [0.619]
 [0.662]
 [0.363]
 [0.619]] [[1.018]
 [1.178]
 [0.768]
 [1.333]
 [1.06 ]
 [1.233]
 [0.949]] [[1.383]
 [1.843]
 [0.705]
 [1.324]
 [1.229]
 [0.745]
 [1.068]]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.317]
 [0.367]
 [0.419]
 [0.39 ]
 [0.36 ]
 [0.334]] [[2.006]
 [2.207]
 [1.938]
 [2.03 ]
 [1.985]
 [2.022]
 [2.364]] [[0.232]
 [0.236]
 [0.067]
 [0.264]
 [0.162]
 [0.138]
 [0.428]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.48360281524358856, 0.33518716674150495, 0.10903492597610913, 0.04597441214098439, 0.013808073694553084, 0.012392606203259975]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.013]
 [0.015]
 [0.013]
 [0.014]
 [0.016]
 [0.016]] [[1.687]
 [1.419]
 [1.218]
 [1.57 ]
 [1.348]
 [1.773]
 [1.636]] [[1.572]
 [1.451]
 [1.361]
 [1.519]
 [1.419]
 [1.613]
 [1.55 ]]
using another actor
from probs:  [0.48360281524358856, 0.33518716674150495, 0.10903492597610913, 0.04597441214098439, 0.013808073694553084, 0.012392606203259975]
siam score:  -0.61380845
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]] [[1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.159]] [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.165]
 [0.178]
 [0.18 ]
 [0.18 ]
 [0.183]
 [0.182]] [[-1.822]
 [ 0.457]
 [-1.676]
 [-1.943]
 [-1.725]
 [-1.403]
 [-0.983]] [[0.18 ]
 [0.165]
 [0.178]
 [0.18 ]
 [0.18 ]
 [0.183]
 [0.182]]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.481]
 [0.517]
 [0.487]
 [0.27 ]
 [0.511]
 [0.496]] [[ 2.276]
 [-0.434]
 [-2.743]
 [-0.287]
 [ 0.426]
 [-2.78 ]
 [ 0.977]] [[1.9  ]
 [0.998]
 [0.248]
 [1.051]
 [1.144]
 [0.232]
 [1.48 ]]
line 256 mcts: sample exp_bonus 4.985229955291528
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.49528442504276915, 0.3191283600869123, 0.1116687060526248, 0.04708494153917902, 0.014141612962509688, 0.01269195431600508]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.49528442504276915, 0.3191283600869123, 0.1116687060526248, 0.04708494153917902, 0.014141612962509688, 0.01269195431600508]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
actor:  1 policy actor:  1  step number:  68 total reward:  0.5449999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[2.375]
 [2.375]
 [2.375]
 [2.375]
 [2.204]
 [2.375]
 [2.375]] [[2.311]
 [2.311]
 [2.311]
 [2.311]
 [2.243]
 [2.311]
 [2.311]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.013]
 [0.19 ]
 [0.19 ]
 [0.161]
 [0.185]
 [0.186]] [[1.61 ]
 [0.403]
 [1.485]
 [1.775]
 [1.785]
 [1.736]
 [1.701]] [[ 1.24 ]
 [-0.224]
 [ 1.108]
 [ 1.394]
 [ 1.361]
 [ 1.348]
 [ 1.314]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
using explorer policy with actor:  1
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[2.449]
 [2.449]
 [2.449]
 [2.449]
 [2.449]
 [2.449]
 [2.449]] [[1.21]
 [1.21]
 [1.21]
 [1.21]
 [1.21]
 [1.21]
 [1.21]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.5085795700198837, 0.31072192679223176, 0.10872714508236497, 0.0458446367911922, 0.013769096630786142, 0.012357624683541289]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.729]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[1.269]
 [0.951]
 [1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.269]] [[0.758]
 [1.497]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.286]
 [0.283]
 [0.283]] [[1.999]
 [1.999]
 [1.999]
 [1.999]
 [1.528]
 [1.999]
 [1.999]] [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.367]
 [0.675]
 [0.675]]
using another actor
in main func line 156:  3790
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.42 ]
 [0.465]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[1.545]
 [0.888]
 [1.216]
 [1.078]
 [1.078]
 [1.078]
 [1.078]] [[1.026]
 [0.487]
 [0.795]
 [0.778]
 [0.778]
 [0.778]
 [0.778]]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.456]] [[1.723]
 [1.651]
 [1.651]
 [1.651]
 [1.651]
 [1.651]
 [1.473]] [[1.114]
 [1.057]
 [1.057]
 [1.057]
 [1.057]
 [1.057]
 [0.949]]
in main func line 156:  3792
siam score:  -0.60197127
siam score:  -0.6009497
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  94 total reward:  0.35499999999999954  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.878]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[2.5  ]
 [1.876]
 [2.5  ]
 [2.5  ]
 [2.5  ]
 [2.5  ]
 [2.5  ]] [[0.534]
 [0.496]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.444]
 [0.519]
 [0.519]
 [0.519]
 [0.525]
 [0.519]] [[-0.255]
 [-0.24 ]
 [-0.255]
 [-0.255]
 [-0.255]
 [-0.484]
 [-0.255]] [[0.29 ]
 [0.146]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.226]
 [0.29 ]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
using another actor
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.49848380971955725, 0.3045538180480537, 0.12641971009256436, 0.04493457966073903, 0.013495767721543848, 0.0121123147575418]
siam score:  -0.5931641
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.49848380971955725, 0.3045538180480537, 0.12641971009256436, 0.04493457966073903, 0.013495767721543848, 0.0121123147575418]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.49848380971955725, 0.3045538180480537, 0.12641971009256436, 0.04493457966073903, 0.013495767721543848, 0.0121123147575418]
actor:  1 policy actor:  1  step number:  62 total reward:  0.4149999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.5091119899354812, 0.2980996837122262, 0.12374061121649958, 0.04398232164830265, 0.013209764090401635, 0.011855629397088804]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.384]
 [0.391]
 [0.374]
 [0.349]
 [0.389]
 [0.376]] [[0.949]
 [2.202]
 [2.933]
 [0.518]
 [0.596]
 [2.597]
 [1.824]] [[ 0.162]
 [ 0.587]
 [ 0.846]
 [ 0.005]
 [-0.02 ]
 [ 0.73 ]
 [ 0.445]]
siam score:  -0.59840715
using another actor
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.425]
 [0.559]
 [0.469]
 [0.559]
 [0.526]
 [0.363]] [[ 0.422]
 [-0.068]
 [ 0.407]
 [-1.169]
 [ 0.407]
 [-3.848]
 [ 0.545]] [[1.728]
 [1.56 ]
 [1.825]
 [1.154]
 [1.825]
 [0.137]
 [1.764]]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.021]
 [-0.012]
 [-0.012]
 [-0.01 ]
 [-0.013]
 [-0.011]] [[1.878]
 [0.975]
 [1.802]
 [1.886]
 [1.944]
 [1.908]
 [1.946]] [[ 0.034]
 [-0.589]
 [-0.019]
 [ 0.037]
 [ 0.079]
 [ 0.048]
 [ 0.079]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6049999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.5138781533682568, 0.29520535387996066, 0.1225391804130626, 0.04355528548357504, 0.013081506945650858, 0.011740519909494028]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.5138781533682568, 0.29520535387996066, 0.1225391804130626, 0.04355528548357504, 0.013081506945650858, 0.011740519909494028]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]] [[-2.489]
 [-2.489]
 [-2.489]
 [-2.489]
 [-2.489]
 [-2.489]
 [-2.489]] [[0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -4.220670880419408
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.773]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]] [[-4.128]
 [-0.458]
 [-4.128]
 [-4.128]
 [-4.128]
 [-4.128]
 [-4.128]] [[0.4  ]
 [0.773]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.504]
 [0.403]
 [0.401]
 [0.399]
 [0.403]
 [0.403]] [[ 0.   ]
 [ 0.24 ]
 [ 0.   ]
 [-3.097]
 [-3.332]
 [ 0.   ]
 [ 0.   ]] [[0.403]
 [0.504]
 [0.403]
 [0.401]
 [0.399]
 [0.403]
 [0.403]]
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.564]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.624]] [[-1.352]
 [-0.302]
 [-1.352]
 [-1.352]
 [-1.352]
 [-1.352]
 [-0.749]] [[0.493]
 [0.564]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.624]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.5138781533682568, 0.29520535387996066, 0.1225391804130626, 0.04355528548357504, 0.013081506945650858, 0.011740519909494028]
Printing some Q and Qe and total Qs values:  [[1.426]
 [1.425]
 [1.426]
 [1.425]
 [1.426]
 [1.425]
 [1.425]] [[0.507]
 [0.515]
 [0.515]
 [0.515]
 [0.506]
 [0.515]
 [0.515]] [[2.614]
 [2.624]
 [2.624]
 [2.624]
 [2.615]
 [2.624]
 [2.624]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.7976449415409659
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.5138781533682568, 0.29520535387996066, 0.1225391804130626, 0.04355528548357504, 0.013081506945650858, 0.011740519909494028]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.622]
 [0.683]
 [0.718]
 [0.736]
 [0.58 ]
 [0.625]] [[-0.5  ]
 [-0.225]
 [-0.697]
 [-0.373]
 [-0.253]
 [-0.67 ]
 [-0.473]] [[0.593]
 [0.622]
 [0.683]
 [0.718]
 [0.736]
 [0.58 ]
 [0.625]]
maxi score, test score, baseline:  -0.9977199999999999 -1.0 -0.9977199999999999
probs:  [0.5138781533682568, 0.29520535387996066, 0.1225391804130626, 0.04355528548357504, 0.013081506945650858, 0.011740519909494028]
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.839]
 [0.833]
 [0.659]
 [0.686]
 [0.696]
 [0.821]] [[-0.389]
 [ 0.448]
 [-0.632]
 [-0.978]
 [-0.516]
 [-0.724]
 [-0.328]] [[0.906]
 [0.839]
 [0.833]
 [0.659]
 [0.686]
 [0.696]
 [0.821]]
actor:  0 policy actor:  1  step number:  72 total reward:  0.4449999999999996  reward:  1.0 rdn_beta:  1
using explorer policy with actor:  1
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.29 ]
 [0.246]
 [0.29 ]
 [0.272]
 [0.249]
 [0.25 ]] [[1.143]
 [1.024]
 [1.082]
 [1.024]
 [1.242]
 [1.188]
 [1.325]] [[-0.093]
 [-0.135]
 [-0.166]
 [-0.135]
 [ 0.046]
 [-0.055]
 [ 0.084]]
line 256 mcts: sample exp_bonus 0.7356599880000676
start point for exploration sampling:  11106
rdn probs:  [0.5139435059844849, 0.29518506179265114, 0.12252298689031016, 0.043556654825839446, 0.013067240024098327, 0.011724550482616084]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
using explorer policy with actor:  1
from probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.271]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]] [[1.234]
 [1.456]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]] [[0.18 ]
 [0.177]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
3814 6202
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.282]
 [0.308]
 [0.308]
 [0.211]
 [0.331]
 [0.308]] [[-1.17 ]
 [-0.063]
 [-1.17 ]
 [-1.17 ]
 [-1.192]
 [-1.211]
 [-1.17 ]] [[0.308]
 [0.282]
 [0.308]
 [0.308]
 [0.211]
 [0.331]
 [0.308]]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.434]
 [0.43 ]
 [0.427]
 [0.425]
 [0.426]
 [0.421]] [[ 0.332]
 [ 1.232]
 [-0.356]
 [-0.269]
 [-0.42 ]
 [ 0.054]
 [-0.044]] [[0.436]
 [0.434]
 [0.43 ]
 [0.427]
 [0.425]
 [0.426]
 [0.421]]
using another actor
from probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
using explorer policy with actor:  0
first move QE:  0.5172180764581552
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.439]
 [0.408]
 [0.408]
 [0.407]
 [0.408]
 [0.409]] [[2.064]
 [3.066]
 [2.064]
 [2.064]
 [2.08 ]
 [2.178]
 [2.262]] [[0.408]
 [0.439]
 [0.408]
 [0.408]
 [0.407]
 [0.408]
 [0.409]]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.175]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.184]] [[-0.903]
 [ 0.903]
 [-0.903]
 [-0.903]
 [-0.903]
 [-0.903]
 [-0.32 ]] [[1.008]
 [1.392]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.128]]
from probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.409]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[-3.173]
 [-2.094]
 [-3.173]
 [-3.173]
 [-3.173]
 [-3.173]
 [-3.173]] [[0.391]
 [0.409]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.117]
 [0.445]
 [0.283]
 [0.282]
 [0.276]
 [0.283]] [[-1.932]
 [ 0.524]
 [-0.672]
 [-2.139]
 [-1.963]
 [-2.125]
 [-2.191]] [[0.286]
 [0.117]
 [0.445]
 [0.283]
 [0.282]
 [0.276]
 [0.283]]
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[-1.902]
 [-1.902]
 [-1.902]
 [-1.902]
 [-1.902]
 [-1.902]
 [-1.902]] [[0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.515528702373571, 0.29468805177422713, 0.12212786530186154, 0.04359154710922197, 0.012723391801417126, 0.011340441639701238]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5181699353350903
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.368]
 [0.564]
 [0.368]
 [0.478]
 [0.368]
 [0.368]] [[0.874]
 [1.2  ]
 [1.068]
 [1.2  ]
 [0.619]
 [1.2  ]
 [1.2  ]] [[0.603]
 [0.565]
 [0.913]
 [0.565]
 [0.592]
 [0.565]
 [0.565]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5060864130417623, 0.3076063615670027, 0.11989100315558919, 0.04279313569520883, 0.012490353473739519, 0.011132733066697369]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5060864130417623, 0.3076063615670027, 0.11989100315558919, 0.04279313569520883, 0.012490353473739519, 0.011132733066697369]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.469]
 [0.427]
 [0.469]
 [0.469]
 [0.46 ]
 [0.427]] [[2.271]
 [2.271]
 [1.646]
 [2.271]
 [2.271]
 [1.054]
 [1.114]] [[0.469]
 [0.469]
 [0.427]
 [0.469]
 [0.469]
 [0.46 ]
 [0.427]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.278]
 [0.278]
 [0.24 ]
 [0.24 ]
 [0.237]
 [0.239]] [[1.555]
 [1.379]
 [1.379]
 [1.101]
 [1.2  ]
 [1.043]
 [1.367]] [[0.704]
 [0.534]
 [0.534]
 [0.085]
 [0.217]
 [0.003]
 [0.44 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.176]
 [0.263]
 [0.263]
 [0.239]
 [0.24 ]
 [0.254]] [[0.877]
 [1.277]
 [0.671]
 [0.671]
 [0.999]
 [1.192]
 [0.823]] [[-0.447]
 [-0.466]
 [-0.493]
 [-0.493]
 [-0.432]
 [-0.366]
 [-0.461]]
using another actor
line 256 mcts: sample exp_bonus -2.9460133379916273
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.636]] [[2.063]
 [2.063]
 [2.063]
 [2.063]
 [2.063]
 [2.063]
 [2.063]]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.447]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.308]] [[2.563]
 [2.342]
 [2.563]
 [2.563]
 [2.563]
 [2.563]
 [2.478]] [[0.146]
 [0.249]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.016]]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.32 ]
 [0.324]
 [0.324]
 [0.324]
 [0.315]
 [0.327]] [[-0.248]
 [ 1.007]
 [-0.535]
 [-0.382]
 [-0.693]
 [-1.262]
 [-0.636]] [[0.327]
 [0.32 ]
 [0.324]
 [0.324]
 [0.324]
 [0.315]
 [0.327]]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5283970977708973, 0.321167106089562, 0.12517636609822835, 0.0005949260224550146, 0.013040987379979293, 0.011623516638877972]
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]] [[0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]] [[-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5283970977708973, 0.321167106089562, 0.12517636609822835, 0.0005949260224550146, 0.013040987379979293, 0.011623516638877972]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  81 total reward:  0.48999999999999966  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.332]
 [0.183]
 [0.18 ]
 [0.177]
 [0.177]
 [0.179]] [[-3.813]
 [-2.279]
 [-3.863]
 [-4.079]
 [-3.773]
 [-3.99 ]
 [-3.838]] [[ 0.021]
 [ 0.568]
 [ 0.004]
 [-0.065]
 [ 0.029]
 [-0.039]
 [ 0.01 ]]
siam score:  -0.6099619
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5394126005850249, 0.3136654195133923, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9070672760152634
3835 6232
using another actor
Printing some Q and Qe and total Qs values:  [[1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]] [[0.503]
 [0.5  ]
 [0.52 ]
 [0.52 ]
 [0.497]
 [0.52 ]
 [0.502]] [[2.012]
 [2.007]
 [2.04 ]
 [2.04 ]
 [2.001]
 [2.04 ]
 [2.009]]
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.572]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[1.519]
 [1.127]
 [1.519]
 [1.519]
 [1.519]
 [1.519]
 [1.519]] [[1.931]
 [1.751]
 [1.931]
 [1.931]
 [1.931]
 [1.931]
 [1.931]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.555]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[0.318]
 [0.763]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]] [[0.317]
 [0.555]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.61423403
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
3843 6247
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
siam score:  -0.6111944
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.513]
 [0.545]
 [0.55 ]
 [0.533]
 [0.556]
 [0.621]] [[ 0.263]
 [-0.083]
 [ 0.588]
 [ 0.819]
 [ 0.998]
 [ 0.317]
 [-2.776]] [[0.526]
 [0.513]
 [0.545]
 [0.55 ]
 [0.533]
 [0.556]
 [0.621]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
first move QE:  0.5216582270418465
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[0.536]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]] [[0.661]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]]
siam score:  -0.6104711
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.2327798569769737
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.323]
 [0.323]
 [0.323]
 [0.324]
 [0.323]
 [0.346]] [[1.874]
 [1.902]
 [1.902]
 [1.902]
 [2.003]
 [1.902]
 [2.011]] [[0.709]
 [0.716]
 [0.716]
 [0.716]
 [0.819]
 [0.716]
 [0.871]]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
line 256 mcts: sample exp_bonus 0.56992751240212
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
3856 6262
from probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.149]
 [0.112]
 [0.149]
 [0.111]
 [0.112]
 [0.127]] [[1.145]
 [0.954]
 [0.875]
 [0.954]
 [0.785]
 [0.798]
 [1.272]] [[-0.215]
 [-0.413]
 [-0.591]
 [-0.413]
 [-0.712]
 [-0.694]
 [-0.033]]
UNIT TEST: sample policy line 217 mcts : [0.061 0.612 0.061 0.02  0.082 0.102 0.061]
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.323]] [[1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.491]] [[1.125]
 [1.125]
 [1.125]
 [1.125]
 [1.125]
 [1.125]
 [0.997]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7940680715943629
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.378]
 [0.381]
 [0.381]
 [0.381]
 [0.884]
 [0.381]] [[1.521]
 [2.596]
 [1.521]
 [1.521]
 [1.521]
 [1.923]
 [1.521]] [[0.381]
 [0.378]
 [0.381]
 [0.381]
 [0.381]
 [0.884]
 [0.381]]
siam score:  -0.6207209
from probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
first move QE:  0.5234853233796791
first move QE:  0.5235386376622658
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.198]
 [0.254]
 [0.254]
 [0.172]
 [0.174]
 [0.175]] [[ 0.   ]
 [-3.327]
 [ 0.   ]
 [ 0.   ]
 [-2.918]
 [-3.078]
 [-2.888]] [[0.254]
 [0.198]
 [0.254]
 [0.254]
 [0.172]
 [0.174]
 [0.175]]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.036]
 [-0.031]
 [-0.069]] [[1.998]
 [1.782]
 [1.782]
 [1.782]
 [1.796]
 [1.403]
 [1.668]] [[0.888]
 [0.624]
 [0.624]
 [0.624]
 [0.64 ]
 [0.264]
 [0.467]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
line 256 mcts: sample exp_bonus -1.763121709143177
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5394126005850249, 0.3136654195133922, 0.12225254903412676, 0.0005810299899166753, 0.012736381465757465, 0.011352019411781992]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.936]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[-0.315]
 [-0.27 ]
 [-0.315]
 [-0.315]
 [-0.315]
 [-0.315]
 [-0.315]] [[0.62 ]
 [1.525]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
siam score:  -0.6153028
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.444]
 [0.342]
 [0.345]
 [0.307]
 [0.331]
 [0.345]] [[-0.976]
 [ 1.906]
 [-1.459]
 [-1.161]
 [-0.087]
 [-1.006]
 [-0.983]] [[0.372]
 [1.689]
 [0.167]
 [0.299]
 [0.747]
 [0.359]
 [0.376]]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.078]
 [0.092]
 [0.09 ]
 [0.089]
 [0.074]
 [0.161]] [[1.544]
 [1.986]
 [1.533]
 [1.55 ]
 [1.605]
 [1.691]
 [2.087]] [[0.15 ]
 [0.658]
 [0.168]
 [0.185]
 [0.245]
 [0.317]
 [0.913]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.6049999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.084]] [[1.455]
 [1.541]
 [1.541]
 [1.541]
 [1.541]
 [1.541]
 [2.19 ]] [[0.084]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.974]]
from probs:  [0.5516527221651196, 0.3053297531986126, 0.1190036845068842, 0.0005655891035022952, 0.012397911811942901, 0.011050339213938328]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.434]
 [0.446]
 [0.449]
 [0.437]
 [0.442]
 [0.444]] [[ 2.32 ]
 [ 0.957]
 [-0.391]
 [-0.951]
 [-0.46 ]
 [-0.414]
 [ 0.163]] [[0.354]
 [0.434]
 [0.446]
 [0.449]
 [0.437]
 [0.442]
 [0.444]]
Printing some Q and Qe and total Qs values:  [[0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]] [[0.515]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.518]] [[1.331]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.335]]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5516527221651196, 0.3053297531986126, 0.1190036845068842, 0.0005655891035022952, 0.012397911811942901, 0.011050339213938328]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5516527221651196, 0.3053297531986126, 0.1190036845068842, 0.0005655891035022952, 0.012397911811942901, 0.011050339213938328]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
probs:  [0.5516527221651196, 0.3053297531986126, 0.1190036845068842, 0.0005655891035022952, 0.012397911811942901, 0.011050339213938328]
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.258]] [[1.97 ]
 [1.97 ]
 [1.97 ]
 [1.97 ]
 [1.97 ]
 [1.97 ]
 [2.234]] [[0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.387]]
maxi score, test score, baseline:  -0.9948300000000001 -0.92775 -0.92775
actor:  0 policy actor:  1  step number:  52 total reward:  0.5149999999999997  reward:  1.0 rdn_beta:  0.333
3876 6284
maxi score, test score, baseline:  -0.9918000000000001 -0.92775 -0.92775
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9918000000000001 -0.92775 -0.92775
probs:  [0.5516527221651196, 0.3053297531986126, 0.1190036845068842, 0.0005655891035022952, 0.012397911811942901, 0.011050339213938328]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9918000000000001 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.32 ]] [[2.407]
 [2.407]
 [2.407]
 [2.407]
 [2.407]
 [2.407]
 [2.561]] [[0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.229]]
line 256 mcts: sample exp_bonus 1.4588563171737776
maxi score, test score, baseline:  -0.9918000000000001 -0.92775 -0.92775
probs:  [0.5516527221651196, 0.3053297531986126, 0.1190036845068842, 0.0005655891035022952, 0.012397911811942901, 0.011050339213938328]
line 256 mcts: sample exp_bonus 0.5163225073836808
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.703]] [[ 0.713]
 [ 0.713]
 [ 0.713]
 [ 0.713]
 [ 0.713]
 [ 0.713]
 [-0.776]] [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.703]]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]] [[0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]] [[1.785]
 [1.785]
 [1.785]
 [1.785]
 [1.785]
 [1.785]
 [1.785]]
maxi score, test score, baseline:  -0.9918000000000001 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.058]
 [-0.058]
 [-0.056]] [[1.302]
 [1.373]
 [1.373]
 [1.373]
 [1.264]
 [1.396]
 [1.428]] [[-0.893]
 [-0.826]
 [-0.826]
 [-0.826]
 [-0.916]
 [-0.829]
 [-0.804]]
maxi score, test score, baseline:  -0.9918000000000001 -0.92775 -0.92775
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.67 ]] [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.757]] [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.67 ]]
first move QE:  0.5280624840992049
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9551585516017067
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]] [[0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]] [[0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]]
Printing some Q and Qe and total Qs values:  [[ 0.026]
 [-0.01 ]
 [ 0.029]
 [ 0.03 ]
 [ 0.027]
 [ 0.029]
 [ 0.069]] [[1.481]
 [0.994]
 [1.471]
 [1.648]
 [1.693]
 [1.704]
 [2.215]] [[ 0.377]
 [-0.181]
 [ 0.373]
 [ 0.553]
 [ 0.592]
 [ 0.607]
 [ 1.197]]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.434]
 [0.441]
 [0.434]
 [0.434]
 [0.43 ]
 [0.586]] [[0.385]
 [0.385]
 [0.199]
 [0.385]
 [0.385]
 [0.331]
 [1.27 ]] [[0.434]
 [0.434]
 [0.441]
 [0.434]
 [0.434]
 [0.43 ]
 [0.586]]
line 256 mcts: sample exp_bonus 1.3134513329820245
maxi score, test score, baseline:  -0.9918000000000001 -0.92775 -0.92775
probs:  [0.5516527221651196, 0.3053297531986126, 0.1190036845068842, 0.0005655891035022952, 0.012397911811942901, 0.011050339213938328]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.325]
 [0.325]
 [0.325]
 [0.329]
 [0.328]
 [0.333]] [[1.755]
 [1.975]
 [1.975]
 [1.975]
 [1.723]
 [1.947]
 [1.834]] [[0.789]
 [1.006]
 [1.006]
 [1.006]
 [0.762]
 [0.984]
 [0.881]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
probs:  [0.5516527221651196, 0.3053297531986126, 0.1190036845068842, 0.0005655891035022952, 0.012397911811942901, 0.011050339213938328]
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.346]
 [0.346]
 [0.346]
 [0.307]
 [0.297]
 [0.301]] [[1.54 ]
 [1.356]
 [1.356]
 [1.356]
 [1.668]
 [1.517]
 [1.674]] [[0.272]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.413]
 [0.242]
 [0.409]]
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
probs:  [0.5516527221651196, 0.3053297531986126, 0.1190036845068842, 0.0005655891035022952, 0.012397911811942901, 0.011050339213938328]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.015]
 [-0.004]
 [ 0.01 ]
 [-0.002]
 [-0.005]
 [-0.004]] [[1.367]
 [1.026]
 [0.986]
 [1.192]
 [1.449]
 [1.032]
 [1.477]] [[-0.295]
 [-0.547]
 [-0.55 ]
 [-0.386]
 [-0.238]
 [-0.523]
 [-0.223]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[5.178]
 [3.948]
 [3.948]
 [3.948]
 [3.948]
 [3.948]
 [3.948]] [[0.743]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.47 ]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[4.079]
 [4.058]
 [4.079]
 [4.079]
 [4.079]
 [4.079]
 [4.079]] [[1.05 ]
 [1.079]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.05 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.875]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]] [[0.048]
 [0.624]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]] [[-0.186]
 [ 1.464]
 [-0.186]
 [-0.186]
 [-0.186]
 [-0.186]
 [-0.186]]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.385]
 [0.385]
 [0.385]
 [1.126]
 [0.385]
 [0.385]] [[0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.726]
 [0.778]
 [0.778]] [[0.704]
 [0.704]
 [0.704]
 [0.704]
 [2.118]
 [0.704]
 [0.704]]
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
probs:  [0.5633131916398931, 0.3117836110766979, 0.10038176852344323, 0.0005775441509654326, 0.012659970651545295, 0.011283913957455052]
Printing some Q and Qe and total Qs values:  [[ 0.109]
 [-0.003]
 [ 0.175]
 [ 0.136]
 [ 0.666]
 [ 0.109]
 [ 0.112]] [[-0.219]
 [ 0.613]
 [-0.28 ]
 [-0.503]
 [ 0.75 ]
 [-0.249]
 [-0.029]] [[-0.431]
 [-0.377]
 [-0.32 ]
 [-0.472]
 [ 1.007]
 [-0.441]
 [-0.362]]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.376]
 [0.494]
 [0.511]
 [0.512]
 [0.464]
 [0.472]] [[2.193]
 [2.279]
 [2.58 ]
 [2.447]
 [2.651]
 [2.686]
 [2.766]] [[ 0.251]
 [-0.013]
 [ 0.424]
 [ 0.37 ]
 [ 0.508]
 [ 0.435]
 [ 0.504]]
from probs:  [0.5633131916398931, 0.3117836110766979, 0.10038176852344323, 0.0005775441509654326, 0.012659970651545295, 0.011283913957455052]
actor:  1 policy actor:  1  step number:  120 total reward:  0.26499999999999946  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
using another actor
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.279]
 [0.303]
 [0.259]
 [0.303]
 [0.266]
 [0.303]] [[ 0.   ]
 [-0.362]
 [ 0.   ]
 [-3.831]
 [ 0.   ]
 [-3.134]
 [ 0.   ]] [[0.303]
 [0.279]
 [0.303]
 [0.259]
 [0.303]
 [0.266]
 [0.303]]
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
probs:  [0.5542059838267289, 0.3229101562515121, 0.09875887447415854, 0.0005682068681142934, 0.012455294131727721, 0.011101484447758603]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]] [[0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]] [[0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.017]
 [0.015]
 [0.019]] [[1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.407]
 [1.27 ]
 [1.349]] [[0.988]
 [0.988]
 [0.988]
 [0.988]
 [1.156]
 [0.988]
 [1.09 ]]
Printing some Q and Qe and total Qs values:  [[0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]] [[3.619]
 [3.619]
 [3.619]
 [3.619]
 [3.619]
 [3.619]
 [3.619]] [[1.996]
 [1.996]
 [1.996]
 [1.996]
 [1.996]
 [1.996]
 [1.996]]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.394]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[4.184]
 [2.744]
 [4.184]
 [4.184]
 [4.184]
 [4.184]
 [4.184]] [[1.922]
 [1.4  ]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]]
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.055]] [[1.275]
 [1.275]
 [1.275]
 [1.275]
 [1.275]
 [1.275]
 [1.264]] [[-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.04 ]]
UNIT TEST: sample policy line 217 mcts : [0.163 0.02  0.061 0.082 0.327 0.082 0.265]
Printing some Q and Qe and total Qs values:  [[-0.17 ]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.165]
 [-0.165]
 [-0.135]] [[0.963]
 [1.58 ]
 [1.58 ]
 [1.58 ]
 [1.535]
 [2.003]
 [1.58 ]] [[-0.412]
 [ 0.48 ]
 [ 0.48 ]
 [ 0.48 ]
 [ 0.362]
 [ 0.985]
 [ 0.48 ]]
from probs:  [0.5542059838267289, 0.3229101562515121, 0.09875887447415854, 0.0005682068681142934, 0.012455294131727721, 0.011101484447758603]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6896969730580804
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
using another actor
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
probs:  [0.5542059838267289, 0.3229101562515121, 0.09875887447415854, 0.0005682068681142934, 0.012455294131727721, 0.011101484447758603]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
probs:  [0.5542059838267289, 0.3229101562515121, 0.09875887447415854, 0.0005682068681142934, 0.012455294131727721, 0.011101484447758603]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.384]
 [0.397]
 [0.384]
 [0.384]
 [0.386]
 [0.369]] [[1.722]
 [1.722]
 [2.366]
 [1.722]
 [1.722]
 [3.157]
 [0.886]] [[0.384]
 [0.384]
 [0.397]
 [0.384]
 [0.384]
 [0.386]
 [0.369]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
using another actor
start point for exploration sampling:  11106
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.402]
 [0.42 ]
 [0.42 ]
 [0.531]
 [0.588]
 [0.553]] [[0.812]
 [0.93 ]
 [1.142]
 [1.142]
 [0.639]
 [0.735]
 [0.818]] [[0.977]
 [0.7  ]
 [0.807]
 [0.807]
 [0.861]
 [1.008]
 [0.964]]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.305]
 [0.488]
 [0.488]
 [0.488]
 [0.636]
 [0.488]] [[1.133]
 [1.446]
 [1.133]
 [1.133]
 [1.133]
 [1.268]
 [1.133]] [[0.921]
 [0.659]
 [0.921]
 [0.921]
 [0.921]
 [1.261]
 [0.921]]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.4  ]
 [0.38 ]
 [0.473]
 [0.349]
 [0.473]
 [0.41 ]] [[1.706]
 [0.526]
 [1.01 ]
 [1.706]
 [1.3  ]
 [1.706]
 [1.322]] [[1.963]
 [1.444]
 [1.633]
 [1.963]
 [1.735]
 [1.963]
 [1.775]]
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.709]
 [0.796]
 [0.671]
 [0.593]
 [0.667]
 [0.664]] [[4.848]
 [5.654]
 [5.571]
 [5.347]
 [4.283]
 [4.835]
 [4.55 ]] [[1.641]
 [1.99 ]
 [2.131]
 [1.816]
 [1.32 ]
 [1.642]
 [1.545]]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.7299999999999999  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
probs:  [0.5677065868097845, 0.3131310168719165, 0.09576802150853506, 0.0005509990657204747, 0.012078093059014625, 0.010765282685029019]
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.206]
 [0.169]
 [0.17 ]] [[0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.969]
 [1.053]
 [1.085]] [[0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.644]
 [0.654]
 [0.686]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.6173],
        [-0.0000],
        [-0.5258],
        [-0.0000],
        [-0.0000],
        [-0.5951],
        [-0.0000]], dtype=torch.float64)
-0.9702 -0.9702
-0.8712 -0.8712
-0.9702 -0.9702
-0.024259925299500003 -0.6415373056924443
-0.95104053 -0.95104053
-0.0530787758985 -0.5788801775287883
-0.9553499999999999 -0.9553499999999999
-0.9514752239519999 -0.9514752239519999
-0.024259925299500003 -0.6193207361835786
-0.9605475 -0.9605475
UNIT TEST: sample policy line 217 mcts : [0.02  0.286 0.122 0.306 0.122 0.122 0.02 ]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.393]
 [0.402]
 [0.402]] [[2.196]
 [2.196]
 [2.196]
 [2.196]
 [4.494]
 [2.196]
 [2.196]] [[0.919]
 [0.919]
 [0.919]
 [0.919]
 [1.677]
 [0.919]
 [0.919]]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.683]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[4.034]
 [2.953]
 [4.034]
 [4.034]
 [4.034]
 [4.034]
 [4.034]] [[0.567]
 [0.683]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]]
maxi score, test score, baseline:  -0.9918 -0.92775 -0.92775
probs:  [0.5677065868097845, 0.3131310168719165, 0.09576802150853506, 0.0005509990657204747, 0.012078093059014625, 0.010765282685029019]
actor:  0 policy actor:  1  step number:  90 total reward:  0.10499999999999932  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9895900000000001 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9895900000000001 -0.92775 -0.92775
probs:  [0.5677065868097845, 0.3131310168719165, 0.09576802150853506, 0.0005509990657204747, 0.012078093059014625, 0.010765282685029019]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.617]] [[-0.202]
 [-0.202]
 [-0.202]
 [-0.202]
 [-0.202]
 [-0.202]
 [-1.602]] [[0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.617]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
from probs:  [0.5677065868097845, 0.3131310168719165, 0.09576802150853506, 0.0005509990657204747, 0.012078093059014625, 0.010765282685029019]
from probs:  [0.5677065868097845, 0.3131310168719165, 0.09576802150853506, 0.0005509990657204747, 0.012078093059014625, 0.010765282685029019]
maxi score, test score, baseline:  -0.9895900000000001 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9895900000000001 -0.92775 -0.92775
siam score:  -0.63193876
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9895900000000001 -0.92775 -0.92775
probs:  [0.5627151275154506, 0.31674657213327284, 0.09687380329116983, 0.0005573611552731776, 0.012217552296693829, 0.010889583608139953]
maxi score, test score, baseline:  -0.9895900000000001 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.12 ]
 [0.431]
 [0.41 ]
 [0.311]
 [0.459]
 [0.338]] [[ 0.959]
 [ 0.876]
 [ 0.333]
 [-0.068]
 [ 1.042]
 [ 0.705]
 [ 1.049]] [[1.361]
 [1.284]
 [1.241]
 [1.006]
 [1.53 ]
 [1.467]
 [1.556]]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.378]] [[ 1.035]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [ 1.15 ]] [[1.915]
 [1.247]
 [1.247]
 [1.247]
 [1.247]
 [1.247]
 [2.038]]
using explorer policy with actor:  1
siam score:  -0.6351968
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.386]
 [0.421]
 [0.425]
 [0.424]
 [0.416]
 [0.417]] [[-0.328]
 [-0.704]
 [-1.323]
 [-1.261]
 [-1.152]
 [-0.195]
 [-0.952]] [[2.197]
 [2.026]
 [1.896]
 [1.924]
 [1.956]
 [2.243]
 [2.006]]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.419]
 [0.39 ]
 [0.402]
 [0.39 ]
 [0.39 ]
 [0.349]] [[-0.466]
 [-0.446]
 [-1.162]
 [-0.505]
 [-1.162]
 [-1.162]
 [-0.127]] [[0.839]
 [1.046]
 [0.749]
 [0.993]
 [0.749]
 [0.749]
 [1.011]]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9895900000000001 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.475]] [[-0.303]
 [-0.303]
 [-0.303]
 [-0.303]
 [-0.303]
 [-0.303]
 [-1.073]] [[0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.475]]
siam score:  -0.6332047
actor:  0 policy actor:  1  step number:  63 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5627151275154505, 0.31674657213327273, 0.0968738032911698, 0.0005573611552731775, 0.012217552296693825, 0.01088958360813995]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.415]
 [0.419]
 [0.415]
 [0.415]
 [0.415]
 [0.108]] [[0.795]
 [0.795]
 [1.99 ]
 [0.795]
 [0.795]
 [0.795]
 [1.504]] [[0.328]
 [0.328]
 [1.531]
 [0.328]
 [0.328]
 [0.328]
 [0.423]]
from probs:  [0.5627151275154505, 0.31674657213327273, 0.0968738032911698, 0.0005573611552731775, 0.012217552296693825, 0.01088958360813995]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.268]
 [0.358]
 [0.358]
 [0.358]
 [0.189]
 [0.358]] [[1.565]
 [0.786]
 [1.565]
 [1.565]
 [1.565]
 [0.704]
 [1.565]] [[0.358]
 [0.268]
 [0.358]
 [0.358]
 [0.358]
 [0.189]
 [0.358]]
from probs:  [0.5627151275154506, 0.31674657213327284, 0.09687380329116983, 0.0005573611552731776, 0.012217552296693829, 0.010889583608139953]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.447]
 [0.603]
 [0.603]] [[0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.409]
 [0.19 ]
 [0.19 ]] [[0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.21 ]
 [0.229]
 [0.229]]
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5627151275154505, 0.31674657213327273, 0.0968738032911698, 0.0005573611552731775, 0.012217552296693825, 0.01088958360813995]
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5627151275154505, 0.31674657213327273, 0.0968738032911698, 0.0005573611552731775, 0.012217552296693825, 0.01088958360813995]
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5627151275154505, 0.31674657213327273, 0.0968738032911698, 0.0005573611552731775, 0.012217552296693825, 0.01088958360813995]
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
actor:  1 policy actor:  1  step number:  67 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.167
from probs:  [0.5627151275154505, 0.31674657213327273, 0.0968738032911698, 0.0005573611552731775, 0.012217552296693825, 0.01088958360813995]
first move QE:  0.53142009486735
siam score:  -0.6340395
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.471]
 [0.504]
 [0.473]
 [0.474]
 [0.504]
 [0.504]] [[ 0.87 ]
 [-0.227]
 [ 0.87 ]
 [-0.691]
 [-0.684]
 [ 0.87 ]
 [ 0.87 ]] [[1.097]
 [0.665]
 [1.097]
 [0.513]
 [0.518]
 [1.097]
 [1.097]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.518]
 [0.52 ]
 [0.542]
 [0.56 ]
 [0.432]
 [0.478]] [[1.911]
 [2.802]
 [1.578]
 [2.638]
 [2.624]
 [1.418]
 [2.022]] [[0.516]
 [0.518]
 [0.52 ]
 [0.542]
 [0.56 ]
 [0.432]
 [0.478]]
siam score:  -0.6372281
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5708956134995672, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.125]
 [0.203]
 [0.224]
 [0.22 ]
 [0.24 ]
 [0.224]] [[ 0.   ]
 [ 0.784]
 [-0.487]
 [ 0.   ]
 [-0.601]
 [-0.796]
 [ 0.   ]] [[0.224]
 [0.125]
 [0.203]
 [0.224]
 [0.22 ]
 [0.24 ]
 [0.224]]
line 256 mcts: sample exp_bonus 2.205882501346206
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5708956134995672, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
using explorer policy with actor:  1
siam score:  -0.63360405
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5708956134995672, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
3943 6411
3944 6412
from probs:  [0.5708956134995672, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
in main func line 156:  3946
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5708956134995672, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5708956134995672, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
using explorer policy with actor:  1
siam score:  -0.6398989
from probs:  [0.5708956134995672, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
siam score:  -0.6416302
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.138]
 [0.497]
 [0.472]
 [0.409]
 [0.516]
 [0.634]] [[1.124]
 [1.228]
 [0.217]
 [1.656]
 [2.164]
 [0.522]
 [1.782]] [[1.548]
 [1.507]
 [1.364]
 [1.787]
 [1.913]
 [1.465]
 [1.899]]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.342]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[0.996]
 [0.712]
 [0.996]
 [0.996]
 [0.996]
 [0.996]
 [0.996]] [[0.317]
 [0.342]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]]
using explorer policy with actor:  1
siam score:  -0.6453835
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.362]] [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[2.4]
 [2.4]
 [2.4]
 [2.4]
 [2.4]
 [2.4]
 [2.4]]
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]] [[0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]] [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5708956134995671, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.576]
 [0.634]] [[ 0.577]
 [ 0.259]
 [ 0.259]
 [ 0.259]
 [ 0.402]
 [ 0.333]
 [-2.363]] [[0.575]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.576]
 [0.634]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5708956134995671, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.954]
 [0.591]] [[0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.362]
 [0.871]] [[0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.759]
 [0.201]]
from probs:  [0.5708956134995671, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5708956134995671, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
line 256 mcts: sample exp_bonus 1.9354088918813965
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.042]
 [ 0.042]
 [ 0.042]
 [ 0.042]
 [ 0.042]
 [ 0.042]] [[1.778]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]] [[1.749]
 [1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]]
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5708956134995672, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
Starting evaluation
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5708956134995672, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[-2.992]
 [-2.992]
 [-2.992]
 [-2.992]
 [-2.992]
 [-2.992]
 [-2.992]] [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.418]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[-2.906]
 [-3.604]
 [-2.906]
 [-2.906]
 [-2.906]
 [-2.906]
 [-2.906]] [[0.391]
 [0.418]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]]
line 256 mcts: sample exp_bonus -1.0072788219327324
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.46 ]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]] [[-0.241]
 [ 0.229]
 [-0.241]
 [-0.241]
 [-0.241]
 [-0.241]
 [-0.241]] [[0.422]
 [0.46 ]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]]
maxi score, test score, baseline:  -0.9867499999999999 -0.92775 -0.92775
probs:  [0.5708956134995672, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2829045427647299
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.297]
 [0.35 ]
 [0.35 ]
 [0.377]
 [0.35 ]
 [0.362]] [[-2.109]
 [ 0.064]
 [-1.133]
 [-1.133]
 [-0.698]
 [-1.133]
 [-0.561]] [[0.463]
 [0.297]
 [0.35 ]
 [0.35 ]
 [0.377]
 [0.35 ]
 [0.362]]
line 256 mcts: sample exp_bonus 1.3996218294581413
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]] [[-1.571]
 [-1.571]
 [-1.571]
 [-1.571]
 [-1.571]
 [-1.571]
 [-1.571]] [[0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.263]
 [0.401]
 [0.36 ]
 [0.357]
 [0.374]
 [0.343]] [[-1.336]
 [ 0.132]
 [-0.43 ]
 [-0.85 ]
 [-0.479]
 [ 0.   ]
 [-0.365]] [[0.363]
 [0.263]
 [0.401]
 [0.36 ]
 [0.357]
 [0.374]
 [0.343]]
actor:  0 policy actor:  1  step number:  39 total reward:  0.6299999999999998  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  39 total reward:  0.6399999999999998  reward:  1.0 rdn_beta:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9802099999999999 -0.92775 -0.92775
actor:  0 policy actor:  1  step number:  45 total reward:  0.5399999999999997  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.162]
 [0.166]
 [0.172]
 [0.167]
 [0.163]
 [0.164]] [[1.139]
 [1.601]
 [1.213]
 [1.173]
 [1.308]
 [1.037]
 [1.308]] [[-0.233]
 [ 0.072]
 [-0.179]
 [-0.193]
 [-0.114]
 [-0.302]
 [-0.119]]
actor:  0 policy actor:  1  step number:  47 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]] [[ 1.107]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]] [[1.441]
 [0.955]
 [0.955]
 [0.955]
 [0.955]
 [0.955]
 [0.955]]
from probs:  [0.5708956134995672, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
using explorer policy with actor:  1
using explorer policy with actor:  0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.92775 -0.92775
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.445]
 [0.437]
 [0.437]
 [0.429]
 [0.43 ]
 [0.434]] [[3.425]
 [3.874]
 [3.885]
 [3.885]
 [3.599]
 [3.696]
 [4.192]] [[0.43 ]
 [0.445]
 [0.437]
 [0.437]
 [0.429]
 [0.43 ]
 [0.434]]
maxi score, test score, baseline:  -0.97413 -0.92775 -0.92775
probs:  [0.5708956134995672, 0.31082105067827487, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349337, 0.010685867239967617]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.92775 -0.92775
probs:  [0.5708956134995672, 0.31082105067827487, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349337, 0.010685867239967617]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.256]
 [0.397]
 [0.397]
 [0.426]
 [0.397]
 [0.397]] [[1.383]
 [1.297]
 [0.726]
 [0.726]
 [1.213]
 [0.726]
 [0.726]] [[1.641]
 [1.702]
 [1.511]
 [1.511]
 [1.758]
 [1.511]
 [1.511]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5256218165411609
maxi score, test score, baseline:  -0.97413 -0.92775 -0.92775
rdn probs:  [0.5708956134995672, 0.3108210506782748, 0.09506154121690955, 0.0005469343479315529, 0.011988993017349335, 0.010685867239967615]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.583]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[1.089]
 [1.868]
 [1.089]
 [1.089]
 [1.089]
 [1.089]
 [1.089]] [[0.538]
 [1.234]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]]
Printing some Q and Qe and total Qs values:  [[-0.054]
 [ 0.045]
 [ 0.003]
 [ 0.003]
 [ 0.089]
 [ 0.003]
 [ 0.003]] [[1.578]
 [1.393]
 [1.135]
 [1.135]
 [1.3  ]
 [1.135]
 [1.135]] [[1.643]
 [1.615]
 [1.367]
 [1.367]
 [1.594]
 [1.367]
 [1.367]]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.606]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[3.749]
 [3.328]
 [3.749]
 [3.749]
 [3.749]
 [3.749]
 [3.749]] [[0.681]
 [0.755]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.331]
 [0.341]
 [0.352]
 [0.352]
 [0.338]
 [0.346]] [[4.936]
 [4.173]
 [4.601]
 [4.08 ]
 [4.08 ]
 [4.163]
 [4.479]] [[0.791]
 [0.523]
 [0.684]
 [0.533]
 [0.533]
 [0.533]
 [0.654]]
siam score:  -0.66373044
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
from probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
from probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
Printing some Q and Qe and total Qs values:  [[2.527]
 [0.107]
 [2.527]
 [2.527]
 [0.123]
 [2.527]
 [2.527]] [[ 0.   ]
 [ 0.005]
 [ 0.   ]
 [ 0.   ]
 [-0.92 ]
 [ 0.   ]
 [ 0.   ]] [[6.184]
 [1.633]
 [6.184]
 [6.184]
 [0.211]
 [6.184]
 [6.184]]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.355]
 [0.391]
 [0.362]
 [0.316]
 [0.391]
 [0.391]] [[-0.312]
 [ 0.028]
 [ 0.   ]
 [-0.605]
 [-0.468]
 [ 0.   ]
 [ 0.   ]] [[0.309]
 [0.355]
 [0.391]
 [0.362]
 [0.316]
 [0.391]
 [0.391]]
Printing some Q and Qe and total Qs values:  [[0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.829]
 [0.851]
 [0.851]] [[1.512]
 [1.512]
 [1.512]
 [1.512]
 [0.696]
 [1.512]
 [1.512]] [[0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.829]
 [0.851]
 [0.851]]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.398]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.308]] [[2.738]
 [2.179]
 [2.738]
 [2.738]
 [2.738]
 [2.738]
 [2.638]] [[-0.157]
 [ 0.097]
 [-0.157]
 [-0.157]
 [-0.157]
 [-0.157]
 [ 0.068]]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[-0.135]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]] [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
3983 6480
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.207]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.134]] [[1.97 ]
 [2.438]
 [1.597]
 [1.597]
 [1.597]
 [1.597]
 [2.566]] [[0.065]
 [0.207]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.134]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
line 256 mcts: sample exp_bonus 0.7749625668010817
using another actor
siam score:  -0.6514732
Printing some Q and Qe and total Qs values:  [[-0.056]
 [ 0.098]
 [-0.024]
 [-0.024]
 [-0.07 ]
 [-0.024]
 [-0.04 ]] [[ 1.014]
 [-0.035]
 [ 0.563]
 [ 0.563]
 [ 0.569]
 [ 0.563]
 [ 0.903]] [[1.18 ]
 [0.622]
 [0.919]
 [0.919]
 [0.894]
 [0.919]
 [1.121]]
first move QE:  0.5238507348751906
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.382]
 [0.318]
 [0.38 ]
 [0.607]
 [0.311]
 [0.312]] [[0.741]
 [0.846]
 [0.929]
 [0.903]
 [0.943]
 [1.012]
 [0.922]] [[0.317]
 [0.382]
 [0.318]
 [0.38 ]
 [0.607]
 [0.311]
 [0.312]]
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.087]
 [0.129]
 [0.135]
 [0.117]
 [0.122]
 [0.129]] [[-0.693]
 [ 1.06 ]
 [-0.797]
 [-0.944]
 [-0.119]
 [-0.366]
 [-0.543]] [[0.28 ]
 [1.212]
 [0.223]
 [0.147]
 [0.588]
 [0.455]
 [0.364]]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[-0.017]
 [-0.016]
 [-0.022]
 [-0.023]
 [-0.023]
 [-0.02 ]
 [-0.019]] [[1.562]
 [2.25 ]
 [2.764]
 [2.1  ]
 [2.1  ]
 [1.974]
 [2.359]] [[-0.243]
 [ 0.447]
 [ 0.948]
 [ 0.284]
 [ 0.284]
 [ 0.163]
 [ 0.55 ]]
3991 6510
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.259]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]] [[3.105]
 [3.121]
 [3.105]
 [3.105]
 [3.105]
 [3.105]
 [3.105]] [[-0.366]
 [-0.263]
 [-0.366]
 [-0.366]
 [-0.366]
 [-0.366]
 [-0.366]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[-0.885]
 [-0.885]
 [-0.885]
 [-0.885]
 [-0.885]
 [-0.885]
 [-0.885]]
using explorer policy with actor:  1
3993 6512
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[-0.106]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]] [[2.072]
 [1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]] [[ 1.016]
 [-0.263]
 [-0.263]
 [-0.263]
 [-0.263]
 [-0.263]
 [-0.263]]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]] [[2.108]
 [2.108]
 [2.108]
 [2.108]
 [2.108]
 [2.108]
 [2.108]] [[1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.341]
 [0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]] [[1.731]
 [1.424]
 [1.731]
 [1.731]
 [1.731]
 [1.731]
 [1.731]] [[0.109]
 [0.61 ]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.017]
 [-0.021]
 [-0.022]
 [-0.017]
 [-0.021]
 [-0.017]
 [-0.017]] [[2.622]
 [3.067]
 [2.872]
 [2.622]
 [3.03 ]
 [2.892]
 [2.622]] [[-0.531]
 [-0.389]
 [-0.457]
 [-0.531]
 [-0.401]
 [-0.44 ]
 [-0.531]]
3999 6527
UNIT TEST: sample policy line 217 mcts : [0.041 0.429 0.163 0.082 0.041 0.041 0.204]
Printing some Q and Qe and total Qs values:  [[-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]] [[1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.609]] [[-0.617]
 [-0.617]
 [-0.617]
 [-0.617]
 [-0.617]
 [-0.617]
 [-0.617]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
line 256 mcts: sample exp_bonus 0.9018473086326593
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.011]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[3.126]
 [2.923]
 [3.126]
 [3.126]
 [3.126]
 [3.126]
 [3.126]] [[0.984]
 [0.74 ]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]]
from probs:  [0.5803911343444853, 0.3059050463024515, 0.09323100294992731, 0.000802666367775116, 0.01055699314885726, 0.00911315688650338]
4008 6542
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5803911343444853, 0.3059050463024515, 0.09323100294992731, 0.000802666367775116, 0.01055699314885726, 0.00911315688650338]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
using explorer policy with actor:  1
from probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
siam score:  -0.652303
using another actor
from probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
using another actor
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.062]
 [-0.058]
 [-0.056]
 [-0.057]
 [-0.058]
 [-0.057]] [[0.311]
 [0.002]
 [0.579]
 [0.404]
 [0.522]
 [0.333]
 [0.443]] [[-0.338]
 [-0.655]
 [-0.069]
 [-0.239]
 [-0.124]
 [-0.316]
 [-0.204]]
Printing some Q and Qe and total Qs values:  [[ 0.01 ]
 [-0.   ]
 [ 0.01 ]
 [ 0.01 ]
 [ 0.01 ]
 [ 0.016]
 [ 0.01 ]] [[1.905]
 [1.847]
 [1.905]
 [1.905]
 [1.905]
 [3.08 ]
 [1.905]] [[0.314]
 [0.274]
 [0.314]
 [0.314]
 [0.314]
 [0.719]
 [0.314]]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]] [[1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]] [[0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.319]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[4.385]
 [2.958]
 [4.385]
 [4.385]
 [4.385]
 [4.385]
 [4.385]] [[0.282]
 [0.319]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]]
siam score:  -0.6452263
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5803911343444853, 0.3059050463024515, 0.09323100294992731, 0.000802666367775116, 0.01055699314885726, 0.00911315688650338]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]] [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]]
siam score:  -0.6430882
Printing some Q and Qe and total Qs values:  [[ 0.847]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]] [[0.508]
 [1.883]
 [1.883]
 [1.883]
 [1.883]
 [1.883]
 [1.883]] [[1.4  ]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1516262036373839
Printing some Q and Qe and total Qs values:  [[-0.088]
 [ 0.109]
 [ 0.109]
 [ 0.109]
 [ 0.109]
 [ 0.109]
 [ 0.109]] [[1.563]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[ 1.686]
 [-0.389]
 [-0.389]
 [-0.389]
 [-0.389]
 [-0.389]
 [-0.389]]
Printing some Q and Qe and total Qs values:  [[ 0.053]
 [-0.004]
 [ 0.055]
 [ 0.069]
 [ 0.075]
 [ 0.062]
 [ 0.06 ]] [[0.638]
 [0.821]
 [0.631]
 [0.863]
 [0.886]
 [0.836]
 [0.865]] [[-0.363]
 [-0.292]
 [-0.365]
 [-0.105]
 [-0.069]
 [-0.145]
 [-0.12 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5214748559065092
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
siam score:  -0.65260667
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5803911343444853, 0.3059050463024516, 0.09323100294992732, 0.0008026663677751161, 0.010556993148857262, 0.009113156886503382]
using explorer policy with actor:  1
siam score:  -0.65484965
actor:  1 policy actor:  1  step number:  60 total reward:  0.6349999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[1.898]
 [1.898]
 [1.898]
 [1.898]
 [1.898]
 [1.898]
 [1.898]] [[0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.327]
 [0.337]
 [0.329]
 [0.266]
 [0.353]
 [0.34 ]] [[ 1.439]
 [-2.318]
 [-4.492]
 [-1.954]
 [-0.776]
 [-4.711]
 [-3.468]] [[0.24 ]
 [0.327]
 [0.337]
 [0.329]
 [0.266]
 [0.353]
 [0.34 ]]
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]] [[-1.613]
 [-0.825]
 [-0.825]
 [-0.825]
 [-0.825]
 [-0.825]
 [-0.825]] [[0.35 ]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]]
line 256 mcts: sample exp_bonus 0.7368999354899769
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
using another actor
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.346]
 [0.352]
 [0.352]
 [0.352]
 [0.349]
 [0.352]] [[ 0.   ]
 [-2.415]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-3.708]
 [ 0.   ]] [[0.352]
 [0.346]
 [0.352]
 [0.352]
 [0.352]
 [0.349]
 [0.352]]
Printing some Q and Qe and total Qs values:  [[ 1.019]
 [ 1.019]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [ 1.019]] [[0.521]
 [0.521]
 [1.99 ]
 [1.99 ]
 [1.99 ]
 [1.99 ]
 [0.521]] [[1.378]
 [1.378]
 [1.531]
 [1.531]
 [1.531]
 [1.531]
 [1.378]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5646102050338533, 0.3247776009487914, 0.09069603682096698, 0.0007808417387270047, 0.010269946788641737, 0.0088653686690196]
Printing some Q and Qe and total Qs values:  [[ 0.069]
 [-0.005]
 [ 0.021]
 [ 0.021]
 [ 0.001]
 [ 0.021]
 [ 0.021]] [[ 0.176]
 [-0.153]
 [-0.147]
 [-0.147]
 [ 0.001]
 [-0.147]
 [-0.147]] [[0.369]
 [0.11 ]
 [0.165]
 [0.165]
 [0.173]
 [0.165]
 [0.165]]
siam score:  -0.64362776
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5646102050338533, 0.3247776009487914, 0.09069603682096698, 0.0007808417387270047, 0.010269946788641737, 0.0088653686690196]
Printing some Q and Qe and total Qs values:  [[1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]] [[0.52 ]
 [0.514]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[2.074]
 [2.067]
 [2.074]
 [2.074]
 [2.074]
 [2.074]
 [2.074]]
start point for exploration sampling:  11106
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.64163953
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.6977],
        [-0.6176],
        [-0.0000],
        [-0.0000],
        [-0.6299],
        [-0.4527],
        [-0.6690],
        [-0.5925],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.0631738157985 0.6345507608458911
-0.024259925299500003 -0.6418355158879183
-0.97515 -0.97515
-0.86625 -0.86625
-0.024259925299500003 -0.6541882001142196
-0.0337698257985 -0.4864392493498852
-0.024259925299500003 -0.6932740781484534
-0.024259925299500003 -0.6167569615950875
-0.9503999999999999 -0.9503999999999999
-0.965448 -0.965448
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.784]
 [0.588]
 [0.588]] [[0.789]
 [0.789]
 [0.789]
 [0.789]
 [1.879]
 [0.789]
 [0.789]] [[0.757]
 [0.757]
 [0.757]
 [0.757]
 [1.555]
 [0.757]
 [0.757]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [ 0.571]
 [-0.016]
 [-0.016]] [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.51 ]
 [0.504]
 [0.504]] [[-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [ 1.16 ]
 [-0.018]
 [-0.018]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.077]
 [-0.067]
 [-0.071]
 [-0.07 ]
 [-0.069]
 [-0.071]] [[0.416]
 [0.505]
 [0.491]
 [0.489]
 [0.419]
 [0.506]
 [0.51 ]] [[0.887]
 [0.923]
 [0.928]
 [0.924]
 [0.902]
 [0.931]
 [0.93 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5646102050338533, 0.3247776009487915, 0.09069603682096698, 0.0007808417387270049, 0.010269946788641739, 0.008865368669019602]
first move QE:  0.5155850012109151
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5646102050338533, 0.3247776009487915, 0.090696036820967, 0.0007808417387270049, 0.010269946788641739, 0.008865368669019602]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.927]
 [0.117]
 [0.117]
 [0.117]
 [0.061]
 [0.117]
 [0.117]] [[0.355]
 [0.164]
 [0.164]
 [0.164]
 [0.65 ]
 [0.164]
 [0.164]] [[1.897]
 [0.359]
 [0.359]
 [0.359]
 [0.406]
 [0.359]
 [0.359]]
siam score:  -0.63197005
siam score:  -0.6321126
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.663]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]] [[-1.857]
 [-1.08 ]
 [-1.857]
 [-1.857]
 [-1.857]
 [-1.857]
 [-1.857]] [[0.396]
 [0.663]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.259]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]] [[-3.691]
 [-0.505]
 [-3.691]
 [-3.691]
 [-3.691]
 [-3.691]
 [-3.691]] [[0.227]
 [0.259]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]]
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [ 0.064]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.007]] [[0.544]
 [1.034]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [1.282]] [[-0.313]
 [ 0.058]
 [-0.313]
 [-0.313]
 [-0.313]
 [-0.313]
 [-0.001]]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.03 ]
 [-0.009]
 [-0.009]
 [-0.01 ]
 [-0.008]
 [-0.009]] [[0.855]
 [1.133]
 [0.516]
 [0.384]
 [0.717]
 [0.647]
 [1.033]] [[ 0.017]
 [ 0.255]
 [-0.319]
 [-0.452]
 [-0.121]
 [-0.187]
 [ 0.197]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5646102050338533, 0.3247776009487915, 0.090696036820967, 0.0007808417387270049, 0.010269946788641739, 0.008865368669019602]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.423]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]] [[-2.554]
 [-0.844]
 [-2.554]
 [-2.554]
 [-2.554]
 [-2.554]
 [-2.554]] [[0.175]
 [0.423]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
using another actor
4048 6686
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
4049 6694
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5646102050338533, 0.3247776009487915, 0.090696036820967, 0.0007808417387270049, 0.010269946788641739, 0.008865368669019602]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.052]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [-0.005]] [[0.718]
 [0.49 ]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.04 ]] [[1.195]
 [1.065]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.83 ]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.253]
 [0.199]
 [0.201]
 [0.204]
 [0.192]
 [0.191]] [[-1.864]
 [-0.794]
 [-1.703]
 [-2.015]
 [-1.577]
 [-2.113]
 [-1.905]] [[0.193]
 [0.253]
 [0.199]
 [0.201]
 [0.204]
 [0.192]
 [0.191]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5703752913763211, 0.31788308257807746, 0.09162211020492428, 0.0007888147084031268, 0.010374810514874813, 0.008955890617399268]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5703752913763211, 0.31788308257807746, 0.09162211020492428, 0.0007888147084031268, 0.010374810514874813, 0.008955890617399268]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5703752913763211, 0.31788308257807746, 0.09162211020492428, 0.0007888147084031268, 0.010374810514874813, 0.008955890617399268]
rdn beta is 0 so we're just using the maxi policy
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.4351489147185776
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
siam score:  -0.6243079
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.269]
 [0.264]
 [0.268]
 [0.273]
 [0.27 ]
 [0.272]] [[ 0.279]
 [ 0.264]
 [ 0.25 ]
 [ 0.106]
 [ 0.286]
 [-0.043]
 [-0.243]] [[0.268]
 [0.269]
 [0.264]
 [0.268]
 [0.273]
 [0.27 ]
 [0.272]]
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.15 ]
 [0.105]
 [0.105]
 [0.105]
 [0.088]
 [0.09 ]] [[4.624]
 [4.885]
 [4.74 ]
 [4.74 ]
 [4.74 ]
 [3.753]
 [4.786]] [[-0.314]
 [-0.042]
 [-0.178]
 [-0.178]
 [-0.178]
 [-0.544]
 [-0.193]]
Printing some Q and Qe and total Qs values:  [[ 0.042]
 [ 0.12 ]
 [ 0.451]
 [ 0.024]
 [ 0.451]
 [-0.023]
 [-0.012]] [[3.927]
 [4.752]
 [2.894]
 [3.689]
 [2.894]
 [4.52 ]
 [6.408]] [[-0.292]
 [ 0.141]
 [ 0.182]
 [-0.408]
 [ 0.182]
 [-0.222]
 [ 0.43 ]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5758958263527026, 0.32095980189307033, 0.0925089001336669, 0.0007964494696795412, 0.0007964494696795412, 0.009042572681200973]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
from probs:  [0.5758958263527026, 0.32095980189307033, 0.0925089001336669, 0.0007964494696795412, 0.0007964494696795412, 0.009042572681200973]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
using explorer policy with actor:  0
4061 6725
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5758958263527026, 0.32095980189307033, 0.09250890013366689, 0.0007964494696795412, 0.0007964494696795412, 0.009042572681200973]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5758958263527026, 0.32095980189307033, 0.09250890013366689, 0.0007964494696795412, 0.0007964494696795412, 0.009042572681200973]
using another actor
from probs:  [0.5758958263527026, 0.32095980189307033, 0.09250890013366689, 0.0007964494696795412, 0.0007964494696795412, 0.009042572681200973]
using explorer policy with actor:  0
from probs:  [0.5758958263527026, 0.32095980189307033, 0.09250890013366689, 0.0007964494696795412, 0.0007964494696795412, 0.009042572681200973]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.312]
 [0.19 ]
 [0.285]
 [0.2  ]
 [0.208]
 [0.254]] [[2.41 ]
 [2.575]
 [2.768]
 [2.919]
 [2.423]
 [2.469]
 [2.924]] [[-0.44 ]
 [-0.045]
 [-0.224]
 [ 0.017]
 [-0.32 ]
 [-0.287]
 [-0.043]]
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]] [[1.342]
 [1.342]
 [1.342]
 [1.342]
 [1.342]
 [1.342]
 [1.342]] [[0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]]
from probs:  [0.5758958263527026, 0.3209598018930704, 0.0925089001336669, 0.0007964494696795414, 0.0007964494696795414, 0.009042572681200975]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5758958263527026, 0.3209598018930704, 0.0925089001336669, 0.0007964494696795414, 0.0007964494696795414, 0.009042572681200975]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5758958263527026, 0.3209598018930704, 0.0925089001336669, 0.0007964494696795414, 0.0007964494696795414, 0.009042572681200975]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5758958263527026, 0.3209598018930704, 0.0925089001336669, 0.0007964494696795414, 0.0007964494696795414, 0.009042572681200975]
siam score:  -0.6128371
first move QE:  0.49877826915953366
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.162]
 [0.152]
 [0.139]
 [0.152]
 [0.152]
 [0.152]] [[ 0.602]
 [-0.262]
 [ 0.   ]
 [-1.649]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.124]
 [0.162]
 [0.152]
 [0.139]
 [0.152]
 [0.152]
 [0.152]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5758958263527026, 0.32095980189307033, 0.09250890013366689, 0.0007964494696795412, 0.0007964494696795412, 0.009042572681200973]
using explorer policy with actor:  0
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.9499831884594108
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5758958263527026, 0.32095980189307033, 0.09250890013366689, 0.0007964494696795412, 0.0007964494696795412, 0.009042572681200973]
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.03 ]
 [-0.014]
 [-0.012]
 [-0.007]
 [-0.012]
 [ 0.204]] [[1.535]
 [1.76 ]
 [1.453]
 [1.421]
 [1.427]
 [1.591]
 [1.184]] [[-0.437]
 [-0.247]
 [-0.522]
 [-0.549]
 [-0.534]
 [-0.38 ]
 [-0.354]]
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.27 ]
 [0.117]] [[1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.48 ]
 [1.609]] [[-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [ 0.148]
 [-0.028]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5758958263527026, 0.3209598018930704, 0.0925089001336669, 0.0007964494696795414, 0.0007964494696795414, 0.009042572681200975]
Printing some Q and Qe and total Qs values:  [[-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]] [[1.339]
 [1.339]
 [1.339]
 [1.339]
 [1.339]
 [1.339]
 [1.339]] [[1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.3657722826499499
siam score:  -0.6276962
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
line 256 mcts: sample exp_bonus 1.7123175887928002
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5758958263527026, 0.3209598018930704, 0.0925089001336669, 0.0007964494696795414, 0.0007964494696795414, 0.009042572681200975]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.239]] [[-1.45 ]
 [-1.45 ]
 [-1.45 ]
 [-1.45 ]
 [-1.45 ]
 [-1.45 ]
 [-2.298]] [[0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.239]]
4076 6746
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.316]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[3.385]
 [2.646]
 [3.385]
 [3.385]
 [3.385]
 [3.385]
 [3.385]] [[0.327]
 [0.316]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]]
in main func line 156:  4078
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5758958263527026, 0.3209598018930704, 0.0925089001336669, 0.0007964494696795414, 0.0007964494696795414, 0.009042572681200975]
using explorer policy with actor:  1
siam score:  -0.6248742
actor:  1 policy actor:  1  step number:  77 total reward:  0.5499999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.017]
 [-0.054]
 [-0.032]
 [-0.034]
 [-0.036]
 [-0.032]
 [-0.034]] [[1.933]
 [1.644]
 [1.549]
 [1.693]
 [1.679]
 [1.789]
 [1.896]] [[-0.478]
 [-0.841]
 [-0.892]
 [-0.752]
 [-0.769]
 [-0.652]
 [-0.548]]
4084 6753
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5859472162654918, 0.3133529630180498, 0.0903164128076166, 0.0007775733900203003, 0.0007775733900203003, 0.008828261128801416]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
4086 6756
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.117]
 [0.154]
 [0.167]
 [0.16 ]
 [0.159]
 [0.15 ]] [[3.269]
 [3.184]
 [3.201]
 [3.195]
 [3.065]
 [3.089]
 [3.062]] [[-0.145]
 [-0.277]
 [-0.196]
 [-0.171]
 [-0.23 ]
 [-0.223]
 [-0.251]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5859472162654918, 0.3133529630180498, 0.0903164128076166, 0.0007775733900203003, 0.0007775733900203003, 0.008828261128801416]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.264]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]] [[0.128]
 [0.937]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]] [[1.044]
 [1.53 ]
 [1.044]
 [1.044]
 [1.044]
 [1.044]
 [1.044]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5859472162654918, 0.3133529630180498, 0.0903164128076166, 0.0007775733900203003, 0.0007775733900203003, 0.008828261128801416]
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.049]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]] [[0.255]
 [1.433]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]] [[0.883]
 [1.552]
 [0.883]
 [0.883]
 [0.883]
 [0.883]
 [0.883]]
from probs:  [0.5859472162654918, 0.3133529630180498, 0.0903164128076166, 0.0007775733900203003, 0.0007775733900203003, 0.008828261128801416]
Printing some Q and Qe and total Qs values:  [[-0.099]
 [-0.113]
 [-0.105]
 [-0.102]
 [-0.098]
 [-0.102]
 [-0.102]] [[2.884]
 [2.605]
 [2.33 ]
 [2.072]
 [2.448]
 [1.963]
 [1.782]] [[1.067]
 [0.91 ]
 [0.777]
 [0.648]
 [0.844]
 [0.593]
 [0.5  ]]
line 256 mcts: sample exp_bonus 1.1163411532122247
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.05 ]
 [-0.04 ]
 [-0.038]
 [-0.051]
 [-0.024]
 [-0.041]] [[1.017]
 [1.199]
 [1.123]
 [0.848]
 [1.079]
 [1.098]
 [1.484]] [[-0.156]
 [ 0.039]
 [-0.033]
 [-0.336]
 [-0.095]
 [-0.044]
 [ 0.364]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.585947216265492, 0.3133529630180498, 0.0903164128076166, 0.0007775733900203003, 0.0007775733900203003, 0.008828261128801416]
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]] [[0.478]
 [0.473]
 [0.494]
 [0.476]
 [0.474]
 [0.482]
 [0.482]] [[0.812]
 [0.806]
 [0.833]
 [0.809]
 [0.806]
 [0.817]
 [0.817]]
in main func line 156:  4092
using explorer policy with actor:  1
siam score:  -0.6291888
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0950131868876996
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[-0.069]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.068]
 [-0.069]] [[1.702]
 [1.702]
 [1.702]
 [1.702]
 [1.702]
 [1.382]
 [1.702]] [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.301]
 [0.779]]
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.08 ]
 [0.059]
 [0.08 ]
 [0.021]
 [0.042]
 [0.048]] [[1.357]
 [1.429]
 [1.43 ]
 [1.429]
 [1.111]
 [1.245]
 [1.75 ]] [[0.93 ]
 [1.047]
 [1.021]
 [1.047]
 [0.626]
 [0.798]
 [1.353]]
from probs:  [0.585947216265492, 0.3133529630180498, 0.0903164128076166, 0.0007775733900203003, 0.0007775733900203003, 0.008828261128801416]
using another actor
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[0.02]
 [0.02]
 [0.02]
 [0.02]
 [0.02]
 [0.02]
 [0.02]] [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.012]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.038]] [[2.102]
 [0.722]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [1.425]] [[1.538]
 [0.763]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [1.188]]
Printing some Q and Qe and total Qs values:  [[0.894]
 [0.765]
 [0.615]
 [0.703]
 [0.756]
 [0.71 ]
 [0.679]] [[0.96 ]
 [1.907]
 [1.566]
 [1.112]
 [1.579]
 [1.309]
 [1.194]] [[0.894]
 [0.765]
 [0.615]
 [0.703]
 [0.756]
 [0.71 ]
 [0.679]]
first move QE:  0.49249780598187626
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.585947216265492, 0.3133529630180498, 0.0903164128076166, 0.0007775733900203003, 0.0007775733900203003, 0.008828261128801416]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.062]
 [-0.062]
 [-0.037]
 [-0.059]
 [-0.062]
 [-0.06 ]] [[2.838]
 [2.328]
 [2.055]
 [1.797]
 [1.955]
 [2.025]
 [2.026]] [[ 0.163]
 [-0.19 ]
 [-0.371]
 [-0.493]
 [-0.432]
 [-0.39 ]
 [-0.386]]
line 256 mcts: sample exp_bonus 0.7784485564594956
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.032]
 [-0.032]
 [-0.006]
 [-0.032]
 [-0.032]
 [-0.032]] [[0.924]
 [0.924]
 [0.924]
 [1.088]
 [0.924]
 [0.924]
 [0.924]] [[0.386]
 [0.386]
 [0.386]
 [0.491]
 [0.386]
 [0.386]
 [0.386]]
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.141]
 [-0.124]
 [-0.112]
 [-0.108]
 [-0.108]
 [-0.091]] [[1.455]
 [1.658]
 [1.763]
 [1.55 ]
 [2.967]
 [1.029]
 [1.056]] [[-0.31 ]
 [-0.245]
 [-0.14 ]
 [-0.257]
 [ 0.693]
 [-0.596]
 [-0.546]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[-0.101]
 [-0.126]
 [-0.119]
 [-0.119]
 [-0.124]
 [-0.1  ]
 [-0.143]] [[2.454]
 [2.182]
 [2.597]
 [2.597]
 [2.512]
 [2.308]
 [2.547]] [[0.382]
 [0.151]
 [0.442]
 [0.442]
 [0.374]
 [0.286]
 [0.36 ]]
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.054]
 [-0.045]
 [-0.045]
 [-0.044]
 [-0.044]
 [-0.043]] [[-1.799]
 [-0.672]
 [-1.612]
 [-1.978]
 [-1.747]
 [-1.205]
 [-0.642]] [[-0.118]
 [ 0.599]
 [ 0.001]
 [-0.233]
 [-0.085]
 [ 0.263]
 [ 0.624]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
line 256 mcts: sample exp_bonus 1.658197661786088
4109 6762
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.585947216265492, 0.3133529630180498, 0.0903164128076166, 0.0007775733900203003, 0.0007775733900203003, 0.008828261128801416]
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.006]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.014]
 [-0.014]] [[4.235]
 [4.811]
 [4.235]
 [4.235]
 [4.235]
 [3.7  ]
 [4.031]] [[ 0.09 ]
 [ 0.296]
 [ 0.09 ]
 [ 0.09 ]
 [ 0.09 ]
 [-0.091]
 [ 0.02 ]]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.002]
 [ 0.025]
 [ 0.021]
 [-0.08 ]
 [-0.063]
 [-0.07 ]] [[1.473]
 [1.686]
 [2.547]
 [2.182]
 [1.272]
 [1.945]
 [1.927]] [[ 0.187]
 [ 0.283]
 [ 0.625]
 [ 0.495]
 [-0.012]
 [ 0.248]
 [ 0.228]]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.585947216265492, 0.3133529630180498, 0.0903164128076166, 0.0007775733900203003, 0.0007775733900203003, 0.008828261128801416]
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.125]
 [0.092]
 [0.092]
 [0.092]
 [0.091]
 [0.092]] [[3.387]
 [4.529]
 [3.387]
 [3.387]
 [3.387]
 [3.382]
 [3.387]] [[-0.481]
 [-0.033]
 [-0.481]
 [-0.481]
 [-0.481]
 [-0.483]
 [-0.481]]
line 256 mcts: sample exp_bonus 0.7607794331769866
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.451]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[4.088]
 [3.23 ]
 [4.088]
 [4.088]
 [4.088]
 [4.088]
 [4.088]] [[0.456]
 [0.451]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.5949999999999998  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
using explorer policy with actor:  1
from probs:  [0.596227121471213, 0.30557318497452657, 0.08807407994896735, 0.0007582681684304237, 0.0007582681684304237, 0.008609077268432486]
using explorer policy with actor:  1
siam score:  -0.63168293
line 256 mcts: sample exp_bonus 0.52361527852269
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.596227121471213, 0.30557318497452657, 0.08807407994896735, 0.0007582681684304237, 0.0007582681684304237, 0.008609077268432486]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
siam score:  -0.6275699
actor:  1 policy actor:  1  step number:  149 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  0.333
from probs:  [0.5912570962686304, 0.3113617823860457, 0.08733991274778313, 0.0007519474027830708, 0.0007519474027830708, 0.008537313791974872]
Printing some Q and Qe and total Qs values:  [[-0.01]
 [ 0.31]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]] [[2.753]
 [2.731]
 [2.753]
 [2.753]
 [2.753]
 [2.753]
 [2.753]] [[1.436]
 [1.627]
 [1.436]
 [1.436]
 [1.436]
 [1.436]
 [1.436]]
Starting evaluation
siam score:  -0.627455
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5912570962686304, 0.3113617823860457, 0.08733991274778313, 0.0007519474027830708, 0.0007519474027830708, 0.008537313791974872]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.476]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[-0.3  ]
 [ 0.808]
 [-0.3  ]
 [-0.3  ]
 [-0.3  ]
 [-0.3  ]
 [-0.3  ]] [[0.362]
 [0.476]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]]
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[-1.037]
 [-1.037]
 [-1.037]
 [-1.037]
 [-1.037]
 [-1.037]
 [-1.037]] [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.237]
 [0.31 ]
 [0.314]
 [0.314]
 [0.311]
 [0.317]] [[-0.405]
 [ 0.55 ]
 [-0.438]
 [-0.568]
 [-0.373]
 [-0.699]
 [-1.743]] [[0.31 ]
 [0.237]
 [0.31 ]
 [0.314]
 [0.314]
 [0.311]
 [0.317]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5912570962686304, 0.3113617823860457, 0.08733991274778313, 0.0007519474027830708, 0.0007519474027830708, 0.008537313791974872]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.245]
 [0.386]
 [0.39 ]
 [0.391]
 [0.384]
 [0.383]] [[-0.338]
 [ 0.818]
 [-0.426]
 [-0.745]
 [-0.451]
 [-0.66 ]
 [-1.437]] [[0.39 ]
 [0.245]
 [0.386]
 [0.39 ]
 [0.391]
 [0.384]
 [0.383]]
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]] [[1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.382]] [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]]
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.625]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[1.899]
 [1.978]
 [1.899]
 [1.899]
 [1.899]
 [1.899]
 [1.899]] [[0.565]
 [0.625]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5912570962686304, 0.3113617823860457, 0.08733991274778313, 0.0007519474027830708, 0.0007519474027830708, 0.008537313791974872]
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
probs:  [0.5912570962686304, 0.3113617823860457, 0.08733991274778313, 0.0007519474027830708, 0.0007519474027830708, 0.008537313791974872]
4125 6783
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.199]] [[-1.098]
 [-1.098]
 [-1.098]
 [-1.098]
 [-1.098]
 [-1.098]
 [-1.569]] [[0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.199]]
using explorer policy with actor:  0
from probs:  [0.5912570962686304, 0.3113617823860457, 0.08733991274778313, 0.0007519474027830708, 0.0007519474027830708, 0.008537313791974872]
line 256 mcts: sample exp_bonus -1.0740380399107017
siam score:  -0.6125349
from probs:  [0.5912570962686304, 0.3113617823860457, 0.08733991274778313, 0.0007519474027830708, 0.0007519474027830708, 0.008537313791974872]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
siam score:  -0.6138214
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.477]
 [0.063]
 [0.063]] [[0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.626]
 [0.447]
 [0.447]] [[-0.388]
 [-0.388]
 [-0.388]
 [-0.388]
 [ 0.5  ]
 [-0.388]
 [-0.388]]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.276]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[-0.421]
 [ 1.056]
 [-0.421]
 [-0.421]
 [-0.421]
 [-0.421]
 [-0.421]] [[0.207]
 [0.739]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]]
line 256 mcts: sample exp_bonus 1.8644369462561206
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -0.6845000000000001 -0.6845000000000001
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[-2.162]
 [-2.162]
 [-2.162]
 [-2.162]
 [-2.162]
 [-2.162]
 [-2.162]] [[0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]]
rdn probs:  [0.5912570962686304, 0.3113617823860457, 0.08733991274778313, 0.0007519474027830708, 0.0007519474027830708, 0.008537313791974872]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.527]
 [0.539]
 [0.539]
 [0.532]
 [0.538]
 [0.525]] [[2.197]
 [3.328]
 [1.951]
 [1.951]
 [1.983]
 [1.995]
 [2.055]] [[0.535]
 [0.527]
 [0.539]
 [0.539]
 [0.532]
 [0.538]
 [0.525]]
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.169]
 [0.257]
 [0.248]
 [0.245]
 [0.247]
 [0.251]] [[ 0.366]
 [ 0.949]
 [-1.103]
 [-0.747]
 [-0.129]
 [-0.154]
 [-0.458]] [[0.253]
 [0.169]
 [0.257]
 [0.248]
 [0.245]
 [0.247]
 [0.251]]
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.57 ]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]] [[-2.192]
 [ 0.411]
 [-2.192]
 [-2.192]
 [-2.192]
 [-2.192]
 [-2.192]] [[0.393]
 [0.57 ]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2565172601077825
maxi score, test score, baseline:  -0.97413 -1.0 -0.97413
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.    0.    0.02  0.918 0.02 ]
maxi score, test score, baseline:  -0.97413 -1.0 -0.97413
probs:  [0.5820361486998817, 0.31696589847265444, 0.08973361903046533, 0.00048206435764254377, 0.00048206435764254377, 0.010300205081713535]
maxi score, test score, baseline:  -0.97413 -1.0 -0.97413
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -1.0 -0.97413
actor:  1 policy actor:  1  step number:  87 total reward:  0.38999999999999957  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[1.833]
 [1.833]
 [1.833]
 [1.833]
 [1.833]
 [1.833]
 [1.833]] [[1.04]
 [1.04]
 [1.04]
 [1.04]
 [1.04]
 [1.04]
 [1.04]]
siam score:  -0.6148853
maxi score, test score, baseline:  -0.97413 -1.0 -0.97413
first move QE:  0.4898863631210489
maxi score, test score, baseline:  -0.97413 -1.0 -0.97413
Printing some Q and Qe and total Qs values:  [[1.229]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[1.453]
 [1.151]
 [1.151]
 [1.151]
 [1.151]
 [1.151]
 [1.151]] [[1.894]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97413 -1.0 -0.97413
probs:  [0.5894026734999649, 0.31137944130747996, 0.08815208290495362, 0.00047356807492631407, 0.00047356807492631407, 0.01011866613774897]
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.562]] [[2.518]
 [2.888]
 [2.888]
 [2.888]
 [2.888]
 [2.888]
 [2.506]] [[1.059]
 [1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]
 [0.996]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[2.421]
 [2.421]
 [2.421]
 [2.421]
 [2.421]
 [2.421]
 [2.421]] [[0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]]
maxi score, test score, baseline:  -0.97413 -1.0 -0.97413
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.465]
 [0.453]
 [0.465]
 [0.459]
 [0.437]
 [0.434]] [[ 1.147]
 [ 2.418]
 [ 1.38 ]
 [-0.048]
 [ 1.17 ]
 [ 1.657]
 [ 0.839]] [[0.432]
 [0.465]
 [0.453]
 [0.465]
 [0.459]
 [0.437]
 [0.434]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6520238002570438
maxi score, test score, baseline:  -0.97413 -1.0 -0.97413
probs:  [0.5894026734999649, 0.31137944130747996, 0.08815208290495362, 0.00047356807492631407, 0.00047356807492631407, 0.01011866613774897]
UNIT TEST: sample policy line 217 mcts : [0.306 0.245 0.02  0.02  0.102 0.02  0.286]
siam score:  -0.61906445
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.432]
 [0.425]
 [0.495]
 [0.425]
 [0.425]
 [0.459]] [[-0.105]
 [ 0.309]
 [ 0.243]
 [ 0.059]
 [ 0.243]
 [ 0.243]
 [ 1.038]] [[2.066]
 [2.112]
 [2.081]
 [2.152]
 [2.081]
 [2.081]
 [2.383]]
siam score:  -0.6232218
maxi score, test score, baseline:  -0.97413 -1.0 -0.97413
from probs:  [0.5894026734999649, 0.31137944130747996, 0.08815208290495362, 0.00047356807492631407, 0.00047356807492631407, 0.01011866613774897]
maxi score, test score, baseline:  -0.97413 -1.0 -0.97413
probs:  [0.5894026734999649, 0.31137944130747996, 0.08815208290495362, 0.00047356807492631407, 0.00047356807492631407, 0.01011866613774897]
siam score:  -0.62627393
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[1.673]
 [1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]] [[1.597]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.4  ]]
using explorer policy with actor:  1
from probs:  [0.5894026734999649, 0.31137944130747996, 0.08815208290495362, 0.00047356807492631407, 0.00047356807492631407, 0.01011866613774897]
maxi score, test score, baseline:  -0.97413 -1.0 -0.97413
probs:  [0.5894026734999649, 0.31137944130747996, 0.08815208290495362, 0.00047356807492631407, 0.00047356807492631407, 0.01011866613774897]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.545]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[-3.567]
 [ 0.3  ]
 [-3.567]
 [-3.567]
 [-3.567]
 [-3.567]
 [-3.567]] [[0.535]
 [0.545]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]]
actor:  0 policy actor:  1  step number:  61 total reward:  0.5599999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.662]
 [0.719]
 [0.691]
 [0.691]
 [0.799]
 [0.691]] [[2.168]
 [1.004]
 [1.082]
 [2.168]
 [2.168]
 [3.341]
 [2.168]] [[0.648]
 [0.2  ]
 [0.341]
 [0.648]
 [0.648]
 [1.256]
 [0.648]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.415]
 [0.468]
 [0.502]
 [0.422]
 [0.422]
 [0.357]] [[2.529]
 [2.273]
 [1.855]
 [1.251]
 [2.529]
 [2.529]
 [2.309]] [[1.775]
 [1.637]
 [1.508]
 [1.257]
 [1.775]
 [1.775]
 [1.569]]
siam score:  -0.626414
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
probs:  [0.5894813977074705, 0.31133296224283435, 0.0881318324597793, 0.00047561418931146783, 0.00047561418931146783, 0.010102579211292873]
siam score:  -0.6263396
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
probs:  [0.5894813977074705, 0.31133296224283435, 0.0881318324597793, 0.00047561418931146783, 0.00047561418931146783, 0.010102579211292873]
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
probs:  [0.5986078391270351, 0.30067091352998465, 0.08949630300841942, 0.00048297772114461114, 0.00048297772114461114, 0.010258988892271692]
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
probs:  [0.5986078391270351, 0.30067091352998465, 0.08949630300841942, 0.00048297772114461114, 0.00048297772114461114, 0.010258988892271692]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.292]
 [0.292]
 [0.292]
 [0.111]
 [0.292]
 [0.292]] [[2.097]
 [1.521]
 [1.521]
 [1.521]
 [1.92 ]
 [1.521]
 [1.521]] [[1.235]
 [0.946]
 [0.946]
 [0.946]
 [1.101]
 [0.946]
 [0.946]]
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
probs:  [0.5986078391270351, 0.30067091352998465, 0.08949630300841942, 0.00048297772114461114, 0.00048297772114461114, 0.010258988892271692]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.049558122493354
actor:  1 policy actor:  1  step number:  67 total reward:  0.5699999999999997  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.485]
 [0.483]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[1.155]
 [0.997]
 [1.12 ]
 [0.997]
 [0.997]
 [0.997]
 [0.997]] [[0.399]
 [0.276]
 [0.355]
 [0.276]
 [0.276]
 [0.276]
 [0.276]]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.467]] [[1.723]
 [1.723]
 [1.723]
 [1.723]
 [1.723]
 [1.723]
 [1.676]] [[1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.052]]
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
probs:  [0.6076087674384454, 0.29392858619572915, 0.08748941327304666, 0.0004721472957706417, 0.0004721472957706417, 0.010028938501237481]
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.398]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.382]] [[-0.039]
 [-1.701]
 [-0.666]
 [-0.666]
 [-0.666]
 [-0.666]
 [-0.166]] [[0.376]
 [0.398]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.382]]
Printing some Q and Qe and total Qs values:  [[ 0.141]
 [-0.025]
 [ 0.131]
 [ 0.165]
 [ 0.168]
 [ 0.118]
 [ 0.186]] [[0.387]
 [0.538]
 [0.223]
 [0.11 ]
 [0.402]
 [0.154]
 [0.135]] [[ 0.15 ]
 [-0.131]
 [ 0.076]
 [ 0.106]
 [ 0.209]
 [ 0.028]
 [ 0.157]]
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]] [[0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]]
using explorer policy with actor:  1
in main func line 156:  4168
siam score:  -0.6414795
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
probs:  [0.6076087674384454, 0.29392858619572915, 0.08748941327304666, 0.0004721472957706417, 0.0004721472957706417, 0.010028938501237481]
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
probs:  [0.6076087674384454, 0.29392858619572915, 0.08748941327304666, 0.0004721472957706417, 0.0004721472957706417, 0.010028938501237481]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.669]
 [0.677]
 [0.698]
 [0.701]
 [0.71 ]
 [0.709]] [[3.349]
 [3.707]
 [3.784]
 [3.392]
 [3.312]
 [3.527]
 [3.778]] [[1.542]
 [1.723]
 [1.781]
 [1.593]
 [1.554]
 [1.692]
 [1.834]]
start point for exploration sampling:  11106
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5374],
        [-0.4804],
        [-0.0000],
        [-0.3292],
        [-0.0000],
        [-0.5648],
        [ 0.2585],
        [-0.5606],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.0337698257985 -0.5711362666259525
-0.0530787758985 -0.533467291893769
-0.00989999999999924 -0.00989999999999924
-0.024259925299500003 -0.35344783283235165
-0.97515 -0.97515
-0.024259925299500003 -0.5890615127975607
-0.043375785898500004 0.21515569346188396
-0.024259925299500003 -0.5848204258157719
-0.9355500000000001 -0.9355500000000001
-0.5095544849999997 -0.5095544849999997
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
probs:  [0.6076087674384454, 0.29392858619572915, 0.08748941327304666, 0.0004721472957706417, 0.0004721472957706417, 0.010028938501237481]
actor:  1 policy actor:  1  step number:  61 total reward:  0.5699999999999997  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.598]
 [0.655]
 [0.591]
 [0.597]
 [0.628]
 [0.569]] [[3.476]
 [3.533]
 [4.386]
 [3.733]
 [3.806]
 [3.632]
 [3.867]] [[0.625]
 [0.727]
 [1.41 ]
 [0.848]
 [0.908]
 [0.853]
 [0.892]]
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.702]
 [0.55 ]
 [0.627]
 [0.622]
 [0.55 ]
 [0.659]] [[2.395]
 [2.65 ]
 [3.39 ]
 [2.529]
 [2.573]
 [3.39 ]
 [2.414]] [[0.503]
 [0.689]
 [0.88 ]
 [0.46 ]
 [0.478]
 [0.88 ]
 [0.447]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.563]
 [0.428]
 [0.417]
 [0.428]
 [0.428]
 [0.428]] [[4.489]
 [6.068]
 [4.489]
 [3.301]
 [4.489]
 [4.489]
 [4.489]] [[0.638]
 [1.456]
 [0.638]
 [0.134]
 [0.638]
 [0.638]
 [0.638]]
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.027644017780553
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.371]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]] [[1.446]
 [2.205]
 [1.446]
 [1.446]
 [1.446]
 [1.446]
 [1.446]] [[1.278]
 [1.905]
 [1.278]
 [1.278]
 [1.278]
 [1.278]
 [1.278]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 2.099939854487083
in main func line 156:  4179
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
probs:  [0.6069114109250263, 0.2723404101293264, 0.10978753990348239, 0.0004716054092646471, 0.0004716054092646471, 0.010017428223635659]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.433]
 [0.497]
 [0.535]
 [0.545]
 [0.524]
 [0.502]] [[0.226]
 [0.881]
 [0.385]
 [0.397]
 [0.566]
 [0.275]
 [0.693]] [[0.347]
 [0.451]
 [0.413]
 [0.492]
 [0.568]
 [0.43 ]
 [0.525]]
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
probs:  [0.6069114109250263, 0.2723404101293264, 0.10978753990348239, 0.0004716054092646471, 0.0004716054092646471, 0.010017428223635659]
using explorer policy with actor:  1
from probs:  [0.6069114109250263, 0.2723404101293264, 0.10978753990348239, 0.0004716054092646471, 0.0004716054092646471, 0.010017428223635659]
using explorer policy with actor:  1
siam score:  -0.65843827
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.536]
 [0.613]
 [0.482]
 [0.466]
 [0.468]
 [0.429]] [[3.999]
 [5.008]
 [4.021]
 [4.644]
 [3.523]
 [3.226]
 [4.084]] [[0.875]
 [1.525]
 [1.351]
 [1.296]
 [0.89 ]
 [0.794]
 [1.003]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  106 total reward:  0.4049999999999996  reward:  1.0 rdn_beta:  0.5
4187 6836
actor:  1 policy actor:  1  step number:  42 total reward:  0.6749999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.406]
 [0.511]
 [0.521]
 [0.517]
 [0.524]
 [0.529]] [[0.687]
 [1.107]
 [0.769]
 [0.731]
 [1.012]
 [1.038]
 [1.05 ]] [[0.314]
 [0.65 ]
 [0.409]
 [0.379]
 [0.744]
 [0.793]
 [0.82 ]]
maxi score, test score, baseline:  -0.97101 -1.0 -0.97101
probs:  [0.60615104816418, 0.261009052152132, 0.12233530359374829, 0.0004519831662276284, 0.0004519831662276284, 0.009600629757484555]
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.631]
 [0.631]
 [0.631]
 [1.12 ]
 [0.631]
 [0.631]] [[1.073]
 [1.073]
 [1.073]
 [1.073]
 [5.538]
 [1.073]
 [1.073]] [[0.078]
 [0.078]
 [0.078]
 [0.078]
 [1.583]
 [0.078]
 [0.078]]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.286]
 [0.254]
 [0.256]
 [0.273]
 [0.268]
 [0.268]] [[-5.298]
 [-4.816]
 [-5.106]
 [-5.034]
 [-5.422]
 [-5.065]
 [-4.942]] [[0.263]
 [0.286]
 [0.254]
 [0.256]
 [0.273]
 [0.268]
 [0.268]]
maxi score, test score, baseline:  -0.97101 -1.0 -0.97101
maxi score, test score, baseline:  -0.97101 -1.0 -0.97101
maxi score, test score, baseline:  -0.97101 -1.0 -0.97101
Printing some Q and Qe and total Qs values:  [[0.989]
 [0.908]
 [1.149]
 [0.987]
 [1.158]
 [1.003]
 [0.8  ]] [[1.039]
 [1.511]
 [1.569]
 [1.496]
 [1.671]
 [0.73 ]
 [1.11 ]] [[1.81 ]
 [1.949]
 [2.189]
 [2.012]
 [2.243]
 [1.686]
 [1.674]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.5549999999999997  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.4221344068656022
first move QE:  0.4774595005211906
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.523]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.423]] [[2.07 ]
 [1.605]
 [2.07 ]
 [2.07 ]
 [2.07 ]
 [2.07 ]
 [1.881]] [[0.665]
 [0.456]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.532]]
Printing some Q and Qe and total Qs values:  [[1.416]
 [1.416]
 [1.416]
 [1.416]
 [1.416]
 [1.416]
 [1.416]] [[0.477]
 [0.471]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]] [[2.116]
 [2.113]
 [2.116]
 [2.116]
 [2.116]
 [2.116]
 [2.116]]
from probs:  [0.5936541981270965, 0.2762446259946366, 0.11981315016700365, 0.0004426647531608859, 0.0004426647531608859, 0.009402696204941479]
in main func line 156:  4196
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
probs:  [0.5936541981270965, 0.2762446259946366, 0.11981315016700365, 0.0004426647531608859, 0.0004426647531608859, 0.009402696204941479]
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.381]
 [0.365]
 [0.374]
 [0.372]
 [0.35 ]
 [0.381]] [[-0.795]
 [ 2.347]
 [-0.685]
 [-0.732]
 [-0.079]
 [-0.538]
 [ 0.819]] [[0.372]
 [0.381]
 [0.365]
 [0.374]
 [0.372]
 [0.35 ]
 [0.381]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 3.8940502627967013
from probs:  [0.5936541981270966, 0.27624462599463656, 0.11981315016700364, 0.0004426647531608858, 0.0004426647531608858, 0.009402696204941477]
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.094]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.297]] [[1.169]
 [1.337]
 [1.255]
 [1.255]
 [1.255]
 [1.255]
 [1.339]] [[1.454]
 [1.323]
 [1.489]
 [1.489]
 [1.489]
 [1.489]
 [1.604]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9710099999999999 -1.0 -0.9710099999999999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.97101 -1.0 -0.97101
probs:  [0.5936541981270966, 0.27624462599463656, 0.11981315016700367, 0.0004426647531608858, 0.0004426647531608858, 0.00940269620494148]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.47598440090262656
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.97101 -1.0 -0.97101
maxi score, test score, baseline:  -0.97101 -1.0 -0.97101
probs:  [0.5936541981270966, 0.27624462599463656, 0.11981315016700367, 0.0004426647531608858, 0.0004426647531608858, 0.00940269620494148]
maxi score, test score, baseline:  -0.97101 -1.0 -0.97101
probs:  [0.5936541981270966, 0.27624462599463656, 0.11981315016700367, 0.0004426647531608858, 0.0004426647531608858, 0.00940269620494148]
maxi score, test score, baseline:  -0.97101 -1.0 -0.97101
4208 6867
from probs:  [0.5936541981270966, 0.27624462599463656, 0.11981315016700367, 0.0004426647531608858, 0.0004426647531608858, 0.00940269620494148]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.363]
 [0.363]
 [0.463]
 [0.363]
 [0.203]
 [0.363]] [[1.385]
 [1.664]
 [1.664]
 [1.882]
 [1.664]
 [2.078]
 [1.664]] [[0.317]
 [0.481]
 [0.481]
 [0.9  ]
 [0.481]
 [0.575]
 [0.481]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.491]
 [1.093]
 [0.465]
 [0.465]
 [0.481]
 [0.465]
 [0.481]] [[0.524]
 [1.026]
 [1.172]
 [1.172]
 [0.719]
 [1.172]
 [0.869]] [[0.895]
 [2.267]
 [1.059]
 [1.059]
 [0.94 ]
 [1.059]
 [0.991]]
maxi score, test score, baseline:  -0.97101 -1.0 -0.97101
probs:  [0.5936541981270966, 0.27624462599463656, 0.11981315016700367, 0.0004426647531608858, 0.0004426647531608858, 0.00940269620494148]
siam score:  -0.62661433
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.995]
 [1.086]
 [0.859]
 [0.859]
 [1.078]
 [0.859]
 [0.98 ]] [[1.783]
 [1.637]
 [1.913]
 [1.913]
 [1.89 ]
 [1.913]
 [2.138]] [[1.   ]
 [1.036]
 [0.858]
 [0.858]
 [1.273]
 [0.858]
 [1.325]]
using another actor
actor:  1 policy actor:  1  step number:  52 total reward:  0.6349999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.582]
 [0.582]
 [0.582]
 [0.636]
 [0.663]
 [0.561]] [[2.151]
 [2.157]
 [2.157]
 [2.157]
 [2.079]
 [2.133]
 [2.208]] [[0.499]
 [0.441]
 [0.441]
 [0.441]
 [0.497]
 [0.588]
 [0.432]]
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.446]
 [0.446]
 [0.446]
 [0.488]
 [0.446]
 [0.446]] [[0.834]
 [0.803]
 [0.803]
 [0.803]
 [1.321]
 [0.803]
 [0.803]] [[0.7  ]
 [0.954]
 [0.954]
 [0.954]
 [1.524]
 [0.954]
 [0.954]]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.565]
 [0.42 ]
 [0.416]
 [0.481]
 [0.476]
 [0.312]] [[1.935]
 [1.562]
 [2.06 ]
 [1.978]
 [1.92 ]
 [1.983]
 [1.952]] [[ 0.079]
 [ 0.044]
 [ 0.085]
 [ 0.024]
 [ 0.115]
 [ 0.147]
 [-0.201]]
maxi score, test score, baseline:  -0.97101 -1.0 -0.97101
actor:  1 policy actor:  1  step number:  105 total reward:  0.26999999999999946  reward:  1.0 rdn_beta:  0.333
from probs:  [0.5947969552732219, 0.27967970903932626, 0.1155973123736861, 0.00042708881017345903, 0.00042708881017345903, 0.009071845693418975]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97101 -1.0 -0.97101
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.324]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[-2.05 ]
 [-0.424]
 [-2.05 ]
 [-2.05 ]
 [-2.05 ]
 [-2.05 ]
 [-2.05 ]] [[0.17 ]
 [0.665]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.633]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[1.441]
 [0.738]
 [1.441]
 [1.441]
 [1.441]
 [1.441]
 [1.441]] [[0.855]
 [1.009]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]]
Printing some Q and Qe and total Qs values:  [[ 0.11 ]
 [ 0.11 ]
 [-0.001]
 [-0.003]
 [ 0.11 ]
 [ 0.11 ]
 [ 0.11 ]] [[2.001]
 [2.001]
 [1.085]
 [1.169]
 [2.001]
 [2.001]
 [2.001]] [[1.898]
 [1.898]
 [1.552]
 [1.579]
 [1.898]
 [1.898]
 [1.898]]
start point for exploration sampling:  11106
using another actor
from probs:  [0.5947969552732219, 0.27967970903932626, 0.1155973123736861, 0.00042708881017345903, 0.00042708881017345903, 0.009071845693418975]
actor:  1 policy actor:  1  step number:  63 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.4  ]
 [0.402]
 [0.398]
 [0.398]
 [0.423]
 [0.412]] [[-0.397]
 [ 1.624]
 [-0.961]
 [-1.074]
 [ 0.028]
 [-0.803]
 [-0.892]] [[0.414]
 [0.4  ]
 [0.402]
 [0.398]
 [0.398]
 [0.423]
 [0.412]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[1.639]
 [1.639]
 [1.639]
 [1.639]
 [1.639]
 [1.639]
 [1.639]] [[1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.572]]
actor:  0 policy actor:  1  step number:  49 total reward:  0.5499999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.60222941582168, 0.2745554481879788, 0.1134800690728072, 0.0004211293970608361, 0.0004211293970608361, 0.008892808123412373]
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.60222941582168, 0.2745554481879788, 0.1134800690728072, 0.0004211293970608361, 0.0004211293970608361, 0.008892808123412373]
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.60222941582168, 0.2745554481879788, 0.1134800690728072, 0.0004211293970608361, 0.0004211293970608361, 0.008892808123412373]
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
siam score:  -0.6213813
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.60222941582168, 0.2745554481879788, 0.1134800690728072, 0.0004211293970608361, 0.0004211293970608361, 0.008892808123412373]
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.60222941582168, 0.2745554481879788, 0.1134800690728072, 0.0004211293970608361, 0.0004211293970608361, 0.008892808123412373]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
UNIT TEST: sample policy line 217 mcts : [0.02  0.082 0.735 0.02  0.061 0.02  0.061]
Printing some Q and Qe and total Qs values:  [[0.979]
 [1.466]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]] [[0.911]
 [0.55 ]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]] [[2.101]
 [2.679]
 [2.101]
 [2.101]
 [2.101]
 [2.101]
 [2.101]]
Printing some Q and Qe and total Qs values:  [[0.87 ]
 [1.274]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]] [[1.087]
 [3.996]
 [1.087]
 [1.087]
 [1.087]
 [1.087]
 [1.087]] [[1.289]
 [2.353]
 [1.289]
 [1.289]
 [1.289]
 [1.289]
 [1.289]]
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.60222941582168, 0.2745554481879788, 0.1134800690728072, 0.0004211293970608361, 0.0004211293970608361, 0.008892808123412373]
line 256 mcts: sample exp_bonus 0.5861459060395151
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
using explorer policy with actor:  1
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 0.42872294183116977
using explorer policy with actor:  0
4228 6886
siam score:  -0.6249478
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.311]
 [0.343]
 [0.343]
 [0.347]
 [0.343]
 [0.343]] [[-1.751]
 [ 0.097]
 [-1.88 ]
 [-1.88 ]
 [-1.616]
 [-1.88 ]
 [-1.88 ]] [[0.344]
 [0.311]
 [0.343]
 [0.343]
 [0.347]
 [0.343]
 [0.343]]
from probs:  [0.6022294158216801, 0.27455544818797883, 0.11348006907280722, 0.0004211293970608362, 0.0004211293970608362, 0.008892808123412374]
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.6022294158216801, 0.27455544818797883, 0.11348006907280722, 0.0004211293970608362, 0.0004211293970608362, 0.008892808123412374]
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.6022294158216801, 0.27455544818797883, 0.11348006907280722, 0.0004211293970608362, 0.0004211293970608362, 0.008892808123412374]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.462]
 [0.442]
 [0.451]
 [0.101]
 [0.442]
 [0.396]] [[ 0.   ]
 [ 0.115]
 [ 0.   ]
 [-1.164]
 [ 0.119]
 [ 0.   ]
 [ 0.256]] [[0.697]
 [0.775]
 [0.697]
 [0.326]
 [0.054]
 [0.697]
 [0.689]]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.374]
 [0.464]
 [0.465]
 [0.465]
 [0.463]
 [0.468]] [[-2.339]
 [-0.575]
 [-2.886]
 [-2.828]
 [-2.854]
 [-2.597]
 [-2.735]] [[0.45 ]
 [1.463]
 [0.124]
 [0.16 ]
 [0.145]
 [0.299]
 [0.219]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5349999999999997  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6096554949512599, 0.26942970343761496, 0.1113614811075718, 0.00041326722637562526, 0.00041326722637562526, 0.008726786050802236]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.6421983
siam score:  -0.6406329
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.6096554949512599, 0.26942970343761496, 0.1113614811075718, 0.00041326722637562526, 0.00041326722637562526, 0.008726786050802236]
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.6096554949512599, 0.26942970343761496, 0.1113614811075718, 0.00041326722637562526, 0.00041326722637562526, 0.008726786050802236]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.291]
 [0.153]
 [0.22 ]
 [0.153]
 [0.153]
 [0.277]] [[-0.038]
 [ 0.578]
 [-0.038]
 [-0.417]
 [-0.038]
 [-0.038]
 [ 0.426]] [[1.416]
 [1.679]
 [1.416]
 [1.344]
 [1.416]
 [1.416]
 [1.626]]
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
Printing some Q and Qe and total Qs values:  [[0.796]
 [1.133]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[0.798]
 [0.43 ]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]] [[0.986]
 [1.537]
 [0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]]
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.801]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]] [[1.069]
 [0.024]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]] [[0.864]
 [1.382]
 [0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.734]
 [0.348]
 [0.348]] [[0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.517]
 [0.503]
 [0.503]] [[0.093]
 [0.093]
 [0.093]
 [0.093]
 [0.87 ]
 [0.093]
 [0.093]]
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.695]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[0.541]
 [0.46 ]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[0.078]
 [0.784]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]]
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.6147663667135574, 0.2716883899821094, 0.11229504810387324, 0.00041673173348670295, 0.00041673173348670295, 0.00041673173348670295]
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.6147663667135574, 0.2716883899821094, 0.11229504810387324, 0.00041673173348670295, 0.00041673173348670295, 0.00041673173348670295]
using another actor
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.6147663667135574, 0.2716883899821094, 0.11229504810387324, 0.00041673173348670295, 0.00041673173348670295, 0.00041673173348670295]
4249 6947
siam score:  -0.6549562
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.9523151313777131
line 256 mcts: sample exp_bonus 0.6123149741780616
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
maxi score, test score, baseline:  -0.96791 -1.0 -0.96791
probs:  [0.6097005315071813, 0.2752611014283908, 0.11377173175512474, 0.00042221176976780665, 0.00042221176976780665, 0.00042221176976780665]
using explorer policy with actor:  1
using explorer policy with actor:  1
4255 6958
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.378]
 [0.378]
 [0.396]
 [0.378]
 [0.341]
 [0.39 ]] [[2.324]
 [1.204]
 [1.204]
 [0.58 ]
 [1.204]
 [0.525]
 [1.041]] [[ 1.535]
 [ 0.702]
 [ 0.702]
 [-0.087]
 [ 0.702]
 [-0.27 ]
 [ 0.51 ]]
using another actor
from probs:  [0.6097005315071812, 0.2752611014283908, 0.11377173175512476, 0.0004222117697678067, 0.0004222117697678067, 0.0004222117697678067]
from probs:  [0.6097005315071812, 0.2752611014283908, 0.11377173175512476, 0.0004222117697678067, 0.0004222117697678067, 0.0004222117697678067]
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.401]
 [0.246]
 [0.246]
 [0.211]
 [0.398]
 [0.415]] [[ 0.94 ]
 [-0.91 ]
 [ 0.246]
 [ 0.246]
 [-0.071]
 [-3.152]
 [-1.379]] [[1.53 ]
 [0.925]
 [1.314]
 [1.314]
 [1.177]
 [0.053]
 [0.748]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
4258 6971
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.927]
 [0.4  ]
 [0.437]
 [0.37 ]
 [0.427]
 [0.443]] [[ 1.501]
 [ 0.682]
 [-3.739]
 [-1.403]
 [ 0.036]
 [-2.949]
 [-0.728]] [[1.746]
 [1.784]
 [0.06 ]
 [0.872]
 [1.335]
 [0.34 ]
 [1.105]]
using explorer policy with actor:  1
siam score:  -0.6648099
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.6096485750158162, 0.2753008873687134, 0.11378766973638656, 0.00042095595969470943, 0.00042095595969470943, 0.00042095595969470943]
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.02  0.02  0.02  0.796 0.102]
4261 6981
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
from probs:  [0.6096485750158162, 0.2753008873687133, 0.11378766973638656, 0.00042095595969470943, 0.00042095595969470943, 0.00042095595969470943]
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.174]
 [0.089]
 [0.112]
 [0.089]
 [0.089]
 [0.147]] [[ 0.427]
 [ 1.271]
 [ 0.427]
 [-0.034]
 [ 0.427]
 [ 0.427]
 [ 1.191]] [[0.089]
 [0.174]
 [0.089]
 [0.112]
 [0.089]
 [0.089]
 [0.147]]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.853]
 [0.622]
 [0.622]
 [0.8  ]
 [0.622]
 [0.816]] [[-0.127]
 [ 0.577]
 [ 1.517]
 [ 1.517]
 [ 0.267]
 [ 1.517]
 [-0.058]] [[1.479]
 [1.784]
 [2.02 ]
 [2.02 ]
 [1.63 ]
 [2.02 ]
 [1.51 ]]
siam score:  -0.66795677
using explorer policy with actor:  1
first move QE:  0.42050161956552373
using another actor
from probs:  [0.6025143405897045, 0.28033240753873584, 0.11586730326336354, 0.0004286495360654183, 0.0004286495360654183, 0.0004286495360654183]
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.6025143405897045, 0.2803324075387359, 0.11586730326336354, 0.0004286495360654183, 0.0004286495360654183, 0.0004286495360654183]
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.547]
 [0.559]
 [0.554]
 [0.559]
 [0.562]
 [0.56 ]] [[-0.341]
 [ 0.157]
 [-1.38 ]
 [-0.99 ]
 [-0.314]
 [-1.296]
 [-1.006]] [[0.554]
 [0.547]
 [0.559]
 [0.554]
 [0.559]
 [0.562]
 [0.56 ]]
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.6025143405897045, 0.2803324075387359, 0.11586730326336354, 0.0004286495360654183, 0.0004286495360654183, 0.0004286495360654183]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.319]
 [0.419]
 [0.423]
 [0.426]
 [0.429]
 [0.434]] [[-1.07 ]
 [ 0.676]
 [-1.65 ]
 [-1.662]
 [-0.766]
 [-1.342]
 [-1.403]] [[0.471]
 [1.29 ]
 [0.175]
 [0.172]
 [0.634]
 [0.341]
 [0.313]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.6025143405897045, 0.2803324075387359, 0.11586730326336354, 0.0004286495360654183, 0.0004286495360654183, 0.0004286495360654183]
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.6025143405897045, 0.2803324075387359, 0.11586730326336354, 0.0004286495360654183, 0.0004286495360654183, 0.0004286495360654183]
line 256 mcts: sample exp_bonus 0.7204757128900995
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.388]
 [0.437]
 [0.416]
 [0.101]
 [0.417]
 [0.402]] [[ 0.75 ]
 [-0.172]
 [ 0.442]
 [-0.443]
 [ 0.534]
 [-2.839]
 [ 0.465]] [[1.836]
 [1.455]
 [1.795]
 [1.372]
 [1.385]
 [0.305]
 [1.757]]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.65 ]
 [0.646]
 [0.646]
 [0.722]
 [0.646]
 [0.881]] [[0.675]
 [0.351]
 [0.958]
 [0.958]
 [0.674]
 [0.958]
 [0.854]] [[1.35 ]
 [1.071]
 [1.267]
 [1.267]
 [1.323]
 [1.267]
 [1.7  ]]
4269 7028
using explorer policy with actor:  1
4269 7032
start point for exploration sampling:  11106
siam score:  -0.6645319
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.34 ]
 [0.402]
 [0.444]
 [0.468]
 [0.671]
 [0.44 ]] [[1.09 ]
 [1.214]
 [1.606]
 [1.395]
 [0.744]
 [1.43 ]
 [1.772]] [[0.762]
 [0.788]
 [1.045]
 [1.057]
 [0.887]
 [1.522]
 [1.176]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.461]
 [0.436]
 [0.436]
 [0.777]
 [0.436]
 [0.436]] [[0.283]
 [0.416]
 [0.283]
 [0.283]
 [0.437]
 [0.283]
 [0.283]] [[1.118]
 [1.201]
 [1.118]
 [1.118]
 [1.76 ]
 [1.118]
 [1.118]]
Printing some Q and Qe and total Qs values:  [[1.108]
 [1.108]
 [1.108]
 [1.108]
 [1.108]
 [1.108]
 [1.108]] [[0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]] [[1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]]
from probs:  [0.5946421447879014, 0.28588438545162403, 0.1181620529649485, 0.0004371389318420237, 0.0004371389318420237, 0.0004371389318420237]
Printing some Q and Qe and total Qs values:  [[1.019]
 [1.019]
 [1.019]
 [1.019]
 [1.242]
 [1.019]
 [1.019]] [[ 0.256]
 [ 0.256]
 [ 0.256]
 [ 0.256]
 [-0.117]
 [ 0.256]
 [ 0.256]] [[1.437]
 [1.437]
 [1.437]
 [1.437]
 [1.634]
 [1.437]
 [1.437]]
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.5946421447879014, 0.28588438545162403, 0.1181620529649485, 0.0004371389318420237, 0.0004371389318420237, 0.0004371389318420237]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.227]
 [0.216]
 [0.215]
 [0.215]
 [0.215]
 [0.212]] [[-4.9  ]
 [-4.876]
 [-5.041]
 [-4.958]
 [-4.887]
 [-4.866]
 [-4.967]] [[0.213]
 [0.227]
 [0.216]
 [0.215]
 [0.215]
 [0.215]
 [0.212]]
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.318]
 [0.093]
 [0.093]
 [0.093]
 [0.093]
 [0.034]] [[0.741]
 [0.558]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.647]] [[ 0.097]
 [ 0.486]
 [ 0.097]
 [ 0.097]
 [ 0.097]
 [ 0.097]
 [-0.051]]
using explorer policy with actor:  1
first move QE:  0.415755970669771
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
using another actor
from probs:  [0.5946421447879016, 0.285884385451624, 0.1181620529649485, 0.0004371389318420237, 0.0004371389318420237, 0.0004371389318420237]
first move QE:  0.41400467717338535
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.158]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[-3.021]
 [-2.687]
 [-3.021]
 [-3.021]
 [-3.021]
 [-3.021]
 [-3.021]] [[0.154]
 [0.158]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]]
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.5946421447879014, 0.28588438545162403, 0.1181620529649485, 0.0004371389318420237, 0.0004371389318420237, 0.0004371389318420237]
Printing some Q and Qe and total Qs values:  [[1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.333]] [[0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.337]] [[2.205]
 [2.205]
 [2.205]
 [2.205]
 [2.205]
 [2.205]
 [2.101]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.5946421447879014, 0.28588438545162403, 0.1181620529649485, 0.0004371389318420237, 0.0004371389318420237, 0.0004371389318420237]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.397]
 [0.394]
 [0.394]] [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.146]
 [0.607]
 [0.607]] [[0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.845]
 [0.993]
 [0.993]]
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.5946421447879016, 0.28588438545162403, 0.1181620529649485, 0.0004371389318420237, 0.0004371389318420237, 0.0004371389318420237]
siam score:  -0.6780696
Printing some Q and Qe and total Qs values:  [[0.823]
 [1.126]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]] [[1.739]
 [1.294]
 [1.739]
 [1.739]
 [1.739]
 [1.739]
 [1.739]] [[1.5  ]
 [1.956]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.5  ]]
siam score:  -0.675168
Starting evaluation
Printing some Q and Qe and total Qs values:  [[1.313]
 [1.313]
 [1.313]
 [1.313]
 [1.313]
 [1.313]
 [1.313]] [[0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[2.428]
 [2.428]
 [2.428]
 [2.428]
 [2.428]
 [2.428]
 [2.428]]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.713]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[1.582]
 [1.759]
 [1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]] [[0.534]
 [0.925]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.27 ]
 [0.256]
 [0.256]
 [0.313]
 [0.252]
 [0.26 ]] [[-3.559]
 [-3.452]
 [-3.487]
 [-3.487]
 [-2.818]
 [-3.307]
 [-3.573]] [[0.254]
 [0.27 ]
 [0.256]
 [0.256]
 [0.313]
 [0.252]
 [0.26 ]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.5946421447879016, 0.2858843854516241, 0.11816205296494849, 0.0004371389318420238, 0.0004371389318420238, 0.0004371389318420238]
siam score:  -0.6679895
line 256 mcts: sample exp_bonus -1.3983723746468473
line 256 mcts: sample exp_bonus -2.011302164711006
from probs:  [0.5946421447879016, 0.2858843854516241, 0.11816205296494849, 0.0004371389318420238, 0.0004371389318420238, 0.0004371389318420238]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.557]
 [0.401]
 [0.401]] [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.59 ]
 [0.615]
 [0.615]] [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.921]
 [0.617]
 [0.617]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
line 256 mcts: sample exp_bonus -1.4280224089014348
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.5946421447879016, 0.2858843854516241, 0.11816205296494849, 0.0004371389318420238, 0.0004371389318420238, 0.0004371389318420238]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
rdn probs:  [0.5946421447879016, 0.2858843854516241, 0.11816205296494849, 0.0004371389318420238, 0.0004371389318420238, 0.0004371389318420238]
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.656]
 [0.753]
 [0.768]
 [0.768]
 [0.756]
 [0.775]] [[0.473]
 [2.564]
 [0.259]
 [0.289]
 [0.418]
 [0.396]
 [0.052]] [[0.775]
 [0.656]
 [0.753]
 [0.768]
 [0.768]
 [0.756]
 [0.775]]
4286 7073
siam score:  -0.6817369
4286 7075
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
4286 7081
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.843]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]] [[2.555]
 [2.807]
 [2.555]
 [2.555]
 [2.555]
 [2.555]
 [2.555]] [[0.832]
 [0.843]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.22 ]
 [0.331]
 [0.332]
 [0.328]
 [0.276]
 [0.272]] [[-0.314]
 [ 0.734]
 [-0.284]
 [-0.258]
 [-0.121]
 [ 0.332]
 [ 0.199]] [[0.25 ]
 [0.381]
 [0.263]
 [0.274]
 [0.31 ]
 [0.359]
 [0.306]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.6317],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.6189],
        [-0.0000],
        [-0.1600],
        [-0.0000],
        [-0.6317]], dtype=torch.float64)
-0.9608890648499999 -0.9608890648499999
-0.024259925299500003 -0.655949201608905
0.9458940149999999 0.9458940149999999
0.99 0.99
-0.9372103732035 -0.9372103732035
-0.024259925299500003 -0.6431265304421516
-0.8415 -0.8415
-0.024259925299500003 -0.18421256176232167
-0.9503999999999999 -0.9503999999999999
-0.024259925299500003 -0.655949201608905
line 256 mcts: sample exp_bonus 0.5309772472307827
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.5946421447879016, 0.28588438545162403, 0.1181620529649485, 0.0004371389318420237, 0.0004371389318420237, 0.0004371389318420237]
using another actor
from probs:  [0.5946421447879016, 0.28588438545162403, 0.1181620529649485, 0.0004371389318420237, 0.0004371389318420237, 0.0004371389318420237]
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.5946421447879016, 0.28588438545162403, 0.1181620529649485, 0.0004371389318420237, 0.0004371389318420237, 0.0004371389318420237]
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.5946421447879016, 0.28588438545162403, 0.1181620529649485, 0.0004371389318420237, 0.0004371389318420237, 0.0004371389318420237]
in main func line 156:  4290
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
using another actor
from probs:  [0.5946421447879016, 0.28588438545162403, 0.1181620529649485, 0.0004371389318420237, 0.0004371389318420237, 0.0004371389318420237]
maxi score, test score, baseline:  -0.9700900000000001 -1.0 -0.9700900000000001
probs:  [0.5946421447879016, 0.28588438545162403, 0.11816205296494849, 0.0004371389318420237, 0.0004371389318420237, 0.0004371389318420237]
siam score:  -0.67037123
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.5946421447879016, 0.2858843854516241, 0.11816205296494849, 0.0004371389318420238, 0.0004371389318420238, 0.0004371389318420238]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.512]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.488]] [[1.146]
 [1.092]
 [1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.918]] [[0.439]
 [0.512]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.488]]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.261]
 [0.394]
 [0.436]
 [0.278]
 [0.436]
 [0.357]] [[ 0.71 ]
 [ 0.199]
 [-4.259]
 [ 0.794]
 [ 0.676]
 [ 0.794]
 [ 0.687]] [[2.086]
 [1.779]
 [0.284]
 [2.191]
 [1.974]
 [2.191]
 [2.064]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.5946421447879016, 0.2858843854516241, 0.11816205296494849, 0.0004371389318420238, 0.0004371389318420238, 0.0004371389318420238]
from probs:  [0.5946421447879016, 0.2858843854516241, 0.11816205296494849, 0.0004371389318420238, 0.0004371389318420238, 0.0004371389318420238]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.261]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]] [[ 1.009]
 [-0.228]
 [ 0.671]
 [ 0.671]
 [ 0.671]
 [ 0.671]
 [ 0.671]] [[1.264]
 [0.095]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
line 256 mcts: sample exp_bonus -0.20192108679438667
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[0.66]
 [0.66]
 [0.66]
 [0.66]
 [0.66]
 [0.66]
 [0.66]] [[0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]]
using explorer policy with actor:  1
from probs:  [0.6054681320471179, 0.2910891640594574, 0.10210741159286724, 0.0004450974335192253, 0.0004450974335192253, 0.0004450974335192253]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6054681320471179, 0.2910891640594574, 0.10210741159286724, 0.0004450974335192253, 0.0004450974335192253, 0.0004450974335192253]
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.087]
 [0.194]
 [0.197]
 [0.198]
 [0.196]
 [0.198]] [[-2.371]
 [ 0.978]
 [-2.504]
 [-2.88 ]
 [-2.919]
 [-2.646]
 [-2.327]] [[0.194]
 [0.087]
 [0.194]
 [0.197]
 [0.198]
 [0.196]
 [0.198]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.1866],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.6520]], dtype=torch.float64)
-0.9702 -0.9702
-0.9454499999999999 -0.9454499999999999
-0.9419653234529999 -0.9419653234529999
-0.8831196 -0.8831196
-0.024259925299500003 -0.21089173567080188
-0.9703485 -0.9703485
-0.9467202737024999 -0.9467202737024999
-0.9703485 -0.9703485
-0.6851980099799998 -0.6851980099799998
-0.024259925299500003 -0.6762351105502808
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6054681320471179, 0.2910891640594574, 0.10210741159286724, 0.0004450974335192253, 0.0004450974335192253, 0.0004450974335192253]
start point for exploration sampling:  11106
from probs:  [0.6054681320471179, 0.2910891640594574, 0.10210741159286724, 0.0004450974335192253, 0.0004450974335192253, 0.0004450974335192253]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
from probs:  [0.6054681320471179, 0.2910891640594574, 0.10210741159286724, 0.0004450974335192253, 0.0004450974335192253, 0.0004450974335192253]
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.198]
 [0.209]
 [0.203]
 [0.195]
 [0.184]
 [0.194]] [[1.799]
 [1.79 ]
 [1.737]
 [1.861]
 [1.88 ]
 [1.863]
 [1.852]] [[0.187]
 [0.198]
 [0.209]
 [0.203]
 [0.195]
 [0.184]
 [0.194]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6054681320471179, 0.2910891640594574, 0.10210741159286724, 0.0004450974335192253, 0.0004450974335192253, 0.0004450974335192253]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.179]
 [0.194]
 [0.198]
 [0.2  ]
 [0.2  ]
 [0.204]] [[0.772]
 [1.084]
 [0.364]
 [0.499]
 [0.532]
 [0.509]
 [0.587]] [[0.745]
 [1.034]
 [0.344]
 [0.488]
 [0.524]
 [0.501]
 [0.587]]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.043]
 [-0.002]
 [-0.   ]
 [-0.   ]
 [ 0.014]
 [ 0.007]] [[1.007]
 [0.908]
 [0.545]
 [0.599]
 [0.51 ]
 [0.269]
 [0.607]] [[ 0.352]
 [ 0.17 ]
 [-0.111]
 [-0.054]
 [-0.143]
 [-0.356]
 [-0.032]]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.376]
 [0.374]
 [0.344]
 [0.374]
 [0.374]
 [0.273]] [[1.703]
 [1.707]
 [1.703]
 [1.686]
 [1.703]
 [1.703]
 [2.214]] [[0.993]
 [1.   ]
 [0.993]
 [0.924]
 [0.993]
 [0.993]
 [1.4  ]]
siam score:  -0.65584105
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.469]
 [0.469]
 [0.469]
 [0.484]
 [0.489]
 [0.486]] [[0.902]
 [1.584]
 [1.584]
 [1.584]
 [1.111]
 [1.179]
 [1.387]] [[2.201]
 [2.471]
 [2.471]
 [2.471]
 [2.286]
 [2.321]
 [2.407]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.456]
 [0.514]
 [0.514]] [[0.457]
 [0.457]
 [0.457]
 [0.457]
 [1.306]
 [0.457]
 [0.457]] [[1.727]
 [1.727]
 [1.727]
 [1.727]
 [2.192]
 [1.727]
 [1.727]]
from probs:  [0.6054681320471179, 0.2910891640594574, 0.10210741159286724, 0.0004450974335192253, 0.0004450974335192253, 0.0004450974335192253]
siam score:  -0.6483979
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
line 256 mcts: sample exp_bonus 0.4081880479211536
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.412]
 [0.412]
 [0.529]
 [0.412]
 [0.412]
 [0.54 ]] [[ 1.797]
 [ 0.901]
 [ 0.901]
 [-0.339]
 [ 0.901]
 [ 0.901]
 [ 0.035]] [[1.624]
 [0.966]
 [0.966]
 [0.19 ]
 [0.966]
 [0.966]
 [0.475]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6054681320471179, 0.2910891640594574, 0.10210741159286724, 0.0004450974335192253, 0.0004450974335192253, 0.0004450974335192253]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[1.062]
 [1.062]
 [1.062]
 [1.062]
 [1.062]
 [1.062]
 [1.062]] [[0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.866]
 [0.615]
 [0.757]
 [0.782]
 [0.808]
 [0.729]] [[4.386]
 [5.942]
 [4.32 ]
 [3.608]
 [4.349]
 [4.819]
 [3.841]] [[0.664]
 [1.541]
 [0.498]
 [0.543]
 [0.841]
 [1.051]
 [0.564]]
siam score:  -0.6422374
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.55 ]
 [0.493]] [[ 0.82]
 [ 0.4 ]
 [ 0.4 ]
 [ 0.4 ]
 [ 0.4 ]
 [-4.02]
 [ 0.4 ]] [[1.865]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [0.172]
 [1.766]]
4317 7136
actor:  1 policy actor:  1  step number:  63 total reward:  0.6199999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.38 ]
 [0.467]
 [0.473]
 [0.434]
 [0.47 ]
 [0.436]] [[-0.361]
 [ 0.209]
 [-0.244]
 [-0.331]
 [-0.015]
 [-0.154]
 [ 0.049]] [[0.411]
 [0.611]
 [0.484]
 [0.438]
 [0.57 ]
 [0.549]
 [0.618]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.579264465967064
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.382]
 [0.485]
 [0.493]
 [0.479]
 [0.482]
 [0.468]] [[0.301]
 [0.614]
 [0.239]
 [0.233]
 [0.459]
 [0.33 ]
 [0.263]] [[0.577]
 [0.734]
 [0.565]
 [0.575]
 [0.772]
 [0.65 ]
 [0.554]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.082 0.041 0.143 0.51  0.082 0.102]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6141526168123669, 0.2846816730151463, 0.09985981049286391, 0.00043529989320770154, 0.00043529989320770154, 0.00043529989320770154]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6141526168123669, 0.2846816730151463, 0.09985981049286391, 0.00043529989320770154, 0.00043529989320770154, 0.00043529989320770154]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6141526168123669, 0.2846816730151463, 0.09985981049286391, 0.00043529989320770154, 0.00043529989320770154, 0.00043529989320770154]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.291]
 [0.382]
 [0.355]
 [0.383]
 [0.385]
 [0.385]] [[0.347]
 [0.563]
 [0.475]
 [0.574]
 [0.564]
 [0.385]
 [0.534]] [[0.074]
 [0.183]
 [0.247]
 [0.325]
 [0.368]
 [0.133]
 [0.331]]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.301]
 [0.59 ]
 [0.509]
 [0.458]
 [0.584]
 [0.327]] [[ 1.291]
 [ 0.858]
 [-2.579]
 [ 0.602]
 [ 0.897]
 [-3.412]
 [ 0.771]] [[1.85 ]
 [1.549]
 [0.511]
 [1.699]
 [1.756]
 [0.167]
 [1.546]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.344]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[0.489]
 [1.125]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[0.086]
 [0.322]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
4333 7152
4333 7153
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]] [[2.341]
 [1.924]
 [1.924]
 [1.924]
 [1.924]
 [1.924]
 [1.924]] [[2.201]
 [1.774]
 [1.774]
 [1.774]
 [1.774]
 [1.774]
 [1.774]]
using another actor
from probs:  [0.6141526168123669, 0.2846816730151463, 0.09985981049286391, 0.00043529989320770154, 0.00043529989320770154, 0.00043529989320770154]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6141526168123669, 0.2846816730151463, 0.09985981049286391, 0.00043529989320770154, 0.00043529989320770154, 0.00043529989320770154]
siam score:  -0.6564908
using explorer policy with actor:  1
siam score:  -0.6541759
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[1.132]
 [1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.128]] [[0.99 ]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]] [[1.401]
 [1.203]
 [1.203]
 [1.203]
 [1.203]
 [1.203]
 [1.203]] [[1.891]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]]
Printing some Q and Qe and total Qs values:  [[0.901]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[1.114]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]] [[1.332]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
rdn beta is 0 so we're just using the maxi policy
first move QE:  0.4052722187867041
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6141526168123669, 0.2846816730151463, 0.09985981049286391, 0.00043529989320770154, 0.00043529989320770154, 0.00043529989320770154]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.452]
 [0.48 ]
 [0.445]
 [0.456]
 [0.555]
 [0.552]] [[-1.851]
 [-0.861]
 [-1.362]
 [-1.073]
 [-0.953]
 [-1.895]
 [-1.934]] [[0.755]
 [0.916]
 [0.806]
 [0.832]
 [0.895]
 [0.777]
 [0.759]]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.278]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[-0.491]
 [ 0.47 ]
 [-0.491]
 [-0.491]
 [-0.491]
 [-0.491]
 [-0.491]] [[-0.116]
 [ 0.077]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.314]
 [0.324]
 [0.324]
 [0.32 ]
 [0.324]
 [0.324]] [[-0.278]
 [ 0.656]
 [-0.278]
 [-0.278]
 [-0.322]
 [-0.278]
 [-0.278]] [[0.182]
 [0.475]
 [0.182]
 [0.182]
 [0.159]
 [0.182]
 [0.182]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6230408634619472, 0.2743293183248642, 0.10130501906438759, 0.00044159971626707, 0.00044159971626707, 0.00044159971626707]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6230408634619472, 0.2743293183248642, 0.10130501906438759, 0.00044159971626707, 0.00044159971626707, 0.00044159971626707]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6230408634619472, 0.2743293183248642, 0.10130501906438759, 0.00044159971626707, 0.00044159971626707, 0.00044159971626707]
line 256 mcts: sample exp_bonus 1.163326887960531
Printing some Q and Qe and total Qs values:  [[1.021]
 [0.76 ]
 [0.76 ]
 [0.191]
 [0.76 ]
 [0.104]
 [0.76 ]] [[0.913]
 [0.696]
 [0.696]
 [1.249]
 [0.696]
 [1.889]
 [0.696]] [[1.712]
 [1.044]
 [1.044]
 [0.275]
 [1.044]
 [0.529]
 [1.044]]
line 256 mcts: sample exp_bonus -2.606563779145043
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.244]
 [0.208]
 [0.208]
 [0.208]
 [0.223]
 [0.217]] [[3.737]
 [4.813]
 [3.737]
 [3.737]
 [3.737]
 [3.714]
 [3.972]] [[0.482]
 [0.914]
 [0.482]
 [0.482]
 [0.482]
 [0.506]
 [0.58 ]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2634888398093478
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6230408634619472, 0.2743293183248642, 0.10130501906438759, 0.00044159971626707, 0.00044159971626707, 0.00044159971626707]
from probs:  [0.6230408634619472, 0.2743293183248642, 0.10130501906438759, 0.00044159971626707, 0.00044159971626707, 0.00044159971626707]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]] [[0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]] [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6230408634619472, 0.2743293183248642, 0.10130501906438759, 0.00044159971626707, 0.00044159971626707, 0.00044159971626707]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6230408634619472, 0.2743293183248642, 0.10130501906438759, 0.00044159971626707, 0.00044159971626707, 0.00044159971626707]
using another actor
siam score:  -0.6637717
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6230408634619472, 0.2743293183248642, 0.10130501906438759, 0.00044159971626707, 0.00044159971626707, 0.00044159971626707]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6230408634619472, 0.2743293183248642, 0.10130501906438759, 0.00044159971626707, 0.00044159971626707, 0.00044159971626707]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.364]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[3.454]
 [2.818]
 [3.454]
 [3.454]
 [3.454]
 [3.454]
 [3.454]] [[0.44 ]
 [0.197]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3137022529712379
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
actor:  1 policy actor:  1  step number:  109 total reward:  0.3099999999999995  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6141526168123669, 0.2846816730151463, 0.09985981049286391, 0.00043529989320770154, 0.00043529989320770154, 0.00043529989320770154]
siam score:  -0.670282
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.271]
 [0.55 ]
 [0.529]
 [0.326]
 [0.536]
 [0.486]] [[ 1.576]
 [ 1.199]
 [-2.593]
 [ 1.095]
 [ 0.853]
 [-2.893]
 [ 1.229]] [[1.898]
 [1.627]
 [0.354]
 [1.917]
 [1.55 ]
 [0.206]
 [1.919]]
siam score:  -0.6702818
actor:  1 policy actor:  1  step number:  52 total reward:  0.6449999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]] [[0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.543]
 [0.625]
 [1.055]
 [0.689]
 [0.555]
 [0.632]] [[ 0.196]
 [ 0.501]
 [ 0.386]
 [-0.11 ]
 [ 0.08 ]
 [ 0.488]
 [ 0.294]] [[0.653]
 [0.933]
 [0.983]
 [1.347]
 [0.804]
 [0.945]
 [0.904]]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]] [[1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.326]
 [0.39 ]
 [0.373]
 [0.348]
 [0.371]
 [0.36 ]] [[ 0.168]
 [ 0.017]
 [-1.895]
 [-0.385]
 [ 0.471]
 [-1.838]
 [-0.49 ]] [[ 0.215]
 [ 0.235]
 [-0.276]
 [ 0.193]
 [ 0.429]
 [-0.296]
 [ 0.132]]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
line 256 mcts: sample exp_bonus -1.4837179201783446
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6227205324780597, 0.2783601877019337, 0.09764237823343903, 0.0004256338621892275, 0.0004256338621892275, 0.0004256338621892275]
using explorer policy with actor:  1
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6227205324780597, 0.2783601877019337, 0.09764237823343903, 0.0004256338621892275, 0.0004256338621892275, 0.0004256338621892275]
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]] [[0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]] [[0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]]
using explorer policy with actor:  0
4368 7213
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.6227205324780598, 0.27836018770193366, 0.09764237823343902, 0.0004256338621892274, 0.0004256338621892274, 0.0004256338621892274]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.342]
 [0.524]
 [0.519]
 [0.684]
 [0.57 ]
 [0.341]] [[ 0.417]
 [ 1.468]
 [-1.264]
 [ 0.45 ]
 [ 1.813]
 [-2.058]
 [ 1.012]] [[0.901]
 [1.071]
 [0.524]
 [1.086]
 [1.871]
 [0.349]
 [0.918]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5849999999999997  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  56 total reward:  0.6149999999999998  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.581]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]] [[0.505]
 [2.525]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[0.391]
 [1.356]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6502495149550749
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.166]
 [0.509]
 [0.515]
 [0.513]
 [0.522]
 [0.53 ]] [[0.688]
 [1.026]
 [0.451]
 [0.631]
 [0.718]
 [0.744]
 [0.956]] [[0.599]
 [0.388]
 [0.306]
 [0.559]
 [0.67 ]
 [0.724]
 [1.023]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.617392095120477, 0.26714684873350725, 0.11423559269949876, 0.00040848781550571615, 0.00040848781550571615, 0.00040848781550571615]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
from probs:  [0.6173920951204771, 0.2671468487335072, 0.11423559269949875, 0.0004084878155057161, 0.0004084878155057161, 0.0004084878155057161]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
probs:  [0.617392095120477, 0.26714684873350725, 0.11423559269949876, 0.00040848781550571615, 0.00040848781550571615, 0.00040848781550571615]
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.45 ]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.295]] [[ 1.048]
 [ 1.989]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [ 1.054]] [[0.306]
 [0.45 ]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.295]]
siam score:  -0.6619115
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]] [[0.9]
 [0.9]
 [0.9]
 [0.9]
 [0.9]
 [0.9]
 [0.9]]
siam score:  -0.6600702
from probs:  [0.617392095120477, 0.26714684873350725, 0.11423559269949878, 0.0004084878155057162, 0.0004084878155057162, 0.0004084878155057162]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.292]
 [0.294]
 [0.294]
 [0.294]
 [0.3  ]
 [0.294]] [[3.303]
 [3.855]
 [3.303]
 [3.303]
 [3.303]
 [3.952]
 [3.303]] [[0.294]
 [0.292]
 [0.294]
 [0.294]
 [0.294]
 [0.3  ]
 [0.294]]
siam score:  -0.656493
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9700899999999999 -1.0 -0.9700899999999999
actor:  0 policy actor:  1  step number:  86 total reward:  0.22499999999999942  reward:  1.0 rdn_beta:  0.167
4382 7237
from probs:  [0.6174502409106783, 0.26709782912050767, 0.11422238264091454, 0.0004098491092998341, 0.0004098491092998341, 0.0004098491092998341]
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
probs:  [0.6174502409106783, 0.26709782912050767, 0.11422238264091454, 0.0004098491092998341, 0.0004098491092998341, 0.0004098491092998341]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.47999999999999965  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[2.891]
 [1.254]
 [1.254]
 [1.254]
 [1.254]
 [1.254]
 [1.254]] [[1.754]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]]
4391 7242
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.39 ]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[-0.553]
 [-0.439]
 [-0.553]
 [-0.553]
 [-0.553]
 [-0.553]
 [-0.553]] [[0.658]
 [0.791]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]]
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[2.515]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]] [[1.911]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]]
siam score:  -0.65474397
siam score:  -0.6580585
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.03 ]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]] [[1.621]
 [1.797]
 [1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.382]] [[-0.75 ]
 [-0.633]
 [-0.89 ]
 [-0.89 ]
 [-0.89 ]
 [-0.89 ]
 [-0.89 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
probs:  [0.6239107400355848, 0.2625870818247695, 0.11229339540314175, 0.00040292757883467694, 0.00040292757883467694, 0.00040292757883467694]
siam score:  -0.6556488
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
probs:  [0.6239107400355848, 0.2625870818247695, 0.11229339540314175, 0.00040292757883467694, 0.00040292757883467694, 0.00040292757883467694]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.13 ]
 [0.095]
 [0.13 ]
 [0.09 ]
 [0.13 ]
 [0.099]] [[0.998]
 [0.998]
 [0.031]
 [0.998]
 [0.976]
 [0.998]
 [1.088]] [[ 0.563]
 [ 0.563]
 [-0.474]
 [ 0.563]
 [ 0.462]
 [ 0.563]
 [ 0.591]]
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
probs:  [0.6239107400355848, 0.2625870818247695, 0.11229339540314175, 0.00040292757883467694, 0.00040292757883467694, 0.00040292757883467694]
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.165]
 [0.165]
 [0.165]
 [0.162]
 [0.165]
 [0.165]] [[-0.042]
 [ 0.812]
 [ 0.812]
 [ 0.812]
 [ 0.442]
 [ 0.812]
 [ 0.812]] [[-0.164]
 [ 0.426]
 [ 0.426]
 [ 0.426]
 [ 0.174]
 [ 0.426]
 [ 0.426]]
4398 7245
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.487]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.338]] [[2.079]
 [2.365]
 [2.079]
 [2.079]
 [2.079]
 [2.079]
 [2.116]] [[0.261]
 [0.487]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]] [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.11 ]
 [0.332]
 [0.355]
 [0.39 ]
 [0.362]
 [0.364]] [[-0.128]
 [ 0.535]
 [-0.153]
 [-0.1  ]
 [ 0.168]
 [-0.204]
 [ 0.24 ]] [[ 0.07 ]
 [ 0.009]
 [-0.006]
 [ 0.076]
 [ 0.325]
 [ 0.021]
 [ 0.321]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
probs:  [0.6239107400355848, 0.2625870818247695, 0.11229339540314175, 0.00040292757883467694, 0.00040292757883467694, 0.00040292757883467694]
from probs:  [0.6239107400355848, 0.2625870818247695, 0.11229339540314175, 0.00040292757883467694, 0.00040292757883467694, 0.00040292757883467694]
siam score:  -0.6513795
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
probs:  [0.6239107400355848, 0.2625870818247695, 0.11229339540314175, 0.00040292757883467694, 0.00040292757883467694, 0.00040292757883467694]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
from probs:  [0.6239107400355849, 0.2625870818247695, 0.11229339540314176, 0.000402927578834677, 0.000402927578834677, 0.000402927578834677]
siam score:  -0.639053
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.519]
 [0.519]
 [0.519]
 [0.54 ]
 [0.519]
 [0.56 ]] [[0.748]
 [0.709]
 [0.709]
 [0.709]
 [0.727]
 [0.709]
 [0.834]] [[1.468]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.454]
 [1.4  ]
 [1.564]]
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
probs:  [0.6239107400355849, 0.2625870818247695, 0.11229339540314176, 0.000402927578834677, 0.000402927578834677, 0.000402927578834677]
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.237]
 [0.239]
 [0.237]
 [0.237]
 [0.239]
 [0.237]] [[1.79 ]
 [1.79 ]
 [2.964]
 [1.79 ]
 [1.79 ]
 [2.048]
 [1.79 ]] [[0.237]
 [0.237]
 [0.239]
 [0.237]
 [0.237]
 [0.239]
 [0.237]]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.466]
 [0.466]
 [0.466]
 [0.441]
 [0.466]
 [0.466]] [[1.315]
 [1.553]
 [1.553]
 [1.553]
 [1.307]
 [1.553]
 [1.553]] [[1.326]
 [1.6  ]
 [1.6  ]
 [1.6  ]
 [1.386]
 [1.6  ]
 [1.6  ]]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
probs:  [0.6239107400355849, 0.2625870818247695, 0.11229339540314176, 0.000402927578834677, 0.000402927578834677, 0.000402927578834677]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.045]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.019]] [[0.802]
 [1.511]
 [1.15 ]
 [1.15 ]
 [1.15 ]
 [1.15 ]
 [2.142]] [[0.178]
 [0.967]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [1.813]]
from probs:  [0.6239107400355849, 0.2625870818247695, 0.11229339540314176, 0.000402927578834677, 0.000402927578834677, 0.000402927578834677]
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.47599836480555285
using explorer policy with actor:  1
siam score:  -0.6403477
line 256 mcts: sample exp_bonus 1.5779586069754257
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
probs:  [0.6346283414645675, 0.24991972856661865, 0.11422238264091454, 0.0004098491092998341, 0.0004098491092998341, 0.0004098491092998341]
4416 7285
from probs:  [0.6475903478929528, 0.23459966694175127, 0.11655532487075615, 0.00041822009817995973, 0.00041822009817995973, 0.00041822009817995973]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
using another actor
using explorer policy with actor:  1
first move QE:  0.40032077485429984
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
probs:  [0.6577306815815105, 0.22261459828253335, 0.11838041366525914, 0.0004247688235656988, 0.0004247688235656988, 0.0004247688235656988]
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.385]
 [0.21 ]
 [0.201]
 [0.203]
 [0.206]
 [0.208]] [[-0.545]
 [ 0.118]
 [ 0.   ]
 [-0.42 ]
 [-0.792]
 [-0.461]
 [-0.883]] [[0.197]
 [0.385]
 [0.21 ]
 [0.201]
 [0.203]
 [0.206]
 [0.208]]
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
probs:  [0.6577306815815105, 0.22261459828253335, 0.11838041366525914, 0.0004247688235656988, 0.0004247688235656988, 0.0004247688235656988]
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
4421 7314
using another actor
maxi score, test score, baseline:  -0.9676400000000001 -1.0 -0.9676400000000001
probs:  [0.6577306815815105, 0.22261459828253335, 0.11838041366525914, 0.0004247688235656988, 0.0004247688235656988, 0.0004247688235656988]
from probs:  [0.6577306815815105, 0.22261459828253335, 0.11838041366525914, 0.0004247688235656988, 0.0004247688235656988, 0.0004247688235656988]
from probs:  [0.6500469891818587, 0.22761213094125582, 0.12103796617084264, 0.0004343045686810418, 0.0004343045686810418, 0.0004343045686810418]
using explorer policy with actor:  0
from probs:  [0.6500469891818587, 0.22761213094125582, 0.12103796617084264, 0.0004343045686810418, 0.0004343045686810418, 0.0004343045686810418]
actor:  0 policy actor:  1  step number:  67 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.96484 -1.0 -0.96484
maxi score, test score, baseline:  -0.96484 -1.0 -0.96484
probs:  [0.6501136777417931, 0.22755568460820083, 0.12102274278783223, 0.0004359649540579784, 0.0004359649540579784, 0.0004359649540579784]
siam score:  -0.64186174
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.532]
 [0.557]
 [0.398]
 [0.405]
 [0.557]
 [0.389]] [[1.424]
 [1.737]
 [1.424]
 [0.721]
 [1.212]
 [1.424]
 [1.008]] [[0.557]
 [0.532]
 [0.557]
 [0.398]
 [0.405]
 [0.557]
 [0.389]]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  60 total reward:  0.4349999999999996  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
probs:  [0.650182264251793, 0.22749762175004845, 0.12100708386844816, 0.0004376767099034919, 0.0004376767099034919, 0.0004376767099034919]
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.322]] [[-0.297]
 [-0.297]
 [-0.297]
 [-0.297]
 [-0.297]
 [-0.297]
 [-0.181]] [[0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.322]]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.343]
 [0.36 ]
 [0.359]
 [0.353]
 [0.357]
 [0.359]] [[2.481]
 [2.84 ]
 [2.801]
 [2.044]
 [2.12 ]
 [1.942]
 [2.012]] [[0.361]
 [0.343]
 [0.36 ]
 [0.359]
 [0.353]
 [0.357]
 [0.359]]
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.099]
 [0.099]
 [0.099]
 [0.182]
 [0.045]
 [0.177]] [[1.311]
 [1.764]
 [1.764]
 [1.764]
 [1.335]
 [1.56 ]
 [1.964]] [[0.331]
 [0.678]
 [0.678]
 [0.678]
 [0.272]
 [0.298]
 [1.102]]
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
probs:  [0.650182264251793, 0.22749762175004845, 0.12100708386844816, 0.0004376767099034919, 0.0004376767099034919, 0.0004376767099034919]
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
probs:  [0.650182264251793, 0.22749762175004845, 0.12100708386844816, 0.0004376767099034919, 0.0004376767099034919, 0.0004376767099034919]
from probs:  [0.650182264251793, 0.22749762175004845, 0.12100708386844816, 0.0004376767099034919, 0.0004376767099034919, 0.0004376767099034919]
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
probs:  [0.650182264251793, 0.22749762175004845, 0.12100708386844816, 0.0004376767099034919, 0.0004376767099034919, 0.0004376767099034919]
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
probs:  [0.650182264251793, 0.22749762175004845, 0.12100708386844816, 0.0004376767099034919, 0.0004376767099034919, 0.0004376767099034919]
siam score:  -0.62916
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.323]
 [0.242]
 [0.241]
 [0.233]
 [0.238]
 [0.241]] [[-2.797]
 [-1.919]
 [-2.908]
 [-2.827]
 [-2.782]
 [-2.758]
 [-2.827]] [[0.24 ]
 [0.323]
 [0.242]
 [0.241]
 [0.233]
 [0.238]
 [0.241]]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[1.53 ]
 [1.53 ]
 [1.53 ]
 [1.53 ]
 [1.53 ]
 [1.53 ]
 [1.566]] [[1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.31 ]]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.342]
 [0.438]
 [0.435]
 [0.437]
 [0.443]
 [0.417]] [[0.401]
 [0.691]
 [0.612]
 [0.637]
 [0.588]
 [0.642]
 [0.766]] [[0.283]
 [0.396]
 [0.509]
 [0.529]
 [0.484]
 [0.55 ]
 [0.622]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
from probs:  [0.650182264251793, 0.22749762175004845, 0.12100708386844816, 0.0004376767099034919, 0.0004376767099034919, 0.0004376767099034919]
Starting evaluation
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
from probs:  [0.650182264251793, 0.22749762175004845, 0.12100708386844816, 0.0004376767099034919, 0.0004376767099034919, 0.0004376767099034919]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.524]] [[0.217]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.606]] [[0.186]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.448]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.35 ]
 [0.428]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[-2.226]
 [-2.226]
 [-4.43 ]
 [-2.226]
 [-2.226]
 [-2.226]
 [-2.226]] [[0.35 ]
 [0.35 ]
 [0.428]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]]
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.328]
 [0.262]
 [0.307]
 [0.338]
 [0.307]
 [0.325]] [[1.883]
 [2.047]
 [1.072]
 [1.448]
 [0.934]
 [1.448]
 [1.885]] [[0.318]
 [0.328]
 [0.262]
 [0.307]
 [0.338]
 [0.307]
 [0.325]]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.335]
 [0.383]
 [0.335]
 [0.335]
 [0.335]
 [0.335]] [[-2.226]
 [-2.226]
 [-3.43 ]
 [-2.226]
 [-2.226]
 [-2.226]
 [-2.226]] [[0.335]
 [0.335]
 [0.383]
 [0.335]
 [0.335]
 [0.335]
 [0.335]]
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.342]
 [0.345]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[-2.196]
 [-2.196]
 [-2.275]
 [-2.196]
 [-2.196]
 [-2.196]
 [-2.196]] [[0.342]
 [0.342]
 [0.345]
 [0.342]
 [0.342]
 [0.342]
 [0.342]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.426]
 [0.446]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[-3.153]
 [-3.153]
 [-2.594]
 [-3.153]
 [-3.153]
 [-3.153]
 [-3.153]] [[0.426]
 [0.426]
 [0.446]
 [0.426]
 [0.426]
 [0.426]
 [0.426]]
Printing some Q and Qe and total Qs values:  [[1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.133]] [[0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[1.927]
 [1.927]
 [1.927]
 [1.927]
 [1.927]
 [1.927]
 [1.927]]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.288]
 [0.402]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[-2.21 ]
 [-0.296]
 [-4.066]
 [-2.21 ]
 [-2.21 ]
 [-2.21 ]
 [-2.21 ]] [[0.315]
 [0.288]
 [0.402]
 [0.315]
 [0.315]
 [0.315]
 [0.315]]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.43 ]
 [0.441]
 [0.434]
 [0.434]
 [0.434]
 [0.434]] [[-1.809]
 [ 0.435]
 [ 0.18 ]
 [-1.809]
 [-1.809]
 [-1.809]
 [-1.809]] [[0.434]
 [0.43 ]
 [0.441]
 [0.434]
 [0.434]
 [0.434]
 [0.434]]
line 256 mcts: sample exp_bonus 1.6173163197963487
maxi score, test score, baseline:  -0.96197 -1.0 -0.96197
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.697]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.722]] [[1.156]
 [0.523]
 [1.156]
 [1.156]
 [1.156]
 [1.156]
 [0.994]] [[0.662]
 [0.697]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.722]]
line 256 mcts: sample exp_bonus 2.3105866226257974
from probs:  [0.650182264251793, 0.22749762175004845, 0.12100708386844816, 0.0004376767099034919, 0.0004376767099034919, 0.0004376767099034919]
actor:  0 policy actor:  1  step number:  46 total reward:  0.6049999999999998  reward:  1.0 rdn_beta:  1
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
siam score:  -0.62231934
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.422]
 [0.457]
 [0.47 ]
 [0.466]
 [0.47 ]
 [0.459]] [[-1.432]
 [ 0.437]
 [-1.599]
 [-2.263]
 [-1.871]
 [-2.594]
 [-1.777]] [[0.485]
 [0.422]
 [0.457]
 [0.47 ]
 [0.466]
 [0.47 ]
 [0.459]]
maxi score, test score, baseline:  -0.9587600000000001 -1.0 -0.9587600000000001
probs:  [0.6502592541346154, 0.2274324327194777, 0.12098950362422062, 0.0004396031738954936, 0.0004396031738954936, 0.0004396031738954936]
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]] [[0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.347]] [[1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.082]
 [0.998]] [[0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.795]]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[-0.407]
 [-2.027]
 [-2.027]
 [-2.027]
 [-2.027]
 [-2.027]
 [-2.027]] [[0.506]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]]
maxi score, test score, baseline:  -0.9587600000000001 -1.0 -0.9587600000000001
probs:  [0.6502592541346154, 0.22743243271947766, 0.12098950362422062, 0.0004396031738954936, 0.0004396031738954936, 0.0004396031738954936]
maxi score, test score, baseline:  -0.9587600000000001 -1.0 -0.9587600000000001
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.439]
 [0.535]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[-1.943]
 [-1.943]
 [ 0.261]
 [-1.943]
 [-1.943]
 [-1.943]
 [-1.943]] [[0.439]
 [0.439]
 [0.535]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
maxi score, test score, baseline:  -0.9587600000000001 -1.0 -0.9587600000000001
probs:  [0.6502592541346154, 0.22743243271947766, 0.12098950362422062, 0.0004396031738954936, 0.0004396031738954936, 0.0004396031738954936]
maxi score, test score, baseline:  -0.9587600000000001 -1.0 -0.9587600000000001
maxi score, test score, baseline:  -0.9587600000000001 -1.0 -0.9587600000000001
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.385]
 [0.487]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[-1.805]
 [-1.805]
 [-1.699]
 [-1.805]
 [-1.805]
 [-1.805]
 [-1.805]] [[0.385]
 [0.385]
 [0.487]
 [0.385]
 [0.385]
 [0.385]
 [0.385]]
maxi score, test score, baseline:  -0.9587600000000001 -1.0 -0.9587600000000001
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.64 ]
 [0.616]] [[1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.945]
 [1.683]] [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.625]
 [0.489]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9587600000000001 -1.0 -0.9587600000000001
maxi score, test score, baseline:  -0.9587600000000001 -1.0 -0.9587600000000001
probs:  [0.6502592541346154, 0.22743243271947766, 0.12098950362422062, 0.0004396031738954936, 0.0004396031738954936, 0.0004396031738954936]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.2521075500250367
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]] [[-1.656]
 [-1.656]
 [-1.656]
 [-1.656]
 [-1.656]
 [-1.656]
 [-1.656]] [[0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]]
maxi score, test score, baseline:  -0.9587600000000001 -1.0 -0.9587600000000001
probs:  [0.6502592541346154, 0.22743243271947766, 0.12098950362422062, 0.0004396031738954936, 0.0004396031738954936, 0.0004396031738954936]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6949999999999998  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 0.7834827864757161
maxi score, test score, baseline:  -0.9587600000000001 -1.0 -0.9587600000000001
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.324]
 [0.329]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[-0.508]
 [-0.508]
 [-1.415]
 [-0.508]
 [-0.508]
 [-0.508]
 [-0.508]] [[0.324]
 [0.324]
 [0.329]
 [0.324]
 [0.324]
 [0.324]
 [0.324]]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.394]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[1.078]
 [2.388]
 [1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]] [[0.749]
 [1.27 ]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.592]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.503]] [[4.292]
 [6.305]
 [4.921]
 [4.921]
 [4.921]
 [4.921]
 [4.076]] [[0.483]
 [0.592]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.503]]
using explorer policy with actor:  0
4466 7368
maxi score, test score, baseline:  -0.9587600000000001 -1.0 -0.9587600000000001
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.465]] [[-0.35 ]
 [-0.35 ]
 [-0.35 ]
 [-0.35 ]
 [-0.35 ]
 [-0.35 ]
 [ 0.048]] [[0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.691]]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.172]
 [0.399]
 [0.335]
 [0.257]
 [0.375]
 [0.21 ]] [[ 2.353]
 [ 1.153]
 [ 0.981]
 [-0.633]
 [ 0.199]
 [ 1.011]
 [ 1.104]] [[ 1.882]
 [ 1.104]
 [ 1.143]
 [-0.052]
 [ 0.485]
 [ 1.148]
 [ 1.096]]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.307]
 [0.414]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[-0.311]
 [ 0.353]
 [-0.974]
 [-0.311]
 [-0.311]
 [-0.311]
 [-0.311]] [[0.349]
 [0.307]
 [0.414]
 [0.349]
 [0.349]
 [0.349]
 [0.349]]
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.834]
 [0.89 ]
 [0.834]
 [0.834]
 [0.834]
 [0.888]
 [0.834]] [[0.729]
 [2.165]
 [0.729]
 [0.729]
 [0.729]
 [1.108]
 [0.729]] [[0.366]
 [0.906]
 [0.366]
 [0.366]
 [0.366]
 [0.58 ]
 [0.366]]
Printing some Q and Qe and total Qs values:  [[ 0.062]
 [ 0.557]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [ 0.045]] [[2.429]
 [5.169]
 [3.295]
 [3.295]
 [3.295]
 [3.295]
 [2.56 ]] [[0.888]
 [1.798]
 [1.065]
 [1.065]
 [1.065]
 [1.065]
 [0.912]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.479]
 [0.527]
 [0.527]
 [0.526]
 [0.527]
 [0.524]] [[-1.742]
 [ 0.186]
 [-2.057]
 [-2.057]
 [-1.884]
 [-2.057]
 [-1.578]] [[0.525]
 [0.479]
 [0.527]
 [0.527]
 [0.526]
 [0.527]
 [0.524]]
rdn probs:  [0.6584715145715652, 0.22209209307823866, 0.1181485497872839, 0.00042928085430414396, 0.00042928085430414396, 0.00042928085430414396]
from probs:  [0.6584715145715652, 0.22209209307823866, 0.1181485497872839, 0.00042928085430414396, 0.00042928085430414396, 0.00042928085430414396]
maxi score, test score, baseline:  -0.9587600000000002 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.339]
 [0.441]
 [0.432]
 [0.442]
 [0.444]
 [0.438]] [[ 0.361]
 [ 0.591]
 [-0.378]
 [ 0.306]
 [ 0.359]
 [-0.146]
 [-0.266]] [[0.414]
 [0.339]
 [0.441]
 [0.432]
 [0.442]
 [0.444]
 [0.438]]
using explorer policy with actor:  1
siam score:  -0.6210756
maxi score, test score, baseline:  -0.9587600000000002 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.348]
 [0.38 ]
 [0.387]
 [0.391]
 [0.38 ]
 [0.388]] [[-1.669]
 [-1.284]
 [-1.723]
 [-1.894]
 [-1.819]
 [-1.683]
 [-1.664]] [[0.397]
 [0.452]
 [0.37 ]
 [0.327]
 [0.36 ]
 [0.385]
 [0.407]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.463]] [[0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [2.961]] [[0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [1.53 ]]
using another actor
maxi score, test score, baseline:  -0.9587600000000002 -0.91975 -0.91975
probs:  [0.6594600907968856, 0.22126410148573583, 0.11791651270644765, 0.0004530983369770325, 0.0004530983369770325, 0.0004530983369770325]
maxi score, test score, baseline:  -0.9587600000000002 -0.91975 -0.91975
probs:  [0.6594600907968856, 0.22126410148573583, 0.11791651270644765, 0.0004530983369770325, 0.0004530983369770325, 0.0004530983369770325]
using another actor
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
line 256 mcts: sample exp_bonus 0.368980186837886
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6594600907968856, 0.22126410148573583, 0.11791651270644765, 0.0004530983369770325, 0.0004530983369770325, 0.0004530983369770325]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6594600907968856, 0.22126410148573583, 0.11791651270644765, 0.0004530983369770325, 0.0004530983369770325, 0.0004530983369770325]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6718181864204368, 0.20667081095362466, 0.12012623481087566, 0.00046158927168768026, 0.00046158927168768026, 0.00046158927168768026]
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.112]
 [0.111]
 [0.11 ]
 [0.11 ]
 [0.117]
 [0.104]] [[2.357]
 [3.   ]
 [2.487]
 [2.18 ]
 [2.269]
 [2.277]
 [2.718]] [[ 0.029]
 [ 0.461]
 [ 0.119]
 [-0.089]
 [-0.028]
 [-0.01 ]
 [ 0.258]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.34 ]
 [0.28 ]
 [0.35 ]
 [0.294]
 [0.28 ]
 [0.31 ]] [[1.083]
 [1.862]
 [1.083]
 [1.577]
 [1.637]
 [1.083]
 [1.954]] [[0.28 ]
 [0.34 ]
 [0.28 ]
 [0.35 ]
 [0.294]
 [0.28 ]
 [0.31 ]]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6718181864204368, 0.20667081095362466, 0.12012623481087566, 0.00046158927168768026, 0.00046158927168768026, 0.00046158927168768026]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6718181864204368, 0.20667081095362466, 0.12012623481087566, 0.00046158927168768026, 0.00046158927168768026, 0.00046158927168768026]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6718181864204368, 0.20667081095362466, 0.12012623481087566, 0.00046158927168768026, 0.00046158927168768026, 0.00046158927168768026]
from probs:  [0.6718181864204368, 0.20667081095362466, 0.12012623481087566, 0.00046158927168768026, 0.00046158927168768026, 0.00046158927168768026]
from probs:  [0.6718181864204368, 0.20667081095362466, 0.12012623481087566, 0.00046158927168768026, 0.00046158927168768026, 0.00046158927168768026]
siam score:  -0.63386387
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6718181864204368, 0.20667081095362466, 0.12012623481087566, 0.00046158927168768026, 0.00046158927168768026, 0.00046158927168768026]
using another actor
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
siam score:  -0.62766385
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.502]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.458]] [[1.119]
 [2.786]
 [1.099]
 [1.099]
 [1.099]
 [1.099]
 [2.758]] [[0.314]
 [0.502]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.458]]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
using another actor
from probs:  [0.6718181864204368, 0.20667081095362466, 0.12012623481087566, 0.00046158927168768026, 0.00046158927168768026, 0.00046158927168768026]
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.735]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[-0.4  ]
 [-0.244]
 [-0.4  ]
 [-0.4  ]
 [-0.4  ]
 [-0.4  ]
 [-0.4  ]] [[-0.203]
 [ 0.815]
 [-0.203]
 [-0.203]
 [-0.203]
 [-0.203]
 [-0.203]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6718181864204368, 0.20667081095362466, 0.12012623481087566, 0.00046158927168768026, 0.00046158927168768026, 0.00046158927168768026]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.331]
 [0.513]
 [0.288]
 [0.244]
 [0.426]
 [0.318]] [[4.24 ]
 [3.505]
 [3.914]
 [3.829]
 [3.927]
 [4.197]
 [3.745]] [[0.599]
 [0.316]
 [0.818]
 [0.339]
 [0.283]
 [0.739]
 [0.372]]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
actor:  1 policy actor:  1  step number:  49 total reward:  0.6499999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.882]
 [0.934]
 [0.882]
 [0.859]
 [0.882]
 [0.882]
 [0.861]] [[3.763]
 [2.88 ]
 [3.763]
 [2.546]
 [3.763]
 [3.763]
 [3.49 ]] [[1.591]
 [1.107]
 [1.591]
 [0.733]
 [1.591]
 [1.591]
 [1.367]]
UNIT TEST: sample policy line 217 mcts : [0.327 0.571 0.02  0.02  0.02  0.02  0.02 ]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
actor:  1 policy actor:  1  step number:  62 total reward:  0.6049999999999998  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.214]] [[0.431]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.877]] [[0.546]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [1.134]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0925531677970548
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.346]
 [0.422]
 [0.422]
 [0.381]
 [0.422]
 [0.422]] [[ 0.   ]
 [-1.76 ]
 [ 0.   ]
 [ 0.   ]
 [-1.581]
 [ 0.   ]
 [ 0.   ]] [[1.231]
 [0.491]
 [1.231]
 [1.231]
 [0.622]
 [1.231]
 [1.231]]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6639280206107688, 0.21970562787222744, 0.11504021334963886, 0.00044204605578836983, 0.00044204605578836983, 0.00044204605578836983]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.62993294
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.542]
 [0.31 ]
 [0.31 ]
 [0.241]
 [0.31 ]
 [0.283]] [[ 0.   ]
 [ 0.426]
 [ 0.   ]
 [ 0.   ]
 [-0.904]
 [ 0.   ]
 [-0.742]] [[ 0.119]
 [ 0.68 ]
 [ 0.119]
 [ 0.119]
 [-0.288]
 [ 0.119]
 [-0.16 ]]
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.248]
 [0.2  ]
 [0.248]
 [0.2  ]
 [0.248]
 [0.197]] [[0.864]
 [0.864]
 [0.199]
 [0.864]
 [0.455]
 [0.864]
 [0.892]] [[ 0.665]
 [ 0.665]
 [-0.095]
 [ 0.665]
 [ 0.16 ]
 [ 0.665]
 [ 0.591]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6639280206107688, 0.21970562787222744, 0.11504021334963886, 0.00044204605578836983, 0.00044204605578836983, 0.00044204605578836983]
4501 7414
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.378]
 [0.292]
 [0.376]
 [0.292]
 [0.292]
 [0.288]] [[1.684]
 [1.651]
 [1.877]
 [1.564]
 [1.877]
 [1.877]
 [1.952]] [[1.021]
 [1.216]
 [1.259]
 [1.144]
 [1.259]
 [1.259]
 [1.313]]
siam score:  -0.6287094
line 256 mcts: sample exp_bonus -1.8470508691945133
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.355]
 [0.398]
 [0.405]
 [2.88 ]
 [2.88 ]
 [2.88 ]] [[-1.427]
 [ 0.378]
 [-1.674]
 [-0.542]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.572]
 [2.194]
 [0.341]
 [1.392]
 [4.175]
 [4.175]
 [4.175]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.379]
 [0.379]
 [0.379]
 [0.216]
 [0.379]
 [0.379]] [[1.237]
 [1.287]
 [1.287]
 [1.287]
 [1.305]
 [1.287]
 [1.287]] [[0.579]
 [1.163]
 [1.163]
 [1.163]
 [0.856]
 [1.163]
 [1.163]]
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.553]
 [0.489]
 [0.509]] [[0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.906]
 [0.867]
 [1.243]] [[0.929]
 [0.929]
 [0.929]
 [0.929]
 [1.094]
 [0.929]
 [1.344]]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.311]
 [0.301]
 [0.313]
 [0.304]
 [0.313]
 [0.314]] [[2.284]
 [3.238]
 [2.231]
 [2.111]
 [2.103]
 [2.654]
 [2.645]] [[0.306]
 [0.311]
 [0.301]
 [0.313]
 [0.304]
 [0.313]
 [0.314]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.5947315244297662
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6639280206107688, 0.21970562787222744, 0.11504021334963886, 0.00044204605578836983, 0.00044204605578836983, 0.00044204605578836983]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6639280206107688, 0.21970562787222744, 0.11504021334963886, 0.00044204605578836983, 0.00044204605578836983, 0.00044204605578836983]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6639280206107688, 0.21970562787222744, 0.11504021334963886, 0.00044204605578836983, 0.00044204605578836983, 0.00044204605578836983]
4514 7426
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.231]
 [0.405]
 [0.408]
 [0.41 ]
 [0.409]
 [0.397]] [[0.56 ]
 [0.947]
 [0.709]
 [0.602]
 [0.447]
 [0.328]
 [0.776]] [[0.579]
 [0.597]
 [0.687]
 [0.604]
 [0.482]
 [0.384]
 [0.727]]
4515 7429
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.376]
 [0.376]
 [0.376]
 [0.367]
 [0.376]
 [0.359]] [[0.831]
 [1.757]
 [1.757]
 [1.757]
 [1.227]
 [1.757]
 [2.196]] [[0.333]
 [1.272]
 [1.272]
 [1.272]
 [0.724]
 [1.272]
 [1.676]]
siam score:  -0.6357105
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]] [[1.487]
 [1.487]
 [1.487]
 [1.487]
 [1.487]
 [1.487]
 [1.487]] [[0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.049]
 [0.284]
 [0.281]
 [0.282]
 [0.282]
 [0.282]] [[0.323]
 [0.668]
 [0.27 ]
 [0.333]
 [0.258]
 [0.221]
 [0.911]] [[0.295]
 [0.291]
 [0.229]
 [0.309]
 [0.21 ]
 [0.159]
 [1.082]]
siam score:  -0.63653636
UNIT TEST: sample policy line 217 mcts : [0.102 0.347 0.061 0.02  0.204 0.041 0.224]
Printing some Q and Qe and total Qs values:  [[ 0.013]
 [ 0.19 ]
 [ 0.047]
 [ 0.221]
 [ 0.214]
 [-0.008]
 [ 0.222]] [[1.065]
 [1.103]
 [1.15 ]
 [1.152]
 [0.98 ]
 [0.721]
 [1.186]] [[0.423]
 [0.733]
 [0.545]
 [0.822]
 [0.674]
 [0.116]
 [0.851]]
siam score:  -0.6411989
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.869]
 [0.717]
 [0.717]
 [0.834]
 [0.717]
 [0.858]] [[0.01 ]
 [0.565]
 [0.01 ]
 [0.01 ]
 [0.358]
 [0.01 ]
 [0.436]] [[0.145]
 [0.634]
 [0.145]
 [0.145]
 [0.495]
 [0.145]
 [0.57 ]]
actor:  1 policy actor:  1  step number:  80 total reward:  0.4449999999999996  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 3.6057709706937837
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8410501185421808
line 256 mcts: sample exp_bonus 0.7481649574956368
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.133850882729743
Printing some Q and Qe and total Qs values:  [[0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]] [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[2.081]
 [2.081]
 [2.081]
 [2.081]
 [2.081]
 [2.081]
 [2.081]]
4524 7432
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]] [[1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]] [[0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  71 total reward:  0.5999999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6758833664869354, 0.21188987132826795, 0.11094779974597442, 0.0004263208129408416, 0.0004263208129408416, 0.0004263208129408416]
using explorer policy with actor:  1
in main func line 156:  4527
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.422]
 [0.422]
 [0.422]
 [0.221]
 [0.422]
 [0.638]] [[1.607]
 [1.806]
 [1.806]
 [1.806]
 [2.109]
 [1.806]
 [2.011]] [[1.073]
 [1.282]
 [1.282]
 [1.282]
 [1.193]
 [1.282]
 [1.842]]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
line 256 mcts: sample exp_bonus 2.211956177760864
line 256 mcts: sample exp_bonus 1.5738410593185972
siam score:  -0.64909446
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6758833664869354, 0.21188987132826795, 0.11094779974597442, 0.0004263208129408416, 0.0004263208129408416, 0.0004263208129408416]
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]] [[1.743]
 [1.743]
 [1.743]
 [1.743]
 [1.743]
 [1.743]
 [1.743]] [[0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6758833664869354, 0.21188987132826795, 0.11094779974597442, 0.0004263208129408416, 0.0004263208129408416, 0.0004263208129408416]
using explorer policy with actor:  1
first move QE:  0.3999151887229077
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6758833664869354, 0.21188987132826795, 0.11094779974597442, 0.0004263208129408416, 0.0004263208129408416, 0.0004263208129408416]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[1.521]
 [1.521]
 [1.521]
 [1.521]
 [1.521]
 [1.521]
 [1.521]] [[1.042]
 [1.042]
 [1.042]
 [1.042]
 [1.042]
 [1.042]
 [1.042]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.6549999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  51 total reward:  0.7099999999999999  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.004]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]] [[0.854]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]] [[1.906]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6475274813722229, 0.22325733358113087, 0.12798987996023029, 0.0004084350288053402, 0.0004084350288053402, 0.0004084350288053402]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.279]
 [0.376]
 [0.469]
 [0.509]
 [0.369]
 [0.332]] [[1.626]
 [1.269]
 [0.822]
 [1.078]
 [1.106]
 [1.063]
 [1.433]] [[1.328]
 [0.725]
 [0.314]
 [0.77 ]
 [0.864]
 [0.603]
 [1.006]]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6475274813722229, 0.22325733358113087, 0.12798987996023029, 0.0004084350288053402, 0.0004084350288053402, 0.0004084350288053402]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.422]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[-3.282]
 [-1.607]
 [-3.282]
 [-3.282]
 [-3.282]
 [-3.282]
 [-3.282]] [[0.275]
 [0.85 ]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.426]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]] [[-3.16 ]
 [-2.011]
 [-3.16 ]
 [-3.16 ]
 [-3.16 ]
 [-3.16 ]
 [-3.16 ]] [[0.298]
 [0.661]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.298]
 [0.298]
 [0.679]
 [0.169]
 [0.298]
 [0.322]] [[ 0.953]
 [ 0.109]
 [-0.064]
 [ 0.253]
 [ 0.139]
 [-0.064]
 [ 0.252]] [[0.034]
 [0.298]
 [0.298]
 [0.679]
 [0.169]
 [0.298]
 [0.322]]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6475274813722229, 0.22325733358113087, 0.12798987996023029, 0.0004084350288053402, 0.0004084350288053402, 0.0004084350288053402]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6849999999999998  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.493]] [[1.164]
 [1.604]
 [1.604]
 [1.604]
 [1.31 ]
 [1.604]
 [1.716]] [[2.248]
 [2.438]
 [2.438]
 [2.438]
 [2.309]
 [2.438]
 [2.5  ]]
line 256 mcts: sample exp_bonus 1.292800290156195
first move QE:  0.40027438765100387
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.622]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]] [[1.006]
 [0.849]
 [1.006]
 [1.006]
 [1.006]
 [1.006]
 [1.006]] [[0.386]
 [1.236]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6547894080319353, 0.21865760368156034, 0.12535292793606945, 0.0004000201168116647, 0.0004000201168116647, 0.0004000201168116647]
actor:  1 policy actor:  1  step number:  79 total reward:  0.5299999999999997  reward:  1.0 rdn_beta:  0.333
from probs:  [0.6547894080319353, 0.21865760368156034, 0.12535292793606945, 0.0004000201168116647, 0.0004000201168116647, 0.0004000201168116647]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6515887214618341, 0.22247689593624687, 0.12474018828566223, 0.00039806477208564406, 0.00039806477208564406, 0.00039806477208564406]
using explorer policy with actor:  1
from probs:  [0.6515887214618341, 0.22247689593624687, 0.12474018828566223, 0.00039806477208564406, 0.00039806477208564406, 0.00039806477208564406]
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.372]
 [0.412]
 [0.412]
 [0.443]
 [0.412]
 [0.412]] [[ 0.014]
 [ 0.481]
 [-0.71 ]
 [-0.71 ]
 [-0.942]
 [-0.71 ]
 [-0.71 ]] [[0.655]
 [1.125]
 [0.806]
 [0.806]
 [0.79 ]
 [0.806]
 [0.806]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.508]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[-4.267]
 [-1.208]
 [-4.267]
 [-4.267]
 [-4.267]
 [-4.267]
 [-4.267]] [[0.365]
 [0.508]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]]
first move QE:  0.399221366504826
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.872]
 [0.636]
 [0.636]] [[2.558]
 [2.558]
 [2.558]
 [2.558]
 [8.55 ]
 [2.558]
 [2.558]] [[0.397]
 [0.397]
 [0.397]
 [0.397]
 [2.043]
 [0.397]
 [0.397]]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.482]
 [0.242]
 [0.242]] [[1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.411]
 [1.104]
 [1.104]] [[-0.22 ]
 [-0.22 ]
 [-0.22 ]
 [-0.22 ]
 [ 0.363]
 [-0.22 ]
 [-0.22 ]]
Printing some Q and Qe and total Qs values:  [[0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.243]
 [0.173]
 [0.173]] [[1.328]
 [1.328]
 [1.328]
 [1.328]
 [1.266]
 [1.328]
 [1.328]] [[-0.274]
 [-0.274]
 [-0.274]
 [-0.274]
 [-0.156]
 [-0.274]
 [-0.274]]
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.571]
 [0.253]
 [0.253]] [[1.382]
 [1.382]
 [1.382]
 [1.382]
 [5.458]
 [1.382]
 [1.382]] [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [ 1.24 ]
 [-0.009]
 [-0.009]]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.087]
 [ 0.136]
 [ 0.081]
 [ 0.081]
 [ 0.105]
 [ 0.089]] [[-0.197]
 [-1.439]
 [-4.298]
 [-2.31 ]
 [-2.31 ]
 [-4.161]
 [-0.415]] [[-0.01 ]
 [ 0.087]
 [ 0.136]
 [ 0.081]
 [ 0.081]
 [ 0.105]
 [ 0.089]]
4570 7465
siam score:  -0.65803754
from probs:  [0.6515887214618341, 0.22247689593624687, 0.12474018828566223, 0.00039806477208564406, 0.00039806477208564406, 0.00039806477208564406]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.461]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[3.669]
 [4.353]
 [3.669]
 [3.669]
 [3.669]
 [3.669]
 [3.669]] [[-0.077]
 [ 0.177]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.103]
 [0.026]
 [0.055]
 [0.055]
 [0.034]
 [0.055]] [[4.448]
 [4.761]
 [4.539]
 [4.448]
 [4.448]
 [4.273]
 [4.448]] [[-0.074]
 [ 0.126]
 [-0.103]
 [-0.074]
 [-0.074]
 [-0.174]
 [-0.074]]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6515887214618341, 0.22247689593624687, 0.12474018828566223, 0.00039806477208564406, 0.00039806477208564406, 0.00039806477208564406]
4573 7466
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6515887214618341, 0.22247689593624687, 0.12474018828566223, 0.00039806477208564406, 0.00039806477208564406, 0.00039806477208564406]
using explorer policy with actor:  1
siam score:  -0.65643936
siam score:  -0.6574544
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6515887214618341, 0.22247689593624687, 0.12474018828566223, 0.00039806477208564406, 0.00039806477208564406, 0.00039806477208564406]
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6515887214618341, 0.22247689593624687, 0.12474018828566223, 0.00039806477208564406, 0.00039806477208564406, 0.00039806477208564406]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6515887214618341, 0.22247689593624687, 0.12474018828566223, 0.00039806477208564406, 0.00039806477208564406, 0.00039806477208564406]
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.14 ]
 [0.291]
 [0.296]
 [0.284]
 [0.273]
 [0.294]] [[-3.321]
 [-0.074]
 [-3.496]
 [-3.678]
 [-3.267]
 [-2.636]
 [-3.398]] [[0.271]
 [0.14 ]
 [0.291]
 [0.296]
 [0.284]
 [0.273]
 [0.294]]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
actor:  1 policy actor:  1  step number:  79 total reward:  0.06999999999999929  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[-0.83]
 [-0.83]
 [-0.83]
 [-0.83]
 [-0.83]
 [-0.83]
 [-0.83]] [[0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]]
Printing some Q and Qe and total Qs values:  [[1.221]
 [1.254]
 [1.221]
 [1.221]
 [1.221]
 [1.221]
 [1.221]] [[0.348]
 [0.382]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[2.377]
 [2.454]
 [2.377]
 [2.377]
 [2.377]
 [2.377]
 [2.377]]
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.448]
 [0.485]
 [0.486]
 [0.485]
 [0.478]
 [0.465]] [[-1.873]
 [-0.99 ]
 [-1.796]
 [-1.757]
 [-1.672]
 [-1.613]
 [ 1.057]] [[1.837]
 [2.04 ]
 [1.859]
 [1.874]
 [1.898]
 [1.903]
 [2.707]]
line 256 mcts: sample exp_bonus -0.2335805831435656
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.866]] [[-0.466]
 [-0.466]
 [-0.466]
 [-0.466]
 [-0.466]
 [-0.466]
 [ 1.749]] [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [1.746]]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6542849274938791, 0.2207552422879186, 0.12377487726156063, 0.0003949843188806128, 0.0003949843188806128, 0.0003949843188806128]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6542849274938791, 0.2207552422879186, 0.12377487726156063, 0.0003949843188806128, 0.0003949843188806128, 0.0003949843188806128]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.568]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.395]] [[-2.959]
 [-1.757]
 [-2.959]
 [-2.959]
 [-2.959]
 [-2.959]
 [-3.366]] [[0.399]
 [0.568]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.395]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
first move QE:  0.3972028859558952
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6542849274938791, 0.2207552422879186, 0.12377487726156063, 0.0003949843188806128, 0.0003949843188806128, 0.0003949843188806128]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6542849274938791, 0.2207552422879186, 0.12377487726156063, 0.0003949843188806128, 0.0003949843188806128, 0.0003949843188806128]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6542849274938791, 0.2207552422879186, 0.12377487726156063, 0.0003949843188806128, 0.0003949843188806128, 0.0003949843188806128]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6542849274938791, 0.2207552422879186, 0.12377487726156063, 0.0003949843188806128, 0.0003949843188806128, 0.0003949843188806128]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6542849274938791, 0.2207552422879186, 0.12377487726156063, 0.0003949843188806128, 0.0003949843188806128, 0.0003949843188806128]
4595 7490
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
probs:  [0.6542849274938791, 0.2207552422879186, 0.12377487726156063, 0.0003949843188806128, 0.0003949843188806128, 0.0003949843188806128]
maxi score, test score, baseline:  -0.95876 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]] [[2.128]
 [2.128]
 [2.128]
 [2.128]
 [2.128]
 [2.128]
 [2.128]] [[0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]]
Printing some Q and Qe and total Qs values:  [[-0.019]
 [ 0.068]
 [ 0.068]
 [ 0.068]
 [-0.02 ]
 [ 0.068]
 [-0.019]] [[1.834]
 [1.437]
 [1.437]
 [1.437]
 [1.844]
 [1.437]
 [1.153]] [[1.843]
 [1.766]
 [1.766]
 [1.766]
 [1.846]
 [1.766]
 [1.645]]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6542849274938791, 0.2207552422879186, 0.12377487726156063, 0.0003949843188806128, 0.0003949843188806128, 0.0003949843188806128]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.954]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]] [[ 0.355]
 [-1.428]
 [-1.428]
 [-1.428]
 [-1.428]
 [-1.428]
 [-1.428]] [[2.256]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5399999999999997  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.155]
 [0.259]
 [0.259]
 [0.25 ]
 [0.25 ]
 [0.258]] [[ 0.117]
 [ 0.245]
 [-0.423]
 [-0.029]
 [ 0.082]
 [-0.091]
 [ 0.077]] [[ 0.21 ]
 [ 0.156]
 [-0.514]
 [ 0.002]
 [ 0.128]
 [-0.098]
 [ 0.139]]
4603 7502
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6600080812306313, 0.2171007409650544, 0.12172584119790161, 0.00038844553547098863, 0.00038844553547098863, 0.00038844553547098863]
Printing some Q and Qe and total Qs values:  [[1.357]
 [1.357]
 [1.357]
 [1.357]
 [1.357]
 [1.357]
 [1.357]] [[0.386]
 [0.388]
 [0.395]
 [0.395]
 [0.391]
 [0.391]
 [0.388]] [[1.697]
 [1.699]
 [1.706]
 [1.706]
 [1.702]
 [1.702]
 [1.7  ]]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6600080812306313, 0.2171007409650544, 0.12172584119790161, 0.00038844553547098863, 0.00038844553547098863, 0.00038844553547098863]
Printing some Q and Qe and total Qs values:  [[ 1.35 ]
 [-0.026]
 [-0.026]
 [ 1.35 ]
 [ 1.35 ]
 [ 1.35 ]
 [ 1.35 ]] [[0.382]
 [1.738]
 [1.738]
 [0.388]
 [0.388]
 [0.392]
 [0.385]] [[1.986]
 [1.955]
 [1.955]
 [1.994]
 [1.994]
 [2.002]
 [1.991]]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.249]
 [0.147]
 [0.249]
 [0.141]
 [0.138]
 [0.142]] [[ 0.378]
 [ 4.052]
 [-0.303]
 [ 4.052]
 [-0.046]
 [ 0.087]
 [ 0.148]] [[-0.236]
 [ 3.657]
 [-0.901]
 [ 3.657]
 [-0.657]
 [-0.529]
 [-0.461]]
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.25 ]] [[1.882]
 [1.494]
 [1.494]
 [1.494]
 [1.494]
 [1.494]
 [2.227]] [[1.343]
 [1.294]
 [1.294]
 [1.294]
 [1.294]
 [1.294]
 [1.523]]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.909]] [[1.091]
 [1.091]
 [1.091]
 [1.091]
 [1.091]
 [1.091]
 [2.55 ]] [[0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [2.093]]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
probs:  [0.6600080812306313, 0.2171007409650544, 0.12172584119790161, 0.00038844553547098863, 0.00038844553547098863, 0.00038844553547098863]
maxi score, test score, baseline:  -0.9587600000000001 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.308]] [[1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]
 [3.378]] [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [1.406]]
siam score:  -0.6292079
siam score:  -0.6296988
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
using explorer policy with actor:  1
siam score:  -0.6360375
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
from probs:  [0.6600080812306313, 0.2171007409650544, 0.12172584119790161, 0.00038844553547098863, 0.00038844553547098863, 0.00038844553547098863]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using another actor
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.488]
 [0.368]
 [0.368]
 [0.637]
 [0.368]
 [0.368]] [[1.039]
 [1.239]
 [0.675]
 [0.675]
 [1.288]
 [0.675]
 [0.675]] [[0.889]
 [0.843]
 [0.228]
 [0.228]
 [1.176]
 [0.228]
 [0.228]]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.268]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]] [[-2.298]
 [-1.037]
 [-2.298]
 [-2.298]
 [-2.298]
 [-2.298]
 [-2.298]] [[0.262]
 [0.268]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.463]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.456]] [[2.873]
 [3.059]
 [2.873]
 [2.873]
 [2.873]
 [2.873]
 [3.265]] [[0.355]
 [0.516]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.639]]
from probs:  [0.6600080812306311, 0.21710074096505447, 0.12172584119790164, 0.0003884455354709887, 0.0003884455354709887, 0.0003884455354709887]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.6600080812306311, 0.21710074096505447, 0.12172584119790164, 0.0003884455354709887, 0.0003884455354709887, 0.0003884455354709887]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.6600080812306311, 0.21710074096505447, 0.12172584119790164, 0.0003884455354709887, 0.0003884455354709887, 0.0003884455354709887]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.757]
 [0.744]
 [0.744]] [[0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.264]
 [0.265]
 [0.265]] [[1.408]
 [1.408]
 [1.408]
 [1.408]
 [1.434]
 [1.408]
 [1.408]]
4621 7538
from probs:  [0.6600080812306313, 0.2171007409650544, 0.12172584119790161, 0.00038844553547098863, 0.00038844553547098863, 0.00038844553547098863]
using another actor
from probs:  [0.6600080812306313, 0.2171007409650544, 0.12172584119790161, 0.00038844553547098863, 0.00038844553547098863, 0.00038844553547098863]
start point for exploration sampling:  11106
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.411]
 [0.39 ]
 [0.39 ]] [[1.169]
 [0.708]
 [0.708]
 [0.708]
 [1.641]
 [0.708]
 [0.708]] [[1.328]
 [0.615]
 [0.615]
 [0.615]
 [1.901]
 [0.615]
 [0.615]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.02  0.02  0.878 0.02  0.02 ]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
using explorer policy with actor:  1
siam score:  -0.6418798
actor:  1 policy actor:  1  step number:  44 total reward:  0.6049999999999998  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]] [[-1.155]
 [-1.155]
 [-1.155]
 [-1.155]
 [-1.155]
 [-1.155]
 [-1.155]] [[0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.515]
 [0.498]
 [0.499]
 [0.492]
 [0.488]
 [0.499]] [[-3.083]
 [-3.144]
 [-3.187]
 [-3.048]
 [-2.525]
 [-2.91 ]
 [-2.838]] [[0.498]
 [0.515]
 [0.498]
 [0.499]
 [0.492]
 [0.488]
 [0.499]]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.51 ]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]] [[-1.616]
 [-0.105]
 [-1.616]
 [-1.616]
 [-1.616]
 [-1.616]
 [-1.616]] [[0.178]
 [0.51 ]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.54 ]
 [0.353]] [[0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [1.282]
 [0.704]] [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [1.074]
 [0.508]]
from probs:  [0.648298736575751, 0.21324911024554902, 0.13730749105848145, 0.00038155404007293465, 0.00038155404007293465, 0.00038155404007293465]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.648298736575751, 0.21324911024554902, 0.13730749105848145, 0.00038155404007293465, 0.00038155404007293465, 0.00038155404007293465]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.648298736575751, 0.21324911024554902, 0.13730749105848145, 0.00038155404007293465, 0.00038155404007293465, 0.00038155404007293465]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.216]
 [0.197]
 [0.203]
 [0.197]
 [0.196]
 [0.201]] [[-3.51 ]
 [-3.422]
 [-3.56 ]
 [-3.583]
 [-2.848]
 [-3.199]
 [-3.484]] [[0.204]
 [0.216]
 [0.197]
 [0.203]
 [0.197]
 [0.196]
 [0.201]]
siam score:  -0.6496568
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]]
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.87 ]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]] [[1.175]
 [1.622]
 [1.175]
 [1.175]
 [1.175]
 [1.175]
 [1.175]] [[0.172]
 [0.87 ]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.191]
 [0.177]
 [0.177]
 [0.174]
 [0.179]
 [0.177]] [[0.467]
 [0.803]
 [0.467]
 [0.467]
 [0.438]
 [0.079]
 [0.467]] [[0.177]
 [0.191]
 [0.177]
 [0.177]
 [0.174]
 [0.179]
 [0.177]]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
siam score:  -0.655728
using another actor
start point for exploration sampling:  11106
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.648298736575751, 0.21324911024554902, 0.13730749105848145, 0.00038155404007293465, 0.00038155404007293465, 0.00038155404007293465]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[0.948]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]] [[0.522]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.648298736575751, 0.21324911024554902, 0.13730749105848145, 0.00038155404007293465, 0.00038155404007293465, 0.00038155404007293465]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
actor:  1 policy actor:  1  step number:  65 total reward:  0.5799999999999997  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
first move QE:  0.39423207808761307
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]] [[ 0.023]
 [-0.514]
 [-0.514]
 [-0.514]
 [-0.514]
 [-0.514]
 [-0.514]] [[ 0.194]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]]
line 256 mcts: sample exp_bonus 5.433821469160984
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.6373589739580697, 0.20965062313937302, 0.1349904861453712, 0.017249685797999237, 0.00037511547959345206, 0.00037511547959345206]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.344]
 [0.399]
 [0.392]
 [0.37 ]
 [0.395]
 [1.124]] [[0.834]
 [1.218]
 [0.85 ]
 [0.687]
 [1.099]
 [1.224]
 [0.701]] [[0.298]
 [0.467]
 [0.181]
 [0.008]
 [0.388]
 [0.551]
 [1.122]]
from probs:  [0.6373589739580697, 0.20965062313937302, 0.1349904861453712, 0.017249685797999237, 0.00037511547959345206, 0.00037511547959345206]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.6373589739580697, 0.20965062313937302, 0.1349904861453712, 0.017249685797999237, 0.00037511547959345206, 0.00037511547959345206]
Printing some Q and Qe and total Qs values:  [[0.875]
 [1.083]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]] [[0.35 ]
 [0.788]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[0.863]
 [1.471]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.6373589739580697, 0.20965062313937302, 0.1349904861453712, 0.017249685797999237, 0.00037511547959345206, 0.00037511547959345206]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.6373589739580697, 0.20965062313937302, 0.1349904861453712, 0.017249685797999237, 0.00037511547959345206, 0.00037511547959345206]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.546]
 [0.677]
 [0.442]
 [0.442]
 [0.571]
 [0.449]] [[2.555]
 [2.514]
 [2.425]
 [2.555]
 [2.555]
 [2.657]
 [2.714]] [[0.442]
 [0.546]
 [0.677]
 [0.442]
 [0.442]
 [0.571]
 [0.449]]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.663]
 [0.455]
 [0.499]
 [0.412]
 [0.38 ]
 [0.39 ]] [[1.593]
 [1.288]
 [1.038]
 [1.5  ]
 [1.285]
 [1.283]
 [1.319]] [[1.131]
 [1.014]
 [0.349]
 [0.897]
 [0.509]
 [0.444]
 [0.498]]
siam score:  -0.64859086
first move QE:  0.3942765982660234
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.444]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]] [[-3.169]
 [-1.2  ]
 [-3.169]
 [-3.169]
 [-3.169]
 [-3.169]
 [-3.169]] [[0.078]
 [0.894]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.297]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[-1.708]
 [ 0.704]
 [-1.708]
 [-1.708]
 [-1.708]
 [-1.708]
 [-1.708]] [[0.18 ]
 [1.043]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]]
actor:  1 policy actor:  1  step number:  98 total reward:  0.22499999999999942  reward:  1.0 rdn_beta:  0.5
in main func line 156:  4653
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
Starting evaluation
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[1.126]
 [1.126]
 [1.126]
 [1.126]
 [1.126]
 [1.126]
 [1.126]] [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.426]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.537]] [[-0.813]
 [-0.902]
 [-0.813]
 [-0.813]
 [-0.813]
 [-0.813]
 [-1.324]] [[0.478]
 [0.426]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.537]]
Printing some Q and Qe and total Qs values:  [[0.47]
 [0.47]
 [0.47]
 [0.47]
 [0.47]
 [0.47]
 [0.47]] [[-1.92]
 [-1.92]
 [-1.92]
 [-1.92]
 [-1.92]
 [-1.92]
 [-1.92]] [[0.47]
 [0.47]
 [0.47]
 [0.47]
 [0.47]
 [0.47]
 [0.47]]
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.233]
 [0.376]
 [0.363]
 [0.352]
 [0.356]
 [0.43 ]] [[0.451]
 [0.831]
 [0.364]
 [0.275]
 [0.559]
 [0.727]
 [0.836]] [[0.553]
 [0.784]
 [0.448]
 [0.303]
 [0.662]
 [0.891]
 [1.186]]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.457]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.473]] [[-0.93 ]
 [ 0.832]
 [-0.93 ]
 [-0.93 ]
 [-0.93 ]
 [-0.93 ]
 [ 0.649]] [[0.459]
 [0.457]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.473]]
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.6311538349981554, 0.20760952651601547, 0.14341196360850805, 0.017081747945446948, 0.00037146346593710177, 0.00037146346593710177]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.619]] [[-1.022]
 [-1.022]
 [-1.022]
 [-1.022]
 [-1.022]
 [-1.022]
 [-0.576]] [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.619]]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.697]] [[-1.753]
 [-1.753]
 [-1.753]
 [-1.753]
 [-1.753]
 [-1.753]
 [-1.107]] [[0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.697]]
line 256 mcts: sample exp_bonus 1.7320486119918965
Printing some Q and Qe and total Qs values:  [[0.84 ]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]] [[ 0.603]
 [-1.249]
 [-1.249]
 [-1.249]
 [-1.249]
 [-1.249]
 [-1.249]] [[0.84 ]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]]
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[-1.791]
 [-2.063]
 [-2.063]
 [-2.063]
 [-2.063]
 [-2.063]
 [-2.063]] [[0.74 ]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]]
Printing some Q and Qe and total Qs values:  [[0.7  ]
 [0.326]
 [0.359]
 [0.359]
 [0.337]
 [0.358]
 [0.359]] [[-4.457]
 [ 0.397]
 [-2.088]
 [-2.091]
 [-1.536]
 [-1.334]
 [-1.72 ]] [[0.7  ]
 [0.326]
 [0.359]
 [0.359]
 [0.337]
 [0.358]
 [0.359]]
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]] [[-2.781]
 [-2.122]
 [-2.122]
 [-2.122]
 [-2.122]
 [-2.122]
 [-2.122]] [[0.769]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[0.327]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[0.364]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]]
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]] [[-3.409]
 [-2.344]
 [-2.344]
 [-2.344]
 [-2.344]
 [-2.344]
 [-2.344]] [[0.763]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.5011832636607187
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[ 0.04 ]
 [-0.01 ]
 [ 0.033]
 [ 0.027]
 [ 0.004]
 [ 0.028]
 [ 0.046]] [[0.969]
 [1.62 ]
 [0.99 ]
 [0.886]
 [1.085]
 [1.105]
 [1.195]] [[-0.122]
 [ 0.427]
 [-0.114]
 [-0.232]
 [-0.079]
 [-0.011]
 [ 0.117]]
Printing some Q and Qe and total Qs values:  [[0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]] [[-0.514]
 [-0.514]
 [-0.514]
 [-0.514]
 [-0.514]
 [-0.514]
 [-0.514]] [[0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]]
from probs:  [0.6311538349981554, 0.20760952651601547, 0.14341196360850805, 0.017081747945446948, 0.00037146346593710177, 0.00037146346593710177]
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[-0.368]
 [-1.894]
 [-1.894]
 [-1.894]
 [-1.894]
 [-1.894]
 [-1.894]] [[0.793]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]] [[ 0.13]
 [-1.49]
 [-1.49]
 [-1.49]
 [-1.49]
 [-1.49]
 [-1.49]] [[0.853]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]]
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]] [[-0.95 ]
 [-2.132]
 [-2.132]
 [-2.132]
 [-2.132]
 [-2.132]
 [-2.132]] [[0.881]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.83 ]
 [0.838]
 [0.838]
 [0.838]
 [0.838]
 [0.838]] [[-0.398]
 [ 1.781]
 [-1.096]
 [-1.096]
 [-1.096]
 [-1.096]
 [-1.096]] [[0.783]
 [0.83 ]
 [0.838]
 [0.838]
 [0.838]
 [0.838]
 [0.838]]
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]] [[-1.981]
 [-2.17 ]
 [-2.17 ]
 [-2.17 ]
 [-2.17 ]
 [-2.17 ]
 [-2.17 ]] [[0.713]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]] [[-0.932]
 [-1.034]
 [-1.034]
 [-1.034]
 [-1.034]
 [-1.034]
 [-1.034]] [[0.813]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[-1.965]
 [-1.965]
 [-1.965]
 [-1.965]
 [-1.965]
 [-1.965]
 [-1.965]] [[0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[-0.298]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]] [[0.503]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]]
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.363]
 [0.357]
 [0.357]
 [0.357]
 [0.357]] [[-1.942]
 [-1.942]
 [-1.923]
 [-1.942]
 [-1.942]
 [-1.942]
 [-1.942]] [[0.357]
 [0.357]
 [0.363]
 [0.357]
 [0.357]
 [0.357]
 [0.357]]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.6311538349981554, 0.20760952651601547, 0.14341196360850805, 0.017081747945446948, 0.00037146346593710177, 0.00037146346593710177]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[-3.01 ]
 [-1.745]
 [-1.745]
 [-1.745]
 [-1.745]
 [-1.745]
 [-1.745]] [[0.421]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]]
from probs:  [0.6311538349981554, 0.20760952651601547, 0.14341196360850805, 0.017081747945446948, 0.00037146346593710177, 0.00037146346593710177]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[0.863]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]] [[0.386]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.6311538349981554, 0.20760952651601547, 0.14341196360850805, 0.017081747945446948, 0.00037146346593710177, 0.00037146346593710177]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]] [[-1.462]
 [-1.17 ]
 [-1.17 ]
 [-1.17 ]
 [-1.17 ]
 [-1.17 ]
 [-1.17 ]] [[0.406]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]]
using explorer policy with actor:  0
siam score:  -0.6412383
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.357]
 [0.357]
 [0.357]
 [0.34 ]
 [0.357]
 [0.423]] [[-1.871]
 [-1.866]
 [-1.866]
 [-1.866]
 [-1.738]
 [-1.866]
 [-2.694]] [[0.365]
 [0.357]
 [0.357]
 [0.357]
 [0.34 ]
 [0.357]
 [0.423]]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -2.655168894259833
using explorer policy with actor:  0
4662 7585
from probs:  [0.6311538349981554, 0.20760952651601547, 0.14341196360850805, 0.017081747945446948, 0.00037146346593710177, 0.00037146346593710177]
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.779]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[0.657]
 [0.264]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[1.099]
 [2.167]
 [1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[-2.159]
 [-2.159]
 [-2.159]
 [-2.159]
 [-2.159]
 [-2.159]
 [-2.159]] [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.702]] [[-1.82 ]
 [-1.82 ]
 [-1.82 ]
 [-1.82 ]
 [-1.82 ]
 [-1.82 ]
 [-1.762]] [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.702]]
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.533]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]] [[ 0.068]
 [-0.096]
 [ 0.068]
 [ 0.068]
 [ 0.068]
 [ 0.068]
 [ 0.068]] [[0.292]
 [0.533]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]]
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.653]] [[-1.839]
 [-1.839]
 [-1.839]
 [-1.839]
 [-1.839]
 [-1.839]
 [-1.732]] [[0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.653]]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.707]] [[-1.805]
 [-1.805]
 [-1.805]
 [-1.805]
 [-1.805]
 [-1.805]
 [-1.722]] [[0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.707]]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.576]] [[-0.583]
 [-0.583]
 [-0.583]
 [-0.583]
 [-0.583]
 [-0.583]
 [ 0.297]] [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.576]]
Printing some Q and Qe and total Qs values:  [[0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]] [[0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.6311538349981554, 0.20760952651601547, 0.14341196360850805, 0.017081747945446948, 0.00037146346593710177, 0.00037146346593710177]
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.895]] [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.37 ]] [[0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.895]]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.974]] [[1.043]
 [1.043]
 [1.043]
 [1.043]
 [1.043]
 [1.043]
 [1.132]] [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [1.085]]
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.934]] [[1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.119]] [[0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.993]]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
from probs:  [0.6311538349981554, 0.20760952651601547, 0.14341196360850805, 0.017081747945446948, 0.00037146346593710177, 0.00037146346593710177]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
siam score:  -0.6472031
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.664]] [[-1.548]
 [-1.548]
 [-1.548]
 [-1.548]
 [-1.548]
 [-1.548]
 [ 0.227]] [[0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.664]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.882]] [[0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.366]] [[0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.882]]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.287]
 [0.287]
 [0.282]
 [0.259]
 [0.282]
 [0.261]] [[4.74 ]
 [3.02 ]
 [3.437]
 [4.74 ]
 [3.344]
 [4.74 ]
 [3.325]] [[1.357]
 [0.801]
 [0.937]
 [1.357]
 [0.852]
 [1.357]
 [0.85 ]]
Printing some Q and Qe and total Qs values:  [[0.899]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[ 1.59 ]
 [-0.663]
 [-0.663]
 [-0.663]
 [-0.663]
 [-0.663]
 [-0.663]] [[0.899]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]]
maxi score, test score, baseline:  -0.96165 -0.91975 -0.91975
probs:  [0.6311538349981554, 0.20760952651601547, 0.14341196360850805, 0.017081747945446948, 0.00037146346593710177, 0.00037146346593710177]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.543]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[-2.749]
 [-2.328]
 [-2.749]
 [-2.749]
 [-2.749]
 [-2.749]
 [-2.749]] [[0.523]
 [0.543]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.349]
 [0.464]
 [0.451]
 [0.427]
 [0.42 ]
 [0.443]] [[-1.197]
 [ 0.477]
 [-0.806]
 [-1.659]
 [-0.796]
 [-1.153]
 [-1.83 ]] [[0.44 ]
 [0.349]
 [0.464]
 [0.451]
 [0.427]
 [0.42 ]
 [0.443]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[ 1.067]
 [-0.671]
 [-0.671]
 [-0.671]
 [-0.671]
 [-0.671]
 [-0.671]] [[1.997]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]]
4671 7589
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.505]
 [0.503]
 [0.503]
 [0.505]
 [0.503]
 [0.502]] [[-1.222]
 [-1.307]
 [-2.284]
 [-2.284]
 [-2.38 ]
 [-2.284]
 [-2.248]] [[0.504]
 [0.505]
 [0.503]
 [0.503]
 [0.505]
 [0.503]
 [0.502]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
using explorer policy with actor:  1
rdn probs:  [0.6311538349981554, 0.20760952651601547, 0.14341196360850805, 0.017081747945446948, 0.00037146346593710177, 0.00037146346593710177]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11106
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.276]
 [0.289]
 [0.289]
 [0.265]
 [0.289]
 [0.289]] [[ 0.074]
 [-0.076]
 [ 0.074]
 [ 0.074]
 [-0.151]
 [ 0.074]
 [ 0.074]] [[0.51 ]
 [0.435]
 [0.51 ]
 [0.51 ]
 [0.386]
 [0.51 ]
 [0.51 ]]
maxi score, test score, baseline:  -0.96468 -1.0 -0.96468
probs:  [0.630496762211814, 0.2081558813909542, 0.14363118267326916, 0.01701759643202104, 0.0003492886459708173, 0.0003492886459708173]
from probs:  [0.630496762211814, 0.2081558813909542, 0.14363118267326916, 0.01701759643202104, 0.0003492886459708173, 0.0003492886459708173]
maxi score, test score, baseline:  -0.96468 -1.0 -0.96468
maxi score, test score, baseline:  -0.96468 -1.0 -0.96468
probs:  [0.630496762211814, 0.2081558813909542, 0.14363118267326916, 0.01701759643202104, 0.0003492886459708173, 0.0003492886459708173]
start point for exploration sampling:  11106
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.65828323
maxi score, test score, baseline:  -0.96468 -1.0 -0.96468
probs:  [0.630496762211814, 0.2081558813909542, 0.14363118267326916, 0.01701759643202104, 0.0003492886459708173, 0.0003492886459708173]
using another actor
maxi score, test score, baseline:  -0.96468 -1.0 -0.96468
probs:  [0.630496762211814, 0.2081558813909542, 0.14363118267326916, 0.01701759643202104, 0.0003492886459708173, 0.0003492886459708173]
maxi score, test score, baseline:  -0.96468 -1.0 -0.96468
probs:  [0.630496762211814, 0.2081558813909542, 0.14363118267326916, 0.01701759643202104, 0.0003492886459708173, 0.0003492886459708173]
