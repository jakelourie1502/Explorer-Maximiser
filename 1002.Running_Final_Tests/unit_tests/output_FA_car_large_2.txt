dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
res_block_channels:[32, 64, 64]
res_block_ds:[False, False, False]
reward_support:[-1, 1, 41]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 41]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
channels:3
env_size:[8, 60]
observable_size:[8, 10]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.
  0. 0. 0. 0. 0. 0. 2. 2. 2. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.
  1. 1. 1. 1. 2. 1. 1. 1. 0. 0. 2. 2. 2. 1. 1. 0. 0. 0. 2. 1. 1. 1. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 2. 1. 1. 1. 2. 2. 2. 2. 2.
  1. 1. 1. 1. 2. 2. 2. 1. 1. 1. 2. 2. 2. 0. 1. 1. 1. 1. 1. 1. 1. 2. 0. 0.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.
  1. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 0. 2. 2. 1. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.
  1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 2. 2. 1. 2. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2.
  2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:200
actions_size:7
optimal_score:0.86
total_frames:305000
exp_gamma:0.975
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
timesteps_in_obs:2
store_prev_actions:True
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 1, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
state_size:[6, 6]
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 480)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.
  0. 0. 0. 0. 0. 0. 2. 2. 2. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.
  1. 1. 1. 1. 2. 1. 1. 1. 0. 0. 2. 2. 2. 1. 1. 0. 0. 0. 2. 1. 1. 1. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 2. 1. 1. 1. 2. 2. 2. 2. 2.
  1. 1. 1. 1. 2. 2. 2. 1. 1. 1. 2. 2. 2. 0. 1. 1. 1. 1. 1. 1. 1. 2. 0. 0.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.
  1. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 0. 2. 2. 1. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.
  1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 2. 2. 1. 2. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2.
  2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Starting evaluation
siam score:  -0.008938467638059095
UNIT TEST: sample policy line 217 mcts : [0.  0.  0.  0.2 0.  0.8 0. ]
maxi score, test score, baseline:  -0.9499 0.0 0.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
siam score:  -0.04464049852279903
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
deleting a thread, now have 2 threads
Frames:  813 train batches done:  27 episodes:  22
first move QE:  0.0001629139930272409
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
siam score:  -0.2939403
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
main train batch thing paused
add a thread
Adding thread: now have 3 threads
deleting a thread, now have 2 threads
Frames:  1090 train batches done:  104 episodes:  31
maxi score, test score, baseline:  -0.9582333333333334 -1.0 -0.9582333333333334
siam score:  -0.320199
maxi score, test score, baseline:  -0.9582333333333334 -1.0 -0.9582333333333334
maxi score, test score, baseline:  -0.9582333333333334 -1.0 -0.9582333333333334
probs:  [0.12413417181258042, 0.14894479381079742, 0.06210761681703794, 0.3722403917947503, 0.1861607268081229, 0.10641229895671112]
maxi score, test score, baseline:  -0.9582333333333334 -1.0 -0.9582333333333334
probs:  [0.12413417181258042, 0.14894479381079742, 0.06210761681703794, 0.3722403917947503, 0.1861607268081229, 0.10641229895671112]
siam score:  -0.32342052
maxi score, test score, baseline:  -0.9582333333333334 -1.0 -0.9582333333333334
probs:  [0.12413417181258042, 0.14894479381079742, 0.06210761681703794, 0.3722403917947503, 0.1861607268081229, 0.10641229895671112]
maxi score, test score, baseline:  -0.9582333333333334 -1.0 -0.9582333333333334
siam score:  -0.34029785
19 15
from probs:  [0.14171426741559298, 0.17003861251140637, 0.07090340467605956, 0.2833359928946599, 0.21252513015512642, 0.12148259234715485]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.002]
 [-0.002]
 [ 0.025]
 [-0.002]
 [-0.002]
 [-0.002]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.427]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.175]
 [-0.175]
 [-0.175]
 [-0.691]
 [-0.175]
 [-0.175]
 [-0.175]]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.11967167802729804, 0.19141239693809564, 0.07368403769986366, 0.2392395428786274, 0.2392395428786274, 0.13675280157748793]
siam score:  -0.3955866
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.11967167802729804, 0.19141239693809564, 0.07368403769986366, 0.2392395428786274, 0.2392395428786274, 0.13675280157748793]
from probs:  [0.11967167802729804, 0.19141239693809564, 0.07368403769986366, 0.2392395428786274, 0.2392395428786274, 0.13675280157748793]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.11729420685597308, 0.17588443301689177, 0.08123868306463847, 0.211038568713443, 0.26376977225826986, 0.15077433609078378]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.13179139836868797, 0.14824939014975658, 0.08476856470849195, 0.16940966529684476, 0.296371316179374, 0.16940966529684476]
siam score:  -0.44699174
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.13179139836868797, 0.14824939014975658, 0.08476856470849195, 0.16940966529684476, 0.296371316179374, 0.16940966529684476]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.13179139836868797, 0.14824939014975658, 0.08476856470849195, 0.16940966529684476, 0.296371316179374, 0.16940966529684476]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.13179139836868797, 0.14824939014975658, 0.08476856470849195, 0.16940966529684476, 0.296371316179374, 0.16940966529684476]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.13464042684668523, 0.15145420275217547, 0.08660106711671306, 0.17307191463066296, 0.3027781859015878, 0.15145420275217547]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.13641843062086276, 0.1666982081015424, 0.09383749353865704, 0.1666982081015424, 0.29992922901653274, 0.13641843062086276]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.13641843062086276, 0.1666982081015424, 0.09383749353865704, 0.1666982081015424, 0.29992922901653274, 0.13641843062086276]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.12080696014346412, 0.1690578901552561, 0.10572854451477913, 0.1690578901552561, 0.281643393516104, 0.15370532151514046]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.12080696014346412, 0.1690578901552561, 0.10572854451477913, 0.1690578901552561, 0.281643393516104, 0.15370532151514046]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.12080696014346412, 0.1690578901552561, 0.10572854451477913, 0.1690578901552561, 0.281643393516104, 0.15370532151514046]
first move QE:  -0.28146877091649003
siam score:  -0.58523786
siam score:  -0.59406286
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.13975961972780268, 0.13975961972780268, 0.11513283100409666, 0.16301825352241395, 0.27931142249547014, 0.16301825352241395]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13975955813263166, 0.13975955813263166, 0.11513271303368432, 0.16301824517052632, 0.27931168035999976, 0.16301824517052632]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
siam score:  -0.6012904
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.15500860950770834, 0.13566119549922429, 0.12061320682595891, 0.1669147104360062, 0.24099711621208184, 0.18080516151902037]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.14921471514125548, 0.13168786253633613, 0.11197015335580181, 0.17213444547076545, 0.24853354656913196, 0.18645927692670916]
58 38
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.15031242502378878, 0.12530005545232853, 0.11279387066659839, 0.1734007661666752, 0.2503619033096299, 0.1878309793809792]
first move QE:  -0.23063687562400412
siam score:  -0.58570665
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.1530253420948075, 0.12756153622733707, 0.10937310346485822, 0.16393840175229485, 0.2548805655646892, 0.19122105089601316]
first move QE:  -0.2268516873606903
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.1530253420948075, 0.12756153622733707, 0.10937310346485822, 0.16393840175229485, 0.2548805655646892, 0.19122105089601316]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.1530253420948075, 0.12756153622733707, 0.10937310346485822, 0.16393840175229485, 0.2548805655646892, 0.19122105089601316]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.15302531147271384, 0.1275614484440019, 0.10937297485206482, 0.1639383956278761, 0.25488076358756157, 0.19122110601578174]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.14004253192276553, 0.14004253192276553, 0.12007443086483968, 0.1679978734038617, 0.2518638978471503, 0.17997873403861722]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.14004253192276553, 0.14004253192276553, 0.12007443086483968, 0.1679978734038617, 0.2518638978471503, 0.17997873403861722]
siam score:  -0.5861716
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.14004253192276553, 0.14004253192276553, 0.12007443086483968, 0.1679978734038617, 0.2518638978471503, 0.17997873403861722]
using another actor
siam score:  -0.6053773
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]]
using another actor
76 49
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.14299088604276547, 0.14299088604276547, 0.1251564309829802, 0.1580093745141636, 0.2307912801832468, 0.20006114223407834]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.14299088604276547, 0.14299088604276547, 0.1251564309829802, 0.1580093745141636, 0.2307912801832468, 0.20006114223407834]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.14299088604276547, 0.14299088604276547, 0.1251564309829802, 0.1580093745141636, 0.2307912801832468, 0.20006114223407834]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
line 256 mcts: sample exp_bonus -0.15509483218193054
using another actor
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.1415747816324288, 0.1415747816324288, 0.12527861664366263, 0.1713081352961425, 0.21689927758050354, 0.20336440721483384]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.14346496864396582, 0.14346496864396582, 0.12226208420627631, 0.16493288913712645, 0.21979535261964803, 0.20607973674901764]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.14432562728051682, 0.13832654161740346, 0.12299554492278052, 0.1659223356677248, 0.22111392376836747, 0.20731602674320682]
using another actor
from probs:  [0.14634487884903544, 0.14026186032269347, 0.12471636853315296, 0.16824374554386642, 0.21021657337562585, 0.21021657337562585]
from probs:  [0.14112006786841286, 0.14112006786841286, 0.1254792930939718, 0.16927346246240682, 0.21150355435339777, 0.21150355435339777]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.14289491057557804, 0.14289491057557804, 0.12705742447962093, 0.1714023855483009, 0.2141635980073852, 0.20158677081353685]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.1428948162604453, 0.1428948162604453, 0.12705726732880204, 0.17140240433740314, 0.21416378645283987, 0.20158690936006435]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.1428948162604453, 0.1428948162604453, 0.12705726732880204, 0.17140240433740314, 0.21416378645283987, 0.20158690936006435]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.14354431331176126, 0.14354431331176126, 0.12308915249475909, 0.17218153845556433, 0.21513737617126888, 0.2025033062548852]
siam score:  -0.6602048
using another actor
siam score:  -0.6589952
siam score:  -0.6561135
94 64
using another actor
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.14771780294607498, 0.14771780294607498, 0.12666767909013052, 0.16111333630894872, 0.20839168935438548, 0.20839168935438548]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.1501063139447962, 0.1501063139447962, 0.12429020276388845, 0.16371844529472937, 0.21176126182390528, 0.2000174622278845]
siam score:  -0.6431452
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.1501063139447962, 0.1501063139447962, 0.12429020276388845, 0.16371844529472937, 0.21176126182390528, 0.2000174622278845]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.1501063139447962, 0.1501063139447962, 0.12429020276388845, 0.16371844529472937, 0.21176126182390528, 0.2000174622278845]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.1501063139447962, 0.1501063139447962, 0.12429020276388845, 0.16371844529472937, 0.21176126182390528, 0.2000174622278845]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.1501063139447962, 0.1501063139447962, 0.12429020276388845, 0.16371844529472937, 0.21176126182390528, 0.2000174622278845]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.1501063139447962, 0.1501063139447962, 0.12429020276388845, 0.16371844529472937, 0.21176126182390528, 0.2000174622278845]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.563]
 [0.784]
 [0.78 ]
 [0.749]
 [0.763]
 [0.715]] [[-0.059]
 [-0.092]
 [-0.104]
 [-0.092]
 [-0.084]
 [-0.088]
 [-0.074]] [[0.269]
 [0.154]
 [0.575]
 [0.587]
 [0.538]
 [0.559]
 [0.485]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.1501063139447962, 0.1501063139447962, 0.12429020276388845, 0.16371844529472937, 0.21176126182390528, 0.2000174622278845]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.1501063139447962, 0.1501063139447962, 0.12429020276388845, 0.16371844529472937, 0.21176126182390528, 0.2000174622278845]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[-0.03 ]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]] [[ 0.073]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]]
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.097]
 [0.288]
 [0.341]
 [0.286]
 [0.293]
 [0.237]] [[-0.02 ]
 [-0.058]
 [-0.095]
 [-0.13 ]
 [-0.183]
 [-0.095]
 [-0.164]] [[0.252]
 [0.097]
 [0.288]
 [0.341]
 [0.286]
 [0.293]
 [0.237]]
main train batch thing paused
add a thread
using another actor
Adding thread: now have 4 threads
main train batch thing paused
add a thread
Adding thread: now have 5 threads
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
siam score:  -0.64426756
siam score:  -0.6436964
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.268]
 [0.511]
 [0.554]
 [0.591]
 [0.54 ]
 [0.598]] [[ 0.845]
 [ 0.004]
 [ 0.027]
 [ 0.017]
 [-0.041]
 [ 0.011]
 [ 0.058]] [[0.455]
 [0.268]
 [0.511]
 [0.554]
 [0.591]
 [0.54 ]
 [0.598]]
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.342]
 [0.379]
 [0.298]
 [0.353]
 [0.456]
 [0.571]] [[ 0.352]
 [ 0.069]
 [ 0.077]
 [-0.003]
 [ 0.003]
 [ 0.036]
 [ 0.094]] [[0.265]
 [0.342]
 [0.379]
 [0.298]
 [0.353]
 [0.456]
 [0.571]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.531]
 [0.626]
 [0.637]
 [0.647]
 [0.658]
 [0.665]] [[ 0.015]
 [ 0.049]
 [-0.121]
 [-0.107]
 [-0.154]
 [-0.125]
 [-0.084]] [[0.607]
 [0.531]
 [0.626]
 [0.637]
 [0.647]
 [0.658]
 [0.665]]
siam score:  -0.6544153
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.11141322910538327
siam score:  -0.66217875
siam score:  -0.66560364
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.15615710013907338, 0.14992630250066294, 0.1293002137666146, 0.17031800386273346, 0.19714918986545782, 0.19714918986545782]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
Printing some Q and Qe and total Qs values:  [[1.152]
 [1.113]
 [1.212]
 [1.199]
 [1.155]
 [1.1  ]
 [1.083]] [[0.281]
 [0.282]
 [0.247]
 [0.183]
 [0.203]
 [0.293]
 [0.31 ]] [[2.32 ]
 [2.242]
 [2.371]
 [2.216]
 [2.169]
 [2.24 ]
 [2.238]]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.15615708562531658, 0.1499262793821477, 0.1293001621633817, 0.1703180089052459, 0.19714923196195408, 0.19714923196195408]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.15615708562531658, 0.1499262793821477, 0.1293001621633817, 0.1703180089052459, 0.19714923196195408, 0.19714923196195408]
siam score:  -0.7017729
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.70652616
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.15839604908155672, 0.15207590621030267, 0.12679533472528653, 0.1727600101525886, 0.19997593639243852, 0.18999676343782687]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
siam score:  -0.71980774
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.1595920704487767, 0.1532242052639818, 0.12775274452480226, 0.16651366304094506, 0.2014859203487431, 0.19143139637275117]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.1595920704487767, 0.1532242052639818, 0.12775274452480226, 0.16651366304094506, 0.2014859203487431, 0.19143139637275117]
siam score:  -0.71817666
line 256 mcts: sample exp_bonus -0.1849542440811696
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.1595920704487767, 0.1532242052639818, 0.12775274452480226, 0.16651366304094506, 0.2014859203487431, 0.19143139637275117]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.1595920704487767, 0.1532242052639818, 0.12775274452480226, 0.16651366304094506, 0.2014859203487431, 0.19143139637275117]
Printing some Q and Qe and total Qs values:  [[1.049]
 [0.61 ]
 [0.953]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[0.264]
 [1.103]
 [0.064]
 [1.103]
 [1.103]
 [1.103]
 [1.103]] [[1.56 ]
 [1.801]
 [1.099]
 [1.801]
 [1.801]
 [1.801]
 [1.801]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.16631116864007045, 0.15967520547694092, 0.1248363988705108, 0.15967520547694092, 0.19949098445571814, 0.1900110370798188]
first move QE:  -0.11169647821127607
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.658]
 [0.658]
 [0.665]
 [0.658]
 [0.658]
 [0.663]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.002]
 [0.   ]
 [0.   ]
 [0.036]] [[0.658]
 [0.658]
 [0.658]
 [0.665]
 [0.658]
 [0.658]
 [0.663]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.16775692181678034, 0.161063262290501, 0.1259215497775344, 0.161063262290501, 0.20122521944817712, 0.18296978437650613]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.16775692181678034, 0.161063262290501, 0.1259215497775344, 0.161063262290501, 0.20122521944817712, 0.18296978437650613]
Printing some Q and Qe and total Qs values:  [[0.986]
 [0.918]
 [1.021]
 [0.918]
 [1.019]
 [0.998]
 [0.936]] [[ 0.031]
 [ 0.   ]
 [-0.084]
 [ 0.   ]
 [-0.03 ]
 [-0.009]
 [ 0.079]] [[0.976]
 [0.808]
 [0.93 ]
 [0.808]
 [0.981]
 [0.959]
 [0.923]]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.16775692181678034, 0.161063262290501, 0.1259215497775344, 0.161063262290501, 0.20122521944817712, 0.18296978437650613]
from probs:  [0.16775692181678034, 0.161063262290501, 0.1259215497775344, 0.161063262290501, 0.20122521944817712, 0.18296978437650613]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.53 ]
 [0.543]
 [0.542]
 [0.534]
 [0.544]
 [0.547]] [[-0.012]
 [-0.056]
 [-0.141]
 [-0.168]
 [-0.149]
 [-0.082]
 [-0.016]] [[-0.201]
 [-0.27 ]
 [-0.328]
 [-0.358]
 [-0.356]
 [-0.268]
 [-0.197]]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.496]
 [0.517]
 [0.517]
 [0.516]
 [0.515]
 [0.513]] [[ 0.057]
 [ 0.004]
 [-0.071]
 [-0.107]
 [-0.126]
 [-0.096]
 [-0.045]] [[ 0.084]
 [-0.002]
 [-0.035]
 [-0.073]
 [-0.093]
 [-0.065]
 [-0.018]]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.426]
 [0.566]
 [0.631]
 [0.623]
 [0.611]
 [0.609]] [[-0.006]
 [ 1.163]
 [ 0.067]
 [-0.114]
 [ 0.051]
 [ 0.061]
 [ 0.112]] [[0.608]
 [0.426]
 [0.566]
 [0.631]
 [0.623]
 [0.611]
 [0.609]]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.17006323488839808, 0.16327754195836458, 0.12016843393226923, 0.1570138254075644, 0.20399169953856572, 0.1854852642748379]
Printing some Q and Qe and total Qs values:  [[1.305]
 [1.302]
 [1.264]
 [1.278]
 [1.302]
 [1.263]
 [1.251]] [[0.419]
 [0.767]
 [0.557]
 [0.53 ]
 [0.767]
 [0.335]
 [0.351]] [[1.701]
 [2.392]
 [1.896]
 [1.868]
 [2.392]
 [1.448]
 [1.458]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.29 ]
 [0.523]
 [0.534]
 [0.551]
 [0.554]
 [0.55 ]] [[ 1.059]
 [ 1.346]
 [ 0.006]
 [-0.072]
 [-0.079]
 [-0.021]
 [ 0.037]] [[0.48 ]
 [0.29 ]
 [0.523]
 [0.534]
 [0.551]
 [0.554]
 [0.55 ]]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.17006323488839808, 0.16327754195836458, 0.12016843393226923, 0.1570138254075644, 0.20399169953856572, 0.1854852642748379]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.17006323488839808, 0.16327754195836458, 0.12016843393226923, 0.1570138254075644, 0.20399169953856572, 0.1854852642748379]
line 256 mcts: sample exp_bonus 1.0107630318722438
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
main train batch thing paused
add a thread
Adding thread: now have 6 threads
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.508]
 [0.513]
 [0.508]
 [0.508]
 [0.519]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.556]
 [ 0.   ]
 [ 0.   ]
 [-0.304]] [[0.508]
 [0.508]
 [0.508]
 [0.513]
 [0.508]
 [0.508]
 [0.519]]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
from probs:  [0.17245101683943717, 0.16557002244268942, 0.12185546980452719, 0.15333714351513786, 0.2068559888231759, 0.17993035857503253]
siam score:  -0.7481281
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.16671720257948028, 0.16671720257948028, 0.12269976620828868, 0.15439956606408511, 0.20828922581893897, 0.18117703674972677]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.16671720257948028, 0.16671720257948028, 0.12269976620828868, 0.15439956606408511, 0.20828922581893897, 0.18117703674972677]
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.778]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.792]] [[ 0.035]
 [ 0.074]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.044]] [[0.424]
 [0.43 ]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.34 ]]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
siam score:  -0.7553737
siam score:  -0.7568857
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.15632509617867663, 0.16879638030889557, 0.1242298796670838, 0.15632509617867663, 0.21088696424838443, 0.183436583418283]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.15632509617867663, 0.16879638030889557, 0.1242298796670838, 0.15632509617867663, 0.21088696424838443, 0.183436583418283]
from probs:  [0.15160160625725383, 0.16974142205133735, 0.12492540656007209, 0.15720031483567465, 0.21206765890419899, 0.18446359139146315]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.306 0.02  0.02  0.02  0.02  0.184 0.429]
Printing some Q and Qe and total Qs values:  [[1.103]
 [0.968]
 [1.094]
 [1.097]
 [1.108]
 [1.104]
 [1.064]] [[-0.003]
 [ 0.007]
 [-0.206]
 [-0.432]
 [-0.232]
 [-0.253]
 [ 0.455]] [[1.12 ]
 [0.854]
 [1.034]
 [0.964]
 [1.052]
 [1.038]
 [1.195]]
first move QE:  -0.10267526954581008
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.15214273412149162, 0.17034732236885275, 0.12180175370922311, 0.1577614341978377, 0.21282469494602868, 0.18512206065656614]
first move QE:  -0.10155772879660148
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
using explorer policy with actor:  0
siam score:  -0.7567104
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.1490309177092865, 0.1728048008069753, 0.12013904588917859, 0.15433758804359204, 0.21589496392153626, 0.1877926836294313]
Printing some Q and Qe and total Qs values:  [[0.855]
 [0.614]
 [0.854]
 [0.868]
 [0.867]
 [0.865]
 [0.856]] [[-0.029]
 [ 0.345]
 [ 0.015]
 [-0.128]
 [-0.084]
 [-0.167]
 [-0.046]] [[0.245]
 [0.014]
 [0.274]
 [0.205]
 [0.233]
 [0.174]
 [0.236]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
Starting evaluation
Printing some Q and Qe and total Qs values:  [[1.303]
 [1.292]
 [1.305]
 [1.303]
 [1.292]
 [1.315]
 [1.281]] [[ 0.001]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.   ]] [[2.222]
 [2.198]
 [2.223]
 [2.221]
 [2.198]
 [2.245]
 [2.176]]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.571]
 [0.583]
 [0.584]
 [0.571]
 [0.571]
 [0.567]] [[0.002]
 [0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.001]] [[0.596]
 [0.571]
 [0.583]
 [0.584]
 [0.571]
 [0.571]
 [0.567]]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.484]
 [0.499]
 [0.509]
 [0.515]
 [0.516]
 [0.51 ]] [[-0.011]
 [ 0.064]
 [-0.078]
 [-0.005]
 [ 0.013]
 [-0.002]
 [ 0.055]] [[-0.13 ]
 [-0.003]
 [-0.256]
 [-0.09 ]
 [-0.042]
 [-0.071]
 [ 0.03 ]]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]] [[0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]]
line 256 mcts: sample exp_bonus 0.25955556820921166
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.419]
 [0.446]
 [0.451]
 [0.45 ]
 [0.446]
 [0.427]] [[-0.012]
 [-0.02 ]
 [-0.011]
 [-0.011]
 [-0.015]
 [-0.013]
 [-0.012]] [[0.435]
 [0.419]
 [0.446]
 [0.451]
 [0.45 ]
 [0.446]
 [0.427]]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.475]
 [0.537]
 [0.564]
 [0.564]
 [0.556]
 [0.567]] [[-0.011]
 [-0.05 ]
 [-0.014]
 [-0.011]
 [-0.011]
 [-0.013]
 [-0.036]] [[0.564]
 [0.475]
 [0.537]
 [0.564]
 [0.564]
 [0.556]
 [0.567]]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.438]
 [0.442]
 [0.447]
 [0.438]
 [0.445]
 [0.434]] [[-0.001]
 [ 0.001]
 [ 0.001]
 [-0.   ]
 [ 0.001]
 [-0.001]
 [-0.002]] [[0.441]
 [0.438]
 [0.442]
 [0.447]
 [0.438]
 [0.445]
 [0.434]]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -0.00920203188900742
line 256 mcts: sample exp_bonus -0.0023181979518032535
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.453]
 [0.455]
 [0.451]
 [0.451]
 [0.454]
 [0.455]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.456]
 [0.453]
 [0.455]
 [0.451]
 [0.451]
 [0.454]
 [0.455]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
siam score:  -0.7354542
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.1552663398156772, 0.16673337153189902, 0.12516538156059487, 0.16079508725028413, 0.20452245332399369, 0.18751736651755108]
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.15579138348394625, 0.16729722821870735, 0.12220675560950836, 0.16133884433820606, 0.20521421654917016, 0.1881515718004619]
siam score:  -0.75063354
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.156]
 [0.355]
 [0.41 ]
 [0.363]
 [0.383]
 [0.353]] [[ 0.007]
 [ 0.002]
 [ 0.005]
 [-0.011]
 [ 0.006]
 [-0.007]
 [ 0.005]] [[0.353]
 [0.156]
 [0.355]
 [0.41 ]
 [0.363]
 [0.383]
 [0.353]]
maxi score, test score, baseline:  -0.9878518072289156 -1.0 -0.9878518072289156
siam score:  -0.76214963
using explorer policy with actor:  1
STARTED EXPV TRAINING ON FRAME NO.  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.903]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[-0.018]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[1.246]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.15139527548710527, 0.17375329680633542, 0.12692230620524525, 0.15642583028393206, 0.2038880211931239, 0.1876152700242581]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.694]
 [0.691]
 [0.694]
 [0.98 ]
 [0.694]
 [0.694]] [[ 0.   ]
 [ 0.   ]
 [-0.   ]
 [ 0.   ]
 [-0.019]
 [ 0.   ]
 [ 0.   ]] [[0.553]
 [0.553]
 [0.547]
 [0.553]
 [1.112]
 [0.553]
 [0.553]]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.15233799060843187, 0.16860832988181199, 0.12771261224872146, 0.1573998739379279, 0.20515764274230347, 0.18878355058080326]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.15374408242372897, 0.16431360983768484, 0.12551231841013644, 0.1588526873404743, 0.20705126416368014, 0.19052603782429528]
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.564]
 [0.714]
 [0.719]
 [0.718]
 [0.712]
 [0.709]] [[-0.001]
 [-0.   ]
 [-0.001]
 [-0.   ]
 [-0.   ]
 [ 0.   ]
 [ 0.   ]] [[1.303]
 [0.986]
 [1.285]
 [1.295]
 [1.293]
 [1.282]
 [1.275]]
Printing some Q and Qe and total Qs values:  [[0.842]
 [0.727]
 [0.839]
 [0.827]
 [0.841]
 [0.837]
 [0.834]] [[-0.]
 [ 0.]
 [ 0.]
 [-0.]
 [-0.]
 [ 0.]
 [ 0.]] [[2.447]
 [2.218]
 [2.441]
 [2.417]
 [2.446]
 [2.437]
 [2.431]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.0005107991483805146
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.597]
 [0.651]
 [0.654]
 [0.651]
 [0.649]
 [0.646]] [[-0.001]
 [ 0.   ]
 [-0.001]
 [-0.001]
 [-0.   ]
 [-0.   ]
 [-0.001]] [[1.046]
 [0.948]
 [1.055]
 [1.061]
 [1.055]
 [1.052]
 [1.046]]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.422]
 [0.477]
 [0.479]
 [0.478]
 [0.477]
 [0.474]] [[-0.001]
 [ 0.   ]
 [-0.001]
 [-0.001]
 [-0.   ]
 [-0.   ]
 [-0.   ]] [[0.46 ]
 [0.355]
 [0.464]
 [0.468]
 [0.465]
 [0.464]
 [0.457]]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.516]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]]
first move QE:  -0.08577856746680994
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  10749
siam score:  -0.8687248
rdn probs:  [0.1572640000015082, 0.1572640000015082, 0.12510667902341013, 0.16248956466044914, 0.2029876907672414, 0.19488806554588292]
siam score:  -0.86907256
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
line 256 mcts: sample exp_bonus 0.011200805131029477
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.16141240593644052, 0.16141240593644052, 0.1284067438980494, 0.15638419961027933, 0.19235521409743211, 0.20002903052135806]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.16141240593644052, 0.16141240593644052, 0.1284067438980494, 0.15638419961027933, 0.19235521409743211, 0.20002903052135806]
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.16141240593644052, 0.16141240593644052, 0.1284067438980494, 0.15638419961027933, 0.19235521409743211, 0.20002903052135806]
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.131]
 [0.129]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]
 [0.001]] [[-0.223]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.207]
 [-0.209]]
from probs:  [0.16591632190281846, 0.16591632190281846, 0.12870305591580214, 0.15132288426085125, 0.19041888386957814, 0.19772253214813149]
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.16643665577213657, 0.16643665577213657, 0.12597056218015923, 0.1517974513256271, 0.191016060768745, 0.19834261418119556]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.109]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.646]] [[-0.006]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[2.005]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [1.076]]
siam score:  -0.89253324
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
Printing some Q and Qe and total Qs values:  [[0.862]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.544]] [[-0.007]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[1.376]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.738]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  10749
siam score:  -0.8945075
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.1678078982374342, 0.1625804640101696, 0.12399702090416923, 0.15304808394868713, 0.1925898086481699, 0.19997672425136995]
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.038]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[-0.196]
 [-0.199]
 [-0.199]
 [-0.199]
 [-0.199]
 [-0.199]
 [-0.196]]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.064]
 [-0.047]
 [-0.046]
 [-0.037]
 [-0.013]
 [-0.011]] [[-0.016]
 [-0.019]
 [-0.012]
 [-0.015]
 [-0.017]
 [-0.017]
 [-0.018]] [[-0.529]
 [-0.656]
 [-0.619]
 [-0.618]
 [-0.602]
 [-0.553]
 [-0.551]]
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[-0.738]
 [-0.738]
 [-0.738]
 [-0.738]
 [-0.738]
 [-0.738]
 [-0.738]]
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.16854234839577167, 0.1632920350910844, 0.12453972260410702, 0.14934120259577252, 0.1934327225809557, 0.20085196873230862]
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.002609346495818777
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.16226884257714835, 0.16732270598158183, 0.12465869631159691, 0.1487918734986591, 0.19114806203105397, 0.2058098195999599]
deleting a thread, now have 5 threads
Frames:  12878 train batches done:  1495 episodes:  247
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.007091058009660206
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.16309309018641854, 0.16309309018641854, 0.1252918743346496, 0.14954765450620136, 0.192119023786884, 0.206855266999428]
start point for exploration sampling:  10749
siam score:  -0.90089613
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.16309308763704375, 0.16309308763704375, 0.12529184481804256, 0.14954764229356832, 0.19211904194449111, 0.2068552956698105]
using explorer policy with actor:  1
deleting a thread, now have 4 threads
Frames:  13011 train batches done:  1522 episodes:  251
siam score:  -0.9023828
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.1613830867774913, 0.16625656310535178, 0.12772209911761787, 0.1524483801764138, 0.18911148657393884, 0.20307838424918645]
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[1.174]
 [1.174]
 [1.174]
 [1.174]
 [1.174]
 [1.174]
 [1.174]] [[1.474]
 [1.474]
 [1.474]
 [1.474]
 [1.474]
 [1.474]
 [1.474]]
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.23876154225016732
siam score:  -0.8977887
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.273]
 [0.283]] [[4.032]
 [4.098]
 [4.098]
 [4.098]
 [4.098]
 [4.052]
 [3.997]] [[1.591]
 [1.62 ]
 [1.62 ]
 [1.62 ]
 [1.62 ]
 [1.609]
 [1.573]]
178 77
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.16087453776878083, 0.16087453776878083, 0.13105095857738727, 0.15220957894965972, 0.19404041462817542, 0.20094997230721595]
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
line 256 mcts: sample exp_bonus -0.06897482541123447
180 79
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.1608745295616639, 0.1608745295616639, 0.13105090811195622, 0.15220955846478937, 0.19404045341521814, 0.20095002088470862]
siam score:  -0.90206873
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
first move QE:  -0.06311668703159183
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[-0.242]
 [-0.242]
 [-0.242]
 [-0.242]
 [-0.242]
 [-0.242]
 [-0.242]] [[-0.591]
 [-0.591]
 [-0.591]
 [-0.591]
 [-0.591]
 [-0.591]
 [-0.591]]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
185 80
siam score:  -0.89047736
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.15563722230316307, 0.16449732978330306, 0.13400207613072826, 0.15563722230316307, 0.19841015496590778, 0.19181599451373463]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.15611058057400376, 0.1649976353685085, 0.13136821211203037, 0.15611058057400376, 0.19901360371988872, 0.19239938765156478]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
188 80
first move QE:  -0.062495833502703144
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
Printing some Q and Qe and total Qs values:  [[-0.089]
 [-0.089]
 [-0.088]
 [-0.084]
 [-0.086]
 [-0.089]
 [-0.091]] [[ 1.017]
 [-0.053]
 [ 1.224]
 [ 1.229]
 [ 1.016]
 [ 1.282]
 [ 1.05 ]] [[ 0.627]
 [-0.629]
 [ 0.871]
 [ 0.881]
 [ 0.629]
 [ 0.938]
 [ 0.663]]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.815]
 [0.737]
 [0.723]
 [0.732]
 [0.746]
 [0.749]] [[0.679]
 [1.46 ]
 [0.824]
 [0.895]
 [0.72 ]
 [0.711]
 [0.945]] [[0.797]
 [1.164]
 [0.796]
 [0.792]
 [0.751]
 [0.775]
 [0.861]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.703688129954698
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.15143548778208063, 0.1636641330006671, 0.1346777147047584, 0.15937338029239115, 0.20170880701404728, 0.1891404772060556]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.882]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]] [[0.194]
 [0.818]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]] [[1.508]
 [2.28 ]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.14955761386485752, 0.16131561682644835, 0.1363189734932886, 0.15719529100230115, 0.2041670053975794, 0.19144549941552483]
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.658]
 [0.622]
 [0.582]
 [0.585]
 [0.622]
 [0.607]] [[4.811]
 [1.577]
 [4.445]
 [5.047]
 [5.168]
 [4.445]
 [4.76 ]] [[1.9  ]
 [0.404]
 [1.756]
 [2.007]
 [2.069]
 [1.756]
 [1.893]]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
Printing some Q and Qe and total Qs values:  [[0.68]
 [0.68]
 [0.68]
 [0.68]
 [0.68]
 [0.68]
 [0.68]] [[2.153]
 [2.153]
 [2.153]
 [1.959]
 [2.153]
 [2.153]
 [2.153]] [[2.525]
 [2.525]
 [2.525]
 [2.137]
 [2.525]
 [2.525]
 [2.525]]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.14853306750975584, 0.1641014382367585, 0.13567223951788412, 0.1559280436050821, 0.20101354302497437, 0.1947516681055449]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.15024871447221066, 0.16599690931781502, 0.13723933612149405, 0.15772910702387274, 0.2033353712904576, 0.1854505617741498]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.01683117422609825
first move QE:  -0.01683117422609825
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.15148889765953555, 0.16736708128106284, 0.13544179719097077, 0.15903103487976103, 0.20501374244823234, 0.18165744654043736]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.15148889765953555, 0.16736708128106284, 0.13544179719097077, 0.15903103487976103, 0.20501374244823234, 0.18165744654043736]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.14850186707288343, 0.16795626696125754, 0.1359185956557223, 0.1595908750092567, 0.20573545642190663, 0.1822969388789733]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.404]
 [0.351]
 [0.404]
 [0.404]
 [0.362]
 [0.35 ]] [[6.047]
 [5.596]
 [6.234]
 [5.596]
 [5.596]
 [6.085]
 [6.21 ]] [[1.309]
 [1.092]
 [1.428]
 [1.092]
 [1.092]
 [1.349]
 [1.413]]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.14990129376457728, 0.16520866588348215, 0.1371994317935711, 0.16109480962652648, 0.20767427885850862, 0.17892152007333445]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
siam score:  -0.9041833
siam score:  -0.9050775
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.15105535954574606, 0.16648058064325805, 0.13825570799674672, 0.15463621444338277, 0.20927312949442048, 0.18029900787644593]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]] [[0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[1.686]
 [1.686]
 [1.686]
 [1.686]
 [1.686]
 [1.686]
 [1.686]]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.15105535954574606, 0.16648058064325805, 0.13825570799674672, 0.15463621444338277, 0.20927312949442048, 0.18029900787644593]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.15179232999074418, 0.1672928078159262, 0.13893023136984847, 0.15539065520016146, 0.21029413339546346, 0.1762998422278563]
line 256 mcts: sample exp_bonus 3.469041636143624
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.1517923190543449, 0.1672928082762983, 0.13893021097655378, 0.15539064690944124, 0.2102941654726852, 0.17629984931067663]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.14930042252777842, 0.1683560357388305, 0.13691427394059458, 0.15637822172045493, 0.21163071864392943, 0.17742032742841204]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
siam score:  -0.9045346
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.717]
 [0.756]
 [0.795]
 [0.795]
 [0.795]
 [0.712]] [[4.686]
 [4.25 ]
 [4.074]
 [4.686]
 [4.686]
 [4.686]
 [4.32 ]] [[2.716]
 [2.196]
 [2.068]
 [2.716]
 [2.716]
 [2.716]
 [2.258]]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.662]] [[4.305]
 [4.305]
 [4.305]
 [4.305]
 [4.305]
 [4.305]
 [4.547]] [[1.967]
 [1.967]
 [1.967]
 [1.967]
 [1.967]
 [1.967]
 [2.069]]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.14999812573590832, 0.16914278867800023, 0.13755409482354858, 0.15710900054297103, 0.21261970064971866, 0.1735762895698531]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.526]] [[4.758]
 [4.758]
 [4.758]
 [4.758]
 [4.758]
 [4.758]
 [4.849]] [[1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.829]]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.14999812573590832, 0.16914278867800023, 0.13755409482354858, 0.15710900054297103, 0.21261970064971866, 0.1735762895698531]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.33 ]
 [0.314]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.274]] [[4.34 ]
 [4.34 ]
 [4.618]
 [4.34 ]
 [4.34 ]
 [4.34 ]
 [4.358]] [[1.495]
 [1.495]
 [1.713]
 [1.495]
 [1.495]
 [1.495]
 [1.438]]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.15066610404255298, 0.16989602284681385, 0.13816665681978343, 0.15780864531270702, 0.21356654813132886, 0.16989602284681385]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
Printing some Q and Qe and total Qs values:  [[-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.099]] [[4.22 ]
 [4.22 ]
 [4.22 ]
 [4.22 ]
 [4.22 ]
 [4.22 ]
 [4.585]] [[0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.77 ]]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.15164578461024456, 0.17100074267194518, 0.13624081798970736, 0.1551566839795763, 0.21495522807658143, 0.17100074267194518]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.1526680792492425, 0.1721535155165547, 0.13715926262832054, 0.1562026467581968, 0.2096629803311307, 0.1721535155165547]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.15447307066402344, 0.17418888282489975, 0.13336555411532056, 0.1580494272885545, 0.20573418228230186, 0.17418888282489975]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.15447307066402344, 0.17418888282489975, 0.13336555411532056, 0.1580494272885545, 0.20573418228230186, 0.17418888282489975]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
Printing some Q and Qe and total Qs values:  [[ 0.01 ]
 [-0.006]
 [ 0.032]
 [ 0.029]
 [ 0.022]
 [ 0.021]
 [ 0.022]] [[-0.082]
 [ 2.586]
 [ 0.083]
 [-0.028]
 [-0.085]
 [-0.067]
 [ 0.148]] [[ 0.01 ]
 [-0.006]
 [ 0.032]
 [ 0.029]
 [ 0.022]
 [ 0.021]
 [ 0.022]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5385],
        [-0.5395],
        [-0.4615],
        [-0.4306],
        [-0.2198],
        [-0.0000],
        [-0.5245],
        [-0.5395],
        [-0.2134],
        [-0.5395]], dtype=torch.float64)
-0.024259925299500003 -0.562790698158986
-0.024259925299500003 -0.5637899110298099
-0.0727797758985 -0.5342425613345877
-0.0727797758985 -0.5034052924084376
-0.0632698753995 -0.2830231281667684
-0.86625 -0.86625
-0.024259925299500003 -0.5487328099471372
-0.024259925299500003 -0.5637899110298099
-0.0727797758985 -0.286163143961533
-0.024259925299500003 -0.5637899110298099
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.1554102982121076, 0.17524574700346804, 0.13417470009429816, 0.15900835636495902, 0.2009151513216992, 0.17524574700346804]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.1554102982121076, 0.17524574700346804, 0.13417470009429816, 0.15900835636495902, 0.2009151513216992, 0.17524574700346804]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.1563044550571965, 0.17625402757469508, 0.13494667742081565, 0.15992321472316134, 0.19631759764943651, 0.17625402757469508]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.406]
 [0.408]
 [0.408]
 [0.408]
 [0.402]
 [0.406]] [[1.718]
 [2.201]
 [2.115]
 [2.115]
 [2.115]
 [1.765]
 [1.793]] [[0.421]
 [0.406]
 [0.408]
 [0.408]
 [0.408]
 [0.402]
 [0.406]]
Printing some Q and Qe and total Qs values:  [[0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]] [[1.125]
 [1.125]
 [1.125]
 [1.125]
 [1.125]
 [1.125]
 [1.125]] [[0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.15715843569754873, 0.17721700414038427, 0.13568396830580712, 0.16079696671741192, 0.19192662099846364, 0.17721700414038427]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.578]] [[1.892]
 [1.845]
 [1.845]
 [1.845]
 [1.845]
 [1.845]
 [1.907]] [[1.232]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.239]]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.1571584287050995, 0.1772170118992092, 0.13568394552081733, 0.16079696240077518, 0.19192663957488965, 0.1772170118992092]
using explorer policy with actor:  1
start point for exploration sampling:  10749
using explorer policy with actor:  1
from probs:  [0.1571584218427241, 0.17721701951370403, 0.135683923159675, 0.16079695816443676, 0.19192665780575596, 0.17721701951370403]
using explorer policy with actor:  1
from probs:  [0.15785501264187968, 0.17357007884006673, 0.13628531393848572, 0.1615096791995976, 0.19277738197118421, 0.17800253340878613]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.15826766219951627, 0.17402382085002907, 0.13402741812180433, 0.161931885141496, 0.1932813480895447, 0.1784678655976096]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.15826766219951627, 0.17402382085002907, 0.13402741812180433, 0.161931885141496, 0.1932813480895447, 0.1784678655976096]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[2.526]
 [2.575]
 [2.575]
 [2.575]
 [2.575]
 [2.575]
 [2.575]] [[1.602]
 [1.684]
 [1.684]
 [1.684]
 [1.684]
 [1.684]
 [1.684]]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.495]
 [0.436]
 [0.456]
 [0.444]
 [0.438]
 [0.428]] [[2.479]
 [1.581]
 [2.523]
 [2.385]
 [2.579]
 [2.539]
 [2.534]] [[1.454]
 [0.361]
 [1.499]
 [1.356]
 [1.588]
 [1.523]
 [1.497]]
line 256 mcts: sample exp_bonus 1.5895170726389818
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.158939541294963, 0.17051738042881268, 0.1345963923468688, 0.16261931962432608, 0.1941018675533213, 0.17922549875170815]
siam score:  -0.89342576
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.4392546275506595
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.499]
 [0.478]
 [0.473]
 [0.481]
 [0.484]
 [0.49 ]] [[0.469]
 [1.027]
 [0.676]
 [0.49 ]
 [0.733]
 [0.699]
 [0.833]] [[0.476]
 [0.499]
 [0.478]
 [0.473]
 [0.481]
 [0.484]
 [0.49 ]]
siam score:  -0.890291
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]] [[2.469]
 [2.124]
 [2.124]
 [2.124]
 [2.124]
 [2.124]
 [2.124]] [[0.742]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]]
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.14 ]
 [0.135]] [[2.604]
 [2.604]
 [2.604]
 [2.604]
 [2.604]
 [2.576]
 [2.597]] [[0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.558]
 [0.582]]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
siam score:  -0.88976485
siam score:  -0.8922258
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
Printing some Q and Qe and total Qs values:  [[ 0.01 ]
 [-0.006]
 [-0.007]
 [-0.007]
 [-0.008]
 [-0.007]
 [-0.004]] [[3.809]
 [3.987]
 [3.866]
 [3.887]
 [3.979]
 [4.036]
 [4.182]] [[0.971]
 [1.063]
 [0.992]
 [1.004]
 [1.058]
 [1.09 ]
 [1.177]]
225 98
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.16009155286255608, 0.17175332620890685, 0.13557192685228003, 0.15654982910551618, 0.19550879043295477, 0.18052457453778611]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.16093885157805768, 0.17266234589219318, 0.136289453276542, 0.1573783829345054, 0.191250949489381, 0.18148001682932074]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]] [[1.255]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.316]] [[0.233]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.16290858646766257, 0.1747755649408573, 0.1353682401996823, 0.1593045411535812, 0.18851627685718805, 0.17912679038102872]
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.105]
 [0.242]
 [0.251]
 [0.244]
 [0.234]
 [0.233]] [[3.531]
 [1.7  ]
 [3.886]
 [4.837]
 [4.703]
 [4.676]
 [4.526]] [[ 0.888]
 [-0.139]
 [ 1.083]
 [ 1.589]
 [ 1.515]
 [ 1.495]
 [ 1.416]]
siam score:  -0.8917917
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.518]
 [0.525]] [[4.454]
 [4.671]
 [4.671]
 [4.671]
 [4.671]
 [4.827]
 [4.713]] [[0.651]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.887]
 [0.826]]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.16358649285755802, 0.17134158438922134, 0.13593154381068306, 0.1599674501427818, 0.18930074372570488, 0.17987218507405103]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.16358649285755802, 0.17134158438922134, 0.13593154381068306, 0.1599674501427818, 0.18930074372570488, 0.17987218507405103]
in main func line 156:  232
siam score:  -0.8922655
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.2360220487649407
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
from probs:  [0.16399709715898012, 0.17177165950608891, 0.13376268803133468, 0.16036896806366266, 0.18977590915202514, 0.18032367808790864]
line 256 mcts: sample exp_bonus 0.9849649146555963
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.726]
 [0.618]
 [0.618]
 [0.647]
 [0.621]
 [0.62 ]] [[1.125]
 [1.488]
 [1.167]
 [1.155]
 [1.306]
 [1.179]
 [1.168]] [[0.309]
 [0.661]
 [0.34 ]
 [0.336]
 [0.444]
 [0.349]
 [0.343]]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
rdn beta is 0 so we're just using the maxi policy
first move QE:  0.11853227325317703
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.15927967268869736, 0.17437849383868195, 0.13579261756649905, 0.15927967268869736, 0.192656014178137, 0.17861352903928737]
siam score:  -0.8959865
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]] [[5.398]
 [2.869]
 [2.869]
 [2.869]
 [2.869]
 [2.869]
 [2.869]] [[1.417]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.16115653208911437, 0.1723476451659077, 0.13739268703715823, 0.1577436394486739, 0.19492620663663104, 0.17643328962251476]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]] [[0.657]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[0.462]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.073]
 [0.369]
 [0.384]
 [0.407]
 [0.383]
 [0.381]] [[0.476]
 [0.623]
 [0.145]
 [0.34 ]
 [0.74 ]
 [0.644]
 [0.616]] [[0.706]
 [0.241]
 [0.516]
 [0.676]
 [0.988]
 [0.876]
 [0.854]]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.16155790687327193, 0.1727768924200015, 0.1352442862273063, 0.15813651411788635, 0.19541168782129792, 0.17687271254023607]
from probs:  [0.16258401858469462, 0.16993978670324184, 0.13368635811897342, 0.1591408930823959, 0.1966528393442817, 0.1779961041664126]
deleting a thread, now have 3 threads
Frames:  18856 train batches done:  2208 episodes:  349
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.16096373572922723, 0.16808364903400727, 0.13521762065390663, 0.16096373572922723, 0.19890537899812075, 0.175865879855511]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.192]
 [0.179]
 [0.18 ]
 [0.177]
 [0.176]
 [0.173]] [[5.078]
 [5.238]
 [5.066]
 [7.003]
 [5.898]
 [6.446]
 [6.807]] [[0.542]
 [0.646]
 [0.541]
 [1.625]
 [1.006]
 [1.311]
 [1.511]]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.16096373215000415, 0.16808364992332117, 0.13521760091613458, 0.16096373215000415, 0.19890539923149617, 0.17586588562903976]
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.047]
 [-0.047]
 [-0.046]
 [-0.046]
 [-0.044]
 [-0.045]] [[4.62 ]
 [5.369]
 [4.637]
 [4.262]
 [4.583]
 [4.887]
 [5.375]] [[0.864]
 [1.117]
 [0.869]
 [0.743]
 [0.851]
 [0.955]
 [1.119]]
line 256 mcts: sample exp_bonus 6.49121483398693
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.052]
 [0.052]
 [0.053]
 [0.052]
 [0.053]
 [0.052]] [[-0.534]
 [-0.577]
 [-0.577]
 [-0.436]
 [-0.577]
 [-0.45 ]
 [-0.416]] [[0.054]
 [0.052]
 [0.052]
 [0.053]
 [0.052]
 [0.053]
 [0.052]]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]] [[-0.503]
 [-0.503]
 [-0.503]
 [-0.503]
 [-0.503]
 [-0.503]
 [-0.503]] [[0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]]
using explorer policy with actor:  0
siam score:  -0.88117385
first move QE:  0.14328876650154046
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.1635151007233373, 0.17074787332555846, 0.13496468255667493, 0.16012473856604617, 0.19199414284458305, 0.17865346198380017]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
siam score:  -0.8872844
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[1.323]
 [1.323]
 [1.323]
 [1.323]
 [1.323]
 [1.323]
 [1.323]] [[0.136]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[2.198]
 [2.201]
 [2.201]
 [2.2  ]
 [2.2  ]
 [2.2  ]
 [2.201]]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
Printing some Q and Qe and total Qs values:  [[0.911]
 [0.94 ]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]] [[1.351]
 [2.802]
 [1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.351]] [[0.649]
 [1.242]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
from probs:  [0.16649356894533496, 0.17385810667365567, 0.13279144713776572, 0.16304144188518466, 0.18190771767902944, 0.18190771767902944]
using explorer policy with actor:  1
siam score:  -0.9025208
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.107]] [[9.467]
 [9.687]
 [9.687]
 [9.687]
 [9.687]
 [9.687]
 [9.476]] [[1.201]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.266]]
siam score:  -0.903319
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]] [[6.138]
 [6.138]
 [6.138]
 [6.138]
 [6.138]
 [6.138]
 [6.138]] [[2.229]
 [2.229]
 [2.229]
 [2.229]
 [2.229]
 [2.229]
 [2.229]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.042]
 [-0.042]
 [-0.045]
 [-0.044]
 [-0.042]
 [-0.042]] [[9.292]
 [7.796]
 [7.796]
 [8.935]
 [8.976]
 [7.796]
 [7.796]] [[1.069]
 [0.428]
 [0.428]
 [0.917]
 [0.934]
 [0.428]
 [0.428]]
Printing some Q and Qe and total Qs values:  [[0.937]
 [1.023]
 [0.926]
 [0.927]
 [0.938]
 [0.948]
 [0.959]] [[3.131]
 [5.71 ]
 [4.036]
 [3.422]
 [3.228]
 [3.056]
 [3.248]] [[1.286]
 [2.269]
 [1.554]
 [1.36 ]
 [1.32 ]
 [1.284]
 [1.365]]
siam score:  -0.896104
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.928]
 [0.995]
 [0.994]
 [0.969]
 [0.934]
 [0.93 ]
 [0.924]] [[1.914]
 [3.364]
 [2.993]
 [3.583]
 [1.82 ]
 [1.942]
 [2.144]] [[0.631]
 [1.329]
 [1.161]
 [1.409]
 [0.592]
 [0.645]
 [0.732]]
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.939]
 [0.9  ]
 [0.905]
 [0.904]
 [0.901]
 [0.927]] [[4.296]
 [4.447]
 [4.24 ]
 [4.141]
 [4.024]
 [4.036]
 [4.864]] [[1.601]
 [1.726]
 [1.564]
 [1.509]
 [1.437]
 [1.44 ]
 [1.968]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.025]
 [0.033]
 [0.035]
 [0.035]
 [0.037]
 [0.037]] [[0.861]
 [2.419]
 [0.618]
 [0.986]
 [0.891]
 [1.11 ]
 [1.08 ]] [[0.032]
 [0.025]
 [0.033]
 [0.035]
 [0.035]
 [0.037]
 [0.037]]
using explorer policy with actor:  0
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -0.3592495671928399
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.107]
 [0.151]
 [0.155]
 [0.172]
 [0.172]
 [0.151]] [[-0.293]
 [ 0.495]
 [-0.297]
 [-0.398]
 [ 0.   ]
 [ 0.   ]
 [-0.318]] [[0.149]
 [0.107]
 [0.151]
 [0.155]
 [0.172]
 [0.172]
 [0.151]]
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.18 ]
 [0.151]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.149]] [[-0.078]
 [ 0.   ]
 [-0.291]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.208]] [[0.15 ]
 [0.18 ]
 [0.151]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.149]]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.16679454428985382, 0.16679454428985382, 0.13584815947856008, 0.16679454428985382, 0.18188410382593925, 0.18188410382593925]
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.151]
 [0.152]
 [0.151]
 [0.151]
 [0.154]
 [0.151]] [[-0.443]
 [-0.286]
 [-0.324]
 [-0.286]
 [-0.286]
 [-0.285]
 [-0.238]] [[0.147]
 [0.151]
 [0.152]
 [0.151]
 [0.151]
 [0.154]
 [0.151]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.107]
 [0.15 ]
 [0.154]
 [0.154]
 [0.151]
 [0.151]] [[-0.024]
 [ 0.495]
 [-0.358]
 [-0.28 ]
 [-0.273]
 [-0.203]
 [-0.252]] [[0.15 ]
 [0.107]
 [0.15 ]
 [0.154]
 [0.154]
 [0.151]
 [0.151]]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.1651909540471853, 0.16861538589748548, 0.13733116950237048, 0.1651909540471853, 0.18386967323064074, 0.17980186327513267]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.244]] [[1.262]
 [1.262]
 [1.262]
 [1.262]
 [1.262]
 [1.262]
 [0.543]] [[0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.244]]
Printing some Q and Qe and total Qs values:  [[ 0.174]
 [-0.   ]
 [ 0.191]
 [ 0.213]
 [ 0.153]
 [ 0.152]
 [ 0.157]] [[1.445]
 [2.136]
 [0.567]
 [0.118]
 [0.242]
 [1.262]
 [2.224]] [[ 0.174]
 [-0.   ]
 [ 0.191]
 [ 0.213]
 [ 0.153]
 [ 0.152]
 [ 0.157]]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.1657358029711175, 0.16917152961670157, 0.13778412856636585, 0.16243750539135682, 0.18447613012884875, 0.18039490332560948]
first move QE:  0.1905967049880809
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.095]
 [0.283]
 [0.308]
 [0.256]
 [0.26 ]
 [0.252]] [[1.345]
 [2.137]
 [0.608]
 [0.109]
 [0.294]
 [1.274]
 [2.208]] [[0.283]
 [0.095]
 [0.283]
 [0.308]
 [0.256]
 [0.26 ]
 [0.252]]
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.233]
 [0.246]
 [0.247]
 [0.233]
 [0.233]
 [0.248]] [[0.512]
 [0.891]
 [0.138]
 [0.085]
 [0.891]
 [0.891]
 [1.545]] [[0.287]
 [0.233]
 [0.246]
 [0.247]
 [0.233]
 [0.233]
 [0.248]]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.1657358024200284, 0.16917153109962837, 0.13778411146735062, 0.1624375028876124, 0.1844761406723919, 0.18039491145298828]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
from probs:  [0.1657358024200284, 0.16917153109962837, 0.13778411146735062, 0.1624375028876124, 0.1844761406723919, 0.18039491145298828]
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]] [[0.424]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]] [[0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.072]
 [0.256]
 [0.255]
 [0.23 ]
 [0.239]
 [0.234]] [[1.471]
 [3.157]
 [0.629]
 [0.074]
 [0.224]
 [1.625]
 [2.324]] [[0.264]
 [0.072]
 [0.256]
 [0.255]
 [0.23 ]
 [0.239]
 [0.234]]
line 256 mcts: sample exp_bonus 2.8834081043985833
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.16626268128420404, 0.1697093322437091, 0.13822213110517992, 0.15977486771337102, 0.18506259560877703, 0.1809683920447589]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.514]
 [0.524]
 [0.523]
 [0.52 ]
 [0.521]
 [0.522]] [[0.673]
 [2.478]
 [0.438]
 [0.215]
 [0.287]
 [0.75 ]
 [0.775]] [[0.519]
 [0.514]
 [0.524]
 [0.523]
 [0.52 ]
 [0.521]
 [0.522]]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.1666442930420481, 0.17009886278820413, 0.13624407927587479, 0.16014157351987196, 0.18548740074835382, 0.18138379062564722]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.51 ]
 [0.51 ]
 [0.501]
 [0.503]
 [0.504]
 [0.503]] [[0.804]
 [2.243]
 [0.853]
 [0.059]
 [0.441]
 [1.087]
 [0.725]] [[0.501]
 [0.51 ]
 [0.51 ]
 [0.501]
 [0.503]
 [0.504]
 [0.503]]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]] [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.1666442930420481, 0.17009886278820413, 0.13624407927587479, 0.16014157351987196, 0.18548740074835382, 0.18138379062564722]
siam score:  -0.871979
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.456]
 [0.451]
 [0.451]] [[-0.616]
 [-0.616]
 [-0.616]
 [-0.616]
 [-1.095]
 [-0.616]
 [-0.616]] [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.456]
 [0.451]
 [0.451]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
264 120
line 256 mcts: sample exp_bonus 0.46183646125587674
using explorer policy with actor:  0
siam score:  -0.88225204
from probs:  [0.16993594521153715, 0.16993594521153715, 0.1366712411954473, 0.15717401473995551, 0.18915135188749815, 0.17713150175402467]
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.583]
 [0.543]
 [0.54 ]
 [0.556]
 [0.541]
 [0.546]] [[0.442]
 [2.055]
 [0.259]
 [0.36 ]
 [0.512]
 [0.384]
 [0.334]] [[0.532]
 [0.583]
 [0.543]
 [0.54 ]
 [0.556]
 [0.541]
 [0.546]]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.667]
 [0.61 ]
 [0.598]
 [0.598]
 [0.617]
 [0.619]] [[1.743]
 [1.706]
 [1.74 ]
 [1.715]
 [1.674]
 [1.666]
 [1.717]] [[0.635]
 [0.667]
 [0.61 ]
 [0.598]
 [0.598]
 [0.617]
 [0.619]]
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.17102633142183465, 0.17102633142183465, 0.13534325773477224, 0.158182496480773, 0.18615351479686282, 0.17826806814392263]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.1722219178949906, 0.1722219178949906, 0.13628934740393306, 0.1563542585613327, 0.18339826925174094, 0.17951428899301208]
rdn probs:  [0.1722219178949906, 0.1722219178949906, 0.13628934740393306, 0.1563542585613327, 0.18339826925174094, 0.17951428899301208]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.16782928781975026, 0.17116861322457688, 0.13820623987370773, 0.1585533839174541, 0.1859777954546775, 0.17826467970983342]
from probs:  [0.16782928781975026, 0.17116861322457688, 0.13820623987370773, 0.1585533839174541, 0.1859777954546775, 0.17826467970983342]
first move QE:  0.2449997215711897
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.548]
 [0.548]
 [0.695]
 [0.548]
 [0.548]
 [0.548]] [[1.23 ]
 [1.23 ]
 [1.23 ]
 [2.483]
 [1.23 ]
 [1.23 ]
 [1.23 ]] [[1.188]
 [1.188]
 [1.188]
 [1.934]
 [1.188]
 [1.188]
 [1.188]]
Printing some Q and Qe and total Qs values:  [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]] [[2.079]
 [2.079]
 [2.079]
 [2.079]
 [2.079]
 [2.079]
 [2.079]] [[1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.451]
 [0.411]
 [0.421]
 [0.396]
 [0.426]
 [0.438]] [[-0.176]
 [ 0.036]
 [-0.867]
 [-0.737]
 [-1.27 ]
 [-0.475]
 [-0.104]] [[0.44 ]
 [0.451]
 [0.411]
 [0.421]
 [0.396]
 [0.426]
 [0.438]]
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.1692486898773705, 0.1692486898773705, 0.13717659038542565, 0.1570029791622643, 0.18755070317440425, 0.17977234752316493]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.47 ]
 [0.481]
 [0.484]
 [0.481]
 [0.477]
 [0.477]] [[-0.621]
 [ 0.542]
 [-0.739]
 [-0.745]
 [-0.633]
 [-0.648]
 [-0.467]] [[0.476]
 [0.47 ]
 [0.481]
 [0.484]
 [0.481]
 [0.477]
 [0.477]]
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.1692486898773705, 0.1692486898773705, 0.13717659038542565, 0.1570029791622643, 0.18755070317440425, 0.17977234752316493]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.602]
 [0.594]
 [0.595]
 [0.594]
 [0.593]
 [0.592]] [[2.928]
 [2.856]
 [2.537]
 [2.458]
 [2.499]
 [2.643]
 [2.847]] [[0.595]
 [0.602]
 [0.594]
 [0.595]
 [0.594]
 [0.593]
 [0.592]]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.16830500767283937, 0.16830500767283937, 0.13907242718785026, 0.1591728461659585, 0.19014278518929373, 0.17500192611121873]
Printing some Q and Qe and total Qs values:  [[1.252]
 [1.224]
 [1.224]
 [1.253]
 [1.224]
 [1.224]
 [1.243]] [[0.314]
 [0.786]
 [0.786]
 [0.226]
 [0.786]
 [0.786]
 [0.265]] [[0.448]
 [0.834]
 [0.834]
 [0.374]
 [0.834]
 [0.834]
 [0.398]]
first move QE:  0.25605527144761175
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.183]
 [0.252]
 [0.255]
 [0.254]
 [0.254]
 [0.254]] [[-2.196]
 [-0.348]
 [-2.577]
 [-3.03 ]
 [-3.186]
 [-2.583]
 [-2.557]] [[0.253]
 [0.183]
 [0.252]
 [0.255]
 [0.254]
 [0.254]
 [0.254]]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.16973073933093744, 0.16973073933093744, 0.13807301045055342, 0.1576706521384102, 0.19175350724772636, 0.17304135150143513]
siam score:  -0.8935896
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.16973073933093744, 0.16973073933093744, 0.13807301045055342, 0.1576706521384102, 0.19175350724772636, 0.17304135150143513]
from probs:  [0.16973073933093744, 0.16973073933093744, 0.13807301045055342, 0.1576706521384102, 0.19175350724772636, 0.17304135150143513]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.16707731134040604, 0.17027317576201928, 0.13851427307223738, 0.1581745461659119, 0.1923663254592589, 0.1735943682001664]
siam score:  -0.88743013
siam score:  -0.88697815
from probs:  [0.16824318779881983, 0.17146135735665483, 0.1352813299034192, 0.15649969186847465, 0.19370870343038354, 0.174805729642248]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.789]
 [0.707]
 [0.789]
 [0.789]
 [0.789]
 [0.741]] [[6.367]
 [5.448]
 [5.474]
 [5.448]
 [5.448]
 [5.448]
 [5.953]] [[1.292]
 [1.124]
 [0.969]
 [1.124]
 [1.124]
 [1.124]
 [1.197]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
from probs:  [0.16071911529445498, 0.17301239907697108, 0.1365050714804082, 0.15791538390546012, 0.1954610042450439, 0.17638702599766176]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.16115537110235204, 0.17348202376223174, 0.13687560071167992, 0.15562963025481974, 0.19599156340201201, 0.17686581076690458]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.585]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[-1.358]
 [ 1.323]
 [-1.358]
 [-1.358]
 [-1.358]
 [-1.358]
 [-1.358]] [[0.628]
 [1.275]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]]
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.602]
 [0.602]
 [0.609]
 [0.617]
 [0.613]
 [0.607]] [[2.235]
 [4.275]
 [2.864]
 [2.328]
 [2.36 ]
 [3.287]
 [4.013]] [[0.616]
 [0.602]
 [0.602]
 [0.609]
 [0.617]
 [0.613]
 [0.607]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.1628089972885717, 0.17526214547340238, 0.13828006904572335, 0.15457725526809038, 0.1938093874508098, 0.17526214547340238]
287 142
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]] [[2.007]
 [2.007]
 [2.007]
 [2.007]
 [2.007]
 [2.007]
 [2.007]] [[1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.379]]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.16483776710827622, 0.17411560085092825, 0.1400031616355207, 0.15391054070026378, 0.19622448125895017, 0.17090844844606087]
line 256 mcts: sample exp_bonus 1.8598647151238632
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.9572549458613615
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.111]
 [0.139]
 [0.122]
 [0.116]
 [0.11 ]
 [0.115]] [[0.463]
 [0.473]
 [1.223]
 [0.798]
 [0.798]
 [0.594]
 [1.146]] [[0.164]
 [0.226]
 [0.532]
 [0.355]
 [0.344]
 [0.263]
 [0.459]]
290 150
Printing some Q and Qe and total Qs values:  [[0.989]
 [1.337]
 [0.989]
 [0.989]
 [0.989]
 [0.989]
 [0.989]] [[0.8 ]
 [1.61]
 [0.8 ]
 [0.8 ]
 [0.8 ]
 [0.8 ]
 [0.8 ]] [[1.654]
 [2.017]
 [1.654]
 [1.654]
 [1.654]
 [1.654]
 [1.654]]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.15796032264060597, 0.17477906990978218, 0.1420353252263135, 0.1554686563785058, 0.1918176700844378, 0.17793895576035468]
Printing some Q and Qe and total Qs values:  [[1.164]
 [1.159]
 [1.164]
 [1.164]
 [1.164]
 [1.164]
 [1.164]] [[1.664]
 [2.371]
 [1.664]
 [1.664]
 [1.664]
 [1.664]
 [1.664]] [[2.242]
 [2.468]
 [2.242]
 [2.242]
 [2.242]
 [2.242]
 [2.242]]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
line 256 mcts: sample exp_bonus 0.3239516627203598
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.15701841166332672, 0.16724739068438801, 0.14185252957307268, 0.15945578557068898, 0.19233733922661375, 0.18208854328190996]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.15812252381751118, 0.16842343032747273, 0.139017126270319, 0.16057703669683795, 0.19368980478586895, 0.18017007810199026]
296 180
using explorer policy with actor:  1
from probs:  [0.15603560034037203, 0.1658758825371321, 0.13950392624981517, 0.15838384950096251, 0.19697488759293036, 0.18322585377878794]
Printing some Q and Qe and total Qs values:  [[0.142]
 [1.021]
 [0.277]
 [0.265]
 [0.189]
 [0.15 ]
 [0.115]] [[1.948]
 [1.889]
 [1.478]
 [1.454]
 [1.742]
 [1.815]
 [2.419]] [[0.707]
 [2.201]
 [0.671]
 [0.635]
 [0.67 ]
 [0.644]
 [0.932]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.408]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[0.659]
 [1.422]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[0.39 ]
 [0.408]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]]
299 187
using another actor
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.174]
 [0.239]
 [0.24 ]
 [0.239]
 [0.24 ]
 [0.241]] [[-0.433]
 [ 1.359]
 [-0.525]
 [-0.797]
 [-0.905]
 [-0.788]
 [-0.696]] [[0.242]
 [0.174]
 [0.239]
 [0.24 ]
 [0.239]
 [0.24 ]
 [0.241]]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.1559585222911283, 0.16310740519151584, 0.14150189242590017, 0.16065223328633224, 0.1961158274723176, 0.1826641193328059]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.37 ]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[1.873]
 [1.653]
 [1.873]
 [1.873]
 [1.873]
 [1.873]
 [1.873]] [[0.342]
 [0.37 ]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
from probs:  [0.15820262293254903, 0.16054771694860773, 0.1435379683521285, 0.16054771694860773, 0.19187147273453492, 0.18529250208357215]
siam score:  -0.87514395
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.15677196838593188, 0.1614192750843009, 0.14431718643430289, 0.1614192750843009, 0.1929130767277125, 0.18315921828345097]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.474]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]] [[4.877]
 [3.836]
 [4.877]
 [4.877]
 [4.877]
 [4.877]
 [4.877]] [[1.51]
 [1.05]
 [1.51]
 [1.51]
 [1.51]
 [1.51]
 [1.51]]
first move QE:  0.34928891753491803
siam score:  -0.8678455
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.1575216475998354, 0.1621220523075574, 0.14709406359566557, 0.15978851368769842, 0.1898831151989832, 0.18359060761026003]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.15787953546111327, 0.16249039441698268, 0.1474282551611427, 0.15787953546111327, 0.19031454328860825, 0.1840077362110398]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.15869396316179193, 0.16332860956556486, 0.14625359649903302, 0.15869396316179193, 0.18807290816197966, 0.18495695944983853]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]] [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]]
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
probs:  [0.16033482918221156, 0.16501740334543413, 0.14586141813225093, 0.15809246972376695, 0.18686941610713945, 0.18382446350919687]
from probs:  [0.16033482918221156, 0.16501740334543413, 0.14586141813225093, 0.15809246972376695, 0.18686941610713945, 0.18382446350919687]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]] [[4.277]
 [4.277]
 [4.277]
 [4.277]
 [4.277]
 [4.277]
 [4.277]] [[1.924]
 [1.924]
 [1.924]
 [1.924]
 [1.924]
 [1.924]
 [1.924]]
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.102]
 [0.106]
 [0.106]
 [0.102]
 [0.106]
 [0.106]] [[0.556]
 [0.343]
 [0.243]
 [0.09 ]
 [0.343]
 [0.402]
 [0.354]] [[0.104]
 [0.102]
 [0.106]
 [0.106]
 [0.102]
 [0.106]
 [0.106]]
siam score:  -0.8749667
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.84 ]
 [0.716]
 [0.71 ]
 [0.72 ]
 [0.748]
 [0.768]] [[2.956]
 [4.879]
 [2.765]
 [2.718]
 [2.852]
 [3.013]
 [2.971]] [[0.754]
 [1.838]
 [0.623]
 [0.593]
 [0.673]
 [0.784]
 [0.784]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.1272941435657384
from probs:  [0.16005685039325512, 0.1646631019193714, 0.14579518701431826, 0.16005685039325512, 0.18919139129594045, 0.18023661898385968]
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
siam score:  -0.8755736
first move QE:  0.39176000344839834
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.03802660623341689
using another actor
from probs:  [0.16168236939791794, 0.16397565249322293, 0.14727584738895064, 0.15945278861081588, 0.19111283578766539, 0.17650050632142716]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
from probs:  [0.16168236530306612, 0.16397565028241895, 0.14727583145841366, 0.15945278268425087, 0.19111285587142757, 0.17650051440042294]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]] [[-1.428]
 [-1.428]
 [-1.428]
 [-1.428]
 [-1.428]
 [-1.428]
 [-1.428]] [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
using explorer policy with actor:  1
siam score:  -0.8831232
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.1592925140469849, 0.16578886994952086, 0.15328955226362895, 0.1592925140469849, 0.19191178257347485, 0.17042476711940563]
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.875]
 [0.715]
 [0.735]
 [0.652]
 [0.732]
 [0.933]] [[3.172]
 [3.149]
 [2.457]
 [2.279]
 [3.669]
 [2.444]
 [3.111]] [[1.532]
 [1.568]
 [1.017]
 [0.997]
 [1.295]
 [1.047]
 [1.67 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.2625603089666075
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.375]
 [0.375]
 [0.344]
 [0.348]
 [0.348]
 [0.351]] [[4.67 ]
 [4.01 ]
 [4.01 ]
 [4.502]
 [4.55 ]
 [4.504]
 [4.376]] [[0.238]
 [0.072]
 [0.072]
 [0.173]
 [0.197]
 [0.181]
 [0.144]]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.759]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[2.907]
 [2.595]
 [2.254]
 [2.254]
 [2.254]
 [2.254]
 [2.254]] [[1.276]
 [1.211]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
from probs:  [0.1585623672785514, 0.16275803739656206, 0.15266220617509896, 0.1606325992446751, 0.1935262849286401, 0.17185850497647245]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.15686162739561146, 0.16308700671596346, 0.15297076532039144, 0.1609572716853167, 0.19391745668342095, 0.17220587219929595]
Printing some Q and Qe and total Qs values:  [[0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]] [[2.686]
 [2.686]
 [2.686]
 [2.686]
 [2.686]
 [2.686]
 [2.686]] [[2.139]
 [2.139]
 [2.139]
 [2.139]
 [2.139]
 [2.139]
 [2.139]]
Printing some Q and Qe and total Qs values:  [[0.742]
 [0.79 ]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.689]] [[3.077]
 [3.45 ]
 [3.955]
 [3.955]
 [3.955]
 [3.955]
 [3.596]] [[1.405]
 [1.704]
 [2.056]
 [2.056]
 [2.056]
 [2.056]
 [1.668]]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.712]
 [0.767]
 [0.746]
 [0.732]
 [0.735]
 [0.808]] [[2.998]
 [2.613]
 [2.398]
 [2.451]
 [2.664]
 [2.377]
 [2.664]] [[1.154]
 [0.847]
 [0.766]
 [0.773]
 [0.919]
 [0.699]
 [1.038]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.602]
 [0.721]
 [0.713]
 [0.702]
 [0.713]
 [0.709]] [[2.792]
 [2.594]
 [2.595]
 [2.584]
 [2.74 ]
 [2.584]
 [2.759]] [[1.179]
 [0.782]
 [1.02 ]
 [0.994]
 [1.128]
 [0.994]
 [1.161]]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.15686162739561146, 0.16308700671596346, 0.15297076532039144, 0.1609572716853167, 0.19391745668342095, 0.17220587219929595]
from probs:  [0.15686162739561146, 0.16308700671596346, 0.15297076532039144, 0.1609572716853167, 0.19391745668342095, 0.17220587219929595]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.1575308275950636, 0.1637827682532159, 0.15174199365233, 0.16164394644911112, 0.19474476008406533, 0.17055570396621417]
Printing some Q and Qe and total Qs values:  [[1.031]
 [1.137]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]] [[5.398]
 [8.298]
 [5.398]
 [5.398]
 [5.398]
 [5.398]
 [5.398]] [[1.387]
 [2.022]
 [1.387]
 [1.387]
 [1.387]
 [1.387]
 [1.387]]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.15818144319530594, 0.1644592076164595, 0.15236869836090453, 0.15818144319530594, 0.19554908855931508, 0.17126011907270916]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.15937883054390453, 0.16354020421362783, 0.1535220824161458, 0.15937883054390453, 0.19397024917347958, 0.17020980310893782]
in main func line 156:  345
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.743]
 [0.656]
 [0.647]
 [0.646]
 [0.622]
 [0.677]] [[0.677]
 [1.093]
 [0.489]
 [0.492]
 [0.469]
 [0.652]
 [1.709]] [[0.489]
 [1.086]
 [0.394]
 [0.388]
 [0.364]
 [0.514]
 [1.608]]
Printing some Q and Qe and total Qs values:  [[0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]] [[1.91]
 [1.91]
 [1.91]
 [1.91]
 [1.91]
 [1.91]
 [1.91]] [[2.226]
 [2.226]
 [2.226]
 [2.226]
 [2.226]
 [2.226]
 [2.226]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[-3.12]
 [-3.12]
 [-3.12]
 [-3.12]
 [-3.12]
 [-3.12]
 [-3.12]] [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]]
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.464]
 [0.777]
 [0.643]
 [0.475]
 [0.62 ]
 [0.649]] [[1.678]
 [2.204]
 [1.884]
 [0.562]
 [1.761]
 [1.613]
 [1.453]] [[0.668]
 [1.171]
 [1.69 ]
 [0.98 ]
 [1.044]
 [1.286]
 [1.29 ]]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
Printing some Q and Qe and total Qs values:  [[0.834]
 [0.841]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]] [[4.715]
 [4.965]
 [4.715]
 [4.715]
 [4.715]
 [4.715]
 [4.715]] [[1.576]
 [1.672]
 [1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.423]
 [0.643]
 [0.643]
 [0.435]
 [0.643]
 [0.54 ]] [[ 1.243]
 [ 1.356]
 [ 1.243]
 [ 1.243]
 [ 0.171]
 [ 1.243]
 [-0.193]] [[2.649]
 [2.442]
 [2.649]
 [2.649]
 [0.835]
 [2.649]
 [0.507]]
using another actor
from probs:  [0.16056523698605948, 0.16470272843099473, 0.15288844563039647, 0.15857406922818437, 0.19194121377681847, 0.17132830594754644]
line 256 mcts: sample exp_bonus 0.3666975492435693
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
siam score:  -0.8700363
siam score:  -0.8693924
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.1610234795205487, 0.16517277909158737, 0.1533247791116336, 0.15902662910198637, 0.1896350676074123, 0.17181726556683172]
using another actor
from probs:  [0.1610234795205487, 0.16517277909158737, 0.1533247791116336, 0.15902662910198637, 0.1896350676074123, 0.17181726556683172]
from probs:  [0.16102347720159982, 0.16517277847770573, 0.153324773629066, 0.15902662596247386, 0.1896350770457928, 0.1718172676833618]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.358]
 [0.358]] [[-0.349]
 [-0.21 ]
 [-0.316]
 [-0.21 ]
 [-0.21 ]
 [-0.367]
 [-0.474]] [[0.361]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.358]
 [0.358]]
siam score:  -0.8692915
using explorer policy with actor:  1
siam score:  -0.8696889
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.16178671079489368, 0.16595568105810407, 0.15405151295712982, 0.15782361560640973, 0.1877508123606231, 0.17263166722283962]
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.254]
 [0.306]
 [0.308]
 [0.297]
 [0.293]
 [0.231]] [[ 1.414]
 [ 0.994]
 [ 0.893]
 [-0.256]
 [ 0.141]
 [ 1.26 ]
 [ 1.071]] [[0.221]
 [0.254]
 [0.306]
 [0.308]
 [0.297]
 [0.293]
 [0.231]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
using explorer policy with actor:  1
using another actor
first move QE:  0.45699907324084293
from probs:  [0.16449022869204832, 0.16449022869204832, 0.154776677325025, 0.1566257630069403, 0.19088823299537047, 0.1687288692885676]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.16479494867208982, 0.16479494867208982, 0.1550634028430111, 0.1550634028430111, 0.1912418555722921, 0.16904144139750601]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.85292846
siam score:  -0.85434294
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.16367798081526333, 0.16573324206006868, 0.15412706091293263, 0.15412706091293263, 0.19233074052225546, 0.1700039147765473]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.16515956899409703, 0.16723343499082954, 0.15552219171516365, 0.1519774092677399, 0.1885646281000781, 0.17154276693209186]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.825]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.808]] [[3.899]
 [3.736]
 [4.216]
 [4.216]
 [4.216]
 [4.216]
 [3.958]] [[1.673]
 [1.757]
 [2.035]
 [2.035]
 [2.035]
 [2.035]
 [1.923]]
using explorer policy with actor:  1
first move QE:  0.46894403980530314
using explorer policy with actor:  1
start point for exploration sampling:  10749
line 256 mcts: sample exp_bonus 4.813798439048819
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.16515956899409703, 0.16723343499082954, 0.15552219171516365, 0.1519774092677399, 0.1885646281000781, 0.17154276693209186]
Printing some Q and Qe and total Qs values:  [[1.072]
 [1.216]
 [1.072]
 [1.072]
 [1.072]
 [1.072]
 [1.072]] [[3.156]
 [3.329]
 [3.156]
 [3.156]
 [3.156]
 [3.156]
 [3.156]] [[2.697]
 [2.929]
 [2.697]
 [2.697]
 [2.697]
 [2.697]
 [2.697]]
Printing some Q and Qe and total Qs values:  [[0.848]
 [1.26 ]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]] [[2.798]
 [2.526]
 [2.798]
 [2.798]
 [2.798]
 [2.798]
 [2.798]] [[2.001]
 [2.681]
 [2.001]
 [2.001]
 [2.001]
 [2.001]
 [2.001]]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.1663228517546996, 0.168411324792226, 0.15481196547810064, 0.1530478449759399, 0.18465500397298684, 0.17275100902604706]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.732]
 [0.725]
 [0.734]
 [0.737]
 [0.74 ]
 [0.739]] [[3.981]
 [5.111]
 [4.114]
 [4.017]
 [3.997]
 [4.016]
 [3.998]] [[1.19 ]
 [1.895]
 [1.29 ]
 [1.239]
 [1.229]
 [1.243]
 [1.232]]
UNIT TEST: sample policy line 217 mcts : [0.122 0.143 0.224 0.163 0.122 0.122 0.102]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.546]
 [0.515]
 [0.517]
 [0.521]
 [0.524]
 [0.537]] [[1.712]
 [3.114]
 [1.34 ]
 [1.18 ]
 [1.188]
 [1.473]
 [2.222]] [[0.521]
 [0.546]
 [0.515]
 [0.517]
 [0.521]
 [0.524]
 [0.537]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.3319079733227674
siam score:  -0.87362754
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.1650365578941721, 0.16918078700688838, 0.15551928754810856, 0.15374710617332432, 0.18297596062866994, 0.17354030074883667]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.1650365572455534, 0.1691807880072544, 0.15551928311257734, 0.15374710103264386, 0.18297596711812203, 0.17354030348384894]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.361]
 [0.391]
 [0.373]
 [0.374]
 [0.373]
 [0.372]] [[4.074]
 [4.519]
 [3.748]
 [3.474]
 [3.502]
 [3.691]
 [3.97 ]] [[ 0.138]
 [ 0.317]
 [ 0.119]
 [-0.008]
 [ 0.004]
 [ 0.064]
 [ 0.156]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.1650365572455534, 0.1691807880072544, 0.15551928311257734, 0.15374710103264386, 0.18297596711812203, 0.17354030348384894]
first move QE:  0.48271483840911233
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.951]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]] [[4.272]
 [4.554]
 [4.272]
 [4.272]
 [4.272]
 [4.272]
 [4.272]] [[1.737]
 [1.858]
 [1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.16336637899243175, 0.16951920137756318, 0.15583036397808475, 0.15405463630229035, 0.18334198043457076, 0.17388743891505906]
Printing some Q and Qe and total Qs values:  [[1.338]
 [1.413]
 [1.338]
 [1.338]
 [1.338]
 [1.338]
 [1.338]] [[1.535]
 [1.498]
 [1.535]
 [1.535]
 [1.535]
 [1.535]
 [1.535]] [[2.62]
 [2.72]
 [2.62]
 [2.62]
 [2.62]
 [2.62]
 [2.62]]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.313]] [[0.669]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.68 ]] [[0.313]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.313]]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.203]
 [0.37 ]
 [0.368]
 [0.37 ]
 [0.369]
 [0.329]] [[3.325]
 [4.504]
 [3.966]
 [3.385]
 [3.125]
 [3.41 ]
 [4.51 ]] [[0.117]
 [0.268]
 [0.422]
 [0.225]
 [0.142]
 [0.235]
 [0.521]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.584]] [[1.179]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [1.151]] [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.584]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
from probs:  [0.1627121580823219, 0.17077932207073973, 0.15714859671099926, 0.1536479288818524, 0.18702415914330717, 0.16868783511077956]
using another actor
from probs:  [0.16300070585569368, 0.17108217585685034, 0.1556539149455513, 0.15392040248360758, 0.18735582092767258, 0.16898697993062453]
siam score:  -0.87233317
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.16140610081111773, 0.17140811221374685, 0.15595045822786543, 0.15421364317327202, 0.1877127609385807, 0.1693089246354173]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
line 256 mcts: sample exp_bonus 1.7639549238283425
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.15984270306899567, 0.1717276698159135, 0.15624119799417208, 0.15450114498049328, 0.18806271543610653, 0.169624568704319]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.15984269773035864, 0.17172767377532192, 0.15624118983794552, 0.1545011354629594, 0.18806273217502026, 0.16962457101839426]
siam score:  -0.8795641
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.16119963655803354, 0.17106455025930295, 0.15581272862677856, 0.15581272862677856, 0.18711503147055747, 0.16899532445854887]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7081364574527014
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.15885872773633616, 0.17246632320678593, 0.15708952190470463, 0.15708952190470463, 0.18615167636563823, 0.1683442288818304]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.602]
 [0.605]
 [0.608]
 [0.608]
 [0.604]
 [0.602]] [[1.894]
 [3.151]
 [2.224]
 [2.404]
 [2.395]
 [2.269]
 [2.412]] [[0.598]
 [0.602]
 [0.605]
 [0.608]
 [0.608]
 [0.604]
 [0.602]]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.15885872773633616, 0.17246632320678593, 0.15708952190470463, 0.15708952190470463, 0.18615167636563823, 0.1683442288818304]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.15885872773633616, 0.17246632320678593, 0.15708952190470463, 0.15708952190470463, 0.18615167636563823, 0.1683442288818304]
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.868]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]] [[2.639]
 [2.411]
 [2.639]
 [2.639]
 [2.639]
 [2.639]
 [2.639]] [[2.159]
 [2.022]
 [2.159]
 [2.159]
 [2.159]
 [2.159]
 [2.159]]
siam score:  -0.8729805
siam score:  -0.8714338
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.718]
 [0.755]
 [0.79 ]
 [0.794]
 [0.787]
 [0.782]] [[2.799]
 [3.854]
 [3.097]
 [2.629]
 [2.479]
 [2.699]
 [2.798]] [[0.941]
 [1.517]
 [1.092]
 [0.838]
 [0.751]
 [0.877]
 [0.932]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.49 ]
 [0.677]
 [0.626]
 [0.608]
 [0.633]
 [0.618]] [[2.186]
 [2.594]
 [2.271]
 [1.879]
 [2.036]
 [2.242]
 [2.485]] [[1.571]
 [1.936]
 [1.91 ]
 [1.628]
 [1.706]
 [1.852]
 [1.986]]
from probs:  [0.1605682881589587, 0.172072241489903, 0.158800087924758, 0.1570707492341661, 0.18536901871657888, 0.1661196144756353]
Printing some Q and Qe and total Qs values:  [[1.185]
 [1.335]
 [1.185]
 [1.185]
 [1.185]
 [1.185]
 [1.183]] [[3.805]
 [3.38 ]
 [3.805]
 [3.805]
 [3.805]
 [3.805]
 [4.011]] [[2.38 ]
 [2.39 ]
 [2.38 ]
 [2.38 ]
 [2.38 ]
 [2.38 ]
 [2.455]]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.16033593121614653, 0.1716853524135443, 0.15858986641654688, 0.15688175954737332, 0.1847808384105417, 0.1677262519958474]
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.934684162411898
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.613]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]] [[5.867]
 [5.693]
 [5.867]
 [5.867]
 [5.867]
 [5.867]
 [5.867]] [[0.953]
 [0.947]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]]
391 277
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
in main func line 156:  392
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.747]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]] [[5.991]
 [9.551]
 [5.991]
 [5.991]
 [5.991]
 [5.991]
 [5.991]] [[1.323]
 [2.082]
 [1.323]
 [1.323]
 [1.323]
 [1.323]
 [1.323]]
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.626]
 [0.651]
 [0.685]
 [0.651]
 [0.651]
 [0.651]] [[4.625]
 [5.216]
 [4.625]
 [4.818]
 [4.625]
 [4.625]
 [4.625]] [[1.383]
 [1.722]
 [1.383]
 [1.547]
 [1.383]
 [1.383]
 [1.383]]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.368]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.369]] [[5.98 ]
 [6.204]
 [5.772]
 [5.772]
 [5.772]
 [5.772]
 [7.355]] [[1.43 ]
 [1.375]
 [1.34 ]
 [1.34 ]
 [1.34 ]
 [1.34 ]
 [1.912]]
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.1592230186863505, 0.17221481815415474, 0.1592230186863505, 0.15586647475089954, 0.18518312431369896, 0.1682895454085456]
siam score:  -0.87706566
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.15975365511856213, 0.17278875185691484, 0.1580516839622488, 0.1547552345647577, 0.1858002769917624, 0.1688503975057541]
from probs:  [0.16012259747720214, 0.1731877980609547, 0.1584166957112744, 0.15511263334358275, 0.18391992711189428, 0.16924034829509188]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.199]
 [0.273]
 [0.256]
 [0.296]
 [0.256]
 [0.272]] [[4.853]
 [5.218]
 [5.04 ]
 [4.787]
 [4.75 ]
 [4.787]
 [5.052]] [[0.83 ]
 [0.958]
 [0.94 ]
 [0.756]
 [0.783]
 [0.756]
 [0.946]]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.7644830377374006
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.717]
 [0.485]
 [0.485]
 [0.485]
 [0.486]
 [0.485]] [[-0.696]
 [ 1.872]
 [-0.711]
 [-0.666]
 [-0.721]
 [-0.957]
 [-0.975]] [[0.165]
 [1.161]
 [0.165]
 [0.176]
 [0.162]
 [0.104]
 [0.099]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.836]
 [0.866]
 [0.866]
 [0.865]
 [0.864]
 [0.864]] [[2.787]
 [4.386]
 [2.547]
 [2.55 ]
 [2.592]
 [2.617]
 [2.665]] [[0.667]
 [1.363]
 [0.564]
 [0.566]
 [0.584]
 [0.594]
 [0.615]]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[6.822]
 [6.822]
 [6.822]
 [6.822]
 [6.822]
 [6.822]
 [6.822]] [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.16383578388998352, 0.16930751226463325, 0.1587096383600485, 0.15707211964909704, 0.18363187435111472, 0.16744307148512297]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.16444223119005816, 0.16806287136160505, 0.159297110946281, 0.15765353086840772, 0.18431159798513266, 0.16623265764851541]
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.61 ]
 [0.691]
 [0.509]
 [0.691]
 [0.517]
 [0.508]] [[ 1.374]
 [ 1.235]
 [ 1.374]
 [ 0.278]
 [ 1.374]
 [-0.198]
 [ 0.208]] [[1.495]
 [1.288]
 [1.495]
 [0.765]
 [1.495]
 [0.623]
 [0.74 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.837]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]] [[4.856]
 [5.689]
 [4.856]
 [4.856]
 [4.856]
 [4.856]
 [4.856]] [[2.16 ]
 [2.357]
 [2.16 ]
 [2.16 ]
 [2.16 ]
 [2.16 ]
 [2.16 ]]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.712]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[4.806]
 [5.303]
 [4.806]
 [4.806]
 [4.806]
 [4.806]
 [4.806]] [[0.694]
 [0.712]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]]
from probs:  [0.16360058251718773, 0.16900320555853518, 0.16018839954370512, 0.1569169251464486, 0.18312813567868458, 0.16716275155543878]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.16250788090436283, 0.1696625528549398, 0.1608133533371209, 0.15752911434081698, 0.18167218077197975, 0.1678149177907798]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.16250788090436283, 0.1696625528549398, 0.1608133533371209, 0.15752911434081698, 0.18167218077197975, 0.1678149177907798]
403 298
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.992]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]] [[3.929]
 [4.434]
 [3.929]
 [3.929]
 [3.929]
 [3.929]
 [3.929]] [[2.462]
 [2.878]
 [2.462]
 [2.462]
 [2.462]
 [2.462]
 [2.462]]
using explorer policy with actor:  1
first move QE:  0.5933967121079713
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.1628525615553819, 0.1699429129200231, 0.1595266750905244, 0.1595266750905244, 0.18182967844309797, 0.16632149690044826]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.1620810788691644, 0.1690604391992843, 0.1604261789970741, 0.1604261789970741, 0.18074680998460135, 0.16725931395280175]
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.33 ]
 [0.357]
 [0.573]
 [0.549]
 [0.511]
 [0.3  ]] [[2.331]
 [2.671]
 [3.068]
 [2.438]
 [3.108]
 [2.419]
 [2.332]] [[1.065]
 [1.21 ]
 [1.473]
 [1.358]
 [1.719]
 [1.275]
 [0.977]]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.759]
 [0.753]
 [0.753]
 [0.753]
 [0.797]
 [0.805]] [[2.253]
 [3.85 ]
 [3.579]
 [3.579]
 [3.579]
 [2.829]
 [3.231]] [[0.609]
 [1.583]
 [1.402]
 [1.402]
 [1.402]
 [0.978]
 [1.247]]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
siam score:  -0.8722858
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
using explorer policy with actor:  1
siam score:  -0.87247986
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.768]
 [0.725]
 [0.791]
 [0.725]
 [0.725]
 [0.725]] [[3.233]
 [4.093]
 [3.233]
 [3.676]
 [3.233]
 [3.233]
 [3.233]] [[0.981]
 [1.48 ]
 [0.981]
 [1.304]
 [0.981]
 [0.981]
 [0.981]]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
siam score:  -0.8716447
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.725]
 [0.744]
 [0.782]
 [0.74 ]
 [0.736]
 [0.782]] [[4.79 ]
 [5.275]
 [4.382]
 [4.889]
 [4.411]
 [4.459]
 [4.889]] [[1.724]
 [2.06 ]
 [1.505]
 [1.917]
 [1.514]
 [1.539]
 [1.917]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]] [[0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.1641459604072997, 0.16582160900413784, 0.15777192456834682, 0.1641459604072997, 0.1805823786661726, 0.16753216694674342]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.114761730329845
siam score:  -0.86893684
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
using explorer policy with actor:  1
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.16467155708354853, 0.16467155708354853, 0.15675609275601174, 0.16467155708354853, 0.18116062546922632, 0.16806861052411642]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.16467155708354853, 0.16467155708354853, 0.15675609275601174, 0.16467155708354853, 0.18116062546922632, 0.16806861052411642]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.334]
 [0.285]
 [0.285]
 [0.285]
 [0.289]
 [0.29 ]] [[-0.374]
 [ 0.093]
 [-0.431]
 [-0.458]
 [-0.461]
 [-0.397]
 [-0.269]] [[0.283]
 [0.334]
 [0.285]
 [0.285]
 [0.285]
 [0.289]
 [0.29 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.16357461635402445, 0.16522723113269963, 0.15728505331285295, 0.16522723113269963, 0.18177194762482962, 0.16691392044289385]
rdn beta is 0 so we're just using the maxi policy
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.429]
 [0.291]
 [0.269]
 [0.269]
 [0.251]
 [0.269]] [[ 0.094]
 [ 3.174]
 [-0.034]
 [ 2.145]
 [ 2.145]
 [-0.238]
 [ 0.081]] [[0.249]
 [0.429]
 [0.291]
 [0.269]
 [0.269]
 [0.251]
 [0.269]]
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.1644720109053815, 0.1661336921732891, 0.15664293570081678, 0.1661336921732891, 0.17878803413658553, 0.16782963491063813]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.16447201001986836, 0.1661336919582413, 0.15664293165638055, 0.1661336919582413, 0.1787880390273889, 0.16782963537987963]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.656]] [[2.807]
 [2.807]
 [2.807]
 [2.807]
 [2.807]
 [2.807]
 [2.72 ]] [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.656]]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.53 ]
 [0.553]
 [0.555]
 [0.553]
 [0.546]
 [0.544]] [[3.114]
 [3.88 ]
 [3.04 ]
 [3.095]
 [3.036]
 [3.07 ]
 [3.014]] [[0.539]
 [0.53 ]
 [0.553]
 [0.555]
 [0.553]
 [0.546]
 [0.544]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.778001392936946
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.477]
 [0.486]
 [0.477]
 [0.477]
 [0.477]
 [0.458]] [[3.787]
 [5.238]
 [4.244]
 [5.238]
 [5.238]
 [5.238]
 [4.896]] [[0.606]
 [0.477]
 [0.486]
 [0.477]
 [0.477]
 [0.477]
 [0.458]]
using explorer policy with actor:  0
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[2.887]
 [2.887]
 [2.887]
 [2.887]
 [2.887]
 [2.887]
 [2.887]] [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.433]
 [0.494]
 [0.494]
 [0.494]
 [0.364]
 [0.46 ]] [[4.698]
 [5.023]
 [5.862]
 [5.862]
 [5.862]
 [4.439]
 [6.595]] [[0.378]
 [0.433]
 [0.494]
 [0.494]
 [0.494]
 [0.364]
 [0.46 ]]
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.489]
 [0.465]
 [0.465]
 [0.465]
 [0.479]
 [0.478]] [[5.116]
 [4.934]
 [5.688]
 [5.688]
 [5.688]
 [5.247]
 [5.418]] [[0.489]
 [0.489]
 [0.465]
 [0.465]
 [0.465]
 [0.479]
 [0.478]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.515]
 [0.537]
 [0.535]
 [0.531]
 [0.527]
 [0.521]] [[3.059]
 [3.347]
 [2.868]
 [2.972]
 [2.994]
 [2.676]
 [2.98 ]] [[0.499]
 [0.515]
 [0.537]
 [0.535]
 [0.531]
 [0.527]
 [0.521]]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.478]
 [0.481]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[3.774]
 [4.564]
 [4.588]
 [4.564]
 [4.564]
 [4.564]
 [4.564]] [[0.608]
 [0.478]
 [0.481]
 [0.478]
 [0.478]
 [0.478]
 [0.478]]
first move QE:  0.6348719047119711
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.16474575491141902, 0.16641021144535595, 0.15690360393421635, 0.16474575491141902, 0.1790856881268754, 0.16810898667071422]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.1647457542314052, 0.16641021135456932, 0.15690360047803584, 0.1647457542314052, 0.17908569252328055, 0.16810898718130382]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.586]
 [0.622]
 [0.626]
 [0.621]
 [0.619]
 [0.619]] [[5.238]
 [6.205]
 [5.293]
 [5.076]
 [5.145]
 [5.164]
 [5.211]] [[0.611]
 [0.586]
 [0.622]
 [0.626]
 [0.621]
 [0.619]
 [0.619]]
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.583]
 [0.583]
 [0.55 ]
 [0.583]
 [0.583]
 [0.583]] [[4.796]
 [4.8  ]
 [4.8  ]
 [4.604]
 [4.8  ]
 [4.8  ]
 [4.8  ]] [[0.527]
 [0.583]
 [0.583]
 [0.55 ]
 [0.583]
 [0.583]
 [0.583]]
Printing some Q and Qe and total Qs values:  [[0.898]
 [1.092]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]] [[3.575]
 [5.678]
 [3.575]
 [3.575]
 [3.575]
 [3.575]
 [3.575]] [[1.491]
 [2.581]
 [1.491]
 [1.491]
 [1.491]
 [1.491]
 [1.491]]
from probs:  [0.16614168986529293, 0.16614168986529293, 0.15674173453393042, 0.1644966976823045, 0.17865793473585714, 0.16782025331732195]
start point for exploration sampling:  10749
main train batch thing paused
add a thread
from probs:  [0.16503428149063845, 0.16668465016157427, 0.157253972041941, 0.16341659338744394, 0.1792418030926077, 0.16836869982579453]
Adding thread: now have 4 threads
rdn probs:  [0.16503428149063845, 0.16668465016157427, 0.157253972041941, 0.16341659338744394, 0.1792418030926077, 0.16836869982579453]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.407]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[3.222]
 [3.462]
 [3.222]
 [3.222]
 [3.222]
 [3.222]
 [3.222]] [[0.399]
 [0.407]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
from probs:  [0.16530709823226386, 0.16530709823226386, 0.1575139219282309, 0.1636867348423164, 0.17953811583093268, 0.16864703093399228]
UNIT TEST: sample policy line 217 mcts : [0.102 0.163 0.102 0.224 0.102 0.204 0.102]
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
siam score:  -0.8594333
433 324
from probs:  [0.165623898160835, 0.165623898160835, 0.15781578154234774, 0.16400042836887233, 0.1779657599126375, 0.16897023385447243]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.16562389781891074, 0.16562389781891074, 0.15781577864013832, 0.16400042749461152, 0.1779657636176155, 0.1689702346098132]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
Printing some Q and Qe and total Qs values:  [[1.264]
 [1.391]
 [1.264]
 [1.264]
 [1.264]
 [1.264]
 [1.264]] [[0.914]
 [1.259]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]] [[1.467]
 [1.904]
 [1.467]
 [1.467]
 [1.467]
 [1.467]
 [1.467]]
Printing some Q and Qe and total Qs values:  [[0.945]
 [0.849]
 [0.824]
 [0.831]
 [0.83 ]
 [0.913]
 [0.968]] [[2.115]
 [1.859]
 [1.688]
 [1.81 ]
 [1.843]
 [1.889]
 [1.972]] [[1.384]
 [1.069]
 [0.898]
 [1.008]
 [1.035]
 [1.159]
 [1.286]]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.16809203696798933, 0.1664443730337587, 0.16016755804621335, 0.15867237334399145, 0.17685067156574172, 0.16977298704230545]
in main func line 156:  438
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
line 256 mcts: sample exp_bonus -1.7278087725297036
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.16783131009818678, 0.16620249189399597, 0.15999454326670268, 0.15999454326670268, 0.17648440680795047, 0.1694927046664614]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.16783131009818678, 0.16620249189399597, 0.15999454326670268, 0.15999454326670268, 0.17648440680795047, 0.1694927046664614]
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.697]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]] [[2.325]
 [2.282]
 [2.325]
 [2.325]
 [2.325]
 [2.325]
 [2.325]] [[2.719]
 [2.705]
 [2.719]
 [2.719]
 [2.719]
 [2.719]
 [2.719]]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[5.097]
 [5.097]
 [5.097]
 [5.097]
 [5.097]
 [5.097]
 [5.097]] [[1.837]
 [1.837]
 [1.837]
 [1.837]
 [1.837]
 [1.837]
 [1.837]]
440 336
from probs:  [0.16807997844138056, 0.16644874688819092, 0.16023160021376998, 0.15874994376332385, 0.17674589606770064, 0.16974383462563403]
Printing some Q and Qe and total Qs values:  [[-0.119]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]] [[3.974]
 [4.046]
 [4.046]
 [4.046]
 [4.046]
 [4.046]
 [4.046]] [[0.314]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.732]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]] [[3.212]
 [3.42 ]
 [3.212]
 [3.212]
 [3.212]
 [3.212]
 [3.212]] [[1.366]
 [1.493]
 [1.366]
 [1.366]
 [1.366]
 [1.366]
 [1.366]]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.16887491797191048, 0.16250644008897608, 0.1609894205932771, 0.15950075660217058, 0.17758182132748493, 0.17054664341618078]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
using explorer policy with actor:  1
siam score:  -0.8654132
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.16806635427435768, 0.16331333764387954, 0.1602927289441364, 0.1602927289441364, 0.1766413842777976, 0.17139346591569238]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[5.592]
 [5.592]
 [5.592]
 [5.592]
 [5.592]
 [5.592]
 [5.592]] [[1.439]
 [1.439]
 [1.439]
 [1.439]
 [1.439]
 [1.439]
 [1.439]]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]] [[5.168]
 [4.627]
 [4.627]
 [4.627]
 [4.627]
 [4.627]
 [4.627]] [[0.171]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.16696608390438716, 0.1638184634431555, 0.15931561750556023, 0.16078851103654934, 0.17718773797952087, 0.171923586130827]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.1672479340580585, 0.16409500019850096, 0.1595845531494117, 0.16105993302528201, 0.17748684298342574, 0.17052573658532127]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.1675251830298059, 0.16436702251273852, 0.15984909843971157, 0.16132692407107552, 0.17778106512131336, 0.1691507068253553]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.16488776454045767, 0.16488776454045767, 0.1603555269335667, 0.16183803456198897, 0.1783443050640103, 0.16968660435951874]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.155]
 [0.128]
 [0.141]
 [0.13 ]
 [0.106]
 [0.109]] [[3.169]
 [3.814]
 [2.847]
 [2.779]
 [2.764]
 [3.211]
 [3.431]] [[0.274]
 [0.611]
 [0.09 ]
 [0.064]
 [0.049]
 [0.261]
 [0.377]]
Printing some Q and Qe and total Qs values:  [[0.9  ]
 [0.926]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]] [[5.095]
 [5.067]
 [5.095]
 [5.095]
 [5.095]
 [5.095]
 [5.095]] [[2.443]
 [2.453]
 [2.443]
 [2.443]
 [2.443]
 [2.443]
 [2.443]]
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.813]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]] [[2.908]
 [3.159]
 [2.908]
 [2.908]
 [2.908]
 [2.908]
 [2.908]] [[2.292]
 [2.466]
 [2.292]
 [2.292]
 [2.292]
 [2.292]
 [2.292]]
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]] [[4.547]
 [4.048]
 [4.048]
 [4.048]
 [4.048]
 [4.048]
 [4.048]] [[0.628]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.16378800757197212, 0.16687448349314735, 0.1608147968222162, 0.15794872880218117, 0.18049316075359062, 0.1700808225568925]
siam score:  -0.8594506
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.068]
 [0.132]
 [0.132]
 [0.13 ]
 [0.13 ]
 [0.13 ]] [[-1.579]
 [ 1.73 ]
 [-1.514]
 [-1.716]
 [-1.825]
 [-1.606]
 [-1.558]] [[0.131]
 [0.068]
 [0.132]
 [0.132]
 [0.13 ]
 [0.13 ]
 [0.13 ]]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
Printing some Q and Qe and total Qs values:  [[1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]] [[0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[2.832]
 [2.832]
 [2.832]
 [2.832]
 [2.832]
 [2.832]
 [2.832]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.008872651430336
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.16375115627433678, 0.1668075238268451, 0.15937311626668965, 0.15796588912137452, 0.18212087053322715, 0.16998144397752685]
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]] [[-1.993]
 [-1.993]
 [-1.993]
 [-1.993]
 [-1.993]
 [-1.993]
 [-1.993]] [[0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.621815156806696
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.451]
 [0.54 ]
 [0.541]
 [0.546]
 [0.537]
 [0.541]] [[0.706]
 [1.452]
 [0.731]
 [0.679]
 [0.606]
 [0.59 ]
 [0.58 ]] [[0.941]
 [1.102]
 [1.039]
 [1.025]
 [1.01 ]
 [0.986]
 [0.992]]
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.16431680847865074, 0.16738373478860222, 0.1599236437643959, 0.15851155510624257, 0.18090324668553126, 0.16896101117657727]
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.704]
 [0.708]
 [0.716]
 [0.708]
 [0.701]
 [0.694]] [[4.464]
 [4.617]
 [4.419]
 [4.366]
 [4.419]
 [4.492]
 [4.453]] [[1.543]
 [1.577]
 [1.453]
 [1.435]
 [1.453]
 [1.488]
 [1.448]]
siam score:  -0.86332726
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.16329557204216602, 0.16786691553564795, 0.16038528729556492, 0.15757802147804706, 0.18142545814216576, 0.16944874550640834]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.025]] [[3.666]
 [3.666]
 [3.666]
 [3.666]
 [3.666]
 [3.666]
 [4.026]] [[0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.443]]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
using another actor
siam score:  -0.8726464
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
line 256 mcts: sample exp_bonus -0.7309612350055309
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.714]
 [0.727]
 [0.721]
 [0.724]
 [0.724]
 [0.729]] [[0.837]
 [3.485]
 [0.708]
 [0.565]
 [0.522]
 [0.741]
 [1.084]] [[0.759]
 [1.521]
 [0.696]
 [0.642]
 [0.634]
 [0.7  ]
 [0.814]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.7595786705069085
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.16514303684588919, 0.16816770473492917, 0.15941208716139238, 0.15941208716139238, 0.1796973793614677, 0.16816770473492917]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.165143036352246, 0.16816770522125282, 0.15941208481096994, 0.15941208481096994, 0.17969738358330847, 0.16816770522125282]
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.703]
 [0.728]
 [0.753]
 [0.783]
 [0.695]
 [0.709]] [[3.93 ]
 [4.814]
 [3.843]
 [3.175]
 [2.63 ]
 [4.4  ]
 [5.235]] [[0.605]
 [0.868]
 [0.593]
 [0.421]
 [0.299]
 [0.715]
 [1.021]]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
siam score:  -0.86583877
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.665]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.716]] [[1.661]
 [1.738]
 [2.497]
 [2.497]
 [2.497]
 [2.497]
 [2.154]] [[0.749]
 [0.665]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.716]]
line 256 mcts: sample exp_bonus 5.213427090611042
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.775]
 [0.778]
 [0.773]
 [0.778]
 [0.778]
 [0.77 ]] [[3.933]
 [5.666]
 [2.899]
 [2.347]
 [3.933]
 [3.933]
 [4.408]] [[1.08 ]
 [1.654]
 [0.736]
 [0.542]
 [1.08 ]
 [1.08 ]
 [1.224]]
Printing some Q and Qe and total Qs values:  [[1.248]
 [1.317]
 [1.248]
 [1.248]
 [1.248]
 [1.248]
 [1.248]] [[1.48 ]
 [1.823]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]] [[1.334]
 [1.639]
 [1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
from probs:  [0.16413982024300922, 0.1700954687601526, 0.15994262829234998, 0.15859139555609786, 0.18164183635098693, 0.16558885079740346]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
using another actor
from probs:  [0.16413981942016964, 0.170095469876704, 0.15994262610274135, 0.15859139292647526, 0.18164184122748506, 0.16558885044642468]
start point for exploration sampling:  10749
actor:  1 policy actor:  1  step number:  121 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.859]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]] [[2.88 ]
 [3.976]
 [2.88 ]
 [2.88 ]
 [2.88 ]
 [2.88 ]
 [2.88 ]] [[2.293]
 [2.55 ]
 [2.293]
 [2.293]
 [2.293]
 [2.293]
 [2.293]]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.882]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.852]] [[2.722]
 [2.924]
 [2.842]
 [2.842]
 [2.842]
 [2.842]
 [3.141]] [[1.174]
 [1.368]
 [1.264]
 [1.264]
 [1.264]
 [1.264]
 [1.49 ]]
maxi score, test score, baseline:  -0.9961121212121212 -1.0 -0.9961121212121212
maxi score, test score, baseline:  -0.9961121212121212 -1.0 -0.9961121212121212
probs:  [0.3037619824001089, 0.1416827081941272, 0.13322579096825513, 0.13210026718605727, 0.15130037876472682, 0.1379288724867248]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
from probs:  [0.3019271084776204, 0.14205610125701268, 0.13357689388089186, 0.13244840352192097, 0.15169912141024813, 0.138292371452306]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.30236790686921516, 0.1422634960194971, 0.13377190942710704, 0.13264177152897452, 0.15046064479661705, 0.13849427135858924]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.30236758499005995, 0.1422635625218695, 0.13377196933203703, 0.13264183055585665, 0.15046071766767175, 0.1384943349325051]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.30270388412624283, 0.1424217908358866, 0.13392075313018564, 0.13167713785192559, 0.15062806301781154, 0.13864837103794792]
Printing some Q and Qe and total Qs values:  [[0.967]
 [0.952]
 [0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]] [[2.238]
 [2.146]
 [2.238]
 [2.238]
 [2.238]
 [2.238]
 [2.238]] [[2.219]
 [2.159]
 [2.219]
 [2.219]
 [2.219]
 [2.219]
 [2.219]]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
siam score:  -0.85536546
using another actor
maxi score, test score, baseline:  -0.9961962962962964 -1.0 -0.9961962962962964
probs:  [0.29868291303438593, 0.1440855567734682, 0.13442413322743177, 0.13442413322743177, 0.14808105202498328, 0.14030221171229906]
486 383
maxi score, test score, baseline:  -0.9961962962962964 -1.0 -0.9961962962962964
probs:  [0.29983945331963613, 0.1433546663619048, 0.13494464149423532, 0.13494464149423532, 0.1472926938793056, 0.139623903450683]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.425]
 [0.372]
 [0.372]
 [0.372]
 [0.386]
 [0.384]] [[ 0.013]
 [ 1.19 ]
 [-0.212]
 [-0.212]
 [-0.212]
 [ 0.02 ]
 [ 0.158]] [[0.388]
 [0.425]
 [0.372]
 [0.372]
 [0.372]
 [0.386]
 [0.384]]
maxi score, test score, baseline:  -0.9961962962962964 -1.0 -0.9961962962962964
Printing some Q and Qe and total Qs values:  [[1.076]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[1.234]
 [1.409]
 [1.409]
 [1.409]
 [1.409]
 [1.409]
 [1.409]] [[1.053]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.438]
 [0.592]
 [0.539]
 [0.53 ]
 [0.526]
 [0.476]] [[3.524]
 [3.772]
 [1.826]
 [2.793]
 [2.5  ]
 [2.149]
 [3.162]] [[0.459]
 [0.438]
 [0.592]
 [0.539]
 [0.53 ]
 [0.526]
 [0.476]]
488 388
from probs:  [0.299478365601388, 0.14310419583699371, 0.13367038032554002, 0.1347802409739463, 0.148345204454468, 0.14062161280766378]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]] [[3.399]
 [3.399]
 [3.399]
 [3.399]
 [3.399]
 [3.399]
 [3.399]] [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]]
siam score:  -0.85604566
using another actor
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
siam score:  -0.86183715
491 394
siam score:  -0.85894895
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.54 ]
 [0.54 ]
 [0.507]
 [0.54 ]
 [0.507]
 [0.503]] [[0.837]
 [0.562]
 [0.562]
 [0.684]
 [0.562]
 [0.663]
 [0.676]] [[0.334]
 [0.323]
 [0.323]
 [0.297]
 [0.323]
 [0.29 ]
 [0.287]]
Printing some Q and Qe and total Qs values:  [[0.968]
 [0.985]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.957]] [[4.407]
 [4.536]
 [4.634]
 [4.634]
 [4.634]
 [4.634]
 [4.567]] [[2.4  ]
 [2.463]
 [2.437]
 [2.437]
 [2.437]
 [2.437]
 [2.426]]
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.779]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.693]] [[1.476]
 [2.863]
 [2.411]
 [2.411]
 [2.411]
 [2.411]
 [1.446]] [[0.453]
 [1.029]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.383]]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.29651347068451506, 0.14269135280857628, 0.13563780639033787, 0.13453022472135825, 0.14916249631154735, 0.14146464908366527]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.322]] [[-2.201]
 [-1.302]
 [-1.302]
 [-1.302]
 [-1.302]
 [-1.302]
 [-1.453]] [[0.325]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.322]]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[1.713]
 [1.713]
 [1.713]
 [1.713]
 [1.713]
 [1.713]
 [1.713]] [[0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.29382648477635553, 0.143685100036254, 0.13546713027188684, 0.1343701179145826, 0.15020131459384523, 0.1424498524070758]
first move QE:  0.7494411591413577
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.605]
 [0.596]
 [0.597]
 [0.597]
 [0.594]
 [0.593]] [[3.892]
 [4.526]
 [3.742]
 [3.668]
 [3.647]
 [3.789]
 [3.676]] [[0.592]
 [0.605]
 [0.596]
 [0.597]
 [0.597]
 [0.594]
 [0.593]]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.61 ]
 [0.597]
 [0.596]
 [0.613]
 [0.594]
 [0.601]] [[4.023]
 [4.614]
 [3.855]
 [3.767]
 [3.675]
 [3.789]
 [3.888]] [[0.594]
 [0.61 ]
 [0.597]
 [0.596]
 [0.613]
 [0.594]
 [0.601]]
using another actor
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
from probs:  [0.2929036479167216, 0.14437327731736363, 0.1350136812512508, 0.1350136812512508, 0.149563598772208, 0.14313211349120517]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.29326734574912633, 0.14331004515920956, 0.1351815181809471, 0.1351815181809471, 0.14974952757056037, 0.14331004515920956]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.85469025
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.29326734574912633, 0.14331004515920956, 0.1351815181809471, 0.1351815181809471, 0.14974952757056037, 0.14331004515920956]
maxi score, test score, baseline:  -0.9962768115942029 -1.0 -0.9962768115942029
501 399
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.2932667614584756, 0.14331016484246348, 0.13518162616879617, 0.13518162616879617, 0.14974965651900513, 0.14331016484246348]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.2932667614584756, 0.14331016484246348, 0.13518162616879617, 0.13518162616879617, 0.14974965651900513, 0.14331016484246348]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.543]
 [0.535]
 [0.289]
 [0.531]
 [0.674]
 [0.704]] [[1.542]
 [1.467]
 [0.91 ]
 [0.67 ]
 [0.839]
 [1.315]
 [1.421]] [[1.88 ]
 [1.748]
 [0.974]
 [0.307]
 [0.87 ]
 [1.719]
 [1.905]]
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.2919976133978262, 0.14381604439453272, 0.13565881228165447, 0.13565881228165447, 0.15027826723720245, 0.1425904504071298]
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.2927483997891708, 0.1429570800122681, 0.13600761920833393, 0.13600761920833393, 0.14932220176962518, 0.1429570800122681]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.29413374241281165, 0.14242032311589317, 0.13555392448598375, 0.13555392448598375, 0.14870430400487275, 0.14363378149445474]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.712]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[4.588]
 [4.478]
 [4.588]
 [4.588]
 [4.588]
 [4.588]
 [4.588]] [[2.064]
 [2.068]
 [2.064]
 [2.064]
 [2.064]
 [2.064]
 [2.064]]
siam score:  -0.84768766
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0130511380122016
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.419611938367991
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.751]
 [0.735]
 [0.731]
 [0.728]
 [0.717]
 [0.732]] [[1.256]
 [3.779]
 [1.135]
 [0.936]
 [1.11 ]
 [1.284]
 [1.863]] [[0.645]
 [1.549]
 [0.633]
 [0.559]
 [0.612]
 [0.647]
 [0.871]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.2887890179285281, 0.14421094252470298, 0.1372582141745237, 0.1361648415710681, 0.150573930523751, 0.1430030532774261]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1927184508123574
using another actor
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.856]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.636]] [[-0.002]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[0.829]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.592]] [[0.011]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]]
siam score:  -0.8520605
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.729]
 [0.734]
 [0.66 ]
 [0.66 ]
 [0.719]
 [0.66 ]] [[1.954]
 [2.563]
 [1.887]
 [1.954]
 [1.954]
 [1.478]
 [1.954]] [[1.016]
 [1.764]
 [1.098]
 [1.016]
 [1.016]
 [0.66 ]
 [1.016]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.2863661201220042, 0.14395602377828634, 0.13707223608323804, 0.13707223608323804, 0.15157736015494697, 0.14395602377828634]
using another actor
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.428]
 [0.414]
 [0.428]
 [0.414]
 [0.409]
 [0.412]] [[4.752]
 [4.265]
 [4.655]
 [4.265]
 [4.6  ]
 [4.564]
 [4.606]] [[0.12 ]
 [0.019]
 [0.122]
 [0.019]
 [0.102]
 [0.081]
 [0.1  ]]
UNIT TEST: sample policy line 217 mcts : [0.224 0.041 0.143 0.061 0.143 0.143 0.245]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[ 0.055]
 [-0.003]
 [ 0.04 ]
 [ 0.042]
 [ 0.036]
 [ 0.021]
 [ 0.028]] [[2.533]
 [2.427]
 [2.635]
 [2.251]
 [2.258]
 [2.815]
 [2.577]] [[-0.607]
 [-0.758]
 [-0.603]
 [-0.728]
 [-0.736]
 [-0.58 ]
 [-0.646]]
using explorer policy with actor:  1
from probs:  [0.2876766377748453, 0.14461540528613878, 0.13554132858711898, 0.1366120696376033, 0.15093915342815478, 0.14461540528613878]
maxi score, test score, baseline:  -0.9963788732394366 -1.0 -0.9963788732394366
probs:  [0.28615317176163013, 0.1449246983216115, 0.135831214639492, 0.13690424571398208, 0.1512619712416727, 0.1449246983216115]
siam score:  -0.8477208
518 417
from probs:  [0.28532782135604673, 0.14557863771055873, 0.13644412177354162, 0.13644412177354162, 0.1506266596757524, 0.14557863771055873]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]]
line 256 mcts: sample exp_bonus 1.1424754826506265
first move QE:  0.7742604253891545
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9963788732394366 -1.0 -0.9963788732394366
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]] [[4.133]
 [4.212]
 [4.212]
 [4.212]
 [4.212]
 [4.212]
 [4.212]] [[0.233]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]]
siam score:  -0.84889525
maxi score, test score, baseline:  -0.9964034965034965 -1.0 -0.9964034965034965
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.306]
 [0.31 ]
 [0.254]
 [0.258]
 [0.275]
 [0.29 ]] [[1.898]
 [1.175]
 [1.795]
 [1.311]
 [2.022]
 [1.895]
 [1.241]] [[0.949]
 [0.462]
 [0.883]
 [0.449]
 [0.931]
 [0.879]
 [0.474]]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.28398455401708617, 0.14580636708689926, 0.13780195241453938, 0.1356757797671938, 0.152125641828236, 0.1446057048860453]
Printing some Q and Qe and total Qs values:  [[1.076]
 [0.484]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.624]] [[3.025]
 [2.301]
 [2.607]
 [2.607]
 [2.607]
 [2.607]
 [2.144]] [[1.972]
 [0.939]
 [1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.049]]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
using explorer policy with actor:  0
siam score:  -0.8459214
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.2809635896103992, 0.14498732967836936, 0.13930313697804952, 0.1361041262025207, 0.1524608073852247, 0.14618101014543655]
UNIT TEST: sample policy line 217 mcts : [0.306 0.163 0.082 0.102 0.082 0.122 0.143]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.761]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]] [[4.695]
 [5.095]
 [4.695]
 [4.695]
 [4.695]
 [4.695]
 [4.695]] [[2.246]
 [2.407]
 [2.246]
 [2.246]
 [2.246]
 [2.246]
 [2.246]]
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
probs:  [0.2822104363198463, 0.1444516115994996, 0.1399215090404644, 0.13464791636677845, 0.1531375908539975, 0.14563093581941372]
siam score:  -0.8490183
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.81 ]
 [0.847]
 [0.847]
 [0.847]
 [0.898]
 [0.815]] [[3.412]
 [4.625]
 [4.321]
 [4.321]
 [4.321]
 [2.628]
 [4.658]] [[1.3  ]
 [1.971]
 [1.838]
 [1.838]
 [1.838]
 [0.975]
 [1.993]]
line 256 mcts: sample exp_bonus 0.24498563179652438
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.02 ]
 [-0.01 ]
 [-0.007]
 [-0.008]
 [-0.009]
 [-0.006]] [[3.792]
 [4.763]
 [3.713]
 [3.288]
 [3.334]
 [3.598]
 [3.556]] [[-0.008]
 [ 0.302]
 [-0.028]
 [-0.166]
 [-0.151]
 [-0.065]
 [-0.073]]
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.999]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]] [[2.843]
 [3.542]
 [2.843]
 [2.843]
 [2.843]
 [2.843]
 [2.843]] [[1.931]
 [2.408]
 [1.931]
 [1.931]
 [1.931]
 [1.931]
 [1.931]]
using explorer policy with actor:  1
first move QE:  0.7968310586452774
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.416]
 [0.375]
 [0.376]
 [0.378]
 [0.376]
 [0.374]] [[0.584]
 [2.104]
 [0.465]
 [0.131]
 [0.067]
 [0.1  ]
 [0.175]] [[0.386]
 [0.416]
 [0.375]
 [0.376]
 [0.378]
 [0.376]
 [0.374]]
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]] [[2.856]
 [2.856]
 [2.856]
 [2.856]
 [2.856]
 [2.856]
 [2.856]] [[0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]]
from probs:  [0.2802733714956965, 0.14304125964494133, 0.14192012571085097, 0.13555004653988306, 0.152700259692489, 0.14651493691613932]
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.2788784722356006, 0.14331848706404277, 0.1421951799347546, 0.13581275306379917, 0.15299621002406383, 0.14679889767773888]
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.2774977620706074, 0.1435928947530411, 0.14246743686065394, 0.13607278974481776, 0.1532891473643767, 0.147079969206503]
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.71 ]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.728]] [[4.193]
 [4.647]
 [4.193]
 [4.193]
 [4.193]
 [4.193]
 [4.567]] [[1.615]
 [1.872]
 [1.615]
 [1.615]
 [1.615]
 [1.615]
 [1.837]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.2774977620706074, 0.1435928947530411, 0.14246743686065394, 0.13607278974481776, 0.1532891473643767, 0.147079969206503]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.7407503784224483
maxi score, test score, baseline:  -0.9964753424657534 -1.0 -0.9964753424657534
probs:  [0.27749750861550443, 0.14359294486085578, 0.14246748624523387, 0.1360728350201094, 0.1532892037031369, 0.14708002155515973]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.2774972568960992, 0.1435929946255278, 0.14246753529162776, 0.1360728799853776, 0.1532892596560512, 0.14708007354531644]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.27679352384224204, 0.14421023002334388, 0.1419674357221502, 0.1366577901227334, 0.1526587221155522, 0.14771229817397824]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.27578834170742317, 0.1446642637633683, 0.1424144082135759, 0.13708804564304466, 0.1518675827925356, 0.14817735788005235]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.27578834170742317, 0.1446642637633683, 0.1424144082135759, 0.13708804564304466, 0.1518675827925356, 0.14817735788005235]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.494]
 [0.493]
 [0.524]
 [0.522]
 [0.512]
 [0.509]] [[3.245]
 [3.484]
 [3.311]
 [3.318]
 [3.317]
 [3.375]
 [3.379]] [[1.025]
 [1.289]
 [1.115]
 [1.153]
 [1.149]
 [1.198]
 [1.199]]
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.658]
 [0.721]
 [0.721]
 [0.586]
 [0.721]
 [0.721]] [[2.008]
 [2.686]
 [2.008]
 [2.008]
 [1.378]
 [2.008]
 [2.008]] [[1.508]
 [2.061]
 [1.508]
 [1.508]
 [0.608]
 [1.508]
 [1.508]]
Printing some Q and Qe and total Qs values:  [[1.04 ]
 [1.038]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.036]] [[4.606]
 [4.644]
 [4.606]
 [4.606]
 [4.606]
 [4.606]
 [4.776]] [[2.692]
 [2.701]
 [2.692]
 [2.692]
 [2.692]
 [2.692]
 [2.742]]
Printing some Q and Qe and total Qs values:  [[0.778]
 [1.017]
 [0.81 ]
 [0.861]
 [0.882]
 [0.814]
 [0.789]] [[2.534]
 [3.496]
 [2.282]
 [1.83 ]
 [2.265]
 [2.394]
 [2.329]] [[0.993]
 [1.801]
 [0.888]
 [0.691]
 [0.959]
 [0.955]
 [0.891]]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.2781069681143235, 0.14473710654792446, 0.14361172426240798, 0.13824058153607938, 0.14942312458925538, 0.1458804949500092]
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.019]
 [-0.019]
 [-0.021]
 [-0.02 ]
 [-0.018]
 [-0.019]] [[4.142]
 [4.176]
 [4.176]
 [3.896]
 [3.961]
 [4.168]
 [3.99 ]] [[1.146]
 [1.18 ]
 [1.18 ]
 [0.914]
 [0.977]
 [1.174]
 [1.005]]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.2767567834123524, 0.1450078140051749, 0.14388032687326338, 0.13849913828914023, 0.14970259648887208, 0.146153340931197]
using another actor
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
using explorer policy with actor:  0
siam score:  -0.8383403
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.277702070190531, 0.1455032820177524, 0.14216188277367528, 0.1389723653134198, 0.1490076763469065, 0.14665272335771495]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
line 256 mcts: sample exp_bonus 1.4815929782472816
maxi score, test score, baseline:  -0.9965216216216216 -1.0 -0.9965216216216216
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.682]] [[3.83 ]
 [3.813]
 [3.813]
 [3.813]
 [3.813]
 [3.813]
 [3.925]] [[2.032]
 [1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.986]
 [2.108]]
maxi score, test score, baseline:  -0.9965216216216216 -1.0 -0.9965216216216216
probs:  [0.27792014746017374, 0.1454554746276021, 0.1421411955272448, 0.1389764327772795, 0.1477533748038499, 0.1477533748038499]
using explorer policy with actor:  1
siam score:  -0.84353065
maxi score, test score, baseline:  -0.9965329966329967 -1.0 -0.9965329966329967
from probs:  [0.2779199031926845, 0.14545552413456145, 0.1421412429507552, 0.13897647821133113, 0.14775342575533382, 0.14775342575533382]
UNIT TEST: sample policy line 217 mcts : [0.122 0.163 0.102 0.102 0.082 0.265 0.163]
line 256 mcts: sample exp_bonus 0.6302361303409141
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.6  ]
 [0.591]
 [0.598]
 [0.599]
 [0.594]
 [0.598]] [[0.314]
 [1.658]
 [0.27 ]
 [0.359]
 [0.331]
 [0.425]
 [0.527]] [[0.585]
 [0.6  ]
 [0.591]
 [0.598]
 [0.599]
 [0.594]
 [0.598]]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.559]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.582]] [[3.517]
 [3.802]
 [3.037]
 [3.037]
 [3.037]
 [3.037]
 [2.98 ]] [[0.571]
 [0.559]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.582]]
554 454
siam score:  -0.84648097
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.48 ]
 [0.587]
 [0.591]
 [0.593]
 [0.591]
 [0.591]] [[2.134]
 [5.171]
 [2.564]
 [2.675]
 [2.673]
 [2.678]
 [2.816]] [[-0.044]
 [ 0.765]
 [ 0.109]
 [ 0.154]
 [ 0.156]
 [ 0.155]
 [ 0.202]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]]
maxi score, test score, baseline:  -0.9965996699669967 -1.0 -0.9965996699669967
maxi score, test score, baseline:  -0.9965996699669967 -1.0 -0.9965996699669967
probs:  [0.2751272767056105, 0.1466940082710278, 0.1444658922079706, 0.13918443191035354, 0.14783438263400983, 0.1466940082710278]
siam score:  -0.8405583
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.132]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.126]] [[4.554]
 [4.682]
 [4.554]
 [4.554]
 [4.554]
 [4.554]
 [4.94 ]] [[0.753]
 [0.824]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.977]]
maxi score, test score, baseline:  -0.9965996699669967 -1.0 -0.9965996699669967
probs:  [0.276042817238896, 0.14385446348987113, 0.14494663109208777, 0.13964759568874036, 0.1483263308375298, 0.14718216165287495]
maxi score, test score, baseline:  -0.9965996699669967 -1.0 -0.9965996699669967
probs:  [0.276042817238896, 0.14385446348987113, 0.14494663109208777, 0.13964759568874036, 0.1483263308375298, 0.14718216165287495]
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.2760425853405451, 0.14385450930539198, 0.14494667756520022, 0.1396476389713158, 0.14832637934555168, 0.1471822094719952]
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.2740780419346068, 0.14360313194289137, 0.14578343480358608, 0.1404538055885546, 0.14918264713758247, 0.1468989385927787]
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.9  ]
 [0.912]
 [0.932]
 [0.893]
 [0.905]
 [0.913]] [[3.978]
 [3.697]
 [3.718]
 [3.651]
 [3.655]
 [3.604]
 [4.157]] [[1.633]
 [1.43 ]
 [1.454]
 [1.421]
 [1.396]
 [1.368]
 [1.767]]
Printing some Q and Qe and total Qs values:  [[0.87 ]
 [0.845]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]] [[2.88 ]
 [2.932]
 [2.88 ]
 [2.88 ]
 [2.88 ]
 [2.88 ]
 [2.88 ]] [[1.874]
 [1.857]
 [1.874]
 [1.874]
 [1.874]
 [1.874]
 [1.874]]
siam score:  -0.83197236
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.27437040048339745, 0.1426896149614991, 0.14593894172831468, 0.14060362740749402, 0.14934177999592468, 0.14705563542337016]
using explorer policy with actor:  1
start point for exploration sampling:  10749
UNIT TEST: sample policy line 217 mcts : [0.694 0.061 0.02  0.02  0.02  0.102 0.082]
from probs:  [0.2731007267089966, 0.14293928678045004, 0.1461942990719232, 0.14084964925999816, 0.14960309147165485, 0.14731294670697723]
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.2734065723245357, 0.14309936454617736, 0.14635802212724702, 0.14100738683981168, 0.14977063203498134, 0.14635802212724702]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.2742786448421561, 0.1424986461041764, 0.14682485365856424, 0.14043097337597635, 0.1502483486130207, 0.14571853340610627]
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
siam score:  -0.8374056
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.2742786448421561, 0.1424986461041764, 0.14682485365856424, 0.14043097337597635, 0.1502483486130207, 0.14571853340610627]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5676181212301734
first move QE:  0.8441286660733762
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.27268465772955036, 0.14347488735959332, 0.14561981452037773, 0.1413930462917732, 0.15011075576327002, 0.1467168383354354]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.27296256819566694, 0.1436211118415797, 0.14576822503553516, 0.14051798474374627, 0.1502637432853794, 0.14686636689809254]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.273249398029678, 0.14272122649845462, 0.14592139869618542, 0.14066564143713847, 0.1504216408492444, 0.14702069448929908]
from probs:  [0.2732491765783046, 0.1427212692626062, 0.14592144331809806, 0.14066568300798366, 0.15042168808363354, 0.1470207397493739]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.481]
 [0.406]
 [0.439]
 [0.417]
 [0.405]
 [0.414]] [[0.854]
 [2.155]
 [0.066]
 [0.605]
 [0.247]
 [0.29 ]
 [0.719]] [[0.425]
 [0.481]
 [0.406]
 [0.439]
 [0.417]
 [0.405]
 [0.414]]
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
probs:  [0.2756262370782551, 0.1429186361532004, 0.14396299478933566, 0.14087532577815318, 0.14942579380912008, 0.14719101239193555]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.447]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.59 ]] [[2.164]
 [2.718]
 [3.616]
 [3.616]
 [3.616]
 [3.616]
 [2.634]] [[0.94 ]
 [1.482]
 [2.168]
 [2.168]
 [2.168]
 [2.168]
 [1.54 ]]
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.284]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]] [[3.681]
 [4.112]
 [3.681]
 [3.681]
 [3.681]
 [3.681]
 [3.681]] [[0.466]
 [0.796]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]]
line 256 mcts: sample exp_bonus 3.239853143983399
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.605]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]] [[4.215]
 [4.633]
 [4.215]
 [4.215]
 [4.215]
 [4.215]
 [4.215]] [[0.666]
 [0.605]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]] [[-0.298]
 [-0.298]
 [-0.298]
 [-0.298]
 [-0.298]
 [-0.298]
 [-0.298]] [[0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]]
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]] [[4.947]
 [4.947]
 [4.947]
 [4.947]
 [4.947]
 [4.947]
 [4.947]] [[1.948]
 [1.948]
 [1.948]
 [1.948]
 [1.948]
 [1.948]
 [1.948]]
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
line 256 mcts: sample exp_bonus 1.7370933236416541
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.367]
 [0.369]
 [0.371]
 [0.379]
 [0.367]
 [0.367]] [[-0.238]
 [-0.289]
 [-0.785]
 [-0.269]
 [-0.183]
 [-0.289]
 [-0.289]] [[0.371]
 [0.367]
 [0.369]
 [0.371]
 [0.379]
 [0.367]
 [0.367]]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.042626534242492
line 256 mcts: sample exp_bonus -0.4770927141966387
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.466]
 [0.466]
 [0.467]
 [0.473]
 [0.464]
 [0.461]] [[-0.237]
 [ 1.23 ]
 [ 0.156]
 [-0.027]
 [-0.042]
 [ 0.127]
 [ 0.046]] [[0.469]
 [0.466]
 [0.466]
 [0.467]
 [0.473]
 [0.464]
 [0.461]]
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
UNIT TEST: sample policy line 217 mcts : [0.367 0.02  0.184 0.102 0.082 0.082 0.163]
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.27819117746219757, 0.14106716447081355, 0.14515856304075375, 0.14206793822173416, 0.14729586080117027, 0.1462192960033308]
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.27819117746219757, 0.14106716447081355, 0.14515856304075375, 0.14206793822173416, 0.14729586080117027, 0.1462192960033308]
line 256 mcts: sample exp_bonus 4.351861601808194
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]] [[0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]] [[0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]] [[0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]]
rdn probs:  [0.27723725095451074, 0.1414614794331699, 0.1455643154190055, 0.14246505082538868, 0.1477075879489196, 0.1455643154190055]
maxi score, test score, baseline:  -0.9969326409495549 -1.0 -0.9969326409495549
probs:  [0.27812449361786584, 0.14191446390915521, 0.1460304399276501, 0.14292124941008202, 0.14497891320759668, 0.1460304399276501]
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.779]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[3.639]
 [3.843]
 [3.639]
 [3.639]
 [3.639]
 [3.639]
 [3.639]] [[1.535]
 [1.735]
 [1.535]
 [1.535]
 [1.535]
 [1.535]
 [1.535]]
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.7  ]
 [0.617]
 [0.669]
 [0.617]
 [0.617]
 [0.648]] [[3.212]
 [2.8  ]
 [2.673]
 [1.587]
 [2.673]
 [2.673]
 [3.07 ]] [[1.71 ]
 [1.716]
 [1.568]
 [0.9  ]
 [1.568]
 [1.568]
 [1.852]]
maxi score, test score, baseline:  -0.9969588235294118 -1.0 -0.9969588235294118
probs:  [0.2818388028936899, 0.1418127460144019, 0.14381010656520185, 0.1418127460144019, 0.14381010656520185, 0.14691549194710254]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[3.014]
 [3.014]
 [3.014]
 [3.014]
 [3.014]
 [3.014]
 [3.014]] [[1.833]
 [1.833]
 [1.833]
 [1.833]
 [1.833]
 [1.833]
 [1.833]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1834966988570623
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.637]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]] [[5.228]
 [5.999]
 [5.228]
 [5.228]
 [5.228]
 [5.228]
 [5.228]] [[0.569]
 [0.637]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]]
siam score:  -0.8273327
Printing some Q and Qe and total Qs values:  [[ 0.096]
 [ 0.405]
 [ 0.047]
 [ 0.151]
 [ 0.119]
 [ 0.089]
 [-0.003]] [[1.402]
 [3.628]
 [1.709]
 [1.294]
 [1.769]
 [2.087]
 [1.951]] [[0.191]
 [1.584]
 [0.323]
 [0.167]
 [0.401]
 [0.553]
 [0.42 ]]
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
siam score:  -0.8265718
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.652]
 [0.65 ]
 [0.661]
 [0.663]
 [0.657]
 [0.655]] [[2.525]
 [4.179]
 [2.702]
 [2.382]
 [2.385]
 [2.594]
 [2.488]] [[0.648]
 [0.652]
 [0.65 ]
 [0.661]
 [0.663]
 [0.657]
 [0.655]]
612 495
maxi score, test score, baseline:  -0.9970014492753624 -1.0 -0.9970014492753624
probs:  [0.2828130428409494, 0.14318664664529576, 0.1452033603180871, 0.13931897658788775, 0.1421994441481252, 0.14727852945965494]
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.398]
 [0.398]
 [0.381]
 [0.398]
 [0.398]
 [0.379]] [[3.418]
 [2.921]
 [2.921]
 [3.279]
 [2.921]
 [2.921]
 [3.259]] [[0.366]
 [0.398]
 [0.398]
 [0.381]
 [0.398]
 [0.398]
 [0.379]]
maxi score, test score, baseline:  -0.9970014492753624 -1.0 -0.9970014492753624
probs:  [0.2828130428409494, 0.14318664664529576, 0.1452033603180871, 0.13931897658788775, 0.1421994441481252, 0.14727852945965494]
in main func line 156:  614
maxi score, test score, baseline:  -0.9970098265895954 -1.0 -0.9970098265895954
probs:  [0.2828128641022999, 0.14318668227129083, 0.14520339692318218, 0.13931901033615673, 0.14219947929484053, 0.1472785670722298]
siam score:  -0.83172625
first move QE:  0.8891140702623029
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
probs:  [0.28019549593263915, 0.1415385734676281, 0.14652084678764338, 0.13964056458381274, 0.1434896735090327, 0.14861484571924402]
actor:  1 policy actor:  1  step number:  62 total reward:  0.49499999999999966  reward:  1.0 rdn_beta:  0.833
624 495
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.369]
 [0.295]
 [0.286]
 [0.285]
 [0.295]
 [0.324]] [[-0.905]
 [-0.046]
 [-0.993]
 [-1.054]
 [-1.005]
 [-0.938]
 [-0.667]] [[0.297]
 [0.369]
 [0.295]
 [0.286]
 [0.285]
 [0.295]
 [0.324]]
maxi score, test score, baseline:  -0.9970428571428571 -1.0 -0.9970428571428571
maxi score, test score, baseline:  -0.9970428571428571 -1.0 -0.9970428571428571
probs:  [0.21345510099010526, 0.1078254616563418, 0.11162100862537866, 0.10567123229553711, 0.34821095792172146, 0.11321623851091588]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970428571428571 -1.0 -0.9970428571428571
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.459]
 [0.46 ]
 [0.453]
 [0.455]
 [0.455]
 [0.456]] [[1.968]
 [2.67 ]
 [2.395]
 [1.695]
 [1.765]
 [1.844]
 [1.946]] [[0.46 ]
 [0.459]
 [0.46 ]
 [0.453]
 [0.455]
 [0.455]
 [0.456]]
using explorer policy with actor:  1
from probs:  [0.21244366408403062, 0.10796411699753214, 0.11176454475123629, 0.10580711746164602, 0.348658730724501, 0.11336182598105397]
maxi score, test score, baseline:  -0.9970428571428571 -1.0 -0.9970428571428571
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.397]
 [0.393]
 [0.397]
 [0.397]
 [0.39 ]
 [0.389]] [[1.712]
 [2.246]
 [1.64 ]
 [1.572]
 [1.56 ]
 [1.629]
 [1.679]] [[0.389]
 [0.397]
 [0.393]
 [0.397]
 [0.397]
 [0.39 ]
 [0.389]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.328]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.301]] [[-0.52 ]
 [ 0.143]
 [-0.52 ]
 [-0.52 ]
 [-0.52 ]
 [-0.52 ]
 [-0.224]] [[0.293]
 [0.328]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.301]]
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.318]
 [0.3  ]
 [0.299]
 [0.3  ]
 [0.299]
 [0.297]] [[-0.543]
 [ 0.638]
 [-1.015]
 [-1.051]
 [-1.017]
 [-0.841]
 [-0.793]] [[0.295]
 [0.318]
 [0.3  ]
 [0.299]
 [0.3  ]
 [0.299]
 [0.297]]
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.305]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]] [[4.984]
 [4.9  ]
 [4.984]
 [4.984]
 [4.984]
 [4.984]
 [4.984]] [[0.82 ]
 [0.812]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[5.319]
 [5.319]
 [5.319]
 [5.319]
 [5.319]
 [5.319]
 [5.319]] [[1.22]
 [1.22]
 [1.22]
 [1.22]
 [1.22]
 [1.22]
 [1.22]]
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.164]
 [0.228]
 [0.196]
 [0.213]
 [0.219]
 [0.227]] [[3.317]
 [3.364]
 [3.236]
 [3.895]
 [3.202]
 [3.174]
 [3.381]] [[0.148]
 [0.083]
 [0.127]
 [0.5  ]
 [0.073]
 [0.066]
 [0.221]]
using explorer policy with actor:  0
siam score:  -0.83448553
maxi score, test score, baseline:  -0.997059090909091 -1.0 -0.997059090909091
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
probs:  [0.21342559867201624, 0.10765270740158953, 0.11216692373147787, 0.10622875996867175, 0.3475690986370362, 0.11295691158920833]
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
probs:  [0.21342559867201624, 0.10765270740158953, 0.11216692373147787, 0.10622875996867175, 0.3475690986370362, 0.11295691158920833]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970751412429378 -1.0 -0.9970751412429378
maxi score, test score, baseline:  -0.9970751412429378 -1.0 -0.9970751412429378
probs:  [0.21274350346902027, 0.10795040443671584, 0.11247710519758848, 0.10582285507910572, 0.3485290266199812, 0.11247710519758848]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.611]
 [0.644]
 [0.655]
 [0.661]
 [0.654]
 [0.653]] [[3.972]
 [3.812]
 [3.916]
 [3.949]
 [3.863]
 [3.92 ]
 [3.82 ]] [[0.854]
 [0.7  ]
 [0.835]
 [0.88 ]
 [0.834]
 [0.857]
 [0.789]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.317]
 [0.439]
 [0.438]
 [0.44 ]
 [0.441]
 [0.44 ]] [[2.524]
 [2.179]
 [2.467]
 [2.49 ]
 [2.481]
 [2.492]
 [2.649]] [[1.488]
 [0.889]
 [1.42 ]
 [1.441]
 [1.438]
 [1.449]
 [1.604]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.367838130891068
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.709]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[0.713]
 [1.717]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[1.526]
 [2.063]
 [1.526]
 [1.526]
 [1.526]
 [1.526]
 [1.526]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.664]] [[5.649]
 [5.624]
 [5.108]
 [5.108]
 [5.108]
 [5.108]
 [5.039]] [[1.902]
 [1.905]
 [1.569]
 [1.569]
 [1.569]
 [1.569]
 [1.482]]
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
probs:  [0.1728305606812284, 0.08636801282817466, 0.09109398127908826, 0.26979769097864587, 0.28818784707568124, 0.09172190715718168]
Printing some Q and Qe and total Qs values:  [[0.896]
 [0.92 ]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]] [[3.374]
 [3.62 ]
 [3.374]
 [3.374]
 [3.374]
 [3.374]
 [3.374]] [[1.634]
 [1.759]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.496]
 [0.848]
 [0.565]
 [0.429]
 [0.563]
 [0.375]] [[3.205]
 [3.011]
 [3.248]
 [1.893]
 [2.403]
 [1.988]
 [2.445]] [[1.241]
 [1.241]
 [1.785]
 [0.386]
 [0.674]
 [0.463]
 [0.654]]
siam score:  -0.83956665
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.69 ]
 [0.743]
 [0.74 ]
 [0.735]
 [0.74 ]
 [0.741]] [[4.057]
 [4.685]
 [4.133]
 [4.337]
 [4.343]
 [4.005]
 [4.137]] [[1.269]
 [1.622]
 [1.36 ]
 [1.489]
 [1.484]
 [1.269]
 [1.358]]
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]] [[5.483]
 [5.483]
 [5.483]
 [5.483]
 [5.483]
 [5.483]
 [5.483]] [[1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.663]]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.1743186286351825, 0.0849117122951191, 0.0912537776895082, 0.27211999163200445, 0.2867581532800504, 0.09063773646813539]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.17389978830544323, 0.08520220156775364, 0.09094781489463515, 0.2712632061227164, 0.28773917421481643, 0.09094781489463515]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.358]
 [0.341]
 [0.347]
 [0.347]
 [0.342]
 [0.345]] [[2.909]
 [3.172]
 [2.602]
 [2.545]
 [2.623]
 [2.519]
 [2.969]] [[0.342]
 [0.358]
 [0.341]
 [0.347]
 [0.347]
 [0.342]
 [0.345]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.17420427914509626, 0.08481628221353468, 0.09110721001776044, 0.27173688387045236, 0.2882414955035328, 0.08989384924962353]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.597]
 [0.782]
 [0.775]
 [0.981]
 [0.632]
 [0.572]] [[2.301]
 [2.371]
 [2.204]
 [1.95 ]
 [2.904]
 [2.095]
 [1.729]] [[2.189]
 [2.205]
 [2.24 ]
 [1.764]
 [3.873]
 [1.77 ]
 [0.996]]
653 526
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]] [[3.346]
 [3.346]
 [3.346]
 [3.346]
 [3.346]
 [3.346]
 [3.346]] [[2.617]
 [2.617]
 [2.617]
 [2.617]
 [2.617]
 [2.617]
 [2.617]]
siam score:  -0.8370073
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.17638758897969625, 0.08534426608509073, 0.09041865014774335, 0.2681212354898509, 0.28990350002535625, 0.08982475927226252]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.5247413878729437
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.401]
 [0.425]
 [0.439]
 [0.434]
 [0.421]
 [0.421]] [[2.869]
 [2.655]
 [2.474]
 [2.64 ]
 [2.657]
 [2.755]
 [2.834]] [[1.78 ]
 [1.406]
 [1.151]
 [1.457]
 [1.475]
 [1.613]
 [1.744]]
from probs:  [0.17808714479422363, 0.08616658779434763, 0.08951469459425554, 0.26729509723794725, 0.28883789481015204, 0.09009858076907393]
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.17557513629907104, 0.08643012821725993, 0.08978847600926405, 0.26811177768563876, 0.2897203336480048, 0.09037414814076146]
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.17557513629907104, 0.08643012821725993, 0.08978847600926405, 0.26811177768563876, 0.2897203336480048, 0.09037414814076146]
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.17557513629907104, 0.08643012821725993, 0.08978847600926405, 0.26811177768563876, 0.2897203336480048, 0.09037414814076146]
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.17557513629907104, 0.08643012821725993, 0.08978847600926405, 0.26811177768563876, 0.2897203336480048, 0.09037414814076146]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[4.292]
 [4.295]
 [4.295]
 [4.295]
 [4.295]
 [4.295]
 [4.295]] [[ 0.002]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]]
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.1756691338077107, 0.0859410311063252, 0.08983654598789159, 0.2682553164558236, 0.28987544097207824, 0.09042253167017075]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.137]
 [0.072]
 [0.109]
 [0.066]
 [0.108]
 [0.12 ]] [[2.242]
 [1.732]
 [1.825]
 [1.8  ]
 [1.923]
 [2.15 ]
 [1.896]] [[0.787]
 [0.006]
 [0.029]
 [0.062]
 [0.181]
 [0.643]
 [0.245]]
using explorer policy with actor:  1
from probs:  [0.17620019893363603, 0.08567063136924814, 0.09010820260422112, 0.26906565128981835, 0.2888471131988553, 0.09010820260422112]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.319]
 [0.511]
 [0.513]
 [0.513]
 [0.511]
 [0.495]] [[1.774]
 [2.347]
 [1.522]
 [1.467]
 [1.487]
 [1.657]
 [2.09 ]] [[0.698]
 [0.87 ]
 [0.428]
 [0.377]
 [0.397]
 [0.564]
 [0.966]]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.28 ]
 [0.495]
 [0.494]
 [0.494]
 [0.492]
 [0.493]] [[1.275]
 [1.579]
 [0.941]
 [1.008]
 [1.023]
 [1.069]
 [1.096]] [[0.764]
 [0.687]
 [0.478]
 [0.544]
 [0.56 ]
 [0.6  ]
 [0.63 ]]
from probs:  [0.176998044080877, 0.08605862040387058, 0.08993358790941358, 0.2669218641509014, 0.29015429554552374, 0.08993358790941358]
maxi score, test score, baseline:  -0.9971451790633609 -1.0 -0.9971451790633609
siam score:  -0.8457239
using explorer policy with actor:  1
siam score:  -0.8458118
siam score:  -0.84578
siam score:  -0.8462541
maxi score, test score, baseline:  -0.9971451790633609 -1.0 -0.9971451790633609
from probs:  [0.1773983295357398, 0.08621492378233622, 0.08893419202725883, 0.2673879020943672, 0.29056555448473903, 0.0894990980755589]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.502]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[-0.872]
 [ 1.106]
 [-0.872]
 [-0.872]
 [-0.872]
 [-0.872]
 [-0.872]] [[0.493]
 [0.502]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]]
maxi score, test score, baseline:  -0.997175204359673 -1.0 -0.997175204359673
probs:  [0.1778882385307095, 0.08694338805051126, 0.08912321193620991, 0.2663531759837196, 0.2911240767993447, 0.08856790869950502]
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.322]
 [0.32 ]
 [0.31 ]
 [0.326]
 [0.334]
 [0.317]] [[4.196]
 [4.214]
 [4.082]
 [3.928]
 [4.004]
 [4.106]
 [4.19 ]] [[1.009]
 [1.054]
 [0.918]
 [0.745]
 [0.854]
 [0.973]
 [1.023]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.484]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[2.425]
 [3.61 ]
 [2.431]
 [2.431]
 [2.431]
 [2.431]
 [2.431]] [[0.536]
 [0.484]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]]
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.041]
 [0.065]
 [0.066]
 [0.067]
 [0.062]
 [0.064]] [[-2.695]
 [ 2.661]
 [-2.684]
 [-2.759]
 [-2.71 ]
 [-2.724]
 [-2.484]] [[0.072]
 [0.041]
 [0.065]
 [0.066]
 [0.067]
 [0.062]
 [0.064]]
maxi score, test score, baseline:  -0.997189972899729 -1.0 -0.997189972899729
probs:  [0.17716649135925933, 0.0870789033910922, 0.08870595688630256, 0.26676672529960777, 0.2915759661774357, 0.08870595688630256]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
siam score:  -0.84885836
deleting a thread, now have 3 threads
Frames:  45694 train batches done:  5345 episodes:  1251
using explorer policy with actor:  1
siam score:  -0.84568423
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
probs:  [0.1779280053586092, 0.08736595063345623, 0.08789654943365699, 0.2660442724766818, 0.2923314825742944, 0.08843373952330125]
using another actor
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.724]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.751]] [[4.374]
 [4.838]
 [4.374]
 [4.374]
 [4.374]
 [4.374]
 [4.719]] [[1.573]
 [1.703]
 [1.573]
 [1.573]
 [1.573]
 [1.573]
 [1.677]]
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
probs:  [0.17802131132727061, 0.08688736265716838, 0.08794264264237012, 0.2661837868746555, 0.29248478206246015, 0.08848011443607534]
siam score:  -0.83446777
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.17830420181813814, 0.0870254979697113, 0.08808245590855487, 0.26501819962438494, 0.29294886236702117, 0.08862078231218948]
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.392]
 [0.374]
 [0.374]
 [0.374]
 [0.381]
 [0.374]] [[2.703]
 [2.698]
 [3.521]
 [3.521]
 [3.521]
 [2.765]
 [3.521]] [[1.46 ]
 [1.536]
 [1.911]
 [1.911]
 [1.911]
 [1.561]
 [1.911]]
deleting a thread, now have 2 threads
Frames:  45985 train batches done:  5384 episodes:  1266
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
probs:  [0.17589787511105137, 0.08569854586665614, 0.08881983628709407, 0.26723601330230473, 0.2935278931457998, 0.08881983628709407]
Printing some Q and Qe and total Qs values:  [[0.823]
 [0.825]
 [0.819]
 [0.822]
 [0.836]
 [0.838]
 [0.832]] [[4.601]
 [4.125]
 [3.288]
 [3.278]
 [4.009]
 [3.124]
 [3.849]] [[2.088]
 [1.872]
 [1.48 ]
 [1.478]
 [1.834]
 [1.43 ]
 [1.755]]
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[2.134]
 [2.134]
 [2.134]
 [2.134]
 [2.134]
 [2.134]
 [2.134]] [[0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]]
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
probs:  [0.1760800898819523, 0.08528813776738825, 0.08891184584724535, 0.2675128463732535, 0.2938319622982488, 0.08837511783191157]
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.668]
 [0.652]
 [0.684]
 [0.684]
 [0.65 ]
 [0.649]] [[2.058]
 [3.756]
 [1.975]
 [2.986]
 [2.986]
 [1.786]
 [1.829]] [[0.424]
 [1.046]
 [0.418]
 [0.819]
 [0.819]
 [0.351]
 [0.364]]
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
probs:  [0.17616699509009434, 0.08483667715744331, 0.0889557287330096, 0.26764487867520387, 0.2939769845313669, 0.0884187358128821]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.327]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]] [[4.798]
 [2.82 ]
 [4.798]
 [4.798]
 [4.798]
 [4.798]
 [4.798]] [[1.49 ]
 [0.492]
 [1.49 ]
 [1.49 ]
 [1.49 ]
 [1.49 ]
 [1.49 ]]
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.105]
 [0.067]
 [0.056]
 [1.29 ]
 [1.29 ]
 [1.29 ]] [[1.021]
 [0.997]
 [1.159]
 [0.987]
 [1.764]
 [1.764]
 [1.764]] [[0.091]
 [0.198]
 [0.283]
 [0.091]
 [3.336]
 [3.336]
 [3.336]]
Printing some Q and Qe and total Qs values:  [[1.016]
 [0.89 ]
 [0.691]
 [0.711]
 [0.692]
 [0.69 ]
 [0.951]] [[2.392]
 [2.255]
 [1.543]
 [1.412]
 [1.479]
 [1.724]
 [2.191]] [[1.603]
 [1.47 ]
 [1.008]
 [0.951]
 [0.976]
 [1.099]
 [1.468]]
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
siam score:  -0.82286817
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
probs:  [0.17603698077665117, 0.08523354267066273, 0.08829943253578573, 0.26730487015528537, 0.29535220726226447, 0.08777296659935047]
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[-0.685]
 [-0.685]
 [-0.685]
 [-0.685]
 [-0.685]
 [-0.685]
 [-0.685]] [[0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]]
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.17654412210744055, 0.08498773663931768, 0.08855394190594547, 0.26807378284177996, 0.29433608062468836, 0.08750433588082783]
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
probs:  [0.17654431229620404, 0.08498788894973094, 0.08855410146041386, 0.26807349604545483, 0.2943357079449714, 0.0875044933032249]
first move QE:  0.9514534316487975
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
probs:  [0.17654431229620404, 0.08498788894973094, 0.08855410146041386, 0.26807349604545483, 0.2943357079449714, 0.0875044933032249]
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
probs:  [0.17709869055481492, 0.08525476493834913, 0.08830253327605866, 0.2673326037272547, 0.29525997061361947, 0.08675143688990292]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.643]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.643]] [[4.524]
 [4.25 ]
 [4.524]
 [4.524]
 [4.524]
 [4.524]
 [4.477]] [[2.166]
 [1.985]
 [2.166]
 [2.166]
 [2.166]
 [2.166]
 [2.17 ]]
siam score:  -0.82248044
696 590
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.17737630883877822, 0.08538847024321197, 0.08844101913136308, 0.266184583900865, 0.2957221280992812, 0.08688748978650046]
actor:  1 policy actor:  1  step number:  79 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  1.0
using another actor
from probs:  [0.17737630883877822, 0.08538847024321197, 0.08844101913136308, 0.266184583900865, 0.2957221280992812, 0.08688748978650046]
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
probs:  [0.15970803296819633, 0.07688306491246727, 0.07915967573398211, 0.23827571841453152, 0.2662649267137903, 0.1797085812570322]
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
siam score:  -0.8224013
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
probs:  [0.15992837720279357, 0.07698913809975022, 0.07926888989081818, 0.23722479097725316, 0.266632284199738, 0.17995651962964695]
first move QE:  0.953608255066339
line 256 mcts: sample exp_bonus -1.1053998661472226
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
probs:  [0.15920744531432407, 0.07705520856709568, 0.0793369167934477, 0.23742837232902111, 0.2668611024468476, 0.18011095454926362]
line 256 mcts: sample exp_bonus 1.5784380369287718
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.1592076129747789, 0.07705534397182366, 0.07933705674475897, 0.2374281204284877, 0.26686076812382603, 0.18011109775632472]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.007]
 [0.006]
 [0.005]
 [0.004]
 [0.008]
 [0.006]] [[2.179]
 [2.24 ]
 [2.397]
 [2.407]
 [2.331]
 [2.369]
 [2.302]] [[-0.295]
 [-0.227]
 [-0.072]
 [-0.066]
 [-0.143]
 [-0.096]
 [-0.168]]
702 595
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9972753280839896 -1.0 -0.9972753280839896
probs:  [0.15856753734256943, 0.07715703484824567, 0.07897386966978699, 0.2377407907366436, 0.2672121474430673, 0.18034861995968712]
from probs:  [0.1585677027892264, 0.07715716915695976, 0.07897400756425614, 0.2377405415992838, 0.26721181669575467, 0.18034876219451937]
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
probs:  [0.1578594782051421, 0.07722211152275252, 0.07904047914354549, 0.23794064527577952, 0.2674367260299567, 0.1805005598228238]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.434]
 [0.434]
 [0.437]
 [0.438]
 [0.438]
 [0.439]] [[-0.418]
 [-0.318]
 [-0.318]
 [-0.397]
 [-0.485]
 [-0.34 ]
 [-0.453]] [[0.433]
 [0.434]
 [0.434]
 [0.437]
 [0.438]
 [0.438]
 [0.439]]
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]] [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using another actor
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]] [[3.66]
 [3.66]
 [3.66]
 [3.66]
 [3.66]
 [3.66]
 [3.66]] [[1.184]
 [1.184]
 [1.184]
 [1.184]
 [1.184]
 [1.184]
 [1.184]]
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.858]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.912]] [[4.102]
 [4.766]
 [4.263]
 [4.263]
 [4.263]
 [4.263]
 [4.416]] [[1.737]
 [1.859]
 [1.726]
 [1.726]
 [1.726]
 [1.726]
 [1.851]]
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]] [[3.19]
 [3.19]
 [3.19]
 [3.19]
 [3.19]
 [3.19]
 [3.19]] [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]]
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.158573225657535, 0.07712805118877836, 0.07939801933894748, 0.23494006542300125, 0.26864409830371994, 0.18131654008801792]
maxi score, test score, baseline:  -0.9973093264248705 -1.0 -0.9973093264248705
probs:  [0.15857338725860004, 0.07712818240257005, 0.07939815493522795, 0.23493982357536988, 0.268643772089283, 0.18131667973894905]
maxi score, test score, baseline:  -0.9973093264248705 -1.0 -0.9973093264248705
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
probs:  [0.15830226429790337, 0.07740468661730897, 0.07968279751197306, 0.235781441369104, 0.2679187165816424, 0.1809100936220682]
line 256 mcts: sample exp_bonus 5.396580110719114
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.554]] [[3.861]
 [3.861]
 [3.861]
 [3.861]
 [3.861]
 [3.861]
 [3.914]] [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.794]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973226804123712 -1.0 -0.9973226804123712
probs:  [0.15767211932728828, 0.07706289950082687, 0.07978429320487057, 0.23608112764536401, 0.26825920126004715, 0.181140359061603]
maxi score, test score, baseline:  -0.9973226804123712 -1.0 -0.9973226804123712
maxi score, test score, baseline:  -0.9973226804123712 -1.0 -0.9973226804123712
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.833]
 [0.764]
 [0.757]
 [0.759]
 [0.756]
 [0.757]] [[1.023]
 [2.81 ]
 [1.061]
 [0.422]
 [0.523]
 [0.656]
 [0.802]] [[0.376]
 [1.137]
 [0.395]
 [0.132]
 [0.174]
 [0.227]
 [0.287]]
maxi score, test score, baseline:  -0.9973226804123712 -1.0 -0.9973226804123712
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.15759439631561717, 0.07699169288224582, 0.0796941217630176, 0.23586206048903902, 0.26785680934249, 0.18200091920759046]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.15759439631561717, 0.07699169288224582, 0.0796941217630176, 0.23586206048903902, 0.26785680934249, 0.18200091920759046]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.1562208758512254, 0.07711722583846005, 0.07982406095935543, 0.2362466274546888, 0.26829354291542623, 0.1822976669808441]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.857906891463535
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
siam score:  -0.80559397
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.202]
 [0.201]
 [0.229]
 [0.229]
 [0.22 ]
 [0.215]] [[2.536]
 [2.962]
 [2.01 ]
 [2.696]
 [2.696]
 [2.089]
 [2.42 ]] [[-0.409]
 [-0.293]
 [-0.614]
 [-0.329]
 [-0.329]
 [-0.55 ]
 [-0.449]]
maxi score, test score, baseline:  -0.9973424552429667 -1.0 -0.9973424552429667
probs:  [0.15694534663384455, 0.07743768202123777, 0.08013941618607384, 0.23590139070742833, 0.2676001671349619, 0.1819759973164536]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.15626874763593235, 0.07749997196416793, 0.08020387998319768, 0.2360905226001858, 0.267814665167092, 0.1821222126494243]
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.15609949843302845, 0.0769476204138735, 0.08052642920777536, 0.23572325788828488, 0.26889171296824915, 0.18181148108878856]
using another actor
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.15630317397076654, 0.07704807068684555, 0.08063155220576658, 0.23472656295399824, 0.26924197836126185, 0.18204866182136123]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.691]
 [0.655]
 [0.666]
 [0.667]
 [0.656]
 [0.665]] [[0.237]
 [1.557]
 [0.228]
 [0.147]
 [0.095]
 [0.035]
 [0.293]] [[0.651]
 [1.134]
 [0.619]
 [0.613]
 [0.598]
 [0.555]
 [0.66 ]]
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.627]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[2.516]
 [2.769]
 [2.516]
 [2.516]
 [2.516]
 [2.516]
 [2.516]] [[1.05 ]
 [1.065]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.05 ]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.15630317397076654, 0.07704807068684555, 0.08063155220576658, 0.23472656295399824, 0.26924197836126185, 0.18204866182136123]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.15630317397076654, 0.07704807068684555, 0.08063155220576658, 0.23472656295399824, 0.26924197836126185, 0.18204866182136123]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.612]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.794]] [[3.02 ]
 [4.491]
 [4.181]
 [4.181]
 [4.181]
 [4.181]
 [3.03 ]] [[0.477]
 [0.701]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.578]]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[5.53]
 [5.62]
 [5.62]
 [5.62]
 [5.62]
 [5.62]
 [5.62]] [[0.89 ]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.352]] [[2.016]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.951]] [[0.352]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.352]]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.647]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[2.904]
 [2.994]
 [2.904]
 [2.904]
 [2.904]
 [2.904]
 [2.904]] [[1.593]
 [1.641]
 [1.593]
 [1.593]
 [1.593]
 [1.593]
 [1.593]]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
in main func line 156:  727
start point for exploration sampling:  10749
siam score:  -0.7972653
maxi score, test score, baseline:  -0.9973619289340102 -1.0 -0.9973619289340102
probs:  [0.15570845459338645, 0.07714516566102834, 0.0802661964377787, 0.23502174753320476, 0.2695805206861797, 0.18227791508842212]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973619289340102 -1.0 -0.9973619289340102
probs:  [0.1566578904953859, 0.07796787350781216, 0.08063986958078442, 0.23368397345969846, 0.2658060868876165, 0.1852443060687025]
siam score:  -0.8000049
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
729 637
siam score:  -0.8031327
maxi score, test score, baseline:  -0.9973619289340102 -1.0 -0.9973619289340102
probs:  [0.1566068125212402, 0.07790272017876336, 0.07965187800612104, 0.23477724410184314, 0.2670496381119271, 0.18401170708010525]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.602]
 [0.695]
 [0.61 ]
 [0.182]
 [0.577]
 [0.541]] [[ 2.477]
 [ 1.758]
 [-1.405]
 [-1.049]
 [ 2.495]
 [-1.834]
 [ 2.191]] [[1.634]
 [1.593]
 [0.375]
 [0.467]
 [1.636]
 [0.131]
 [1.731]]
from probs:  [0.15685501180112857, 0.07802623542221127, 0.07977816694911904, 0.23514887317100536, 0.265888416655805, 0.18430329600073078]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.127]
 [0.14 ]
 [0.139]
 [0.127]
 [0.132]
 [0.136]] [[-0.792]
 [-0.684]
 [-0.631]
 [-0.638]
 [-0.684]
 [-0.535]
 [-0.615]] [[0.139]
 [0.127]
 [0.14 ]
 [0.139]
 [0.127]
 [0.132]
 [0.136]]
maxi score, test score, baseline:  -0.9973683544303797 -1.0 -0.9973683544303797
probs:  [0.15705346765473138, 0.07812495565420335, 0.07987910375718754, 0.2341811694201915, 0.26622482358399047, 0.18453647992969574]
from probs:  [0.15712045681649708, 0.07773174156351405, 0.07991317517428644, 0.2342810564234843, 0.26633837840095465, 0.1846151916212635]
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.376]
 [0.278]
 [0.282]
 [0.285]
 [0.278]
 [0.293]] [[-2.339]
 [-0.785]
 [-2.459]
 [-2.557]
 [-2.538]
 [-2.305]
 [-2.327]] [[0.292]
 [0.376]
 [0.278]
 [0.282]
 [0.285]
 [0.278]
 [0.293]]
maxi score, test score, baseline:  -0.9973683544303797 -1.0 -0.9973683544303797
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9973683544303797 -1.0 -0.9973683544303797
probs:  [0.15728327933429356, 0.07781229426893839, 0.07999598847976316, 0.2345238397774268, 0.26661438246965286, 0.1837702156699252]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.485]
 [0.499]
 [0.507]
 [0.481]
 [0.48 ]
 [0.492]] [[4.946]
 [3.983]
 [3.925]
 [4.073]
 [4.729]
 [4.661]
 [4.414]] [[0.484]
 [0.485]
 [0.499]
 [0.507]
 [0.481]
 [0.48 ]
 [0.492]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.103]
 [0.107]
 [0.109]
 [0.109]
 [0.11 ]
 [0.112]] [[-0.616]
 [ 1.948]
 [ 0.05 ]
 [-0.618]
 [-0.466]
 [-0.116]
 [ 0.16 ]] [[0.108]
 [0.103]
 [0.107]
 [0.109]
 [0.109]
 [0.11 ]
 [0.112]]
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.81 ]
 [0.781]
 [0.635]
 [0.563]
 [0.505]
 [0.332]] [[2.134]
 [3.037]
 [1.759]
 [1.343]
 [1.464]
 [1.981]
 [1.723]] [[0.678]
 [1.479]
 [0.612]
 [0.222]
 [0.245]
 [0.54 ]
 [0.233]]
maxi score, test score, baseline:  -0.9973874371859297 -1.0 -0.9973874371859297
probs:  [0.15859310760202455, 0.07846045267151841, 0.07932636265073639, 0.2352116661044799, 0.26414109748699127, 0.18426731348424943]
first move QE:  0.971017291718829
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9973874371859297 -1.0 -0.9973874371859297
probs:  [0.15897285390758373, 0.07822152535132706, 0.07907989095834286, 0.23577487318565865, 0.26324232071301906, 0.18470853588406871]
maxi score, test score, baseline:  -0.9973874371859297 -1.0 -0.9973874371859297
probs:  [0.15904149099096673, 0.078255297767346, 0.07868228049366485, 0.23587666980836927, 0.26335597650186976, 0.18478828443778342]
first move QE:  0.9718912072862069
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.173]
 [0.13 ]
 [0.13 ]
 [0.13 ]
 [0.13 ]
 [0.13 ]
 [0.13 ]] [[1.751]
 [1.81 ]
 [1.81 ]
 [1.81 ]
 [1.81 ]
 [1.81 ]
 [1.81 ]] [[0.886]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]]
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[2.148]
 [1.997]
 [1.997]
 [1.997]
 [1.997]
 [1.997]
 [1.997]] [[0.269]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]]
from probs:  [0.15836037345282325, 0.078696321308998, 0.0791257103827407, 0.23720600043875342, 0.2618121966357782, 0.18479939778090643]
maxi score, test score, baseline:  -0.9973874371859297 -1.0 -0.9973874371859297
probs:  [0.15852188357054953, 0.07877658287849365, 0.07920640988174069, 0.23744792440130114, 0.2620792162049978, 0.1839679830629172]
using explorer policy with actor:  0
siam score:  -0.79892653
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.304]
 [0.291]
 [0.291]
 [0.291]
 [0.298]
 [0.293]] [[-1.408]
 [-0.034]
 [-1.408]
 [-1.408]
 [-1.408]
 [-1.33 ]
 [-1.283]] [[0.291]
 [0.304]
 [0.291]
 [0.291]
 [0.291]
 [0.298]
 [0.293]]
maxi score, test score, baseline:  -0.9973937343358396 -1.0 -0.9973937343358396
maxi score, test score, baseline:  -0.9973937343358396 -1.0 -0.9973937343358396
probs:  [0.15868208319085583, 0.07885624323985747, 0.07928650498933607, 0.2376874307133812, 0.2623435215969611, 0.18314421626960833]
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.1589191890367446, 0.07897412162777342, 0.07940502665149594, 0.23804213481206793, 0.26124169611851045, 0.1834178317534076]
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
UNIT TEST: sample policy line 217 mcts : [0.347 0.122 0.143 0.082 0.082 0.102 0.122]
maxi score, test score, baseline:  -0.9974062344139651 -1.0 -0.9974062344139651
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.353]
 [0.372]
 [0.345]
 [0.327]
 [0.327]
 [0.289]] [[0.273]
 [1.579]
 [0.515]
 [0.724]
 [0.273]
 [0.273]
 [1.686]] [[0.327]
 [0.353]
 [0.372]
 [0.345]
 [0.327]
 [0.327]
 [0.289]]
siam score:  -0.78285617
first move QE:  0.9731016221706988
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.1590785694640515, 0.07905342403320487, 0.0794847619453908, 0.2382799675661748, 0.261502617830666, 0.18260065916051205]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.456]
 [0.462]
 [0.462]
 [0.462]
 [0.457]
 [0.455]] [[-0.723]
 [ 0.759]
 [-0.675]
 [-0.714]
 [-0.466]
 [-0.481]
 [-0.362]] [[0.464]
 [0.456]
 [0.462]
 [0.462]
 [0.462]
 [0.457]
 [0.455]]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.451]
 [0.524]
 [0.471]
 [0.524]
 [0.524]
 [0.453]] [[3.674]
 [3.566]
 [3.622]
 [3.32 ]
 [3.622]
 [3.622]
 [5.071]] [[0.861]
 [0.748]
 [0.886]
 [0.609]
 [0.886]
 [0.886]
 [1.771]]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.488]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.561]] [[2.392]
 [2.308]
 [2.306]
 [2.306]
 [2.306]
 [2.306]
 [2.295]] [[0.242]
 [0.277]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.38 ]]
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.667]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.669]] [[3.351]
 [4.121]
 [3.351]
 [3.351]
 [3.351]
 [3.351]
 [4.142]] [[1.368]
 [1.808]
 [1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.823]]
line 256 mcts: sample exp_bonus 3.9599056713553056
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
probs:  [0.1594615479461831, 0.07924379286001136, 0.07924379286001136, 0.23885317398758782, 0.26213164380258774, 0.18106604854361863]
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.435]
 [0.409]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[3.708]
 [3.708]
 [3.079]
 [3.708]
 [3.708]
 [3.708]
 [3.708]] [[0.801]
 [0.801]
 [0.33 ]
 [0.801]
 [0.801]
 [0.801]
 [0.801]]
from probs:  [0.1594615479461831, 0.07924379286001136, 0.07924379286001136, 0.23885317398758782, 0.26213164380258774, 0.18106604854361863]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]] [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]]
siam score:  -0.79151917
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]] [[-1.616]
 [-1.616]
 [-1.616]
 [-1.616]
 [-1.616]
 [-1.616]
 [-1.616]] [[0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]]
line 256 mcts: sample exp_bonus -1.7635377457662627
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.476]
 [0.478]] [[1.883]
 [1.883]
 [1.883]
 [1.883]
 [1.883]
 [1.799]
 [1.802]] [[0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.823]
 [0.83 ]]
maxi score, test score, baseline:  -0.9974247524752475 -1.0 -0.9974247524752475
probs:  [0.15973281632120448, 0.0789502946062722, 0.07937864774310405, 0.23798717795138224, 0.26257703503594104, 0.18137402834209612]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.561]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[2.537]
 [3.361]
 [2.537]
 [2.537]
 [2.537]
 [2.537]
 [2.537]] [[0.567]
 [0.561]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]]
maxi score, test score, baseline:  -0.9974247524752475 -1.0 -0.9974247524752475
probs:  [0.15973281632120448, 0.0789502946062722, 0.07937864774310405, 0.23798717795138224, 0.26257703503594104, 0.18137402834209612]
maxi score, test score, baseline:  -0.9974247524752475 -1.0 -0.9974247524752475
probs:  [0.15993406850654224, 0.07904976645988694, 0.07947865929173493, 0.2370270950502937, 0.2629078637493986, 0.1816025469421436]
line 256 mcts: sample exp_bonus -1.8271197575871043
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
probs:  [0.1599347940201291, 0.07905036560143948, 0.07947926215149761, 0.23702599099071048, 0.26290641932734826, 0.18160316790887512]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.082]
 [0.087]
 [0.084]
 [0.084]
 [0.085]
 [0.133]] [[-1.777]
 [ 1.189]
 [-1.122]
 [-1.254]
 [-1.183]
 [-1.201]
 [-1.317]] [[0.204]
 [0.082]
 [0.087]
 [0.084]
 [0.084]
 [0.085]
 [0.133]]
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
probs:  [0.1600026686171519, 0.07865952453589117, 0.07951299229085379, 0.23712658225805155, 0.2630179939686005, 0.18168023832945118]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
probs:  [0.1600026686171519, 0.07865952453589117, 0.07951299229085379, 0.23712658225805155, 0.2630179939686005, 0.18168023832945118]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[2.583]
 [2.462]
 [2.462]
 [2.462]
 [2.462]
 [2.462]
 [2.462]] [[0.592]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]]
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[-1.098]
 [-1.001]
 [-1.001]
 [-1.001]
 [-1.001]
 [-1.001]
 [-1.001]] [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9711958341678227
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.135]
 [0.135]] [[-0.688]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.861]
 [-0.843]] [[0.13 ]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.135]
 [0.135]]
using explorer policy with actor:  0
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.574]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]] [[2.558]
 [4.258]
 [2.558]
 [2.558]
 [2.558]
 [2.558]
 [2.558]] [[0.569]
 [0.574]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
line 256 mcts: sample exp_bonus 4.148093497497921
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.631]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[3.221]
 [3.166]
 [3.221]
 [3.221]
 [3.221]
 [3.221]
 [3.221]] [[0.613]
 [0.631]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]]
siam score:  -0.7861969
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.403]
 [0.496]
 [0.504]
 [0.509]
 [0.509]
 [0.499]] [[5.059]
 [4.582]
 [3.428]
 [3.69 ]
 [3.632]
 [3.703]
 [3.755]] [[ 0.923]
 [ 0.402]
 [-0.067]
 [ 0.086]
 [ 0.063]
 [ 0.102]
 [ 0.113]]
maxi score, test score, baseline:  -0.9975470588235295 -1.0 -0.9975470588235295
probs:  [0.15960300301814603, 0.07839635001501638, 0.07839635001501638, 0.2376038269642913, 0.26494007993399005, 0.18106039005353974]
maxi score, test score, baseline:  -0.9975470588235295 -1.0 -0.9975470588235295
probs:  [0.15960300301814603, 0.07839635001501638, 0.07839635001501638, 0.2376038269642913, 0.26494007993399005, 0.18106039005353974]
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.1  ]
 [0.111]
 [0.105]
 [0.102]
 [0.104]
 [0.105]] [[-3.519]
 [-1.165]
 [-3.461]
 [-3.503]
 [-3.6  ]
 [-3.534]
 [-3.526]] [[0.107]
 [0.1  ]
 [0.111]
 [0.105]
 [0.102]
 [0.104]
 [0.105]]
rdn probs:  [0.15966920612515184, 0.0780149472424042, 0.07842891208603334, 0.237701986015954, 0.2650494917876923, 0.18113545674276427]
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.13 ]
 [0.11 ]
 [0.106]
 [0.105]
 [0.107]
 [0.107]] [[-3.333]
 [-0.509]
 [-3.405]
 [-3.5  ]
 [-3.604]
 [-3.527]
 [-3.559]] [[0.11 ]
 [0.13 ]
 [0.11 ]
 [0.106]
 [0.105]
 [0.107]
 [0.107]]
maxi score, test score, baseline:  -0.9975580796252927 -1.0 -0.9975580796252927
maxi score, test score, baseline:  -0.9975580796252927 -1.0 -0.9975580796252927
maxi score, test score, baseline:  -0.9975689976689976 -1.0 -0.9975689976689976
probs:  [0.15973501970680126, 0.07763752488241762, 0.07846136842405643, 0.23779877832072463, 0.2651572999786944, 0.18121000868730563]
maxi score, test score, baseline:  -0.9975689976689976 -1.0 -0.9975689976689976
probs:  [0.15986602751117318, 0.07729551884613393, 0.07811124300852774, 0.2379938106679664, 0.2653747706106645, 0.1813586293555343]
maxi score, test score, baseline:  -0.9975689976689976 -1.0 -0.9975689976689976
siam score:  -0.770265
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9975689976689976 -1.0 -0.9975689976689976
maxi score, test score, baseline:  -0.9975744186046511 -1.0 -0.9975744186046511
probs:  [0.1596517165669887, 0.07756877109126624, 0.07838737913635792, 0.23758518071833354, 0.2648073254500539, 0.18199962703699965]
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[1.817]
 [1.817]
 [1.817]
 [1.817]
 [1.817]
 [1.817]
 [1.817]] [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
siam score:  -0.7604913
maxi score, test score, baseline:  -0.9975851851851852 -1.0 -0.9975851851851852
siam score:  -0.7595291
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9975905311778291 -1.0 -0.9975905311778291
probs:  [0.15976359890536596, 0.07759526857220879, 0.07882347970319249, 0.2389054090740791, 0.2647824762828228, 0.1801297674623308]
maxi score, test score, baseline:  -0.9975905311778291 -1.0 -0.9975905311778291
maxi score, test score, baseline:  -0.9975905311778291 -1.0 -0.9975905311778291
maxi score, test score, baseline:  -0.9975905311778291 -1.0 -0.9975905311778291
maxi score, test score, baseline:  -0.9975958525345622 -1.0 -0.9975958525345622
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]] [[2.464]
 [2.464]
 [2.464]
 [2.464]
 [2.464]
 [2.464]
 [2.464]] [[0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.391]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]] [[3.243]
 [2.449]
 [2.711]
 [2.711]
 [2.711]
 [2.711]
 [2.711]] [[0.921]
 [0.929]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]]
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.83 ]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]] [[4.383]
 [4.764]
 [4.383]
 [4.383]
 [4.383]
 [4.383]
 [4.383]] [[2.186]
 [2.347]
 [2.186]
 [2.186]
 [2.186]
 [2.186]
 [2.186]]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
using explorer policy with actor:  0
using another actor
from probs:  [0.15956804046130682, 0.07747275247859373, 0.07910762110654974, 0.23976558325628433, 0.26425099868874213, 0.1798350040085233]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9976064220183486 -1.0 -0.9976064220183486
maxi score, test score, baseline:  -0.9976064220183486 -1.0 -0.9976064220183486
probs:  [0.1592859821505154, 0.07691354250841499, 0.07934766484173104, 0.24049261485279416, 0.2635796316934886, 0.18038056395305577]
766 690
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997616894977169 -1.0 -0.997616894977169
probs:  [0.15954828849042482, 0.07664866344788152, 0.07947841512330964, 0.23963432502258636, 0.26401276960599845, 0.18067753830979918]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997616894977169 -1.0 -0.997616894977169
probs:  [0.15976011469710677, 0.07636233226719459, 0.0795839355983204, 0.2399524784201602, 0.26436328933911735, 0.17997784967810082]
maxi score, test score, baseline:  -0.997616894977169 -1.0 -0.997616894977169
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9976220956719818 -1.0 -0.9976220956719818
siam score:  -0.7593956
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[-6.029]
 [-3.241]
 [-3.241]
 [-3.241]
 [-3.241]
 [-3.241]
 [-3.241]] [[0.669]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]]
maxi score, test score, baseline:  -0.9976220956719818 -1.0 -0.9976220956719818
probs:  [0.15918147228880625, 0.07606542489918088, 0.07967528179570585, 0.2402273908419578, 0.2646661310362949, 0.18018429913805453]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[-2.985]
 [-3.066]
 [-3.066]
 [-3.066]
 [-3.066]
 [-3.066]
 [-3.066]] [[0.596]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.7549466
769 694
maxi score, test score, baseline:  -0.9976220956719818 -1.0 -0.9976220956719818
probs:  [0.15854647781657694, 0.07612287026075042, 0.07973545335692406, 0.24040881294462949, 0.26486600951738853, 0.18032037610373058]
maxi score, test score, baseline:  -0.9976324263038548 -1.0 -0.9976324263038548
maxi score, test score, baseline:  -0.9976324263038548 -1.0 -0.9976324263038548
probs:  [0.15854672621475452, 0.07612306711418268, 0.07973566099021645, 0.24040843649981897, 0.2648655190873174, 0.18032059009371004]
line 256 mcts: sample exp_bonus 3.3567537703032437
maxi score, test score, baseline:  -0.9976324263038548 -1.0 -0.9976324263038548
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.384]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.453]] [[4.617]
 [6.056]
 [4.735]
 [4.735]
 [4.735]
 [4.735]
 [4.622]] [[0.128]
 [0.537]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.196]]
maxi score, test score, baseline:  -0.9976324263038548 -1.0 -0.9976324263038548
maxi score, test score, baseline:  -0.9976324263038548 -1.0 -0.9976324263038548
probs:  [0.15854672621475452, 0.07612306711418268, 0.07973566099021645, 0.24040843649981897, 0.2648655190873174, 0.18032059009371004]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
main train batch thing paused
add a thread
Adding thread: now have 4 threads
main train batch thing paused
add a thread
Adding thread: now have 5 threads
using another actor
line 256 mcts: sample exp_bonus -3.344091517834864
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.301]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]] [[-2.434]
 [-0.994]
 [-2.478]
 [-2.478]
 [-2.478]
 [-2.478]
 [-2.478]] [[0.273]
 [0.301]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.377]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.368]] [[1.938]
 [3.158]
 [1.97 ]
 [1.97 ]
 [1.97 ]
 [1.97 ]
 [1.884]] [[0.349]
 [0.377]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.368]]
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.307]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]] [[-2.623]
 [-0.879]
 [-2.623]
 [-2.623]
 [-2.623]
 [-2.623]
 [-2.623]] [[0.273]
 [0.307]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]]
maxi score, test score, baseline:  -0.997637556561086 -1.0 -0.997637556561086
probs:  [0.15874810561164998, 0.07579898516315779, 0.08017077706942706, 0.24049267666186866, 0.2662365717357135, 0.17855288375818296]
siam score:  -0.74814147
using explorer policy with actor:  1
from probs:  [0.15812381509897935, 0.07585545105423178, 0.08023050143791853, 0.2406708453607747, 0.2664337378015747, 0.17868564924652086]
779 706
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.498]
 [0.511]
 [0.516]
 [0.518]
 [0.513]
 [0.51 ]] [[-1.258]
 [-0.14 ]
 [-0.986]
 [-1.118]
 [-1.095]
 [-1.1  ]
 [-0.828]] [[0.509]
 [0.498]
 [0.511]
 [0.516]
 [0.518]
 [0.513]
 [0.51 ]]
line 256 mcts: sample exp_bonus 2.514052863637823
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.886]
 [0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.887]] [[1.293]
 [1.446]
 [1.446]
 [1.446]
 [1.446]
 [1.446]
 [1.158]] [[0.793]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.683]]
using explorer policy with actor:  1
from probs:  [0.15962098656315063, 0.07655250416200184, 0.0796960413156045, 0.24159603328593288, 0.2658071490786449, 0.17672728559466533]
line 256 mcts: sample exp_bonus 2.4422304445735863
from probs:  [0.15948949529199097, 0.0768451940644887, 0.07959192541681111, 0.24128960233863112, 0.2653809208197476, 0.1774028620683305]
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.1599728893775049, 0.07670034157603646, 0.07983323854951124, 0.24079933236983495, 0.2647537143734837, 0.17794048375362875]
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.1599728893775049, 0.07670034157603646, 0.07983323854951124, 0.24079933236983495, 0.2647537143734837, 0.17794048375362875]
Printing some Q and Qe and total Qs values:  [[1.297]
 [1.342]
 [1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]] [[0.934]
 [1.077]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]] [[2.182]
 [2.257]
 [2.069]
 [2.069]
 [2.069]
 [2.069]
 [2.069]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.755]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]] [[-0.193]
 [ 2.145]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]] [[1.165]
 [1.806]
 [1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.336]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[-1.446]
 [-1.01 ]
 [-1.446]
 [-1.446]
 [-1.446]
 [-1.446]
 [-1.446]] [[0.322]
 [0.336]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]]
maxi score, test score, baseline:  -0.9976777777777778 -1.0 -0.9976777777777778
probs:  [0.1594363107534015, 0.07677444975913161, 0.07989381883154403, 0.23860242059534081, 0.2663077749263519, 0.17898522513423007]
line 256 mcts: sample exp_bonus -0.3116609729664979
maxi score, test score, baseline:  -0.9976777777777778 -1.0 -0.9976777777777778
probs:  [0.15907624658448397, 0.07695230447019129, 0.07967397350075679, 0.2379729152997243, 0.26692470012101066, 0.17939986002383293]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9976777777777778 -1.0 -0.9976777777777778
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.9011376246245377
siam score:  -0.72320044
from probs:  [0.15841485502813332, 0.07545713237369368, 0.0803846390609359, 0.24012195049507187, 0.2663720076807882, 0.17924941536137684]
789 757
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.615]
 [0.696]
 [0.734]
 [0.597]
 [0.597]
 [0.491]] [[ 1.377]
 [ 1.138]
 [-2.574]
 [-1.868]
 [ 0.658]
 [ 0.658]
 [ 1.694]] [[1.873]
 [1.909]
 [0.373]
 [0.736]
 [1.673]
 [1.673]
 [1.991]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.652]
 [0.626]
 [0.695]
 [0.626]
 [0.626]
 [0.626]] [[ 1.574]
 [ 0.032]
 [ 0.6  ]
 [-1.447]
 [ 0.6  ]
 [ 0.6  ]
 [ 0.6  ]] [[2.314]
 [1.378]
 [1.687]
 [0.525]
 [1.687]
 [1.687]
 [1.687]]
maxi score, test score, baseline:  -0.9976876106194691 -1.0 -0.9976876106194691
probs:  [0.15934753850714278, 0.07588418681413175, 0.08041151357455656, 0.24143007777129252, 0.26356581010104246, 0.17936087323183403]
line 256 mcts: sample exp_bonus 0.9115832740203313
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.5961426657335454
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.466]] [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]] [[2.953]
 [2.953]
 [2.953]
 [2.953]
 [2.953]
 [2.953]
 [2.953]]
maxi score, test score, baseline:  -0.9976924944812362 -1.0 -0.9976924944812362
siam score:  -0.716974
line 256 mcts: sample exp_bonus 0.9335066428815935
maxi score, test score, baseline:  -0.9976924944812362 -1.0 -0.9976924944812362
probs:  [0.16006350681266648, 0.07620806096669164, 0.07954985944042342, 0.24123496980095577, 0.26456073166103666, 0.17838287131822617]
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.7188988
siam score:  -0.7186255
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
probs:  [0.15955896713280582, 0.0766425517811638, 0.0796151883339791, 0.24260846772680356, 0.26470357879266343, 0.17687124623258424]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 1.7999319386791877
maxi score, test score, baseline:  -0.9977165938864629 -1.0 -0.9977165938864629
maxi score, test score, baseline:  -0.9977165938864629 -1.0 -0.9977165938864629
from probs:  [0.16020099271584057, 0.07695097802598584, 0.07993557770333685, 0.24358431172154701, 0.2617452391176758, 0.1775829007156139]
siam score:  -0.7126187
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
801 786
maxi score, test score, baseline:  -0.9977260869565218 -1.0 -0.9977260869565218
maxi score, test score, baseline:  -0.9977401727861771 -1.0 -0.9977401727861771
maxi score, test score, baseline:  -0.9977401727861771 -1.0 -0.9977401727861771
probs:  [0.1599201536806594, 0.07644142528017159, 0.07975978571074771, 0.2442254834688281, 0.26243404263587056, 0.17721910922372267]
maxi score, test score, baseline:  -0.9977401727861771 -1.0 -0.9977401727861771
probs:  [0.15938986051527018, 0.07616992098980313, 0.07984353109533382, 0.24448191290700386, 0.26270959051545906, 0.17740518397712998]
804 795
maxi score, test score, baseline:  -0.9977401727861771 -1.0 -0.9977401727861771
maxi score, test score, baseline:  -0.9977401727861771 -1.0 -0.9977401727861771
maxi score, test score, baseline:  -0.9977401727861771 -1.0 -0.9977401727861771
probs:  [0.1603653404676246, 0.07628547691576938, 0.07956411438274381, 0.2436170742052927, 0.26167707473644525, 0.1784909192921242]
maxi score, test score, baseline:  -0.9977448275862069 -1.0 -0.9977448275862069
maxi score, test score, baseline:  -0.9977448275862069 -1.0 -0.9977448275862069
Printing some Q and Qe and total Qs values:  [[0.924]
 [1.295]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]] [[0.263]
 [1.785]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[1.368]
 [2.589]
 [1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.368]]
using explorer policy with actor:  0
siam score:  -0.7092659
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.573]
 [0.686]
 [0.573]
 [0.573]
 [0.74 ]
 [0.584]] [[1.556]
 [1.898]
 [1.528]
 [1.898]
 [1.898]
 [0.986]
 [1.965]] [[1.95 ]
 [2.02 ]
 [1.972]
 [2.02 ]
 [2.02 ]
 [1.829]
 [2.049]]
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.145]
 [0.119]
 [0.099]
 [0.081]
 [0.062]
 [0.049]] [[-2.904]
 [ 0.   ]
 [-2.312]
 [-2.567]
 [-2.78 ]
 [-2.811]
 [-3.081]] [[0.128]
 [0.145]
 [0.119]
 [0.099]
 [0.081]
 [0.062]
 [0.049]]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.595]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.383]] [[1.76 ]
 [2.17 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.878]] [[1.291]
 [1.884]
 [1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.167]]
first move QE:  0.9023470243540163
siam score:  -0.7080326
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 1.351361027796369
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.7470952211866897
line 256 mcts: sample exp_bonus -4.0758362927187335
maxi score, test score, baseline:  -0.9977813559322034 -1.0 -0.9977813559322034
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus -1.308571783306565
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.307]
 [0.36 ]
 [0.35 ]
 [0.307]
 [0.311]
 [0.308]] [[-3.213]
 [-3.45 ]
 [-2.916]
 [-3.081]
 [-3.45 ]
 [-3.311]
 [-3.478]] [[0.329]
 [0.307]
 [0.36 ]
 [0.35 ]
 [0.307]
 [0.311]
 [0.308]]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 1.697861552781166
818 834
maxi score, test score, baseline:  -0.9977947368421053 -1.0 -0.9977947368421053
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4893],
        [-0.4039],
        [-0.4235],
        [-0.3528],
        [-0.5476],
        [-0.4117],
        [-0.4241],
        [-0.4324],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.0530787758985 -0.5423532318376669
-0.0727797758985 -0.47668849284429604
-0.0727797758985 -0.49628289905448175
-0.0632698753995 -0.4160445053590886
-0.024259925299500003 -0.57182541103624
-0.0727797758985 -0.48444635931424057
-0.0727797758985 -0.496830664764835
-0.0727797758985 -0.5052159237407231
-0.9702 -0.9702
-0.945846 -0.945846
using another actor
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.686]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[-3.454]
 [ 0.043]
 [-3.454]
 [-3.454]
 [-3.454]
 [-3.454]
 [-3.454]] [[0.32 ]
 [1.294]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]]
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[2.732]
 [2.732]
 [2.732]
 [2.732]
 [2.732]
 [2.732]
 [2.732]] [[2.128]
 [2.128]
 [2.128]
 [2.128]
 [2.128]
 [2.128]
 [2.128]]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.855]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[1.443]
 [2.406]
 [1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]] [[0.656]
 [1.524]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]]
line 256 mcts: sample exp_bonus 2.1890211570853695
first move QE:  0.869446150063619
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5957628647215896
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
probs:  [0.16046162394090274, 0.07718939233434735, 0.08002557190529644, 0.23945009888656116, 0.26401227083157075, 0.17886104210132148]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.591]
 [0.595]
 [0.59 ]
 [0.583]
 [0.597]
 [0.555]] [[2.339]
 [2.588]
 [2.132]
 [1.939]
 [0.977]
 [2.107]
 [2.527]] [[0.557]
 [0.591]
 [0.595]
 [0.59 ]
 [0.583]
 [0.597]
 [0.555]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
probs:  [0.16046162394090274, 0.07718939233434735, 0.08002557190529644, 0.23945009888656116, 0.26401227083157075, 0.17886104210132148]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.026]
 [0.49 ]
 [0.49 ]] [[1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.227]
 [1.349]
 [1.349]] [[1.076]
 [1.076]
 [1.076]
 [1.076]
 [0.067]
 [1.076]
 [1.076]]
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
Printing some Q and Qe and total Qs values:  [[0.696]
 [1.2  ]
 [0.861]
 [1.106]
 [1.106]
 [0.661]
 [0.669]] [[1.574]
 [1.129]
 [1.228]
 [1.248]
 [1.248]
 [1.029]
 [1.307]] [[1.35 ]
 [1.78 ]
 [1.338]
 [1.727]
 [1.727]
 [0.879]
 [1.105]]
Printing some Q and Qe and total Qs values:  [[1.188]
 [1.265]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.187]] [[2.373]
 [2.061]
 [2.373]
 [2.373]
 [2.373]
 [2.373]
 [2.122]] [[2.463]
 [2.353]
 [2.463]
 [2.463]
 [2.463]
 [2.463]
 [2.288]]
Printing some Q and Qe and total Qs values:  [[0.323]
 [1.353]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]] [[1.941]
 [1.447]
 [1.941]
 [1.941]
 [1.941]
 [1.941]
 [1.941]] [[0.958]
 [2.038]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]]
maxi score, test score, baseline:  -0.9978338842975206 -1.0 -0.9978338842975206
maxi score, test score, baseline:  -0.9978338842975206 -1.0 -0.9978338842975206
probs:  [0.161184334926609, 0.07650667459020422, 0.0785564279645269, 0.23830592924823776, 0.26502990315968084, 0.1804167301107412]
Printing some Q and Qe and total Qs values:  [[0.654]
 [1.075]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[-2.316]
 [-0.174]
 [-2.316]
 [-2.316]
 [-2.316]
 [-2.316]
 [-2.316]] [[0.881]
 [2.266]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]]
siam score:  -0.6976332
start point for exploration sampling:  10749
using another actor
siam score:  -0.6972746
from probs:  [0.1601971115600955, 0.0766703032884268, 0.0787244405740633, 0.2388156062037515, 0.2655967360311442, 0.17999580234251877]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9978423868312757 -1.0 -0.9978423868312757
probs:  [0.1600111456815035, 0.07689706253880055, 0.07825810846594505, 0.2395211150131988, 0.26638129959975965, 0.17893126870079235]
maxi score, test score, baseline:  -0.9978423868312757 -1.0 -0.9978423868312757
probs:  [0.16013728546087155, 0.07695768193166459, 0.07831980079677368, 0.23970993398873372, 0.2665912929612606, 0.1782840048606958]
siam score:  -0.69197905
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.02  0.02  0.02  0.02  0.878]
maxi score, test score, baseline:  -0.9978508196721312 -1.0 -0.9978508196721312
probs:  [0.1603102190880483, 0.07636174960112015, 0.07803556880627083, 0.24095328864250154, 0.2666938987215667, 0.17764527514049255]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.668]
 [0.668]
 [0.672]
 [0.674]
 [0.668]
 [0.676]] [[-2.664]
 [-2.727]
 [-2.727]
 [-2.679]
 [-2.657]
 [-2.727]
 [-2.49 ]] [[0.798]
 [0.742]
 [0.742]
 [0.788]
 [0.81 ]
 [0.742]
 [0.959]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.062]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.255]] [[-1.96 ]
 [ 1.431]
 [-1.602]
 [-1.602]
 [-1.602]
 [-1.602]
 [ 0.945]] [[0.787]
 [1.824]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [1.732]]
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.529]
 [0.652]
 [0.65 ]
 [0.646]
 [0.643]
 [0.64 ]] [[-2.665]
 [-3.17 ]
 [-2.702]
 [-2.856]
 [-2.859]
 [-2.868]
 [-2.855]] [[0.852]
 [0.183]
 [0.737]
 [0.595]
 [0.589]
 [0.577]
 [0.586]]
from probs:  [0.1603106304579468, 0.07633182224560611, 0.07798978763958808, 0.2386733233663925, 0.2675963693575177, 0.17909806693294897]
UNIT TEST: sample policy line 217 mcts : [0.041 0.816 0.02  0.02  0.02  0.02  0.061]
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.16080644096155097, 0.07656790215571878, 0.07823099532353958, 0.23836902652304498, 0.26715401684286366, 0.17887161819328196]
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.16085855578592367, 0.07626863241682347, 0.07825634875190607, 0.23844627815475117, 0.2672405972346998, 0.17892958765589592]
siam score:  -0.70023
Printing some Q and Qe and total Qs values:  [[0.632]
 [1.452]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[1.276]
 [1.151]
 [1.276]
 [1.276]
 [1.276]
 [1.276]
 [1.276]] [[1.213]
 [2.526]
 [1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.449]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[-0.452]
 [ 1.114]
 [-0.616]
 [-0.616]
 [-0.616]
 [-0.616]
 [-0.616]] [[0.429]
 [0.449]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]]
siam score:  -0.70012206
UNIT TEST: sample policy line 217 mcts : [0.102 0.286 0.122 0.082 0.102 0.122 0.184]
using another actor
maxi score, test score, baseline:  -0.9978716024340771 -1.0 -0.9978716024340771
probs:  [0.15995008854084455, 0.07645290200310807, 0.07810598851795296, 0.23902080569413245, 0.2678843823826492, 0.17858583286131277]
maxi score, test score, baseline:  -0.9978716024340771 -1.0 -0.9978716024340771
probs:  [0.15995008854084455, 0.07645290200310807, 0.07810598851795296, 0.23902080569413245, 0.2678843823826492, 0.17858583286131277]
maxi score, test score, baseline:  -0.9978757085020243 -1.0 -0.9978757085020243
probs:  [0.15957720454047616, 0.07658138680264774, 0.07823725176154157, 0.23838417873309686, 0.26833411500885257, 0.17888586315338512]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9978757085020243 -1.0 -0.9978757085020243
Printing some Q and Qe and total Qs values:  [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]] [[-4.297]
 [-4.297]
 [-4.297]
 [-4.297]
 [-4.297]
 [-4.297]
 [-4.297]] [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]]
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.133]
 [0.109]
 [0.107]
 [0.104]
 [0.1  ]
 [0.102]] [[1.201]
 [1.074]
 [0.951]
 [0.975]
 [0.963]
 [1.028]
 [1.141]] [[1.053]
 [1.039]
 [0.982]
 [0.983]
 [0.977]
 [0.986]
 [1.013]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.362]
 [0.345]
 [0.346]
 [0.344]
 [0.345]
 [0.347]] [[-4.582]
 [-4.318]
 [-4.84 ]
 [-4.888]
 [-4.82 ]
 [-4.609]
 [-4.647]] [[0.349]
 [0.362]
 [0.345]
 [0.346]
 [0.344]
 [0.345]
 [0.347]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9978919678714859 -1.0 -0.9978919678714859
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.747]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.596]] [[0.98 ]
 [3.687]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [1.854]] [[1.368]
 [1.982]
 [1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.478]]
from probs:  [0.1594143335752041, 0.07711081161882714, 0.07809808754083909, 0.23878441232496214, 0.2673063079204464, 0.17928604701972112]
first move QE:  0.80729432197118
849 914
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.044]] [[-4.959]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-4.895]] [[0.044]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.044]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16028273743621232, 0.07688324098768355, 0.07753086999772868, 0.23905791069485216, 0.26751476638533245, 0.17873047449819085]
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.323]
 [0.316]
 [0.316]
 [0.311]
 [0.32 ]
 [0.32 ]] [[ 0.324]
 [ 1.659]
 [ 0.393]
 [ 0.564]
 [-0.106]
 [ 0.322]
 [ 0.233]] [[0.304]
 [0.323]
 [0.316]
 [0.316]
 [0.311]
 [0.32 ]
 [0.32 ]]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.14 ]
 [0.253]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[-4.622]
 [-2.605]
 [-3.795]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.196]
 [0.14 ]
 [0.253]
 [0.204]
 [0.204]
 [0.204]
 [0.204]]
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.455]
 [0.413]
 [0.426]
 [0.433]
 [0.413]
 [0.433]] [[-3.591]
 [ 0.024]
 [-2.88 ]
 [-3.564]
 [-3.416]
 [-2.88 ]
 [-3.21 ]] [[0.427]
 [0.455]
 [0.413]
 [0.426]
 [0.433]
 [0.413]
 [0.433]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15976243603538612, 0.07720151615612672, 0.07720151615612672, 0.23999831581956574, 0.26714510700024147, 0.17869110883255335]
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.801]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[4.265]
 [4.107]
 [4.265]
 [4.265]
 [4.265]
 [4.265]
 [4.265]] [[1.053]
 [1.021]
 [1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.15959317803450918, 0.07772014864184774, 0.07772014864184774, 0.23857492655732748, 0.26650005931897647, 0.17989153880549136]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15975211304225756, 0.07779754826869098, 0.07779754826869098, 0.2378166416391511, 0.2667654603186586, 0.18007068846255084]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.359]] [[-5.908]
 [-4.256]
 [-4.256]
 [-4.256]
 [-4.256]
 [-4.256]
 [-4.429]] [[0.623]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.359]]
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.724]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]] [[0.848]
 [2.724]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]] [[1.175]
 [1.808]
 [1.175]
 [1.175]
 [1.175]
 [1.175]
 [1.175]]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[-2.177]
 [-2.177]
 [-2.177]
 [-2.177]
 [-2.177]
 [-2.177]
 [-2.177]] [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]]
siam score:  -0.689406
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.4858],
        [-0.0000],
        [-0.4410],
        [-0.4425],
        [-0.0000],
        [-0.4453],
        [-0.3504],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.9702 -0.9702
-0.0727797758985 -0.5585903406076816
-0.8464499999999999 -0.8464499999999999
-0.0727797758985 -0.5138154872693528
-0.0727797758985 -0.5152841547382032
-0.955892025 -0.955892025
-0.0727797758985 -0.5181203824358661
-0.024259925299500003 -0.3746109127101635
-0.9409455 -0.9409455
-0.9752489999999999 -0.9752489999999999
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -1.0729956692178766
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.68772835
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.03 ]
 [0.029]
 [0.021]
 [0.016]
 [0.019]
 [0.019]] [[0.802]
 [0.805]
 [0.763]
 [0.78 ]
 [0.745]
 [0.763]
 [0.783]] [[0.105]
 [0.147]
 [0.074]
 [0.087]
 [0.019]
 [0.054]
 [0.089]]
using explorer policy with actor:  1
using another actor
868 954
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.6859702558811747
line 256 mcts: sample exp_bonus 1.225850606374568
siam score:  -0.69093823
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15881456268189262, 0.07755064086077305, 0.07850928254008122, 0.23703250086686525, 0.26789037992047515, 0.1802026331299128]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.558]
 [0.59 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.587]] [[2.143]
 [3.098]
 [2.222]
 [2.176]
 [2.176]
 [2.176]
 [2.24 ]] [[0.628]
 [0.558]
 [0.59 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.587]]
from probs:  [0.1589157087221233, 0.07728558774721597, 0.07823684592033872, 0.2371834624566829, 0.2680609943193519, 0.18031740083428724]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15908489270444479, 0.07736786711164303, 0.07800005811124158, 0.23743597143167286, 0.26834637596531186, 0.17976483467568588]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.648]
 [0.628]
 [0.542]
 [0.628]
 [0.628]
 [0.587]] [[ 0.004]
 [ 0.681]
 [ 0.004]
 [ 1.661]
 [ 0.004]
 [ 0.004]
 [-0.3  ]] [[1.465]
 [1.732]
 [1.465]
 [1.846]
 [1.465]
 [1.465]
 [1.282]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.549]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[-0.45 ]
 [ 0.341]
 [-0.45 ]
 [-0.45 ]
 [-0.45 ]
 [-0.45 ]
 [-0.45 ]] [[1.709]
 [1.805]
 [1.709]
 [1.709]
 [1.709]
 [1.709]
 [1.709]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -3.45174123309531
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.043]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.066]] [[-3.211]
 [-1.699]
 [-3.638]
 [-3.6  ]
 [-3.577]
 [-3.357]
 [-3.501]] [[0.07 ]
 [0.043]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.066]]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.478]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.476]] [[-1.35 ]
 [-0.054]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-1.481]] [[0.476]
 [0.478]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.476]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15871934261807982, 0.07653485734094269, 0.07777476169643664, 0.2377099911445716, 0.26855078982495645, 0.1807102573750128]
using another actor
Printing some Q and Qe and total Qs values:  [[1.204]
 [1.446]
 [1.204]
 [1.204]
 [1.204]
 [1.204]
 [1.204]] [[1.266]
 [1.242]
 [1.266]
 [1.266]
 [1.266]
 [1.266]
 [1.266]] [[2.651]
 [3.038]
 [2.651]
 [2.651]
 [2.651]
 [2.651]
 [2.651]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.022]
 [0.163]
 [0.164]
 [0.169]
 [0.142]
 [0.214]] [[-4.19 ]
 [-1.347]
 [-2.761]
 [-2.806]
 [-2.765]
 [-2.955]
 [-3.036]] [[0.376]
 [0.022]
 [0.163]
 [0.164]
 [0.169]
 [0.142]
 [0.214]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15786936313527253, 0.0757608143809317, 0.07788998476469729, 0.23999089952572455, 0.2675372336361019, 0.18095170455727203]
from probs:  [0.15786936313527253, 0.0757608143809317, 0.07788998476469729, 0.23999089952572455, 0.2675372336361019, 0.18095170455727203]
using another actor
from probs:  [0.15802359332961755, 0.07583482877418478, 0.07796607924718085, 0.23924840998283775, 0.2677986036620446, 0.1811284850041344]
siam score:  -0.68691564
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.545]
 [0.54 ]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[ 1.688]
 [ 1.688]
 [-1.687]
 [ 1.688]
 [ 1.688]
 [ 1.688]
 [ 1.688]] [[2.682]
 [2.682]
 [0.425]
 [2.682]
 [2.682]
 [2.682]
 [2.682]]
Printing some Q and Qe and total Qs values:  [[1.228]
 [1.058]
 [1.058]
 [1.058]
 [1.058]
 [1.058]
 [1.058]] [[1.053]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]] [[1.264]
 [1.022]
 [1.022]
 [1.022]
 [1.022]
 [1.022]
 [1.022]]
880 994
from probs:  [0.1582937603692698, 0.07596448075058125, 0.07809937494296704, 0.23868681430377545, 0.268256448939765, 0.1806991206936416]
siam score:  -0.68873245
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.737]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.726]] [[2.184]
 [2.73 ]
 [2.184]
 [2.184]
 [2.184]
 [2.184]
 [2.167]] [[1.195]
 [1.444]
 [1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.234]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1583405126621207, 0.07569156551311482, 0.07812244170719256, 0.2387573108022853, 0.2683356788729647, 0.18075249044232194]
UNIT TEST: sample policy line 217 mcts : [0.02  0.49  0.02  0.02  0.02  0.184 0.245]
siam score:  -0.68403083
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.369]
 [0.486]
 [0.591]
 [0.34 ]
 [0.219]
 [0.176]] [[ 2.573]
 [ 1.813]
 [ 1.117]
 [-0.821]
 [ 1.847]
 [ 2.49 ]
 [ 1.917]] [[1.93 ]
 [1.851]
 [1.713]
 [1.061]
 [1.83 ]
 [1.942]
 [1.661]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15801411824042802, 0.07581511948600282, 0.07793687664809586, 0.23914704265821538, 0.2687736925270305, 0.18031315044022758]
siam score:  -0.6841803
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15806050673644828, 0.07554380482672356, 0.07795975672066457, 0.2392172494964406, 0.2688525969154799, 0.18036608530424314]
Printing some Q and Qe and total Qs values:  [[1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.466]] [[0.709]
 [0.709]
 [0.718]
 [0.718]
 [0.718]
 [0.727]
 [0.713]] [[2.406]
 [2.406]
 [2.418]
 [2.418]
 [2.418]
 [2.431]
 [2.412]]
UNIT TEST: sample policy line 217 mcts : [0.163 0.061 0.286 0.082 0.245 0.061 0.102]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.637]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]] [[-1.211]
 [-0.347]
 [-1.211]
 [-1.211]
 [-1.211]
 [-1.211]
 [-1.211]] [[1.748]
 [2.31 ]
 [1.748]
 [1.748]
 [1.748]
 [1.748]
 [1.748]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1576441276619714, 0.07531747073854259, 0.07801568657842847, 0.23841548790174624, 0.2701217485111006, 0.18048547860821074]
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]] [[-2.51]
 [-2.51]
 [-2.51]
 [-2.51]
 [-2.51]
 [-2.51]
 [-2.51]] [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[-0.37 ]
 [-0.386]
 [-0.386]
 [-0.386]
 [-0.386]
 [-0.386]
 [-0.386]] [[0.399]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15691325763315805, 0.07551698288379301, 0.07791197476341598, 0.23904703839044134, 0.26964717087743884, 0.18096357545175287]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[-2.041]
 [-2.041]
 [-2.041]
 [-2.041]
 [-2.041]
 [-2.041]
 [-2.041]] [[0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.436]
 [0.467]
 [0.429]
 [0.292]
 [0.504]
 [0.313]] [[ 1.273]
 [ 0.114]
 [-2.961]
 [-1.853]
 [ 0.024]
 [-2.094]
 [ 0.193]] [[0.254]
 [0.436]
 [0.467]
 [0.429]
 [0.292]
 [0.504]
 [0.313]]
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.68 ]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]] [[2.359]
 [2.586]
 [2.359]
 [2.359]
 [2.359]
 [2.359]
 [2.359]] [[0.681]
 [0.68 ]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[-1.818]
 [-2.119]
 [-2.119]
 [-2.119]
 [-2.119]
 [-2.119]
 [-2.119]] [[0.853]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]]
Printing some Q and Qe and total Qs values:  [[0.907]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.725]] [[-3.   ]
 [-2.913]
 [-2.913]
 [-2.913]
 [-2.913]
 [-2.913]
 [-2.659]] [[0.907]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.725]]
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]] [[-3.45]
 [-3.45]
 [-3.45]
 [-3.45]
 [-3.45]
 [-3.45]
 [-3.45]] [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.653]
 [0.67 ]
 [0.683]
 [0.685]
 [0.682]
 [0.662]] [[2.451]
 [2.611]
 [2.7  ]
 [2.517]
 [2.469]
 [2.706]
 [2.528]] [[0.674]
 [0.653]
 [0.67 ]
 [0.683]
 [0.685]
 [0.682]
 [0.662]]
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.822]
 [0.702]
 [0.703]
 [0.711]
 [0.702]
 [0.701]] [[-2.866]
 [ 0.   ]
 [-3.444]
 [-3.482]
 [-3.239]
 [-3.393]
 [-3.353]] [[0.83 ]
 [0.822]
 [0.702]
 [0.703]
 [0.711]
 [0.702]
 [0.701]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.698]] [[-2.204]
 [-1.551]
 [-1.551]
 [-1.551]
 [-1.551]
 [-1.551]
 [-1.395]] [[0.707]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.698]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]] [[-1.071]
 [-1.527]
 [-1.527]
 [-1.527]
 [-1.527]
 [-1.527]
 [-1.527]] [[0.738]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.672]
 [0.7  ]
 [0.711]
 [0.707]
 [0.707]
 [0.704]] [[2.212]
 [2.418]
 [1.684]
 [1.797]
 [1.818]
 [1.812]
 [1.891]] [[0.706]
 [0.672]
 [0.7  ]
 [0.711]
 [0.707]
 [0.707]
 [0.704]]
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.691]
 [0.711]
 [0.7  ]
 [0.709]
 [0.694]
 [0.693]] [[-2.116]
 [-0.368]
 [-1.863]
 [ 0.   ]
 [-2.001]
 [-2.122]
 [-2.101]] [[0.709]
 [0.691]
 [0.711]
 [0.7  ]
 [0.709]
 [0.694]
 [0.693]]
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.824]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]] [[2.12 ]
 [2.691]
 [2.12 ]
 [2.12 ]
 [2.12 ]
 [2.12 ]
 [2.12 ]] [[1.871]
 [2.064]
 [1.871]
 [1.871]
 [1.871]
 [1.871]
 [1.871]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15721172185418095, 0.07562962846181973, 0.07831656458784257, 0.23836937897482632, 0.2685824886799694, 0.18189021744136097]
using explorer policy with actor:  1
from probs:  [0.1567354351034234, 0.07567236919627837, 0.07836082379628827, 0.23850408917421567, 0.26873427327897353, 0.18199300945082073]
890 1024
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]] [[-2.208]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.623]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[-1.501]
 [-1.612]
 [-1.612]
 [-1.612]
 [-1.612]
 [-1.612]
 [-1.612]] [[0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]]
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.65 ]
 [0.794]
 [0.709]
 [0.713]
 [0.708]
 [0.706]] [[-1.224]
 [ 0.574]
 [-1.484]
 [ 0.   ]
 [-2.143]
 [-2.176]
 [-2.114]] [[0.709]
 [0.65 ]
 [0.794]
 [0.709]
 [0.713]
 [0.708]
 [0.706]]
from probs:  [0.1567206657699001, 0.07593705641879551, 0.07801853655500368, 0.23933833006812455, 0.2673558256039436, 0.1826295855842325]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]] [[0.813]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]]
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.329]
 [0.305]
 [0.372]
 [0.305]
 [0.305]
 [0.354]] [[ 0.508]
 [-0.903]
 [ 0.   ]
 [-1.935]
 [ 0.   ]
 [ 0.   ]
 [-0.945]] [[0.248]
 [0.329]
 [0.305]
 [0.372]
 [0.305]
 [0.305]
 [0.354]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.666402170038961
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15718730204025091, 0.07558748730309578, 0.07794543955971513, 0.2391021579764202, 0.26700424737650846, 0.18317336574400966]
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [0.628]
 [0.637]
 [0.635]
 [0.634]
 [0.632]
 [0.66 ]] [[-1.703]
 [-0.211]
 [-3.33 ]
 [-3.269]
 [-3.281]
 [-3.242]
 [-3.142]] [[0.86 ]
 [0.628]
 [0.637]
 [0.635]
 [0.634]
 [0.632]
 [0.66 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.876]] [[0.661]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.823]] [[0.994]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.876]]
siam score:  -0.6644909
Printing some Q and Qe and total Qs values:  [[0.975]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[-0.334]
 [-2.106]
 [-2.106]
 [-2.106]
 [-2.106]
 [-2.106]
 [-2.106]] [[0.975]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.524]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[-0.726]
 [ 1.829]
 [-0.726]
 [-0.726]
 [-0.726]
 [-0.726]
 [-0.726]] [[0.491]
 [0.524]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.697]
 [0.711]
 [0.705]
 [0.701]
 [0.706]
 [0.703]] [[-1.583]
 [-0.229]
 [-1.977]
 [-2.084]
 [ 0.   ]
 [-1.869]
 [-1.904]] [[0.694]
 [0.697]
 [0.711]
 [0.705]
 [0.701]
 [0.706]
 [0.703]]
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.615]
 [0.623]
 [0.624]
 [0.628]
 [0.622]
 [0.639]] [[-1.92 ]
 [-1.298]
 [-3.303]
 [-3.343]
 [-3.35 ]
 [-3.347]
 [-3.427]] [[0.785]
 [0.615]
 [0.623]
 [0.624]
 [0.628]
 [0.622]
 [0.639]]
line 256 mcts: sample exp_bonus -1.0996636363285872
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]] [[ 0.841]
 [-0.356]
 [-0.356]
 [-0.356]
 [-0.356]
 [-0.356]
 [-0.356]] [[0.929]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]]
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]] [[0.875]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]] [[0.799]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15722251082428546, 0.07502483437212155, 0.07793782727522729, 0.23906684860228464, 0.2668778223508854, 0.18387015657519568]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]] [[-3.781]
 [-4.13 ]
 [-4.13 ]
 [-4.13 ]
 [-4.13 ]
 [-4.13 ]
 [-4.13 ]] [[0.453]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15675140239758853, 0.07506677288155994, 0.07798139413321696, 0.2392004857821654, 0.2670270057268332, 0.18397293907863607]
Printing some Q and Qe and total Qs values:  [[0.976]
 [0.797]
 [0.814]
 [0.797]
 [0.797]
 [0.797]
 [0.799]] [[ 0.114]
 [-1.845]
 [-1.627]
 [-1.845]
 [-1.845]
 [-1.845]
 [-1.814]] [[0.976]
 [0.797]
 [0.814]
 [0.797]
 [0.797]
 [0.797]
 [0.799]]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.1783600248503905
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.325]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.409]] [[-3.718]
 [-0.87 ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-3.731]] [[0.402]
 [0.325]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.409]]
first move QE:  0.706144784185766
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]] [[0.654]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[0.994]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]]
Printing some Q and Qe and total Qs values:  [[0.984]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.82 ]] [[ 0.191]
 [-1.485]
 [-1.485]
 [-1.485]
 [-1.485]
 [-1.485]
 [-1.393]] [[0.984]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.82 ]]
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.708]] [[-0.811]
 [-1.336]
 [-1.336]
 [-1.336]
 [-1.336]
 [-1.336]
 [-1.389]] [[0.721]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.708]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.623]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[-3.172]
 [-0.449]
 [-1.918]
 [-1.918]
 [-1.918]
 [-1.918]
 [-1.901]] [[0.617]
 [0.623]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
Printing some Q and Qe and total Qs values:  [[0.93 ]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]] [[0.761]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[0.93 ]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]]
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]] [[0.644]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]] [[0.83 ]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[-2.187]
 [-2.187]
 [-2.187]
 [-2.187]
 [-2.187]
 [-2.187]
 [-2.187]] [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]]
line 256 mcts: sample exp_bonus -1.093049091778702
Printing some Q and Qe and total Qs values:  [[0.846]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]] [[0.719]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]] [[0.846]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]]
line 256 mcts: sample exp_bonus -0.2511189943765466
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.697]] [[-0.665]
 [-1.368]
 [-1.368]
 [-1.368]
 [-1.368]
 [-1.368]
 [-1.254]] [[0.85 ]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.697]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.425]
 [0.44 ]
 [0.439]
 [0.443]
 [0.445]
 [0.45 ]] [[-2.543]
 [-1.538]
 [-2.815]
 [-2.657]
 [-2.696]
 [-2.843]
 [-2.76 ]] [[0.426]
 [0.425]
 [0.44 ]
 [0.439]
 [0.443]
 [0.445]
 [0.45 ]]
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]] [[-0.577]
 [-0.706]
 [-0.706]
 [-0.706]
 [-0.706]
 [-0.706]
 [-0.706]] [[0.844]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.628]
 [0.708]
 [0.686]
 [0.651]
 [0.851]
 [0.626]] [[-0.424]
 [-0.496]
 [-1.611]
 [-1.607]
 [-1.686]
 [ 0.   ]
 [-1.949]] [[0.884]
 [0.628]
 [0.708]
 [0.686]
 [0.651]
 [0.851]
 [0.626]]
Printing some Q and Qe and total Qs values:  [[0.99 ]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]] [[0.399]
 [0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.038]] [[0.99 ]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]]
Printing some Q and Qe and total Qs values:  [[0.995]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]] [[0.671]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[0.995]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.979]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]] [[ 1.049]
 [-0.543]
 [-0.543]
 [-0.543]
 [-0.543]
 [-0.543]
 [-0.543]] [[0.979]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]]
Printing some Q and Qe and total Qs values:  [[0.995]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]] [[0.349]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[0.995]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.4  ]
 [0.448]
 [0.444]
 [0.443]
 [0.443]
 [0.441]] [[-3.691]
 [ 0.837]
 [-3.663]
 [-3.696]
 [-3.71 ]
 [-3.756]
 [-3.594]] [[0.47 ]
 [0.4  ]
 [0.448]
 [0.444]
 [0.443]
 [0.443]
 [0.441]]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]] [[-2.105]
 [-2.189]
 [-2.189]
 [-2.189]
 [-2.189]
 [-2.189]
 [-2.03 ]] [[0.602]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]] [[-2.777]
 [-3.293]
 [-3.293]
 [-3.293]
 [-3.293]
 [-3.293]
 [-3.293]] [[0.73 ]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn probs:  [0.15772521292769867, 0.07525216175038174, 0.07816210293580086, 0.23880646897336516, 0.26641556404732036, 0.18363848936543328]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.649]
 [0.694]
 [0.716]
 [0.377]
 [0.673]
 [0.556]] [[ 1.221]
 [-0.364]
 [-3.507]
 [-2.766]
 [ 0.693]
 [-3.323]
 [ 0.324]] [[2.188]
 [2.261]
 [1.302]
 [1.592]
 [2.071]
 [1.32 ]
 [2.306]]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[1.185]
 [1.185]
 [1.185]
 [1.185]
 [1.185]
 [1.185]
 [1.185]] [[2.463]
 [2.463]
 [2.463]
 [2.463]
 [2.463]
 [2.463]
 [2.463]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15789584575397414, 0.07504313181372657, 0.07852468294581211, 0.23898098007531254, 0.2665254823312774, 0.18302987707989732]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.672]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[-3.249]
 [-1.863]
 [-3.249]
 [-3.249]
 [-3.249]
 [-3.249]
 [-3.249]] [[0.812]
 [1.331]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.965]
 [0.762]
 [0.792]
 [0.722]
 [0.707]
 [0.674]] [[-3.775]
 [ 1.021]
 [-3.503]
 [-3.414]
 [-3.809]
 [-3.966]
 [-3.855]] [[0.183]
 [1.993]
 [0.316]
 [0.36 ]
 [0.191]
 [0.13 ]
 [0.155]]
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]] [[2.514]
 [2.514]
 [2.514]
 [2.514]
 [2.514]
 [2.514]
 [2.514]] [[1.742]
 [1.742]
 [1.742]
 [1.742]
 [1.742]
 [1.742]
 [1.742]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15791104694831667, 0.07531605355638835, 0.07850642200584108, 0.23892077577835613, 0.2663743049399954, 0.18297139677110233]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -4.216990684203067
line 256 mcts: sample exp_bonus 2.4172758611125325
Printing some Q and Qe and total Qs values:  [[ 0.032]
 [-0.023]
 [ 0.021]
 [ 0.045]
 [ 0.055]
 [ 0.061]
 [-0.003]] [[2.764]
 [3.515]
 [3.127]
 [3.192]
 [2.845]
 [3.233]
 [4.222]] [[-0.511]
 [ 0.054]
 [-0.218]
 [-0.133]
 [-0.413]
 [-0.079]
 [ 0.676]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1568358122031594, 0.07559474274298458, 0.07879691639393335, 0.23887931975455223, 0.2662447698925588, 0.18364843901281164]
907 1063
from probs:  [0.15650316918101762, 0.07568158730455805, 0.07917854002539017, 0.2372891422440547, 0.2675342276606118, 0.18381333358436766]
Printing some Q and Qe and total Qs values:  [[0.834]
 [0.846]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]] [[0.974]
 [3.045]
 [0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]] [[1.377]
 [1.84 ]
 [1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.377]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.648268
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 5.024625080514376
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6404454
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15740110191267606, 0.07556037445518694, 0.07902366833902752, 0.23774374246949587, 0.266843332953817, 0.18342777986979658]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.47609415547491646
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15694291594428533, 0.07560146245496989, 0.07906663959864571, 0.2378730220675579, 0.26698843624222107, 0.1835275236923202]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[-1.522]
 [-1.522]
 [-1.522]
 [-1.522]
 [-1.522]
 [-1.522]
 [-1.522]] [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]]
from probs:  [0.15724381513190244, 0.07574640954288714, 0.07891648025909319, 0.23742695162039262, 0.2675003204714969, 0.18316602297422777]
using explorer policy with actor:  1
917 1077
from probs:  [0.15655369729395516, 0.07593468772705862, 0.07881249209076911, 0.2380171091683771, 0.26706070624545436, 0.1836213074743858]
siam score:  -0.64709425
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15629927339529978, 0.07552030963529635, 0.07895416070329016, 0.23844495445936623, 0.26754075856595594, 0.1832405432407916]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.64674395
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 0.5062457538436714
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.722]
 [0.672]
 [0.706]
 [0.695]
 [0.68 ]
 [0.679]] [[ 2.37 ]
 [ 1.143]
 [ 0.694]
 [ 0.711]
 [ 0.386]
 [-0.016]
 [ 1.033]] [[1.877]
 [1.161]
 [0.796]
 [0.854]
 [0.624]
 [0.336]
 [1.031]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.439]] [[-3.436]
 [-2.341]
 [-2.341]
 [-2.341]
 [-2.341]
 [-2.341]
 [-2.382]] [[0.552]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.439]]
921 1084
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
922 1101
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15658218097768553, 0.07588302969255747, 0.0784209083816224, 0.2386152177312208, 0.2685596093939054, 0.18193905382300835]
line 256 mcts: sample exp_bonus 5.128711411246343
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6363661
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15761222012774112, 0.0761088022506538, 0.07893678193320809, 0.23928951919990846, 0.2649167792750533, 0.18313589721343523]
siam score:  -0.6325428
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15761222012774112, 0.0761088022506538, 0.07893678193320809, 0.23928951919990846, 0.2649167792750533, 0.18313589721343523]
first move QE:  0.6560307811566447
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]] [[1.079]
 [1.132]
 [1.132]
 [1.132]
 [1.132]
 [1.132]
 [1.132]] [[0.103]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]]
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.555]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.297]] [[0.926]
 [1.008]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.876]] [[1.673]
 [1.117]
 [1.673]
 [1.673]
 [1.673]
 [1.673]
 [0.557]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.09 ]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]] [[-5.016]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.09 ]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.017]] [[1.364]
 [1.364]
 [1.364]
 [1.364]
 [1.364]
 [1.364]
 [0.65 ]] [[1.369]
 [1.369]
 [1.369]
 [1.369]
 [1.369]
 [1.369]
 [0.034]]
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]] [[-4.783]
 [-4.909]
 [-4.909]
 [-4.909]
 [-4.909]
 [-4.909]
 [-4.909]] [[0.124]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.097]
 [0.096]
 [0.096]
 [0.095]
 [0.096]
 [0.108]] [[-5.031]
 [-4.727]
 [-5.043]
 [-4.961]
 [-4.936]
 [-4.889]
 [-4.996]] [[0.099]
 [0.097]
 [0.096]
 [0.096]
 [0.095]
 [0.096]
 [0.108]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15658577887614628, 0.07636223667646262, 0.07918856945546808, 0.2374060436069663, 0.26675015290640836, 0.18370721847854843]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[2.782]
 [3.673]
 [3.673]
 [3.673]
 [3.673]
 [3.673]
 [3.673]] [[0.807]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.15635539785249325, 0.07623234666041057, 0.079335722304213, 0.23784720545477148, 0.266180733711149, 0.1840485940169628]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.202681016218931
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.403]
 [0.404]
 [0.404]
 [0.407]
 [0.408]
 [0.415]] [[-4.821]
 [-3.983]
 [-4.897]
 [-4.809]
 [-4.799]
 [-4.788]
 [-4.614]] [[0.399]
 [0.403]
 [0.404]
 [0.404]
 [0.407]
 [0.408]
 [0.415]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.15535124258192287, 0.07649779169974788, 0.07961197346071626, 0.23780221054608625, 0.26604732081299676, 0.18468946089853006]
from probs:  [0.15545920629584062, 0.0765509550060797, 0.07966730101512966, 0.2379674748169199, 0.26623221445372636, 0.18412284841230378]
using explorer policy with actor:  0
937 1120
siam score:  -0.6368229
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15502853121215654, 0.07658999230208613, 0.07970792749734028, 0.23808882675503212, 0.26636798004621637, 0.18421674218716846]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.665]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.601]] [[2.037]
 [3.208]
 [2.037]
 [2.037]
 [2.037]
 [2.037]
 [2.588]] [[1.451]
 [1.776]
 [1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.569]]
first move QE:  0.6369511022648513
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15582098777252557, 0.07643853905932016, 0.07952622244068021, 0.2367082068848606, 0.2677295684687436, 0.18377647537386996]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15586284291007188, 0.07619046084769582, 0.07954758401094347, 0.23677178917052785, 0.2678014834146932, 0.18382583964606786]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15593868893704213, 0.0759439900773015, 0.0795547773484572, 0.23766360191832153, 0.26774742824960385, 0.18315151346927389]
siam score:  -0.63536036
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[0.625]
 [0.618]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[2.456]
 [2.44 ]
 [2.456]
 [2.456]
 [2.456]
 [2.456]
 [2.456]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.403]] [[0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [1.643]] [[0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.403]]
950 1151
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15734562704686592, 0.07582242203781621, 0.07965994672438359, 0.23800834709742694, 0.26579276072176117, 0.18337089637174628]
Printing some Q and Qe and total Qs values:  [[1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]] [[0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]] [[2.607]
 [2.607]
 [2.607]
 [2.607]
 [2.607]
 [2.607]
 [2.607]]
siam score:  -0.6363515
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
952 1155
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.061 0.633 0.102 0.041 0.02  0.02  0.122]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6360894
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.401904290454791
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1579751588183741, 0.07532793685801002, 0.08020919107356059, 0.23965926379133684, 0.26550739558056724, 0.1813210538781512]
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[3.691]
 [3.691]
 [3.691]
 [3.691]
 [3.691]
 [3.691]
 [3.691]] [[0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.645]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.623]] [[1.062]
 [1.482]
 [1.062]
 [1.062]
 [1.062]
 [1.062]
 [1.882]] [[0.938]
 [1.155]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [1.295]]
siam score:  -0.64109725
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.082]] [[-0.182]
 [-0.182]
 [-0.182]
 [-0.182]
 [-0.182]
 [-0.182]
 [-0.182]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15714654019307395, 0.07565993168678986, 0.08056269918446544, 0.23986033372521295, 0.26465030020688424, 0.18212019500357357]
from probs:  [0.1580351435676011, 0.07557770879451299, 0.08072715124387936, 0.2386731810729816, 0.26513909991047313, 0.18184771541055172]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15825308895113743, 0.07542919325539638, 0.0805491244544907, 0.23816534099830022, 0.26550475176180166, 0.18209850057887353]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.303]
 [0.367]
 [0.329]
 [0.341]
 [0.374]
 [0.378]] [[-3.018]
 [-0.744]
 [-2.884]
 [-2.221]
 [-2.449]
 [-3.07 ]
 [-2.983]] [[0.389]
 [0.303]
 [0.367]
 [0.329]
 [0.341]
 [0.374]
 [0.378]]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[2.309]
 [2.309]
 [2.309]
 [2.309]
 [2.309]
 [2.309]
 [2.309]] [[0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15829855820475897, 0.07545086555981285, 0.08028494796776418, 0.23823377062811335, 0.26558103654698717, 0.18215082109256334]
using explorer policy with actor:  0
siam score:  -0.6420035
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.818]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.724]] [[2.326]
 [4.662]
 [2.326]
 [2.326]
 [2.326]
 [2.326]
 [2.783]] [[1.642]
 [2.413]
 [1.642]
 [1.642]
 [1.642]
 [1.642]
 [1.792]]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.612]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]] [[0.637]
 [1.69 ]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[0.605]
 [0.612]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15868002431267642, 0.07537165648486714, 0.08044935197431956, 0.23790657543680888, 0.26506666707232945, 0.18252572471899864]
line 256 mcts: sample exp_bonus 1.7589201216843842
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
siam score:  -0.6474738
deleting a thread, now have 4 threads
Frames:  65999 train batches done:  7726 episodes:  2169
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15773283432936708, 0.07537854381689858, 0.08070350204126917, 0.23950585232834978, 0.2648583854704566, 0.1818208820136588]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15731532307671872, 0.07541590890903861, 0.08074350671152547, 0.23962457521403013, 0.2649896755893159, 0.1819110104993712]
first move QE:  0.5835369766949555
deleting a thread, now have 3 threads
Frames:  66119 train batches done:  7750 episodes:  2172
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15648700461217252, 0.07549003911643477, 0.0808228736910528, 0.23986011463422405, 0.26525014768192134, 0.18208982026419454]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64497495
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.692]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[-3.259]
 [-0.953]
 [-3.259]
 [-3.259]
 [-3.259]
 [-3.259]
 [-3.259]] [[0.377]
 [0.998]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.545]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.552]] [[2.365]
 [2.328]
 [2.356]
 [2.356]
 [2.356]
 [2.356]
 [2.351]] [[1.772]
 [1.725]
 [1.746]
 [1.746]
 [1.746]
 [1.746]
 [1.752]]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.579]
 [0.667]
 [0.641]
 [0.504]
 [0.682]
 [0.572]] [[-0.172]
 [-0.311]
 [-1.161]
 [-0.447]
 [ 1.538]
 [-1.268]
 [-0.46 ]] [[0.606]
 [0.674]
 [0.567]
 [0.753]
 [1.141]
 [0.56 ]
 [0.61 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15592051809165494, 0.07568619597776056, 0.08103288764007348, 0.24048337841570555, 0.2649514827444988, 0.18192553713030657]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15571000805314855, 0.07581837980180675, 0.08117440931154071, 0.24090337590887953, 0.26541421309741764, 0.18097961382720684]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3572970480478506
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.449]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]] [[-4.111]
 [-3.107]
 [-4.111]
 [-4.111]
 [-4.111]
 [-4.111]
 [-4.111]] [[0.434]
 [0.449]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15530472052860966, 0.07585477516804054, 0.08121337575164966, 0.2410190176123303, 0.2655416208251294, 0.18106649011424053]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15553189360649244, 0.07596573227667429, 0.08133217119116982, 0.24053522592380494, 0.26593004370824264, 0.1807049332936158]
rdn beta is 0 so we're just using the maxi policy
using another actor
from probs:  [0.1557249576630514, 0.07606002967184337, 0.0814331300269096, 0.2408338058831004, 0.2662601466331714, 0.1796879301219237]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -4.1907371135497895
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[2.457]
 [2.457]
 [2.457]
 [2.457]
 [2.457]
 [2.457]
 [2.457]] [[2.074]
 [2.074]
 [2.074]
 [2.074]
 [2.074]
 [2.074]
 [2.074]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.653358
siam score:  -0.6513794
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.201]
 [0.205]
 [0.197]
 [0.191]
 [0.193]
 [0.189]] [[-3.093]
 [-1.071]
 [-3.21 ]
 [-3.174]
 [-3.502]
 [-3.161]
 [-3.095]] [[0.226]
 [0.201]
 [0.205]
 [0.197]
 [0.191]
 [0.193]
 [0.189]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15626919752424545, 0.0763097803241975, 0.08196951109831686, 0.24075188693552615, 0.26506038821701877, 0.17963923590069514]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.3593],
        [-0.5416],
        [-0.0000],
        [-0.4858],
        [-0.4668],
        [-0.0000],
        [-0.4394],
        [-0.3816],
        [-0.0000],
        [-0.3761]], dtype=torch.float64)
-0.0530787758985 -0.41242253151590996
-0.024259925299500003 -0.5658647842266269
-0.9507464999999999 -0.9507464999999999
-0.0530787758985 -0.538907777740379
-0.0727797758985 -0.5395483191228485
-0.965448 -0.965448
-0.0727797758985 -0.512220722603904
-0.0727797758985 -0.45434643668148045
-0.9702 -0.9702
-0.0727797758985 -0.4489279252715838
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
979 1220
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[-0.998]
 [-2.316]
 [-2.316]
 [-2.316]
 [-2.316]
 [-2.316]
 [-2.316]] [[0.633]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]]
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.399]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.368]] [[ 0.525]
 [ 0.856]
 [-1.209]
 [-1.209]
 [-1.209]
 [-1.209]
 [-0.17 ]] [[0.652]
 [0.399]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.368]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.156398297894734, 0.07637282295235347, 0.08203722946136738, 0.24012464104035128, 0.26527936543621994, 0.17978764321497384]
line 256 mcts: sample exp_bonus 4.81247546798929
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[1.362]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]] [[2.056]
 [1.836]
 [1.836]
 [1.836]
 [1.836]
 [1.836]
 [1.836]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
981 1224
from probs:  [0.15715416039825192, 0.07649096727809629, 0.08128350165004054, 0.23882886177802, 0.265585964036985, 0.18065654485860633]
line 256 mcts: sample exp_bonus 3.458232197133783
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.683]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[-1.106]
 [-0.004]
 [-1.106]
 [-1.106]
 [-1.106]
 [-1.106]
 [-1.106]] [[0.065]
 [0.496]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]]
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]] [[-2.477]
 [-2.349]
 [-2.349]
 [-2.349]
 [-2.349]
 [-2.349]
 [-2.349]] [[0.077]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.299]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[0.892]
 [1.17 ]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]] [[-0.572]
 [-0.46 ]
 [-0.613]
 [-0.613]
 [-0.613]
 [-0.613]
 [-0.613]]
start point for exploration sampling:  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[6.574]
 [6.574]
 [6.574]
 [6.574]
 [6.574]
 [6.574]
 [6.574]] [[1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15725433085733917, 0.07602920662191892, 0.08130165902618211, 0.23971509540513966, 0.26559958720271876, 0.18010012088670152]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
using explorer policy with actor:  1
first move QE:  0.5604774131913174
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15762917707517657, 0.07596462209329964, 0.08093322110563426, 0.24028650285942885, 0.26526561645547475, 0.17992086041098593]
from probs:  [0.15762917707517657, 0.07596462209329964, 0.08093322110563426, 0.24028650285942885, 0.26526561645547475, 0.17992086041098593]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
siam score:  -0.64850837
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.447]
 [0.511]
 [0.552]
 [0.57 ]
 [0.546]
 [0.554]] [[7.079]
 [4.481]
 [7.098]
 [6.623]
 [6.903]
 [6.985]
 [7.074]] [[1.957]
 [0.444]
 [1.962]
 [1.773]
 [1.953]
 [1.958]
 [2.02 ]]
line 256 mcts: sample exp_bonus 6.699311259439444
Printing some Q and Qe and total Qs values:  [[1.44]
 [1.44]
 [1.44]
 [1.44]
 [1.44]
 [1.44]
 [1.44]] [[0.556]
 [0.549]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]] [[2.855]
 [2.851]
 [2.855]
 [2.855]
 [2.855]
 [2.855]
 [2.855]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.693]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]] [[0.321]
 [0.769]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[2.43 ]
 [2.783]
 [2.386]
 [2.386]
 [2.386]
 [2.386]
 [2.386]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15838792373647648, 0.07583049537289563, 0.08101308874982761, 0.24136342602508376, 0.26447679857411033, 0.17892826754160623]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15843171505152298, 0.07585146109449023, 0.0807590059611604, 0.24143015852324665, 0.264549921489896, 0.17897773787968374]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
993 1260
siam score:  -0.6430159
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.084]
 [0.541]
 [0.567]
 [0.529]
 [0.362]
 [0.239]] [[ 0.424]
 [ 1.152]
 [-2.571]
 [-2.588]
 [-2.84 ]
 [ 0.344]
 [ 0.894]] [[1.405]
 [1.701]
 [0.217]
 [0.221]
 [0.089]
 [1.461]
 [1.654]]
from probs:  [0.15902290587584483, 0.07587278402374815, 0.08018799545884761, 0.23975738298110238, 0.26620160740035326, 0.1789573242601037]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15906510238146515, 0.0758737982482452, 0.08069581022776866, 0.23968237888626917, 0.266931968190671, 0.17775094206558092]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.781]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.764]] [[0.793]
 [1.997]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [1.915]] [[0.858]
 [1.674]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [1.589]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1590986692404607, 0.07587102988566773, 0.08039143603413214, 0.24117812049428808, 0.26575460578772037, 0.17770613855773099]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.727]
 [0.506]
 [0.954]
 [0.25 ]
 [0.404]
 [0.682]] [[1.75 ]
 [2.112]
 [1.97 ]
 [2.067]
 [1.996]
 [1.916]
 [2.133]] [[1.466]
 [1.953]
 [1.699]
 [2.132]
 [1.487]
 [1.587]
 [1.923]]
siam score:  -0.63820463
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.836697357344363
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.63644576
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15863881131579224, 0.07585852522286668, 0.0800830119629345, 0.24264202603809082, 0.26456770429240006, 0.17820992116791576]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1586759818058648, 0.07564199062311176, 0.08010177612774676, 0.242698879231488, 0.2646296948679998, 0.17825167734378888]
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4702],
        [-0.0000],
        [-0.0000],
        [-0.4036],
        [-0.3865],
        [-0.0000],
        [-0.0000],
        [-0.3324],
        [-0.2980],
        [-0.0000]], dtype=torch.float64)
-0.024259925299500003 -0.49444579954210643
-0.9256500000000001 -0.9256500000000001
-0.9467202737024999 -0.9467202737024999
-0.0727797758985 -0.4764283198080136
-0.0628797758985 -0.4493670734784079
-0.955892025 -0.955892025
-0.9514752239519999 -0.9514752239519999
-0.024259925299500003 -0.3566869025376441
-0.024259925299500003 -0.3222178696049704
-0.9467202737024999 -0.9467202737024999
from probs:  [0.15891632466466635, 0.07598142416421004, 0.08046122237308387, 0.24218879098334054, 0.2639745038175244, 0.17847773399717481]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15891632466466635, 0.07598142416421004, 0.08046122237308387, 0.24218879098334054, 0.2639745038175244, 0.17847773399717481]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.678]
 [0.697]
 [0.696]
 [0.696]
 [0.699]
 [0.693]] [[3.037]
 [3.582]
 [2.606]
 [2.566]
 [2.646]
 [2.962]
 [2.799]] [[0.31 ]
 [0.493]
 [0.206]
 [0.19 ]
 [0.216]
 [0.328]
 [0.263]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.254]
 [0.254]
 [0.043]
 [0.041]
 [0.254]
 [0.254]] [[1.527]
 [1.126]
 [1.126]
 [0.669]
 [1.111]
 [1.126]
 [1.126]] [[2.022]
 [1.913]
 [1.913]
 [1.523]
 [1.78 ]
 [1.913]
 [1.913]]
siam score:  -0.63258576
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15909466531825855, 0.07603776336949462, 0.08100079199735923, 0.2414458214865497, 0.26387320291574673, 0.17854775491259128]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15909466531825855, 0.07603776336949462, 0.08100079199735923, 0.2414458214865497, 0.26387320291574673, 0.17854775491259128]
first move QE:  0.5012111356521118
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.041 0.694 0.02  0.102 0.02  0.02  0.102]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1595424459474731, 0.07625177580955696, 0.08122877315269776, 0.2413443350240673, 0.26371150045358166, 0.17792116961262341]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.381]
 [0.406]
 [0.484]
 [0.584]
 [0.472]
 [0.592]] [[3.209]
 [4.187]
 [3.092]
 [2.222]
 [2.357]
 [3.912]
 [4.656]] [[0.339]
 [0.473]
 [0.156]
 [0.022]
 [0.266]
 [0.561]
 [1.05 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15875994794263518, 0.07632276911808111, 0.08130440023537543, 0.241569035795361, 0.2639570259080947, 0.17808682100045253]
using another actor
siam score:  -0.63490593
line 256 mcts: sample exp_bonus 0.9781962327957029
Printing some Q and Qe and total Qs values:  [[1.47 ]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.474]
 [1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.093]] [[2.892]
 [0.997]
 [0.997]
 [0.997]
 [0.997]
 [0.997]
 [0.997]]
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.301]
 [0.301]
 [0.296]
 [0.296]
 [0.183]
 [0.304]] [[1.608]
 [2.639]
 [2.234]
 [2.768]
 [2.768]
 [1.666]
 [2.558]] [[0.937]
 [1.59 ]
 [1.36 ]
 [1.658]
 [1.658]
 [0.936]
 [1.546]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
from probs:  [0.15853097683304054, 0.07642330267560465, 0.08112993218878717, 0.24262676381770307, 0.26242243706955753, 0.178866587415307]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15867133437591374, 0.07649096507948361, 0.08120176167071536, 0.2428415767021313, 0.2617694128059126, 0.17902494936584343]
UNIT TEST: sample policy line 217 mcts : [0.898 0.02  0.02  0.02  0.02  0.    0.02 ]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15867133437591374, 0.07649096507948361, 0.08120176167071536, 0.2428415767021313, 0.2617694128059126, 0.17902494936584343]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15867133437591374, 0.07649096507948361, 0.08120176167071536, 0.2428415767021313, 0.2617694128059126, 0.17902494936584343]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.092]
 [0.08 ]
 [0.085]
 [0.076]
 [0.081]
 [0.078]] [[4.165]
 [4.137]
 [4.107]
 [4.177]
 [4.158]
 [4.12 ]
 [4.195]] [[0.753]
 [0.705]
 [0.665]
 [0.731]
 [0.704]
 [0.677]
 [0.737]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.465]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]] [[-3.928]
 [-2.003]
 [-3.928]
 [-3.928]
 [-3.928]
 [-3.928]
 [-3.928]] [[0.436]
 [0.465]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.521]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.509]] [[1.541]
 [2.588]
 [1.541]
 [1.541]
 [1.541]
 [1.541]
 [2.469]] [[0.533]
 [0.521]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.509]]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[1.881]
 [1.881]
 [1.881]
 [1.881]
 [1.881]
 [1.881]
 [1.881]] [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.479]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]] [[-3.264]
 [-0.78 ]
 [-3.264]
 [-3.264]
 [-3.264]
 [-3.264]
 [-3.264]] [[0.495]
 [0.479]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.502]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[-3.943]
 [-1.23 ]
 [-3.943]
 [-3.943]
 [-3.943]
 [-3.943]
 [-3.943]] [[0.465]
 [0.502]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.724]] [[2.286]
 [2.627]
 [2.627]
 [2.627]
 [2.627]
 [2.627]
 [2.253]] [[0.663]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.724]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[1.741]
 [1.741]
 [1.741]
 [1.741]
 [1.741]
 [1.741]
 [1.741]] [[0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]]
first move QE:  0.49536216579915077
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.012]] [[4.054]
 [4.386]
 [4.386]
 [4.012]
 [4.386]
 [4.386]
 [4.23 ]] [[0.641]
 [0.94 ]
 [0.94 ]
 [0.598]
 [0.94 ]
 [0.94 ]
 [0.801]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.4355616905501659
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.55 ]] [[2.05 ]
 [2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.134]] [[0.551]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.55 ]]
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]] [[-1.041]
 [ 0.148]
 [ 0.148]
 [ 0.148]
 [ 0.148]
 [ 0.148]
 [ 0.148]] [[0.656]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15905568029193332, 0.07644329241890589, 0.08087330638518819, 0.24264712381508233, 0.2615219997145705, 0.1794585973743196]
siam score:  -0.6338209
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.56 ]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]] [[0.279]
 [1.702]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]] [[0.595]
 [0.56 ]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.543]
 [0.557]
 [0.568]
 [0.566]
 [0.557]
 [0.552]] [[2.385]
 [3.03 ]
 [2.467]
 [2.659]
 [2.507]
 [2.613]
 [2.557]] [[0.578]
 [0.543]
 [0.557]
 [0.568]
 [0.566]
 [0.557]
 [0.552]]
using explorer policy with actor:  1
using another actor
from probs:  [0.15923099274244698, 0.07651684977021202, 0.08067673150212021, 0.24283764914541597, 0.2616897658594528, 0.17904801098035197]
using explorer policy with actor:  1
siam score:  -0.6369462
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[0.076]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]] [[0.656]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15910765707908675, 0.07667844662323715, 0.0808471136592823, 0.242573170522972, 0.26136746749258816, 0.17942614462283366]
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.773]
 [0.773]
 [0.773]
 [0.609]
 [0.603]
 [0.602]] [[0.165]
 [0.236]
 [0.236]
 [0.236]
 [0.276]
 [0.386]
 [0.464]] [[0.584]
 [0.773]
 [0.773]
 [0.773]
 [0.609]
 [0.603]
 [0.602]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15884598399796634, 0.07677282107066846, 0.08094661882682305, 0.2420984431712554, 0.26168915385175934, 0.17964697908152727]
rdn probs:  [0.15898432604444745, 0.07683968400867751, 0.08101711680620144, 0.24230929139816826, 0.2610461446246792, 0.17980343711782615]
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.96 ]
 [0.503]
 [0.96 ]
 [0.96 ]
 [0.637]
 [0.96 ]
 [0.541]] [[1.857]
 [2.961]
 [1.857]
 [1.857]
 [1.349]
 [1.857]
 [1.775]] [[ 0.68 ]
 [ 0.134]
 [ 0.68 ]
 [ 0.68 ]
 [-0.136]
 [ 0.68 ]
 [-0.185]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.48650902397804097
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.4869893096154805
in main func line 156:  1027
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1591567825349112, 0.07666853063404103, 0.08105162842490303, 0.24242167366971495, 0.2602344703912764, 0.18046691434515333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]] [[1.881]
 [1.881]
 [1.881]
 [1.881]
 [1.881]
 [1.881]
 [1.881]] [[1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1587686785351009, 0.07691822355571351, 0.08105772553846034, 0.2416822746512195, 0.2610819981147703, 0.18049109960473564]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15889420251342906, 0.07674888166048409, 0.08112181052220964, 0.2418733508816544, 0.2612884119451026, 0.1800733424771202]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15889420251342906, 0.07674888166048409, 0.08112181052220964, 0.2418733508816544, 0.2612884119451026, 0.1800733424771202]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1586518725885605, 0.07684931496624409, 0.08122796622668, 0.24218986546373983, 0.2607719949604767, 0.18030898579429885]
siam score:  -0.6440407
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1586518725885605, 0.07684931496624409, 0.08122796622668, 0.24218986546373983, 0.2607719949604767, 0.18030898579429885]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1586518725885605, 0.07684931496624409, 0.08122796622668, 0.24218986546373983, 0.2607719949604767, 0.18030898579429885]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1030 1376
line 256 mcts: sample exp_bonus 1.325161564664993
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65272826
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[-3.424]
 [-4.008]
 [-4.008]
 [-4.008]
 [-4.008]
 [-4.008]
 [-4.008]] [[0.49 ]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15814906819122254, 0.0770392928300287, 0.08117140508551904, 0.24202791311147412, 0.2614166449038003, 0.18019567587795537]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.343]
 [0.321]
 [0.326]
 [0.322]
 [0.355]
 [0.324]] [[2.827]
 [3.108]
 [3.396]
 [3.065]
 [2.976]
 [4.08 ]
 [3.374]] [[-0.001]
 [ 0.168]
 [ 0.221]
 [ 0.119]
 [ 0.082]
 [ 0.517]
 [ 0.219]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15808576158718307, 0.07722510460009846, 0.08136718313466686, 0.2426116624332748, 0.2611903161077183, 0.1795199721370584]
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.261]
 [0.261]
 [0.236]
 [0.243]
 [0.259]
 [0.234]] [[1.837]
 [1.932]
 [1.932]
 [2.063]
 [2.017]
 [2.105]
 [1.975]] [[0.271]
 [0.43 ]
 [0.43 ]
 [0.56 ]
 [0.513]
 [0.65 ]
 [0.444]]
from probs:  [0.15751719805277276, 0.07737904633377506, 0.08127256191385225, 0.24309528832304916, 0.2608580753138417, 0.17987783006270913]
siam score:  -0.64633393
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15714794236005902, 0.07741296115469419, 0.08130818324176393, 0.24320183568387085, 0.2609724080089155, 0.17995666955069642]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.134]
 [0.194]
 [0.203]
 [0.207]
 [0.253]
 [0.287]] [[-5.337]
 [-2.114]
 [-4.007]
 [-3.834]
 [-3.755]
 [-4.028]
 [-3.701]] [[0.378]
 [0.134]
 [0.194]
 [0.203]
 [0.207]
 [0.253]
 [0.287]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15744117378336228, 0.07755741047067378, 0.08120415514708954, 0.24289468997485208, 0.2606101095284111, 0.18029246109561123]
siam score:  -0.641264
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.163]
 [0.22 ]
 [0.221]
 [0.192]
 [0.221]
 [0.22 ]] [[-4.104]
 [-1.94 ]
 [-3.912]
 [-3.923]
 [-3.897]
 [-3.924]
 [-3.87 ]] [[0.247]
 [0.163]
 [0.22 ]
 [0.221]
 [0.192]
 [0.221]
 [0.22 ]]
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.268]
 [0.545]
 [0.619]
 [0.661]
 [0.655]
 [0.42 ]] [[ 0.898]
 [ 2.319]
 [ 1.292]
 [-0.152]
 [-0.446]
 [-0.725]
 [ 4.251]] [[0.861]
 [1.15 ]
 [0.958]
 [0.432]
 [0.347]
 [0.228]
 [2.059]]
from probs:  [0.15769346368503745, 0.07768169150206873, 0.08133427986494679, 0.24252653801643895, 0.2601826581366619, 0.18058136879484607]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1580252463190163, 0.07784513160678966, 0.08099696823539906, 0.24228259607519803, 0.2598887508230207, 0.18096130694057624]
siam score:  -0.6440825
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1582761707710726, 0.07796874000132586, 0.08112558135479682, 0.2419166234483782, 0.2594642334179802, 0.18124865100644647]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15816591387110304, 0.07813111061018994, 0.08104182724279665, 0.24242041701286296, 0.25917106812406643, 0.18106966313898099]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15846442610477188, 0.07804527346247833, 0.08119478040621066, 0.24287794580903843, 0.25800617177281, 0.18141140244469087]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15858320089046898, 0.07810377120142227, 0.08125563881386422, 0.24231045560354808, 0.25819955667637556, 0.1815473768143209]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]] [[-4.084]
 [-3.926]
 [-3.926]
 [-3.926]
 [-3.926]
 [-3.926]
 [-3.926]] [[0.252]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15878952148206382, 0.0782053861025567, 0.0813613543712269, 0.24187985782037225, 0.25853548056354564, 0.18122839966023466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1581711146441966, 0.07833284967770282, 0.08074131599755352, 0.2422740873353725, 0.2589568563574268, 0.1815237759877476]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[3.879]
 [3.967]
 [3.967]
 [3.967]
 [3.967]
 [3.967]
 [3.967]] [[0.235]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.696]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[0.192]
 [1.944]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]] [[1.046]
 [1.577]
 [1.046]
 [1.046]
 [1.046]
 [1.046]
 [1.046]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.66 ]
 [0.719]
 [0.77 ]
 [0.719]
 [0.719]
 [0.793]] [[1.674]
 [1.869]
 [2.396]
 [1.962]
 [2.396]
 [2.396]
 [2.198]] [[1.803]
 [1.795]
 [2.027]
 [1.908]
 [2.027]
 [2.027]
 [2.009]]
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.862]] [[4.391]
 [4.391]
 [4.391]
 [4.391]
 [4.391]
 [4.391]
 [8.387]] [[0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [2.269]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.486]
 [0.581]
 [0.589]
 [0.483]
 [0.614]
 [0.601]] [[3.345]
 [2.688]
 [3.372]
 [3.349]
 [3.532]
 [3.657]
 [3.802]] [[1.542]
 [1.193]
 [1.566]
 [1.561]
 [1.572]
 [1.717]
 [1.774]]
siam score:  -0.64347076
siam score:  -0.6433516
line 256 mcts: sample exp_bonus 0.14703090446188546
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.61 ]
 [0.68 ]
 [0.704]
 [0.704]
 [0.686]
 [0.682]] [[4.225]
 [3.536]
 [4.24 ]
 [4.057]
 [4.057]
 [4.428]
 [4.291]] [[1.94 ]
 [1.158]
 [1.969]
 [1.821]
 [1.821]
 [2.167]
 [2.023]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1580318158309429, 0.07731384950781027, 0.0808662042563316, 0.24264966650618208, 0.2593296430620476, 0.18180882083668556]
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.716]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.578]] [[2.384]
 [2.568]
 [2.384]
 [2.384]
 [2.384]
 [2.384]
 [1.685]] [[2.073]
 [2.16 ]
 [2.073]
 [2.073]
 [2.073]
 [2.073]
 [1.74 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64408606
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15834068709746713, 0.07723910746459978, 0.0807764902216883, 0.24164305154248197, 0.2598364997027733, 0.18216416397098947]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
siam score:  -0.6455966
using explorer policy with actor:  1
from probs:  [0.15832113380384344, 0.07721622928726639, 0.08049579784065145, 0.240809062838972, 0.26051672509810475, 0.18264105113116186]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.874]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[1.522]
 [1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.396]] [[0.77 ]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15906315989616387, 0.07735334645559995, 0.08062815488256958, 0.24048215202875145, 0.2600865337537392, 0.1823866529831759]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6524577
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.014]
 [0.008]
 [0.006]
 [0.007]
 [0.008]
 [0.01 ]] [[0.453]
 [0.454]
 [0.451]
 [0.517]
 [0.479]
 [0.5  ]
 [0.958]] [[-0.536]
 [-0.521]
 [-0.538]
 [-0.43 ]
 [-0.493]
 [-0.456]
 [ 0.313]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15857501690635445, 0.07707865891696628, 0.08055219799197445, 0.2409855083936717, 0.26059294390164217, 0.18221567388939106]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.613]
 [0.426]
 [0.608]
 [0.426]
 [0.594]
 [0.531]] [[1.672]
 [1.719]
 [1.976]
 [1.006]
 [1.976]
 [1.313]
 [1.755]] [[1.034]
 [1.078]
 [1.098]
 [0.624]
 [1.098]
 [0.807]
 [1.038]]
first move QE:  0.44830772843685446
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15812840469891543, 0.07726486848292384, 0.08073568326422088, 0.242261067616094, 0.2595246868598665, 0.18208528907797916]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using another actor
siam score:  -0.65311587
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15741586358843812, 0.0773302637231208, 0.08080401612368465, 0.24246611191395143, 0.2597443426952737, 0.18223940195553126]
first move QE:  0.44673420447533735
1076 1443
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.026]] [[4.005]
 [4.005]
 [4.005]
 [4.005]
 [4.005]
 [4.005]
 [4.048]] [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.219]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15780601958548335, 0.0773007269752231, 0.08100428927759257, 0.24161807740631158, 0.25957980354010407, 0.18269108321528524]
UNIT TEST: sample policy line 217 mcts : [0.102 0.082 0.245 0.102 0.082 0.204 0.184]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15793022649324445, 0.0773615692947398, 0.08082566702557416, 0.24180825161089928, 0.25978411516775707, 0.18229017040778522]
UNIT TEST: sample policy line 217 mcts : [0.02  0.061 0.    0.02  0.    0.02  0.878]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -4.312984818376298
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[-2.716]
 [-2.706]
 [-2.706]
 [-2.706]
 [-2.706]
 [-2.706]
 [-2.706]] [[0.198]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[-4.479]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.338]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[-1.032]
 [-1.068]
 [-1.068]
 [-1.068]
 [-1.068]
 [-1.068]
 [-1.068]] [[0.641]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15781869627427797, 0.07729312142608898, 0.0807431711080937, 0.24228353390992022, 0.26029472963473044, 0.1815667476468886]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.225]
 [0.321]
 [0.321]
 [0.314]
 [0.309]
 [0.307]] [[-1.712]
 [-2.466]
 [-4.508]
 [-4.493]
 [-4.52 ]
 [-4.518]
 [-4.535]] [[0.696]
 [0.225]
 [0.321]
 [0.321]
 [0.314]
 [0.309]
 [0.307]]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.225]
 [0.333]
 [0.321]
 [0.314]
 [0.309]
 [0.309]] [[-2.244]
 [-2.464]
 [-4.245]
 [-4.491]
 [-4.518]
 [-4.516]
 [-4.419]] [[0.678]
 [0.225]
 [0.333]
 [0.321]
 [0.314]
 [0.309]
 [0.309]]
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.208]] [[3.32 ]
 [2.641]
 [2.641]
 [2.641]
 [2.641]
 [2.641]
 [2.355]] [[ 0.589]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.126]]
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[4.186]
 [2.171]
 [2.171]
 [2.171]
 [2.171]
 [2.171]
 [2.171]] [[0.486]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15806019586554207, 0.07719185120344263, 0.08062647925388218, 0.24265428449771864, 0.26069304157304307, 0.1807741476063713]
UNIT TEST: sample policy line 217 mcts : [0.204 0.286 0.082 0.082 0.143 0.102 0.102]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15806019586554207, 0.07719185120344263, 0.08062647925388218, 0.24265428449771864, 0.26069304157304307, 0.1807741476063713]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
1085 1455
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15800223935760052, 0.0769330313775021, 0.08081180792408763, 0.24177097214871612, 0.26129227268388466, 0.1811896765082091]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15800223935760052, 0.0769330313775021, 0.08081180792408763, 0.24177097214871612, 0.26129227268388466, 0.1811896765082091]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]] [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.498]
 [0.511]
 [0.514]
 [0.513]
 [0.514]] [[1.455]
 [1.532]
 [1.5  ]
 [1.346]
 [1.339]
 [1.241]
 [1.389]] [[0.433]
 [0.517]
 [0.482]
 [0.337]
 [0.335]
 [0.228]
 [0.389]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.44]
 [1.44]
 [1.44]
 [1.44]
 [1.44]
 [1.44]
 [1.44]] [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[2.698]
 [2.698]
 [2.698]
 [2.698]
 [2.698]
 [2.698]
 [2.698]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15800217689619156, 0.07713756097659796, 0.08102664940870068, 0.2409860208505536, 0.26117621522089907, 0.18167137664705701]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.57520885050848
siam score:  -0.64914703
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15804010075861116, 0.077156075621822, 0.08080607637863438, 0.24104386258969485, 0.2612389030334842, 0.1817149816177535]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1580743794131657, 0.07695591219532635, 0.08082360309220137, 0.24109614463240775, 0.2612955653493794, 0.18175439531751936]
siam score:  -0.64372903
using explorer policy with actor:  1
start point for exploration sampling:  10749
line 256 mcts: sample exp_bonus 3.6894848817257486
line 256 mcts: sample exp_bonus 1.5567236343248865
line 256 mcts: sample exp_bonus 3.6221748309136537
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1582478645816385, 0.07702820486196604, 0.08112685772456002, 0.24129027176154225, 0.2614669058939055, 0.18083989517638777]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15828577475493008, 0.07704665789197929, 0.08090673064639228, 0.24134807573912032, 0.2615295434261, 0.1808832175414781]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.606]
 [0.626]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[ 0.774]
 [ 1.037]
 [-4.396]
 [ 0.173]
 [ 0.173]
 [ 0.173]
 [ 0.173]] [[1.625]
 [1.757]
 [0.104]
 [1.502]
 [1.502]
 [1.502]
 [1.502]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15873835847915962, 0.07705109000811108, 0.08089927933734355, 0.24203815803566758, 0.2598727004043795, 0.1814004137353388]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
first move QE:  0.4307485912304477
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.113]
 [0.157]
 [0.148]
 [0.148]
 [0.163]
 [0.161]] [[-1.236]
 [ 0.958]
 [-0.698]
 [ 0.563]
 [ 0.563]
 [-0.756]
 [-0.622]] [[-0.238]
 [ 1.002]
 [ 0.078]
 [ 0.796]
 [ 0.796]
 [ 0.048]
 [ 0.123]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.39 ]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.366]] [[1.861]
 [1.634]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [1.497]] [[0.268]
 [0.39 ]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.366]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15888884316338453, 0.07689853379178448, 0.08047983308262929, 0.24219725549844157, 0.2600123984372337, 0.18152313602652648]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.84 ]
 [0.752]
 [0.715]
 [0.715]
 [0.715]
 [0.715]] [[ 0.87 ]
 [ 0.627]
 [-0.303]
 [ 0.87 ]
 [ 0.87 ]
 [ 0.87 ]
 [ 0.87 ]] [[2.638]
 [2.794]
 [2.347]
 [2.638]
 [2.638]
 [2.638]
 [2.638]]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]] [[4.026]
 [4.026]
 [4.026]
 [4.026]
 [4.026]
 [4.026]
 [4.026]] [[1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.517]
 [0.517]
 [0.525]
 [0.517]
 [0.53 ]
 [0.529]] [[5.171]
 [5.596]
 [5.596]
 [5.093]
 [5.596]
 [5.174]
 [5.139]] [[0.665]
 [0.778]
 [0.778]
 [0.625]
 [0.778]
 [0.661]
 [0.649]]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1590012278024148, 0.07695292536384254, 0.08053675776538317, 0.24166124940637534, 0.260196309396367, 0.18165153026561728]
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.12 ]
 [0.282]
 [0.283]
 [0.283]
 [0.261]
 [0.272]] [[2.472]
 [1.608]
 [2.66 ]
 [2.656]
 [2.656]
 [2.672]
 [2.614]] [[ 0.555]
 [-0.375]
 [ 0.649]
 [ 0.649]
 [ 0.649]
 [ 0.615]
 [ 0.599]]
Printing some Q and Qe and total Qs values:  [[1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]] [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[2.626]
 [2.626]
 [2.626]
 [2.626]
 [2.626]
 [2.626]
 [2.626]]
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.076]
 [0.536]
 [0.536]
 [0.525]
 [0.516]
 [0.525]] [[-0.727]
 [ 1.072]
 [-0.789]
 [-0.722]
 [-0.7  ]
 [-0.558]
 [-0.716]] [[0.299]
 [1.57 ]
 [0.229]
 [0.298]
 [0.306]
 [0.441]
 [0.29 ]]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.689]
 [0.747]
 [0.59 ]
 [0.747]
 [0.745]
 [0.656]] [[4.149]
 [4.064]
 [4.611]
 [4.865]
 [4.611]
 [4.285]
 [5.238]] [[1.469]
 [1.471]
 [1.703]
 [1.634]
 [1.703]
 [1.596]
 [1.818]]
from probs:  [0.1584243722101283, 0.0770781318546169, 0.0806677953415438, 0.24205444507569207, 0.2598281685787493, 0.18194708693926953]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.15894870546618944, 0.07690619522572117, 0.08093477956373535, 0.24285556673115474, 0.2583347393178808, 0.1820200136953185]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.388]
 [0.365]
 [0.365]
 [0.365]
 [0.364]
 [0.365]] [[-0.999]
 [ 0.235]
 [-1.311]
 [-1.311]
 [-1.311]
 [-1.109]
 [-1.035]] [[0.364]
 [0.388]
 [0.365]
 [0.365]
 [0.365]
 [0.364]
 [0.365]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1589861047197768, 0.07692429058729867, 0.08071853147828162, 0.24291270854239908, 0.25839552324435466, 0.1820628414278892]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1589861047197768, 0.07692429058729867, 0.08071853147828162, 0.24291270854239908, 0.25839552324435466, 0.1820628414278892]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.234]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.181]] [[ 1.525]
 [-0.141]
 [-1.202]
 [-1.202]
 [-1.202]
 [-1.202]
 [ 0.193]] [[0.001]
 [0.234]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.181]]
siam score:  -0.6259487
Printing some Q and Qe and total Qs values:  [[0.8  ]
 [0.458]
 [0.506]
 [0.545]
 [0.513]
 [0.53 ]
 [0.504]] [[0.834]
 [3.016]
 [1.186]
 [1.165]
 [0.938]
 [1.353]
 [1.371]] [[0.8  ]
 [0.458]
 [0.506]
 [0.545]
 [0.513]
 [0.53 ]
 [0.504]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1590197763570368, 0.07672879255616176, 0.08073562684090393, 0.24296415497932067, 0.2584502487836942, 0.18210140048288262]
from probs:  [0.1590197763570368, 0.07672879255616176, 0.08073562684090393, 0.24296415497932067, 0.2584502487836942, 0.18210140048288262]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1590197763570368, 0.07672879255616176, 0.08073562684090393, 0.24296415497932067, 0.2584502487836942, 0.18210140048288262]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1591370566462139, 0.0765746398139461, 0.08079517099245359, 0.24314334593913983, 0.2586408610497258, 0.1817089255585206]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15934413490381769, 0.07667428312345234, 0.08090030629899074, 0.24345973799442944, 0.2582005413205511, 0.18142099635875858]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]] [[-0.024]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]] [[0.798]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15934413490381769, 0.07667428312345234, 0.08090030629899074, 0.24345973799442944, 0.2582005413205511, 0.18142099635875858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.542]
 [0.594]
 [0.601]
 [0.608]
 [0.636]
 [0.592]] [[-2.259]
 [ 1.322]
 [-1.614]
 [-1.554]
 [-1.498]
 [-1.357]
 [-1.473]] [[0.826]
 [0.542]
 [0.594]
 [0.601]
 [0.608]
 [0.636]
 [0.592]]
siam score:  -0.63031006
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15937757557747823, 0.07648050985404975, 0.08091728439952525, 0.2435108315452066, 0.25825472844232056, 0.18145907018141963]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15914991050718913, 0.07657157533809575, 0.08101363275920007, 0.24380078034114727, 0.2577889672236322, 0.18167513383073555]
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1591538594131566, 0.0767737778684896, 0.08075825587408648, 0.24373333094136299, 0.2584697104530888, 0.18111106544981553]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.734]] [[-0.289]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.434]] [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.734]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.8225534524908582
from probs:  [0.1591538594131566, 0.0767737778684896, 0.08075825587408648, 0.24373333094136299, 0.2584697104530888, 0.18111106544981553]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.167]
 [0.622]
 [0.609]
 [0.591]
 [0.589]
 [0.586]] [[-0.145]
 [ 0.613]
 [-0.566]
 [-0.473]
 [-0.222]
 [-0.396]
 [-0.431]] [[0.862]
 [0.687]
 [0.418]
 [0.485]
 [0.7  ]
 [0.522]
 [0.482]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15975881541594086, 0.07643831912921267, 0.08059972236373636, 0.24324439044042634, 0.2586786073233606, 0.18128014532732317]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15975881541594086, 0.07643831912921267, 0.08059972236373636, 0.24324439044042634, 0.2586786073233606, 0.18128014532732317]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]] [[-2.05 ]
 [-3.491]
 [-3.491]
 [-3.491]
 [-3.491]
 [-3.491]
 [-3.491]] [[0.477]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]]
siam score:  -0.6409584
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.313]
 [0.308]
 [0.309]
 [0.312]
 [0.313]
 [0.314]] [[3.125]
 [3.553]
 [3.311]
 [3.297]
 [3.302]
 [3.393]
 [3.39 ]] [[ 0.208]
 [ 0.134]
 [-0.037]
 [-0.044]
 [-0.034]
 [ 0.028]
 [ 0.027]]
siam score:  -0.6434296
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.231]
 [0.417]
 [0.417]
 [0.409]
 [0.417]
 [0.396]] [[-2.958]
 [ 0.451]
 [-4.368]
 [-4.368]
 [-4.576]
 [-4.368]
 [-2.107]] [[0.413]
 [0.231]
 [0.417]
 [0.417]
 [0.409]
 [0.417]
 [0.396]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15994097879039104, 0.0763184902777357, 0.08046067727116665, 0.2428194424893768, 0.2589735629906608, 0.1814868481806691]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.498]
 [0.413]
 [0.584]
 [0.57 ]
 [0.492]
 [0.504]] [[1.042]
 [1.102]
 [1.265]
 [0.011]
 [0.401]
 [1.457]
 [4.624]] [[0.627]
 [0.568]
 [0.565]
 [0.182]
 [0.334]
 [0.712]
 [2.052]]
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.633]
 [0.645]
 [0.649]
 [0.648]
 [0.648]
 [0.651]] [[1.895]
 [1.583]
 [1.642]
 [1.978]
 [1.88 ]
 [1.733]
 [1.812]] [[1.029]
 [0.752]
 [0.819]
 [1.1  ]
 [1.019]
 [0.9  ]
 [0.969]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16071209420379243, 0.07647961790303906, 0.08038857036056907, 0.24189753782090861, 0.25867975426181034, 0.18184242544988047]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.005]
 [-0.004]
 [-0.004]
 [-0.005]
 [-0.007]
 [-0.004]] [[0.277]
 [0.399]
 [0.409]
 [0.398]
 [0.551]
 [0.596]
 [0.476]] [[-0.035]
 [ 0.053]
 [ 0.061]
 [ 0.055]
 [ 0.153]
 [ 0.181]
 [ 0.106]]
siam score:  -0.6420689
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.617]
 [0.628]
 [0.648]
 [0.628]
 [0.628]
 [0.631]] [[1.97 ]
 [3.131]
 [1.97 ]
 [1.793]
 [1.97 ]
 [1.97 ]
 [2.974]] [[0.628]
 [0.617]
 [0.628]
 [0.648]
 [0.628]
 [0.628]
 [0.631]]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.054]
 [0.036]
 [0.033]
 [0.046]
 [0.044]
 [0.004]] [[1.159]
 [1.234]
 [1.137]
 [1.66 ]
 [1.469]
 [1.444]
 [1.827]] [[-0.415]
 [-0.28 ]
 [-0.412]
 [ 0.103]
 [-0.062]
 [-0.09 ]
 [ 0.214]]
line 256 mcts: sample exp_bonus -0.030709498020007775
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16081766380634555, 0.07652316244693369, 0.08042249425204105, 0.241308392682621, 0.25952495673608267, 0.1814033300759761]
siam score:  -0.6393234
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.647]
 [0.647]
 [0.612]
 [0.639]
 [0.647]
 [0.59 ]] [[4.017]
 [3.692]
 [3.692]
 [3.906]
 [3.871]
 [3.692]
 [4.329]] [[1.232]
 [1.05 ]
 [1.05 ]
 [1.122]
 [1.153]
 [1.05 ]
 [1.36 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16094762009073477, 0.07657835954758231, 0.08046871858368398, 0.24213172591186652, 0.25887622437128405, 0.1809973514948484]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6298433
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.1157932440995775
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]] [[3.495]
 [3.495]
 [3.495]
 [3.495]
 [3.495]
 [3.495]
 [3.495]] [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]]
first move QE:  0.40550637034577663
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16098058178868205, 0.07638924494512081, 0.08048519838993813, 0.2421813139257089, 0.2589292416194338, 0.18103441933111628]
siam score:  -0.6306862
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16098058178868205, 0.07638924494512081, 0.08048519838993813, 0.2421813139257089, 0.2589292416194338, 0.18103441933111628]
using explorer policy with actor:  1
first move QE:  0.4044739157746125
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.661]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[2.009]
 [3.283]
 [2.009]
 [2.009]
 [2.009]
 [2.009]
 [2.009]] [[0.671]
 [1.441]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[2.264]
 [2.046]
 [2.046]
 [2.046]
 [2.046]
 [2.046]
 [2.046]] [[0.772]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[1.735]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[1.425]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6466954
1136 1542
Printing some Q and Qe and total Qs values:  [[1.052]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[1.317]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[1.875]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.709048397684721
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[1.606]
 [0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]] [[1.856]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16110315554377366, 0.07602663493609237, 0.08026604467790394, 0.24015784369444648, 0.2603764057323017, 0.18206991541548184]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16116231037418544, 0.07604989063335141, 0.08027796814882909, 0.23952639129460032, 0.26037783925077285, 0.18260560029826092]
first move QE:  0.3968055211633813
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.604]
 [0.631]
 [0.639]
 [0.612]
 [0.611]
 [0.646]] [[ 0.433]
 [ 2.009]
 [-0.005]
 [-0.293]
 [-0.719]
 [-0.619]
 [-0.055]] [[0.616]
 [0.604]
 [0.631]
 [0.639]
 [0.612]
 [0.611]
 [0.646]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16137328239495602, 0.07594984390477666, 0.07993786078301096, 0.23917567902944958, 0.26071869089768135, 0.18284464299012546]
using another actor
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.404]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]] [[-4.155]
 [-3.774]
 [-4.155]
 [-4.155]
 [-4.155]
 [-4.155]
 [-4.155]] [[0.401]
 [0.404]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16128802822282906, 0.07590567327872351, 0.08010073968107445, 0.23900111301482818, 0.26048724425661957, 0.18321720154592525]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.457]
 [0.573]
 [0.553]
 [0.58 ]
 [0.574]
 [0.566]] [[0.625]
 [1.505]
 [0.485]
 [0.355]
 [0.553]
 [0.527]
 [0.665]] [[0.592]
 [0.457]
 [0.573]
 [0.553]
 [0.58 ]
 [0.574]
 [0.566]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16104581840503474, 0.07617978603558002, 0.07994738935803011, 0.238546685842174, 0.26142792330687903, 0.18285239705230222]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16108116284665022, 0.07619650507880935, 0.07974546703491665, 0.2385990392624231, 0.2614852984319428, 0.18289252734525782]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.507]
 [0.504]] [[3.109]
 [3.109]
 [3.109]
 [3.109]
 [3.109]
 [2.658]
 [3.109]] [[0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.609]
 [0.974]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.412]
 [0.412]
 [0.412]
 [0.142]
 [0.412]
 [0.412]] [[ 0.606]
 [-0.131]
 [-0.131]
 [-0.131]
 [ 0.802]
 [-0.131]
 [-0.131]] [[1.819]
 [1.607]
 [1.607]
 [1.607]
 [1.928]
 [1.607]
 [1.607]]
from probs:  [0.16150480087445737, 0.07580272471996406, 0.07973639621144235, 0.2379198765395098, 0.26217299595144555, 0.1828632057031809]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16177356625953118, 0.07553739939374358, 0.07986908813834567, 0.23831580673408378, 0.2618449173961552, 0.18265922207814064]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16108578723371977, 0.07559937911291746, 0.07993462208170643, 0.23851134890120396, 0.2620597655982057, 0.18280909707224666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16120838775206928, 0.07565691692072354, 0.07999545939249332, 0.2386928771136533, 0.26149812787210774, 0.18294823094895266]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[1.05]
 [1.05]
 [1.05]
 [1.05]
 [1.05]
 [1.05]
 [1.05]] [[2.046]
 [2.046]
 [2.046]
 [2.046]
 [2.046]
 [2.046]
 [2.046]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1612900386354218, 0.07569523660237562, 0.08003597651455295, 0.23881377333089143, 0.2616305747840428, 0.18253440013271535]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16143801443742484, 0.0755699294147881, 0.07989082532115498, 0.2390328734063641, 0.26187060817023156, 0.1821977492500363]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[2.369]
 [2.27 ]
 [2.27 ]
 [2.27 ]
 [2.27 ]
 [2.27 ]
 [2.27 ]] [[0.957]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]]
line 256 mcts: sample exp_bonus 2.761331808079375
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1618802133380458, 0.07577692489738044, 0.07967499221469224, 0.23838570135073664, 0.26258790449874103, 0.18169426370040376]
from probs:  [0.16153719635079664, 0.07580793809250477, 0.07970760077044228, 0.23848326538201464, 0.26269537375710084, 0.18176862564714075]
using explorer policy with actor:  1
siam score:  -0.6614299
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[-3.015]
 [-3.015]
 [-3.015]
 [-3.015]
 [-3.015]
 [-3.015]
 [-3.015]] [[0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16185764854350565, 0.07576360979698953, 0.07986572209493892, 0.2376655937244852, 0.2632165002247917, 0.18163092561528899]
1160 1587
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Starting evaluation
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.008]
 [ 0.958]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[0.459]
 [0.384]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[-0.008]
 [ 0.958]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.384]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]] [[-3.67 ]
 [-3.096]
 [-3.67 ]
 [-3.67 ]
 [-3.67 ]
 [-3.67 ]
 [-3.67 ]] [[0.358]
 [0.384]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.381]
 [0.384]
 [0.384]
 [0.415]
 [0.408]
 [0.414]] [[ 0.   ]
 [-0.48 ]
 [ 0.   ]
 [ 0.   ]
 [-1.871]
 [-1.568]
 [-3.218]] [[0.384]
 [0.381]
 [0.384]
 [0.384]
 [0.415]
 [0.408]
 [0.414]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16201604048248705, 0.07583775115001151, 0.07972764472935447, 0.23789817040255962, 0.26271172579131424, 0.1818086674442731]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.488]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.489]] [[-0.219]
 [ 0.213]
 [ 0.364]
 [ 0.364]
 [ 0.364]
 [ 0.364]
 [ 1.807]] [[0.404]
 [0.488]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.489]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.672]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.665]] [[0.302]
 [1.699]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [1.256]] [[0.664]
 [0.672]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.665]]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.647]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]] [[0.073]
 [1.754]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]] [[0.621]
 [0.647]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16133280623650967, 0.07589958401463275, 0.07979264914454827, 0.23809213614048136, 0.26292592278857396, 0.18195690167525405]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16133280623650967, 0.07589958401463275, 0.07979264914454827, 0.23809213614048136, 0.26292592278857396, 0.18195690167525405]
Printing some Q and Qe and total Qs values:  [[1.38]
 [1.38]
 [1.38]
 [1.38]
 [1.38]
 [1.38]
 [1.38]] [[0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[2.385]
 [2.385]
 [2.385]
 [2.385]
 [2.385]
 [2.385]
 [2.385]]
rdn probs:  [0.16133280623650967, 0.07589958401463275, 0.07979264914454827, 0.23809213614048136, 0.26292592278857396, 0.18195690167525405]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.582]
 [0.617]
 [0.62 ]
 [0.623]
 [0.638]
 [0.61 ]] [[3.654]
 [4.025]
 [3.048]
 [2.842]
 [2.713]
 [2.814]
 [3.703]] [[1.537]
 [1.865]
 [0.958]
 [0.758]
 [0.636]
 [0.767]
 [1.599]]
siam score:  -0.65915984
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.539]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.525]] [[1.096]
 [1.377]
 [1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.775]] [[1.922]
 [2.017]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [2.165]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16098419335746195, 0.0759217509977561, 0.07980470116758243, 0.23812897692957796, 0.2636767702896876, 0.1814836072579339]
siam score:  -0.6555306
first move QE:  0.3809326713624591
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16098419335746195, 0.0759217509977561, 0.07980470116758243, 0.23812897692957796, 0.2636767702896876, 0.1814836072579339]
siam score:  -0.6532667
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16068942495758884, 0.07596759122123777, 0.0800569808812325, 0.23824013873580557, 0.2629885484828983, 0.18205731572123696]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.395]
 [0.383]
 [0.384]
 [0.381]
 [0.378]
 [0.384]] [[-2.582]
 [-1.311]
 [-2.33 ]
 [-2.267]
 [-2.526]
 [-2.636]
 [-2.413]] [[0.385]
 [0.395]
 [0.383]
 [0.384]
 [0.381]
 [0.378]
 [0.384]]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.683]
 [0.773]
 [0.773]
 [0.773]
 [0.737]
 [0.736]] [[0.965]
 [0.992]
 [0.577]
 [0.577]
 [0.577]
 [1.093]
 [1.039]] [[2.142]
 [1.951]
 [1.992]
 [1.992]
 [1.992]
 [2.094]
 [2.072]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1604648720825313, 0.07585681985694802, 0.08014400477044203, 0.23849911157253254, 0.26327442344414004, 0.181760768273406]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.6099731538009268
first move QE:  0.3780868326157152
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16013126363845848, 0.0758869632956241, 0.08017585181989136, 0.23859388463786194, 0.263379041545097, 0.18183299506306708]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16021007527439535, 0.07592431250269158, 0.08021531188472182, 0.23871131313961472, 0.2635086685315081, 0.1814303186670684]
siam score:  -0.63891584
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16033124651032604, 0.07598173612456668, 0.08027598090609885, 0.23889185699613702, 0.26295164011283073, 0.18156753935004066]
siam score:  -0.63591504
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.654]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]] [[2.888]
 [3.624]
 [2.888]
 [2.888]
 [2.888]
 [2.888]
 [2.888]] [[1.74 ]
 [2.041]
 [1.74 ]
 [1.74 ]
 [1.74 ]
 [1.74 ]
 [1.74 ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
1180 1606
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]] [[4.88]
 [4.88]
 [4.88]
 [4.88]
 [4.88]
 [4.88]
 [4.88]] [[1.332]
 [1.332]
 [1.332]
 [1.332]
 [1.332]
 [1.332]
 [1.332]]
using explorer policy with actor:  1
siam score:  -0.62968236
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16009899118973528, 0.07624724578777453, 0.08033986203097496, 0.23972663782281037, 0.262365678378016, 0.18122158479068878]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1181 1608
using explorer policy with actor:  1
start point for exploration sampling:  10749
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.558]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.528]] [[-0.715]
 [ 0.512]
 [-0.715]
 [-0.715]
 [-0.715]
 [-0.715]
 [-0.609]] [[0.51 ]
 [0.558]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.528]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16040816847774553, 0.07619529208456186, 0.08069323553527438, 0.24013612813177035, 0.2605484909494974, 0.1820186848211505]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.691]] [[2.36 ]
 [2.36 ]
 [2.36 ]
 [2.36 ]
 [2.36 ]
 [2.36 ]
 [2.077]] [[1.28 ]
 [1.28 ]
 [1.28 ]
 [1.28 ]
 [1.28 ]
 [1.28 ]
 [1.001]]
1185 1616
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.3546084399381108
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.198]
 [0.197]
 [0.199]
 [0.198]
 [0.202]
 [0.204]] [[-2.837]
 [-1.753]
 [-2.144]
 [-2.051]
 [-1.753]
 [-1.924]
 [-2.088]] [[0.216]
 [0.198]
 [0.197]
 [0.199]
 [0.198]
 [0.202]
 [0.204]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16073835558246985, 0.07615959631786388, 0.08085933596560206, 0.23998765132473546, 0.2603510529900504, 0.18190400781927835]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[3.585]
 [3.585]
 [3.585]
 [3.585]
 [3.585]
 [3.585]
 [3.585]] [[2.063]
 [2.063]
 [2.063]
 [2.063]
 [2.063]
 [2.063]
 [2.063]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.7200648520017854
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16068830071273735, 0.07593133181338209, 0.0807783220134718, 0.24039647498794223, 0.25999589658938355, 0.18220967388308307]
siam score:  -0.6529669
Printing some Q and Qe and total Qs values:  [[0.935]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]] [[-2.444]
 [-1.393]
 [-1.393]
 [-1.393]
 [-1.393]
 [-1.393]
 [-1.393]] [[0.935]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]]
first move QE:  0.3710201422901912
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.41 ]
 [0.471]
 [0.465]
 [0.465]
 [0.46 ]
 [0.456]] [[-3.985]
 [-1.211]
 [-3.941]
 [-4.007]
 [-4.088]
 [-4.015]
 [-4.113]] [[ 0.029]
 [ 1.013]
 [ 0.049]
 [ 0.024]
 [-0.006]
 [ 0.019]
 [-0.018]]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.414]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[1.947]
 [3.147]
 [1.947]
 [1.947]
 [1.947]
 [1.947]
 [1.947]] [[1.126]
 [1.946]
 [1.126]
 [1.126]
 [1.126]
 [1.126]
 [1.126]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.388]
 [0.387]] [[-0.765]
 [-0.765]
 [-0.765]
 [-0.765]
 [-0.765]
 [-0.599]
 [-0.765]] [[0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.388]
 [0.387]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1608848107722989, 0.07620951419787426, 0.0808587209225592, 0.24063793181656062, 0.25950239348495446, 0.1819066288057525]
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.874]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.723]] [[0.418]
 [0.353]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.66 ]] [[2.922]
 [3.03 ]
 [2.922]
 [2.922]
 [2.922]
 [2.922]
 [2.838]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16063457348148089, 0.07627599884535378, 0.08092926150548725, 0.2408478626662113, 0.2597287815591015, 0.18158352194236527]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.137]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]] [[2.613]
 [3.232]
 [2.613]
 [2.613]
 [2.613]
 [2.613]
 [2.613]] [[0.05]
 [0.71]
 [0.05]
 [0.05]
 [0.05]
 [0.05]
 [0.05]]
Printing some Q and Qe and total Qs values:  [[ 0.015]
 [ 0.015]
 [ 0.025]
 [ 0.015]
 [ 0.036]
 [ 0.015]
 [-0.002]] [[2.73 ]
 [2.73 ]
 [2.857]
 [2.73 ]
 [2.343]
 [2.73 ]
 [3.115]] [[-0.187]
 [-0.187]
 [-0.04 ]
 [-0.187]
 [-0.532]
 [-0.187]
 [ 0.164]]
siam score:  -0.6574582
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.102]
 [0.175]
 [0.163]
 [0.163]
 [0.161]
 [0.163]] [[-6.071]
 [-2.799]
 [-5.579]
 [-5.568]
 [-5.628]
 [-5.707]
 [-5.666]] [[0.204]
 [0.102]
 [0.175]
 [0.163]
 [0.163]
 [0.161]
 [0.163]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.02  0.041 0.02  0.02  0.02  0.02  0.857]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1604447942593455, 0.07637066858640999, 0.08081489670623454, 0.24050960342446748, 0.26005114320453726, 0.18180889381900517]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1604447942593455, 0.07637066858640999, 0.08081489670623454, 0.24050960342446748, 0.26005114320453726, 0.18180889381900517]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64976066
Printing some Q and Qe and total Qs values:  [[1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]] [[0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[2.732]
 [2.732]
 [2.732]
 [2.732]
 [2.732]
 [2.732]
 [2.732]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.54 ]
 [0.558]
 [0.559]
 [0.528]
 [0.562]
 [0.564]] [[2.701]
 [3.155]
 [2.727]
 [2.783]
 [2.789]
 [2.913]
 [2.837]] [[1.576]
 [1.839]
 [1.588]
 [1.628]
 [1.57 ]
 [1.72 ]
 [1.674]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.7715532753911252
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]] [[1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.957]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16033982790150383, 0.07649306977012305, 0.08028459664967312, 0.24145863580863028, 0.2588982881527472, 0.18252558171732247]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.408]
 [0.363]
 [0.363]
 [0.363]
 [0.37 ]
 [0.363]] [[-2.796]
 [-0.506]
 [-2.796]
 [-2.796]
 [-2.796]
 [-3.641]
 [-2.796]] [[0.363]
 [0.408]
 [0.363]
 [0.363]
 [0.363]
 [0.37 ]
 [0.363]]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.512]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[0.23 ]
 [1.251]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]] [[0.476]
 [0.512]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.878 0.02  0.02  0.02  0.02  0.02 ]
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.689]
 [0.689]
 [0.707]
 [0.689]
 [0.719]
 [0.715]] [[1.915]
 [2.302]
 [2.302]
 [2.005]
 [2.302]
 [2.125]
 [2.196]] [[1.897]
 [2.105]
 [2.105]
 [1.945]
 [2.105]
 [2.048]
 [2.088]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16004151984264756, 0.0763327446563782, 0.08029284646093436, 0.2414729365885685, 0.2602780659311301, 0.1815818865203414]
Printing some Q and Qe and total Qs values:  [[0.964]
 [1.126]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [1.011]] [[1.899]
 [1.91 ]
 [2.08 ]
 [2.08 ]
 [2.08 ]
 [2.08 ]
 [1.727]] [[2.036]
 [2.363]
 [2.09 ]
 [2.09 ]
 [2.09 ]
 [2.09 ]
 [2.072]]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.512]
 [0.511]
 [0.522]
 [0.503]
 [0.515]
 [0.506]] [[2.889]
 [3.911]
 [3.365]
 [3.335]
 [2.943]
 [3.384]
 [3.12 ]] [[0.252]
 [0.589]
 [0.404]
 [0.416]
 [0.247]
 [0.419]
 [0.312]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16004151984264756, 0.0763327446563782, 0.08029284646093436, 0.2414729365885685, 0.2602780659311301, 0.1815818865203414]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.418]
 [0.404]
 [0.399]
 [0.404]
 [0.404]
 [0.404]] [[ 0.105]
 [-1.495]
 [ 0.105]
 [-3.16 ]
 [ 0.105]
 [ 0.105]
 [ 0.105]] [[0.404]
 [0.418]
 [0.404]
 [0.399]
 [0.404]
 [0.404]
 [0.404]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
1217 1664
siam score:  -0.63865644
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[0.448]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]] [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1600419074187238, 0.07632116979557538, 0.08005180517733354, 0.24199035322142792, 0.2600944221364223, 0.181500342250517]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.533]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[0.547]
 [1.196]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[0.453]
 [0.533]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1595530523633047, 0.07625606837149544, 0.0799629811834875, 0.24296762473633285, 0.2604353377870793, 0.1808249355583002]
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]] [[0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]] [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
siam score:  -0.63136995
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16002416666186894, 0.07648123062085373, 0.07958727634947296, 0.24368503828879087, 0.25979286641999144, 0.18042942165902204]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]] [[0.915]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[0.912]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.309]] [[1.109]
 [1.109]
 [1.109]
 [1.109]
 [1.109]
 [1.109]
 [2.607]] [[0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [1.207]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15970831872081107, 0.07650998911397033, 0.0796172027787483, 0.24377666880309298, 0.2598905538042037, 0.18049726677917366]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[2.705]
 [2.705]
 [2.705]
 [2.705]
 [2.705]
 [2.705]
 [2.705]] [[1.575]
 [1.575]
 [1.575]
 [1.575]
 [1.575]
 [1.575]
 [1.575]]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.49 ]] [[2.407]
 [2.407]
 [2.407]
 [2.407]
 [2.407]
 [2.407]
 [2.708]] [[1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.466]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1595891647851794, 0.07644630936274945, 0.07934134041981002, 0.24353036584207988, 0.26030660205473655, 0.18078621753544477]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1595891647851794, 0.07644630936274945, 0.07934134041981002, 0.24353036584207988, 0.26030660205473655, 0.18078621753544477]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1595891647851794, 0.07644630936274945, 0.07934134041981002, 0.24353036584207988, 0.26030660205473655, 0.18078621753544477]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.987]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[1.61 ]
 [1.019]
 [1.61 ]
 [1.61 ]
 [1.61 ]
 [1.61 ]
 [1.61 ]] [[1.336]
 [1.777]
 [1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[2.953]
 [2.953]
 [2.953]
 [2.953]
 [2.953]
 [2.953]
 [2.953]] [[1.867]
 [1.867]
 [1.867]
 [1.867]
 [1.867]
 [1.867]
 [1.867]]
siam score:  -0.6373162
siam score:  -0.63699484
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.552]
 [0.538]
 [0.542]
 [0.544]
 [0.543]
 [0.54 ]] [[-3.509]
 [-3.017]
 [-3.673]
 [-3.521]
 [-3.555]
 [-3.647]
 [-3.547]] [[0.285]
 [0.688]
 [0.154]
 [0.278]
 [0.253]
 [0.18 ]
 [0.257]]
siam score:  -0.6472666
using explorer policy with actor:  1
using another actor
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15964351064813068, 0.0760916371909587, 0.07933914643072881, 0.24348537097162168, 0.26020947825800067, 0.1812308565005596]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.644]
 [0.676]
 [0.718]
 [0.731]
 [0.724]
 [0.739]] [[2.587]
 [3.028]
 [2.69 ]
 [2.407]
 [2.444]
 [2.432]
 [2.319]] [[0.688]
 [0.644]
 [0.676]
 [0.718]
 [0.731]
 [0.724]
 [0.739]]
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]] [[3.061]
 [3.061]
 [3.061]
 [3.061]
 [3.061]
 [3.061]
 [3.061]] [[0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.654]
 [0.654]
 [0.64 ]
 [0.654]
 [0.654]
 [0.604]] [[3.841]
 [3.841]
 [3.841]
 [3.928]
 [3.841]
 [3.841]
 [3.751]] [[1.386]
 [1.386]
 [1.386]
 [1.416]
 [1.386]
 [1.386]
 [1.225]]
first move QE:  0.35632295906293227
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15978671654955842, 0.07615989409317042, 0.07921114842668206, 0.24370378600078824, 0.25974502791155274, 0.18139342701824815]
using explorer policy with actor:  1
using explorer policy with actor:  1
first move QE:  0.35589083834725266
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.256]
 [0.416]
 [0.396]
 [0.399]
 [0.402]
 [0.397]] [[2.543]
 [2.739]
 [2.812]
 [2.686]
 [2.66 ]
 [2.675]
 [2.667]] [[ 0.063]
 [-0.219]
 [ 0.151]
 [ 0.026]
 [ 0.014]
 [ 0.03 ]
 [ 0.015]]
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[4.288]
 [5.665]
 [5.665]
 [5.665]
 [5.665]
 [5.665]
 [5.665]] [[0.492]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1598183819086736, 0.07617498690218928, 0.07902867324799427, 0.2437520814290196, 0.25979650227540985, 0.1814293742367135]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.963]
 [0.353]
 [0.561]
 [0.599]
 [0.597]
 [0.597]
 [0.6  ]] [[1.957]
 [3.25 ]
 [2.834]
 [2.622]
 [2.625]
 [2.709]
 [2.745]] [[1.219]
 [0.859]
 [0.998]
 [0.933]
 [0.931]
 [0.989]
 [1.017]]
Printing some Q and Qe and total Qs values:  [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]] [[0.349]
 [0.348]
 [0.353]
 [0.353]
 [0.353]
 [0.349]
 [0.347]] [[1.638]
 [1.636]
 [1.642]
 [1.642]
 [1.642]
 [1.638]
 [1.636]]
siam score:  -0.63785785
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16039049853496845, 0.07554018597556647, 0.07872108424304744, 0.24462466327357368, 0.26002980181819985, 0.18069376615464403]
from probs:  [0.1604191840633091, 0.07537484813799368, 0.07873516335567374, 0.24466841391825184, 0.26007630764190887, 0.18072608288286265]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1603143115603265, 0.07550120488271317, 0.07847781564075297, 0.24507857068913896, 0.2605122938684035, 0.1801158033586647]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.483]
 [0.484]] [[3.629]
 [3.629]
 [3.629]
 [3.629]
 [3.629]
 [3.498]
 [3.537]] [[1.464]
 [1.464]
 [1.464]
 [1.464]
 [1.464]
 [1.341]
 [1.38 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1603143115603265, 0.07550120488271317, 0.07847781564075297, 0.24507857068913896, 0.2605122938684035, 0.1801158033586647]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.3541075572973099
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16021466902474055, 0.07562978537500117, 0.07861146537766049, 0.24486101225821846, 0.26026052276363837, 0.18042254520074105]
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[3.187]
 [3.028]
 [3.028]
 [3.028]
 [3.028]
 [3.028]
 [3.028]] [[1.005]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16031845835001457, 0.07567877941807145, 0.07846876147144215, 0.24501963668008256, 0.26042912320589207, 0.1800852408744972]
from probs:  [0.16031845835001457, 0.07567877941807145, 0.07846876147144215, 0.24501963668008256, 0.26042912320589207, 0.1800852408744972]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus -1.4082355466227363
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.449]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[-0.261]
 [ 1.367]
 [-0.261]
 [-0.261]
 [-0.261]
 [-0.261]
 [-0.261]] [[0.364]
 [0.932]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.415]
 [0.382]
 [0.45 ]
 [0.437]
 [0.437]
 [0.433]] [[2.492]
 [3.112]
 [1.787]
 [2.724]
 [2.492]
 [2.492]
 [3.329]] [[ 0.258]
 [ 0.422]
 [-0.087]
 [ 0.361]
 [ 0.258]
 [ 0.258]
 [ 0.529]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16039093377313252, 0.07571299164550818, 0.07850423497054959, 0.2451304031010135, 0.26054685581816167, 0.17971458069163454]
siam score:  -0.64334047
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]] [[ 2.304]
 [-2.32 ]
 [-2.32 ]
 [-2.32 ]
 [-2.32 ]
 [-2.32 ]
 [-2.32 ]] [[0.195]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1601797774906546, 0.07560982259119013, 0.07858280604641947, 0.245375742215614, 0.26080762451151723, 0.17944422714460453]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16028122680383008, 0.07565770981323129, 0.07863257619734429, 0.244897803527865, 0.2609728061267756, 0.17955787753095365]
using explorer policy with actor:  1
in main func line 156:  1249
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.027]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.028]] [[5.1  ]
 [4.917]
 [5.093]
 [5.108]
 [5.388]
 [5.252]
 [5.269]] [[-0.044]
 [-0.167]
 [-0.049]
 [-0.038]
 [ 0.147]
 [ 0.057]
 [ 0.065]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1604936613955107, 0.07575798552868568, 0.07873679476086777, 0.24459150484702707, 0.2606241923305344, 0.17979586113737442]
siam score:  -0.64200544
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.462]
 [0.422]
 [0.495]
 [0.422]
 [0.422]
 [0.668]] [[3.144]
 [3.115]
 [3.144]
 [3.087]
 [3.144]
 [3.144]
 [2.43 ]] [[1.279]
 [1.293]
 [1.279]
 [1.302]
 [1.279]
 [1.279]
 [1.118]]
siam score:  -0.6414765
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.615]
 [0.521]
 [0.501]
 [0.512]
 [0.536]
 [0.591]] [[-1.367]
 [ 0.643]
 [-1.366]
 [-1.541]
 [-1.51 ]
 [-1.331]
 [-1.256]] [[0.21 ]
 [1.134]
 [0.274]
 [0.176]
 [0.208]
 [0.316]
 [0.452]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
siam score:  -0.64392644
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.531]
 [0.611]
 [0.61 ]
 [0.606]
 [0.605]
 [0.606]] [[3.005]
 [3.61 ]
 [2.998]
 [2.964]
 [2.94 ]
 [2.994]
 [3.025]] [[1.001]
 [1.423]
 [0.992]
 [0.959]
 [0.928]
 [0.978]
 [1.009]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.1609678993964403, 0.07562227228730006, 0.07915225367430616, 0.24399913498847878, 0.26130496512115053, 0.17895347453232419]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.448]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.44 ]] [[0.291]
 [0.988]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.266]] [[0.414]
 [0.448]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.44 ]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
from probs:  [0.1611277789634814, 0.0755204600199763, 0.07903635816149346, 0.24361968142687296, 0.26156450335732, 0.17913121807085586]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16115896688480152, 0.07553507777354004, 0.07885809627651541, 0.24366683649538598, 0.2616151318287315, 0.1791658907410255]
siam score:  -0.64331865
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.168]
 [0.217]
 [0.196]
 [0.193]
 [0.192]
 [0.199]] [[-4.978]
 [-1.714]
 [-4.48 ]
 [-4.926]
 [-4.74 ]
 [-4.629]
 [-4.684]] [[0.207]
 [0.168]
 [0.217]
 [0.196]
 [0.193]
 [0.192]
 [0.199]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16129891459021556, 0.07542441975680794, 0.07892657530588959, 0.24387843263125228, 0.2611501823630289, 0.1793214753528057]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16129891459021556, 0.07542441975680794, 0.07892657530588959, 0.24387843263125228, 0.2611501823630289, 0.1793214753528057]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.521]
 [0.534]
 [0.521]
 [0.521]
 [0.539]
 [0.539]] [[0.169]
 [0.321]
 [0.384]
 [0.321]
 [0.321]
 [0.364]
 [0.484]] [[0.537]
 [0.521]
 [0.534]
 [0.521]
 [0.521]
 [0.539]
 [0.539]]
line 256 mcts: sample exp_bonus 1.615362078330999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 4.333914034990506
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16167248655148878, 0.07542330033025872, 0.07872396669604767, 0.24382256333705024, 0.26106497746484086, 0.1792927056203136]
actor:  1 policy actor:  1  step number:  137 total reward:  0.01999999999999924  reward:  1.0 rdn_beta:  0.5
from probs:  [0.14991862296722866, 0.06993989865774095, 0.14570229460995845, 0.22609625003935996, 0.2420851114579404, 0.1662578222677716]
line 256 mcts: sample exp_bonus -6.421624231943939
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14998007470388755, 0.06996856706563583, 0.14576201807100522, 0.22618892703266463, 0.24218434229547017, 0.16591607083133658]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.431]
 [0.529]
 [0.552]
 [0.554]
 [0.547]
 [0.532]] [[1.385]
 [1.576]
 [1.624]
 [1.584]
 [1.631]
 [1.372]
 [1.566]] [[-0.182]
 [-0.373]
 [-0.16 ]
 [-0.128]
 [-0.108]
 [-0.209]
 [-0.174]]
line 256 mcts: sample exp_bonus -1.517584737365932
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.254]
 [0.284]
 [0.286]
 [0.287]
 [0.286]
 [0.284]] [[-3.127]
 [-1.366]
 [-2.97 ]
 [-3.093]
 [-2.946]
 [-2.674]
 [-2.858]] [[0.283]
 [0.254]
 [0.284]
 [0.286]
 [0.287]
 [0.286]
 [0.284]]
from probs:  [0.1499968437178783, 0.07013798630326255, 0.14575392136392018, 0.22502267660812772, 0.24277075828707306, 0.16631781371973814]
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]] [[2.837]
 [2.837]
 [2.837]
 [2.837]
 [2.837]
 [2.837]
 [2.837]] [[1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.445]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]] [[1.01 ]
 [1.358]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]] [[0.45 ]
 [0.445]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.701]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.696]] [[1.797]
 [2.286]
 [1.797]
 [1.797]
 [1.797]
 [1.797]
 [1.336]] [[1.5  ]
 [1.638]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.391]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.478]
 [0.553]
 [0.586]
 [0.579]
 [0.553]
 [0.568]] [[2.125]
 [2.326]
 [2.276]
 [2.043]
 [1.864]
 [1.993]
 [1.932]] [[0.066]
 [0.035]
 [0.167]
 [0.157]
 [0.082]
 [0.073]
 [0.082]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15039474834739497, 0.07016091870509542, 0.145422019964227, 0.2244884901201843, 0.24277480900601162, 0.16675901385708664]
using explorer policy with actor:  1
1278 1746
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15059470297221417, 0.07025419988898518, 0.14525830836563441, 0.22422494583048047, 0.24309758587411223, 0.16657025706857345]
using explorer policy with actor:  1
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15048412013911996, 0.0703643426796555, 0.14548604073575208, 0.2245764801346316, 0.2434787082348494, 0.16561030807599145]
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.017]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[3.577]
 [3.67 ]
 [2.624]
 [2.624]
 [2.624]
 [2.624]
 [2.624]] [[-0.496]
 [-0.459]
 [-0.822]
 [-0.822]
 [-0.822]
 [-0.822]
 [-0.822]]
1279 1752
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1504551265610103, 0.07051247141565932, 0.14579231323826805, 0.22392962634463573, 0.24335151690588339, 0.1659589455345432]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15047967787078245, 0.07036079737991709, 0.14581610366885372, 0.2239661672439006, 0.2433912270747001, 0.16598602676184607]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.25 ]
 [0.453]
 [0.418]
 [0.415]
 [0.44 ]
 [0.431]] [[2.944]
 [2.378]
 [2.699]
 [2.996]
 [2.91 ]
 [2.876]
 [2.955]] [[0.917]
 [0.021]
 [0.747]
 [0.975]
 [0.884]
 [0.9  ]
 [0.96 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15047967787078245, 0.07036079737991709, 0.14581610366885372, 0.2239661672439006, 0.2433912270747001, 0.16598602676184607]
siam score:  -0.65508944
line 256 mcts: sample exp_bonus 1.2141214769289104
Printing some Q and Qe and total Qs values:  [[1.248]
 [1.019]
 [1.019]
 [1.019]
 [1.019]
 [1.019]
 [1.019]] [[2.835]
 [1.199]
 [1.199]
 [1.199]
 [1.199]
 [1.199]
 [1.199]] [[2.736]
 [1.892]
 [1.892]
 [1.892]
 [1.892]
 [1.892]
 [1.892]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.192]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]] [[2.944]
 [1.119]
 [1.119]
 [1.119]
 [1.119]
 [1.119]
 [1.119]] [[1.976]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]]
1284 1756
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.932]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[5.282]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[1.623]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6573148
using another actor
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.413]
 [0.439]
 [0.439]
 [0.45 ]
 [0.439]
 [0.432]] [[2.263]
 [3.06 ]
 [2.263]
 [2.263]
 [0.132]
 [2.263]
 [0.473]] [[ 0.938]
 [ 1.335]
 [ 0.938]
 [ 0.938]
 [-0.184]
 [ 0.938]
 [-0.023]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14990798257992924, 0.07057557700813982, 0.14590348723676877, 0.22298605610465588, 0.24413419019067384, 0.16649270687983256]
line 256 mcts: sample exp_bonus 1.1682181101472353
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.14990798257992924, 0.07057557700813982, 0.14590348723676877, 0.22298605610465588, 0.24413419019067384, 0.16649270687983256]
siam score:  -0.66513395
using another actor
using explorer policy with actor:  1
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15018680218336733, 0.07070684322929208, 0.1458182495064068, 0.22230336933440964, 0.2445882647297555, 0.1663964710167688]
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[-6.191]
 [-5.917]
 [-5.917]
 [-5.917]
 [-5.917]
 [-5.917]
 [-5.917]] [[0.69 ]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
line 256 mcts: sample exp_bonus 3.6085970447464613
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15038280599697237, 0.07079912038493719, 0.14565324953103317, 0.22204815048474683, 0.24490746876068986, 0.16620920484162055]
using explorer policy with actor:  1
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.3351835723838363
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15029493650027836, 0.07075515210037292, 0.14589953711988063, 0.2218800580667019, 0.24468006536333928, 0.16649025084942676]
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.611]
 [0.87 ]
 [0.826]
 [0.764]
 [0.704]
 [0.893]] [[1.299]
 [2.853]
 [1.493]
 [1.889]
 [1.662]
 [1.914]
 [1.973]] [[1.381]
 [1.962]
 [1.638]
 [1.786]
 [1.557]
 [1.596]
 [1.943]]
siam score:  -0.6684388
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1503726852097306, 0.07062877132494232, 0.14562068743055462, 0.22199483830194852, 0.2448066402151185, 0.16657637751770543]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.135]
 [0.18 ]
 [0.183]
 [0.178]
 [0.177]
 [0.181]] [[-6.388]
 [-1.878]
 [-6.2  ]
 [-6.144]
 [-6.027]
 [-5.931]
 [-5.92 ]] [[0.182]
 [0.135]
 [0.18 ]
 [0.183]
 [0.178]
 [0.177]
 [0.181]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.8413484458058744
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15020338014466872, 0.0705470920382985, 0.14578709169257636, 0.2222485171431358, 0.2444471906305009, 0.1667667283508198]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15036986471470098, 0.07046349354310938, 0.1459486812736718, 0.22195248422282737, 0.24471813450265958, 0.16654734174303087]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.274]
 [0.347]
 [0.526]
 [0.438]
 [0.436]
 [0.372]] [[3.479]
 [2.681]
 [1.721]
 [3.313]
 [2.356]
 [1.543]
 [2.781]] [[1.984]
 [1.177]
 [0.622]
 [1.952]
 [1.18 ]
 [0.622]
 [1.379]]
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.525]
 [0.521]
 [0.535]
 [0.444]
 [0.456]
 [0.508]] [[ 0.367]
 [-0.922]
 [-2.534]
 [-2.315]
 [-2.035]
 [ 0.   ]
 [-1.687]] [[1.528]
 [1.018]
 [0.118]
 [0.248]
 [0.353]
 [1.494]
 [0.583]]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.442]
 [0.419]
 [0.503]
 [0.419]
 [0.419]
 [0.431]] [[3.243]
 [3.374]
 [3.243]
 [3.267]
 [3.243]
 [3.243]
 [3.462]] [[1.637]
 [1.815]
 [1.637]
 [1.83 ]
 [1.637]
 [1.637]
 [1.881]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.473]
 [0.486]
 [0.5  ]
 [0.5  ]
 [0.495]
 [0.51 ]] [[3.999]
 [3.989]
 [3.967]
 [3.941]
 [3.915]
 [4.017]
 [3.976]] [[1.632]
 [1.605]
 [1.602]
 [1.597]
 [1.572]
 [1.664]
 [1.645]]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.446]
 [0.506]
 [0.506]
 [0.506]
 [0.473]
 [0.506]] [[3.923]
 [3.761]
 [3.626]
 [3.626]
 [3.626]
 [3.865]
 [3.626]] [[1.587]
 [1.376]
 [1.36 ]
 [1.36 ]
 [1.36 ]
 [1.534]
 [1.36 ]]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.498]
 [0.39 ]
 [0.578]
 [0.39 ]
 [0.39 ]
 [0.39 ]] [[3.293]
 [3.275]
 [3.293]
 [3.291]
 [3.293]
 [3.293]
 [3.293]] [[1.746]
 [1.822]
 [1.746]
 [1.895]
 [1.746]
 [1.746]
 [1.746]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15014875808532743, 0.07067871736732367, 0.14604001885463486, 0.22101433154446404, 0.24546560202256093, 0.16665257212568915]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1502288941597562, 0.07071643939065378, 0.14611796204890268, 0.22059857816559922, 0.24559660976451114, 0.16674151647057692]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4822],
        [-0.6104],
        [-0.6101],
        [-0.5974],
        [-0.5389],
        [-0.4222],
        [-0.5647],
        [-0.0000],
        [-0.4666],
        [-0.4874]], dtype=torch.float64)
-0.024259925299500003 -0.5064882132287505
-0.024259925299500003 -0.6346714310654457
-0.024259925299500003 -0.6344063204557163
-0.024259925299500003 -0.6216139998382235
-0.0628797758985 -0.6018074045895279
-0.024259925299500003 -0.44649471775580357
-0.0628797758985 -0.6276211602469911
-0.946189035 -0.946189035
-0.0727797758985 -0.5393583097194505
-0.0727797758985 -0.5601663349403962
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.698]
 [0.667]] [[0.44 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.635]
 [0.488]] [[0.712]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.698]
 [0.667]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15039519448151892, 0.07079472104672502, 0.14557488533353805, 0.2208427762922946, 0.24586848020232607, 0.16652394264359735]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]] [[0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15044782442325313, 0.07081949525614457, 0.1452758841310317, 0.22092005896404987, 0.24595452047663097, 0.16658221674888973]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.465]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]] [[-0.005]
 [ 1.104]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.464]
 [0.465]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.482]
 [0.492]
 [0.496]
 [0.501]
 [0.497]
 [0.493]] [[3.524]
 [3.07 ]
 [3.575]
 [3.575]
 [3.59 ]
 [3.601]
 [3.552]] [[1.561]
 [0.973]
 [1.594]
 [1.602]
 [1.628]
 [1.634]
 [1.57 ]]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.463]
 [0.491]
 [0.51 ]
 [0.478]
 [0.497]
 [0.476]] [[1.644]
 [3.306]
 [1.238]
 [2.191]
 [2.144]
 [0.711]
 [2.625]] [[0.478]
 [0.463]
 [0.491]
 [0.51 ]
 [0.478]
 [0.497]
 [0.476]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15055242334995586, 0.0708687325463601, 0.14468163675331402, 0.2210736537477786, 0.24612552048250122, 0.16669803312009016]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.3  ]
 [0.3  ]
 [0.319]
 [0.3  ]
 [0.321]
 [0.328]] [[3.371]
 [3.42 ]
 [3.42 ]
 [3.347]
 [3.42 ]
 [3.37 ]
 [3.33 ]] [[1.486]
 [1.501]
 [1.501]
 [1.441]
 [1.501]
 [1.476]
 [1.437]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15076944895132718, 0.07097089184049213, 0.14489019944878648, 0.22139233770904862, 0.24583919722331202, 0.16613792482703352]
line 256 mcts: sample exp_bonus -0.7523713343423061
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15076944895132718, 0.07097089184049213, 0.14489019944878648, 0.22139233770904862, 0.24583919722331202, 0.16613792482703352]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.562]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[3.533]
 [3.492]
 [3.44 ]
 [3.44 ]
 [3.44 ]
 [3.44 ]
 [3.44 ]] [[2.153]
 [2.195]
 [2.052]
 [2.052]
 [2.052]
 [2.052]
 [2.052]]
siam score:  -0.6809099
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.33 ]
 [0.364]
 [0.304]
 [0.19 ]
 [0.345]
 [0.275]] [[ 1.034]
 [ 1.201]
 [-1.345]
 [ 0.569]
 [-0.159]
 [-1.658]
 [ 1.608]] [[0.115]
 [0.33 ]
 [0.364]
 [0.304]
 [0.19 ]
 [0.345]
 [0.275]]
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.489]
 [0.713]
 [0.763]
 [0.625]
 [0.635]
 [0.592]] [[2.655]
 [3.826]
 [1.646]
 [1.802]
 [2.253]
 [2.052]
 [2.323]] [[0.551]
 [0.489]
 [0.713]
 [0.763]
 [0.625]
 [0.635]
 [0.592]]
line 256 mcts: sample exp_bonus 0.2978982442545398
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.712]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]] [[3.018]
 [2.236]
 [3.018]
 [3.018]
 [3.018]
 [3.018]
 [3.018]] [[1.892]
 [1.14 ]
 [1.892]
 [1.892]
 [1.892]
 [1.892]
 [1.892]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15089376882401515, 0.07118959967042095, 0.14430178637159535, 0.22100787165230326, 0.24595706882648666, 0.1666499046551786]
rdn probs:  [0.15089376882401515, 0.07118959967042095, 0.14430178637159535, 0.22100787165230326, 0.24595706882648666, 0.1666499046551786]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.1854973427540856
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.571]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[-0.351]
 [ 1.589]
 [-0.351]
 [-0.351]
 [-0.351]
 [-0.351]
 [-0.351]] [[1.392]
 [2.057]
 [1.392]
 [1.392]
 [1.392]
 [1.392]
 [1.392]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1507976071895924, 0.07146436077650146, 0.14451577184144693, 0.22132934624736958, 0.24499985792031057, 0.16689305602477905]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.539]] [[3.056]
 [3.056]
 [3.056]
 [3.056]
 [3.056]
 [3.056]
 [3.278]] [[1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.906]]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.697]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.696]] [[3.363]
 [2.161]
 [3.363]
 [3.363]
 [3.363]
 [3.363]
 [2.619]] [[1.517]
 [0.368]
 [1.517]
 [1.517]
 [1.517]
 [1.517]
 [0.838]]
siam score:  -0.6754844
from probs:  [0.15097240960374975, 0.07154720123413709, 0.1446832924425526, 0.22105634850836414, 0.24465423215980997, 0.1670865160513864]
siam score:  -0.6670268
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15124108815057966, 0.07167453044685741, 0.14494077854127796, 0.22092181301819164, 0.24383791772217486, 0.16738387212091854]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.259]
 [0.261]
 [0.308]
 [0.331]
 [0.261]
 [0.335]] [[5.263]
 [6.689]
 [5.263]
 [5.488]
 [3.104]
 [5.263]
 [5.803]] [[1.08 ]
 [1.577]
 [1.08 ]
 [1.191]
 [0.373]
 [1.08 ]
 [1.32 ]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.644]
 [0.703]
 [0.71 ]
 [0.703]
 [0.703]
 [0.723]] [[3.432]
 [3.614]
 [3.432]
 [2.85 ]
 [3.432]
 [3.432]
 [3.6  ]] [[1.983]
 [2.001]
 [1.983]
 [1.83 ]
 [1.983]
 [1.983]
 [2.04 ]]
siam score:  -0.6552083
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.996]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[1.992]
 [1.619]
 [1.73 ]
 [1.73 ]
 [1.73 ]
 [1.73 ]
 [1.73 ]] [[2.412]
 [2.985]
 [2.351]
 [2.351]
 [2.351]
 [2.351]
 [2.351]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15167282216532993, 0.07187913313230195, 0.1446696076711932, 0.22102556352370495, 0.24329159992043914, 0.16746127358703086]
line 256 mcts: sample exp_bonus 3.749510799755731
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.66217726
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.65 ]
 [0.65 ]
 [0.714]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[1.695]
 [1.684]
 [1.684]
 [1.675]
 [1.684]
 [1.684]
 [1.684]] [[0.381]
 [0.343]
 [0.343]
 [0.469]
 [0.343]
 [0.343]
 [0.343]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15158881298690685, 0.07167191786001244, 0.14491328374902818, 0.22139785071856022, 0.24308394158495716, 0.167344193100535]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[2.049]
 [2.049]
 [2.049]
 [2.049]
 [2.049]
 [2.049]
 [2.049]] [[1.763]
 [1.763]
 [1.763]
 [1.763]
 [1.763]
 [1.763]
 [1.763]]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.841]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]] [[0.683]
 [0.796]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[0.596]
 [0.841]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.426]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[1.264]
 [2.051]
 [1.264]
 [1.264]
 [1.264]
 [1.264]
 [1.264]] [[0.438]
 [0.426]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1517296498048833, 0.07157257993287641, 0.1453719974418145, 0.22157176403856904, 0.24385340819020368, 0.165900600591653]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.482]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.45 ]] [[1.479]
 [2.655]
 [1.479]
 [1.479]
 [1.479]
 [1.479]
 [2.148]] [[0.449]
 [0.482]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.45 ]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.524]
 [0.575]
 [0.704]
 [0.698]
 [0.65 ]
 [0.66 ]] [[-0.179]
 [ 2.937]
 [-0.132]
 [-0.988]
 [-0.499]
 [-0.158]
 [ 1.307]] [[ 0.429]
 [ 1.722]
 [ 0.233]
 [-0.008]
 [ 0.234]
 [ 0.334]
 [ 1.098]]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.482]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[3.937]
 [4.799]
 [3.937]
 [3.937]
 [3.937]
 [3.937]
 [3.937]] [[0.681]
 [1.227]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]]
from probs:  [0.1518478158226348, 0.07146624270154565, 0.14548521216321122, 0.22174432261915156, 0.24342660384410575, 0.1660298028493509]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15189975232916406, 0.07149068629963484, 0.14519294248111284, 0.22182016582701822, 0.24350986304234615, 0.1660865900207239]
siam score:  -0.6533422
Printing some Q and Qe and total Qs values:  [[0.987]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]] [[0.802]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]] [[0.987]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]]
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]] [[0.343]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]] [[0.914]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15181422007683568, 0.07144806363507974, 0.1454346560595323, 0.22218944648991698, 0.24391525217056806, 0.16519836156806736]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15181422007683568, 0.07144806363507974, 0.1454346560595323, 0.22218944648991698, 0.24391525217056806, 0.16519836156806736]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15142950219591222, 0.07158398098897965, 0.14571131987113556, 0.22261212276825024, 0.24315045182813824, 0.16551262234758424]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15142950219591222, 0.07158398098897965, 0.14571131987113556, 0.22261212276825024, 0.24315045182813824, 0.16551262234758424]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1512739671317094, 0.07134676356695999, 0.14588389210587846, 0.22234830561973504, 0.24343842545220426, 0.16570864612351285]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15137763247178992, 0.07123567644812946, 0.14598386372408995, 0.22197537462069888, 0.24360524944473697, 0.16582220329055483]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]] [[2.722]
 [2.937]
 [2.722]
 [2.722]
 [2.722]
 [2.722]
 [2.722]] [[-0.743]
 [-0.671]
 [-0.743]
 [-0.743]
 [-0.743]
 [-0.743]
 [-0.743]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.031]
 [0.046]
 [0.044]
 [0.049]
 [0.05 ]
 [0.043]] [[2.263]
 [2.875]
 [2.848]
 [2.909]
 [2.653]
 [2.694]
 [2.715]] [[-0.683]
 [-0.732]
 [-0.71 ]
 [-0.693]
 [-0.77 ]
 [-0.754]
 [-0.76 ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.008]
 [ 0.156]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.009]
 [-0.015]] [[3.1  ]
 [3.658]
 [4.187]
 [4.187]
 [4.187]
 [2.962]
 [3.054]] [[-0.994]
 [-0.48 ]
 [-0.696]
 [-0.696]
 [-0.696]
 [-1.043]
 [-1.025]]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15117349416112444, 0.0712967348199404, 0.14610899115284615, 0.22164239379394718, 0.24381405127883654, 0.16596433479330536]
using explorer policy with actor:  1
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15094959549068832, 0.07134801524113485, 0.14621408054614776, 0.22180181084631298, 0.24398941537193675, 0.16569708250377943]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.495]
 [0.501]
 [0.494]
 [0.496]
 [0.501]
 [0.501]] [[1.68 ]
 [1.986]
 [1.68 ]
 [0.831]
 [0.891]
 [1.68 ]
 [0.94 ]] [[0.501]
 [0.495]
 [0.501]
 [0.494]
 [0.496]
 [0.501]
 [0.501]]
siam score:  -0.64894956
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.09 ]
 [-0.037]
 [-0.035]
 [-0.037]
 [-0.033]
 [-0.037]] [[4.175]
 [4.569]
 [3.475]
 [3.512]
 [3.475]
 [3.572]
 [3.596]] [[ 0.087]
 [ 0.201]
 [-0.241]
 [-0.221]
 [-0.241]
 [-0.19 ]
 [-0.184]]
Printing some Q and Qe and total Qs values:  [[0.202]
 [0.043]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.163]] [[-0.119]
 [ 0.639]
 [ 0.042]
 [ 0.042]
 [ 0.042]
 [ 0.042]
 [ 0.329]] [[1.385]
 [1.693]
 [1.596]
 [1.596]
 [1.596]
 [1.596]
 [1.595]]
line 256 mcts: sample exp_bonus -0.09634494873810562
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.15084279412361418, 0.07129456307856886, 0.14643186145987894, 0.22213217712056585, 0.24374008937146155, 0.16555851484591066]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.13 ]
 [-0.131]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]] [[3.131]
 [3.022]
 [3.131]
 [3.131]
 [3.131]
 [3.131]
 [3.131]] [[0.444]
 [0.297]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15107821718829412, 0.07140583378680844, 0.14666040030253272, 0.22091814480405958, 0.24412049891743007, 0.16581690500087518]
from probs:  [0.15107821718829412, 0.07140583378680844, 0.14666040030253272, 0.22091814480405958, 0.24412049891743007, 0.16581690500087518]
from probs:  [0.15119410589913637, 0.07146060759985826, 0.14677290020515923, 0.22108760615646023, 0.2443077582751074, 0.16517702186427857]
1354 1831
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.3157226239501064
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[-1.885]
 [-1.885]
 [-1.885]
 [-1.885]
 [-1.885]
 [-1.885]
 [-1.885]] [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15124219494503488, 0.07116527483411225, 0.14681958303510587, 0.22115792564396156, 0.2443854632023675, 0.16522955833941808]
siam score:  -0.6596623
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15128986992781382, 0.07087248498440885, 0.14686586390993284, 0.22122763965664388, 0.24446249906364836, 0.16528164245755236]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15128986992781382, 0.07087248498440885, 0.14686586390993284, 0.22122763965664388, 0.24446249906364836, 0.16528164245755236]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.151313553813143, 0.07072703341166106, 0.14688885523302192, 0.22126227204843438, 0.24450076878916183, 0.16530751670457794]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.541]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[1.65 ]
 [2.298]
 [1.65 ]
 [1.65 ]
 [1.65 ]
 [1.65 ]
 [1.65 ]] [[-0.276]
 [ 0.392]
 [-0.276]
 [-0.276]
 [-0.276]
 [-0.276]
 [-0.276]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.02  0.02  0.041 0.122 0.755]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15145308858795153, 0.07079022171321853, 0.14732308934954477, 0.22088858857599525, 0.24335920193095117, 0.1661858098423387]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 2.0622121739400976
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15145308858795153, 0.07079022171321853, 0.14732308934954477, 0.22088858857599525, 0.24335920193095117, 0.1661858098423387]
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[-1.752]
 [-1.89 ]
 [-1.89 ]
 [-1.89 ]
 [-1.89 ]
 [-1.89 ]
 [-1.89 ]] [[0.638]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6538476
from probs:  [0.15153060770396895, 0.07082645468450388, 0.14739849458400917, 0.22048981144668242, 0.24348376188799317, 0.1662708696928424]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.654]
 [0.654]
 [0.551]
 [0.654]
 [0.654]
 [0.654]] [[2.879]
 [2.879]
 [2.879]
 [2.574]
 [2.879]
 [2.879]
 [2.879]] [[2.194]
 [2.194]
 [2.194]
 [1.887]
 [2.194]
 [2.194]
 [2.194]]
from probs:  [0.15130841110140153, 0.07087702669338543, 0.14750374110538927, 0.2206472471499439, 0.24365761589527846, 0.1660059580546014]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.713]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.593]] [[2.215]
 [2.625]
 [2.215]
 [2.215]
 [2.215]
 [2.215]
 [2.728]] [[0.598]
 [0.713]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.593]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1510291604774254, 0.07090034783169832, 0.14755227523991032, 0.22071984817751528, 0.24373778817713598, 0.16606058009631464]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1510291604774254, 0.07090034783169832, 0.14755227523991032, 0.22071984817751528, 0.24373778817713598, 0.16606058009631464]
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]] [[-2.725]
 [-2.692]
 [-2.692]
 [-2.692]
 [-2.692]
 [-2.692]
 [-2.692]] [[0.18 ]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]]
siam score:  -0.6545492
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.26 ]
 [0.268]
 [0.268]
 [0.271]
 [0.27 ]
 [0.271]] [[-2.148]
 [ 0.819]
 [-2.033]
 [-2.06 ]
 [-2.188]
 [-2.078]
 [-2.079]] [[0.277]
 [0.26 ]
 [0.268]
 [0.268]
 [0.271]
 [0.27 ]
 [0.271]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15093272663034302, 0.07100909515681714, 0.14777859169304852, 0.2210583894375231, 0.242905912397818, 0.16631528468445037]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65508616
line 256 mcts: sample exp_bonus 1.7119421145109885
Printing some Q and Qe and total Qs values:  [[0.76 ]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[3.479]
 [2.868]
 [2.868]
 [2.868]
 [2.868]
 [2.868]
 [2.868]] [[1.03 ]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]]
Printing some Q and Qe and total Qs values:  [[1.059]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[1.184]
 [2.11 ]
 [2.11 ]
 [2.11 ]
 [2.11 ]
 [2.11 ]
 [2.11 ]] [[0.861]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]]
siam score:  -0.65135384
Printing some Q and Qe and total Qs values:  [[0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]] [[0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]] [[-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15113899046702964, 0.07110613579693331, 0.14763446092742818, 0.22033997628206542, 0.24323786628590843, 0.16654257024063499]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.3933],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.5029],
        [-0.6230],
        [-0.7446],
        [-0.6172],
        [-0.0000]], dtype=torch.float64)
-0.9702 -0.9702
-0.0530787758985 -0.44639799376980566
-0.9702 -0.9702
-0.7253234999999999 -0.7253234999999999
-0.5345999999999997 -0.5345999999999997
-0.0727797758985 -0.5756991733752893
-0.024259925299500003 -0.6472578926738409
-0.024259925299500003 -0.7688890457587364
-0.024259925299500003 -0.6414442722804157
-0.9360449999999999 -0.9360449999999999
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.89390438943552
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.588]] [[1.925]
 [2.209]
 [2.209]
 [2.209]
 [2.209]
 [2.209]
 [3.389]] [[0.749]
 [0.997]
 [0.997]
 [0.997]
 [0.997]
 [0.997]
 [2.255]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.653]] [[1.842]
 [2.324]
 [2.324]
 [2.324]
 [2.324]
 [2.324]
 [3.532]] [[1.53 ]
 [1.673]
 [1.673]
 [1.673]
 [1.673]
 [1.673]
 [1.982]]
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.651]
 [0.308]
 [0.769]
 [0.21 ]
 [0.379]
 [0.607]] [[3.13 ]
 [3.648]
 [2.526]
 [3.787]
 [1.549]
 [2.749]
 [4.045]] [[1.692]
 [1.877]
 [1.341]
 [1.988]
 [0.992]
 [1.449]
 [1.969]]
1380 1855
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15069175973469146, 0.0711977943990627, 0.14709864682429813, 0.2210839949292963, 0.24282296748738819, 0.16710483662526324]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.274]
 [0.274]
 [0.059]
 [0.274]
 [0.274]
 [0.065]] [[0.965]
 [1.022]
 [1.022]
 [1.455]
 [1.022]
 [1.022]
 [0.375]] [[1.74 ]
 [1.892]
 [1.892]
 [2.016]
 [1.892]
 [1.892]
 [1.408]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.59 ]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]] [[-2.067]
 [ 0.021]
 [-2.067]
 [-2.067]
 [-2.067]
 [-2.067]
 [-2.067]] [[0.249]
 [0.841]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15111627653579227, 0.07108745000643206, 0.14649531995381948, 0.22119818670382457, 0.24291170612352858, 0.16719106067660308]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.32074845565792565
siam score:  -0.6425087
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1513333966421974, 0.07118958668596975, 0.14636925535498668, 0.22100900317164118, 0.24266748151227852, 0.16743127663292656]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1513333966421974, 0.07118958668596975, 0.14636925535498668, 0.22100900317164118, 0.24266748151227852, 0.16743127663292656]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1513333966421974, 0.07118958668596975, 0.14636925535498668, 0.22100900317164118, 0.24266748151227852, 0.16743127663292656]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1513333966421974, 0.07118958668596975, 0.14636925535498668, 0.22100900317164118, 0.24266748151227852, 0.16743127663292656]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[-1.119]
 [-1.119]
 [-1.119]
 [-1.119]
 [-1.119]
 [-1.119]
 [-1.119]] [[0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -6.410623629801155
siam score:  -0.6449685
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1516466468600074, 0.07133694446706752, 0.146672230125513, 0.22096078435997854, 0.2419896267457579, 0.1673937674416757]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.403]
 [0.405]
 [0.404]
 [0.405]
 [0.401]
 [0.408]] [[-1.535]
 [ 0.612]
 [-0.779]
 [-0.95 ]
 [-0.966]
 [-0.907]
 [-1.171]] [[0.389]
 [0.403]
 [0.405]
 [0.404]
 [0.405]
 [0.401]
 [0.408]]
using explorer policy with actor:  1
first move QE:  0.302906748289142
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15184762654313533, 0.07127634016545223, 0.14686661713600527, 0.22125362715831198, 0.2411401719660311, 0.1676156170310641]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.204 0.02  0.061 0.286 0.143 0.082 0.204]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [0.543]
 [0.727]
 [0.543]
 [0.543]
 [0.588]] [[2.327]
 [2.327]
 [2.327]
 [2.59 ]
 [2.327]
 [2.327]
 [2.585]] [[0.578]
 [0.578]
 [0.578]
 [1.121]
 [0.578]
 [0.578]
 [0.839]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.592]
 [0.695]
 [0.714]
 [0.695]
 [0.695]
 [0.664]] [[2.362]
 [2.764]
 [3.339]
 [2.774]
 [3.339]
 [3.339]
 [2.911]] [[1.176]
 [2.024]
 [2.998]
 [2.281]
 [2.998]
 [2.998]
 [2.366]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.302]
 [0.304]
 [0.309]
 [0.306]
 [0.304]
 [0.305]] [[-4.771]
 [-2.63 ]
 [-4.104]
 [-4.873]
 [-4.644]
 [-4.527]
 [-4.563]] [[0.315]
 [0.302]
 [0.304]
 [0.309]
 [0.306]
 [0.304]
 [0.305]]
using another actor
from probs:  [0.15225229896104225, 0.07130902833746458, 0.1472156882839946, 0.22128126624839647, 0.23992612522769896, 0.1680155929414032]
using explorer policy with actor:  0
first move QE:  0.3042935781579377
UNIT TEST: sample policy line 217 mcts : [0.306 0.306 0.061 0.041 0.082 0.082 0.122]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15261295787926302, 0.07101862700163894, 0.14722831139940706, 0.22080422453398, 0.2399222868550838, 0.16841359233062736]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15261295787926302, 0.07101862700163894, 0.14722831139940706, 0.22080422453398, 0.2399222868550838, 0.16841359233062736]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.30473350632799406
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15286835197904883, 0.07098548142250474, 0.14747469442333472, 0.22117373507186217, 0.23918576021531898, 0.16831197688793045]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.643]
 [0.682]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[ 3.789]
 [ 1.735]
 [-1.483]
 [ 1.735]
 [ 1.735]
 [ 1.735]
 [ 1.735]] [[1.813]
 [1.153]
 [0.175]
 [1.153]
 [1.153]
 [1.153]
 [1.153]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15296767829081034, 0.07088018946237132, 0.1475705162025689, 0.22081910701958102, 0.23934117131958285, 0.16842133770508544]
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.367]
 [0.367]
 [0.223]
 [0.367]
 [0.367]
 [0.367]] [[2.975]
 [2.975]
 [2.975]
 [2.342]
 [2.975]
 [2.975]
 [2.975]] [[1.912]
 [1.912]
 [1.912]
 [1.644]
 [1.912]
 [1.912]
 [1.912]]
in main func line 156:  1400
Printing some Q and Qe and total Qs values:  [[0.813]
 [1.223]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]] [[0.733]
 [3.378]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]] [[1.097]
 [2.547]
 [1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.097]]
line 256 mcts: sample exp_bonus -3.0371595690629043
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15305786245979913, 0.07122548667542533, 0.14795245863003204, 0.21991444587548736, 0.23937374304834183, 0.1684760033109144]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.557]] [[2.299]
 [2.56 ]
 [2.56 ]
 [2.56 ]
 [2.56 ]
 [2.56 ]
 [2.732]] [[0.343]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.629]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.850159558765556
siam score:  -0.6127399
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15328823765060778, 0.07133269178609125, 0.148175149417292, 0.22024545031796386, 0.2386097219918029, 0.16834874883624232]
using explorer policy with actor:  1
first move QE:  0.3034201990172479
line 256 mcts: sample exp_bonus 6.649855759018997
rdn beta is 0 so we're just using the maxi policy
first move QE:  0.30342329900031717
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
line 256 mcts: sample exp_bonus 3.8061614055284583
from probs:  [0.15346147793148154, 0.07111023134612067, 0.14800633018713724, 0.22000356139914207, 0.23887938922714083, 0.16853900990897777]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15324339877633092, 0.07116045270580143, 0.14811085915579125, 0.2201589381681234, 0.23904809698549848, 0.16827825420845444]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.496]
 [0.496]
 [0.646]
 [0.496]
 [0.496]
 [0.452]] [[2.472]
 [2.472]
 [2.472]
 [2.618]
 [2.472]
 [2.472]
 [2.411]] [[0.458]
 [0.458]
 [0.458]
 [0.904]
 [0.458]
 [0.458]
 [0.309]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -4.920059582779066
siam score:  -0.62578756
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1407 1914
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15304241521425044, 0.07106742415368235, 0.14789578698598072, 0.22033755503342026, 0.23924203878996939, 0.16841477982269676]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15309350112782094, 0.0710911466248869, 0.14761135262104902, 0.220411104220925, 0.23932189833804923, 0.16847099706726895]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.56 ]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[0.946]
 [1.042]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]] [[0.557]
 [0.56 ]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[1.648]
 [1.648]
 [1.648]
 [1.648]
 [1.648]
 [1.648]
 [1.648]]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.801]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[-0.458]
 [ 1.274]
 [-0.458]
 [-0.458]
 [-0.458]
 [-0.458]
 [-0.458]] [[0.959]
 [1.674]
 [0.959]
 [0.959]
 [0.959]
 [0.959]
 [0.959]]
siam score:  -0.6295408
from probs:  [0.15339173051351532, 0.07093042204176962, 0.1472163043743183, 0.2203280877357974, 0.23973457033156528, 0.16839888500303407]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.692]
 [0.501]
 [0.692]
 [0.483]
 [0.692]
 [0.529]] [[0.632]
 [0.883]
 [0.464]
 [0.883]
 [1.037]
 [0.883]
 [1.627]] [[0.503]
 [0.692]
 [0.501]
 [0.692]
 [0.483]
 [0.692]
 [0.529]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15334708496548624, 0.07091079115298506, 0.14748491383747833, 0.21975632896234898, 0.24017198773931336, 0.1683288933423879]
Printing some Q and Qe and total Qs values:  [[1.474]
 [1.475]
 [1.472]
 [1.473]
 [1.472]
 [1.475]
 [1.472]] [[0.292]
 [0.297]
 [0.296]
 [0.293]
 [0.296]
 [0.297]
 [0.297]] [[1.424]
 [1.43 ]
 [1.427]
 [1.424]
 [1.426]
 [1.43 ]
 [1.427]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1535135556204199, 0.07083921188803019, 0.1476450206318454, 0.21999489221913612, 0.23987163523633465, 0.16813568440423382]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.7  ]
 [0.671]
 [0.756]
 [0.642]
 [0.721]
 [0.716]] [[1.548]
 [1.682]
 [1.328]
 [1.872]
 [2.676]
 [1.187]
 [2.021]] [[0.79 ]
 [0.907]
 [0.598]
 [1.11 ]
 [1.639]
 [0.535]
 [1.19 ]]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.661]
 [0.41 ]
 [0.724]
 [0.375]
 [0.575]
 [0.382]] [[2.392]
 [2.907]
 [2.644]
 [3.214]
 [2.051]
 [2.355]
 [2.969]] [[0.985]
 [1.891]
 [1.293]
 [2.296]
 [0.621]
 [1.196]
 [1.602]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.677]
 [0.677]
 [0.597]
 [0.677]
 [0.566]
 [0.645]] [[2.553]
 [2.659]
 [2.659]
 [2.523]
 [2.659]
 [1.267]
 [2.646]] [[1.144]
 [1.635]
 [1.635]
 [1.427]
 [1.635]
 [0.503]
 [1.58 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]] [[0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]] [[2.187]
 [2.187]
 [2.187]
 [2.187]
 [2.187]
 [2.187]
 [2.187]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15383960075282024, 0.07098966622495063, 0.1476278812092723, 0.21997695950765725, 0.2398214769398673, 0.16774441536543225]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.056]
 [0.081]
 [0.094]
 [0.086]
 [0.077]
 [0.094]] [[1.939]
 [1.656]
 [1.567]
 [2.337]
 [2.084]
 [3.832]
 [2.568]] [[-0.144]
 [-0.311]
 [-0.29 ]
 [-0.007]
 [-0.109]
 [ 0.457]
 [ 0.071]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15342304343924093, 0.07109642492981778, 0.14784989326029735, 0.2198240328315034, 0.24018213549256842, 0.167624470046572]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [ 0.004]
 [-0.009]
 [-0.018]] [[2.753]
 [3.64 ]
 [3.64 ]
 [3.64 ]
 [2.765]
 [3.184]
 [3.511]] [[-0.965]
 [-0.374]
 [-0.374]
 [-0.374]
 [-0.944]
 [-0.692]
 [-0.493]]
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.07 ]
 [-0.063]
 [-0.052]
 [-0.046]
 [-0.051]
 [-0.058]] [[2.252]
 [1.919]
 [1.97 ]
 [1.858]
 [1.732]
 [1.855]
 [2.07 ]] [[-0.584]
 [-0.951]
 [-0.886]
 [-0.977]
 [-1.091]
 [-0.976]
 [-0.777]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15375708096503962, 0.0711024481374165, 0.1471850122254414, 0.2198200003270301, 0.24014603097538578, 0.16798942736968656]
from probs:  [0.15377985913795558, 0.0709648376356773, 0.1472068167864936, 0.21985256531816036, 0.24018160714387382, 0.16801431397783925]
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.2  ]
 [0.425]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[-2.219]
 [ 0.445]
 [-2.248]
 [-2.186]
 [-2.281]
 [-2.269]
 [-2.152]] [[0.426]
 [0.794]
 [0.345]
 [0.374]
 [0.343]
 [0.347]
 [0.386]]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[2.384]
 [2.384]
 [2.384]
 [2.384]
 [2.384]
 [2.384]
 [2.384]] [[1.469]
 [1.469]
 [1.469]
 [1.469]
 [1.469]
 [1.469]
 [1.469]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15380772049505637, 0.07068532572234972, 0.1475430256416153, 0.2193934849931793, 0.24017239761880177, 0.16839804552899748]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15373193808626265, 0.07079870700802801, 0.14745242047693596, 0.21879113259569044, 0.24055764101901503, 0.16866816081406796]
siam score:  -0.6365376
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15378203887375919, 0.0708217800988834, 0.147174577722567, 0.21886243598376165, 0.24063603804830766, 0.16872312927272118]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.456]
 [0.456]
 [0.433]
 [0.456]
 [0.456]
 [0.456]] [[1.566]
 [1.566]
 [1.566]
 [0.842]
 [1.566]
 [1.566]
 [1.566]] [[1.651]
 [1.651]
 [1.651]
 [1.456]
 [1.651]
 [1.651]
 [1.651]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -4.982719760240698
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]] [[-5.398]
 [-5.983]
 [-5.983]
 [-5.983]
 [-5.983]
 [-5.983]
 [-5.983]] [[0.138]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.531 0.122 0.02  0.122 0.122 0.041]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.559]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[-1.389]
 [ 0.076]
 [-1.389]
 [-1.389]
 [-1.389]
 [-1.389]
 [-1.389]] [[0.416]
 [0.559]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.635]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[1.293]
 [2.047]
 [1.293]
 [1.293]
 [1.293]
 [1.293]
 [1.293]] [[0.581]
 [0.938]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15351120164093615, 0.07041531260458249, 0.14652704302115324, 0.21935355988904506, 0.24110604947799, 0.1690868333662931]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.676]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]] [[-1.915]
 [ 0.702]
 [-1.915]
 [-1.915]
 [-1.915]
 [-1.915]
 [-1.915]] [[0.731]
 [1.369]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.672]
 [0.55 ]] [[3.378]
 [3.378]
 [3.378]
 [3.378]
 [3.378]
 [4.244]
 [3.378]] [[1.726]
 [1.726]
 [1.726]
 [1.726]
 [1.726]
 [2.24 ]
 [1.726]]
siam score:  -0.6277991
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.633567436554406
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9687421564052536
siam score:  -0.62710756
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1535963983560613, 0.07045439218486259, 0.1466083636193178, 0.21947529825441375, 0.24068487323142493, 0.16918067435391962]
from probs:  [0.1535963983560613, 0.07045439218486259, 0.1466083636193178, 0.21947529825441375, 0.24068487323142493, 0.16918067435391962]
from probs:  [0.15364548891420893, 0.07047690993575599, 0.14633561326711658, 0.21954544420188546, 0.24076179791775273, 0.16923474576328024]
from probs:  [0.15373043761429322, 0.07051587575198211, 0.14641652042690975, 0.2196668281762942, 0.24034202445888303, 0.16932831357163755]
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.438]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.446]] [[1.007]
 [1.147]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.428]] [[1.235]
 [1.509]
 [1.531]
 [1.531]
 [1.531]
 [1.531]
 [1.581]]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]] [[2.796]
 [2.796]
 [2.796]
 [2.796]
 [2.796]
 [2.796]
 [2.796]] [[0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.431]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[-2.091]
 [-1.841]
 [-2.091]
 [-2.091]
 [-2.091]
 [-2.091]
 [-2.091]] [[0.442]
 [0.431]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]]
line 256 mcts: sample exp_bonus 1.8171825210392054
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.62686974
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15375252115515772, 0.070382354368974, 0.1464375533157915, 0.2196983835497901, 0.24037654984631418, 0.16935263776397255]
Printing some Q and Qe and total Qs values:  [[1.305]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]] [[1.814]
 [1.982]
 [1.982]
 [1.982]
 [1.982]
 [1.982]
 [1.982]] [[2.583]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9178464332512253
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8124170464391862
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.3744],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.4627],
        [-0.4997],
        [-0.6057],
        [-0.0000],
        [-0.4697]], dtype=torch.float64)
-0.024259925299500003 -0.398641025452247
-0.95128310475 -0.95128310475
-0.9219315600000001 -0.9219315600000001
-0.01950497504999926 -0.01950497504999926
-0.96074352 -0.96074352
-0.043375785898500004 -0.5060587793043744
-0.0530787758985 -0.5527640544720016
-0.024259925299500003 -0.6299930759578143
-0.9702 -0.9702
-0.0727797758985 -0.5425063703239199
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15375252115515772, 0.070382354368974, 0.1464375533157915, 0.2196983835497901, 0.24037654984631418, 0.16935263776397255]
Printing some Q and Qe and total Qs values:  [[1.073]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]] [[1.843]
 [1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]] [[1.195]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1534829979007762, 0.07040477060261105, 0.14648419254003733, 0.21976835577989645, 0.24045310791190508, 0.16940657526477373]
using explorer policy with actor:  0
using explorer policy with actor:  0
UNIT TEST: sample policy line 217 mcts : [0.612 0.143 0.041 0.02  0.061 0.02  0.102]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15388833024497509, 0.070447238933453, 0.14687104178042498, 0.21987349839666304, 0.2394396771187941, 0.1694802135256899]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15394560945303976, 0.07047346029832212, 0.14692570906382083, 0.21995533812968585, 0.23952879963421028, 0.1691710834209212]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.409031282788173
siam score:  -0.6219112
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15421961995965236, 0.07059889725380071, 0.14686718600743523, 0.22034684051710174, 0.23886647294244417, 0.16910098331956583]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.565]
 [0.583]
 [0.599]
 [0.636]
 [0.607]
 [0.748]] [[2.101]
 [2.209]
 [2.289]
 [2.028]
 [1.969]
 [2.103]
 [1.896]] [[0.879]
 [0.725]
 [0.843]
 [0.613]
 [0.629]
 [0.705]
 [0.779]]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.064]
 [-0.056]
 [-0.053]
 [-0.055]
 [-0.055]
 [-0.038]] [[3.797]
 [3.843]
 [3.607]
 [3.708]
 [3.64 ]
 [3.822]
 [4.869]] [[-0.535]
 [-0.545]
 [-0.686]
 [-0.613]
 [-0.663]
 [-0.541]
 [ 0.19 ]]
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.051]
 [-0.06 ]
 [-0.057]
 [-0.059]
 [-0.059]
 [-0.04 ]] [[3.797]
 [3.595]
 [3.607]
 [3.708]
 [3.64 ]
 [3.822]
 [4.919]] [[-0.329]
 [-0.47 ]
 [-0.481]
 [-0.408]
 [-0.457]
 [-0.336]
 [ 0.434]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6275571
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]] [[3.649]
 [3.649]
 [3.649]
 [3.649]
 [3.649]
 [3.649]
 [3.649]] [[-0.452]
 [-0.452]
 [-0.452]
 [-0.452]
 [-0.452]
 [-0.452]
 [-0.452]]
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.063]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.058]] [[3.445]
 [3.357]
 [3.445]
 [3.445]
 [3.445]
 [3.445]
 [3.385]] [[-0.128]
 [-0.179]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.153]]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.636]
 [0.636]
 [0.723]
 [0.636]
 [0.636]
 [0.823]] [[1.804]
 [2.254]
 [2.254]
 [2.201]
 [2.254]
 [2.254]
 [2.209]] [[1.101]
 [1.471]
 [1.471]
 [1.61 ]
 [1.471]
 [1.471]
 [1.815]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15426877299557495, 0.07062139860696005, 0.14659527463932331, 0.2204170696887869, 0.23894260471042747, 0.16915487935892728]
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.109]
 [-0.09 ]
 [-0.072]
 [-0.078]
 [-0.098]
 [-0.07 ]] [[3.434]
 [2.555]
 [3.041]
 [3.053]
 [2.209]
 [3.192]
 [4.691]] [[-0.274]
 [-0.869]
 [-0.532]
 [-0.491]
 [-1.027]
 [-0.452]
 [ 0.53 ]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.15431287678415184, 0.07035569922692488, 0.14663718465697315, 0.2204800845663381, 0.2390109158398643, 0.16920323892574776]
siam score:  -0.63277704
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
line 256 mcts: sample exp_bonus 0.6837110030483734
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.563]
 [0.563]
 [0.671]
 [0.563]
 [0.563]
 [0.511]] [[2.568]
 [2.568]
 [2.568]
 [1.695]
 [2.568]
 [2.568]
 [1.502]] [[2.181]
 [2.181]
 [2.181]
 [1.75 ]
 [2.181]
 [2.181]
 [1.489]]
siam score:  -0.63865256
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15436469029650218, 0.07024454753657337, 0.14665762028435075, 0.22051632542855476, 0.23899707041579862, 0.1692197460382203]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.51 ]
 [0.588]
 [0.515]
 [0.321]
 [0.594]
 [0.491]] [[ 0.136]
 [ 0.501]
 [-3.969]
 [-4.492]
 [-2.286]
 [-4.11 ]
 [-1.129]] [[ 1.045]
 [ 1.34 ]
 [ 0.162]
 [-0.043]
 [ 0.408]
 [ 0.129]
 [ 0.871]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.2754023315864006
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15444066268156062, 0.07014212870303334, 0.1470323157765385, 0.21966488054137098, 0.2390679268015193, 0.16965208549597732]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15444066268156062, 0.07014212870303334, 0.1470323157765385, 0.21966488054137098, 0.2390679268015193, 0.16965208549597732]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.8414412867365386
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1544896087511949, 0.07016435848019295, 0.14676198919969732, 0.2197344977804614, 0.23914369334634875, 0.1697058524421048]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.306]
 [0.523]
 [0.523]
 [0.153]
 [0.523]
 [0.739]] [[ 0.268]
 [ 0.282]
 [ 0.519]
 [ 0.519]
 [-0.934]
 [ 0.519]
 [ 0.585]] [[0.404]
 [0.306]
 [0.523]
 [0.523]
 [0.153]
 [0.523]
 [0.739]]
line 256 mcts: sample exp_bonus -2.944373417757928
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.612]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[-0.067]
 [ 0.164]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]] [[0.424]
 [0.612]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.565]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[-2.882]
 [-2.017]
 [-2.882]
 [-2.882]
 [-2.882]
 [-2.882]
 [-2.882]] [[0.417]
 [0.565]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6271386
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1545600316784571, 0.07005617540491786, 0.14651321403484172, 0.21983466210011546, 0.2392527052019543, 0.1697832115797135]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1545600316784571, 0.07005617540491786, 0.14651321403484172, 0.21983466210011546, 0.2392527052019543, 0.1697832115797135]
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.485]
 [0.643]
 [0.654]
 [0.704]
 [0.629]
 [0.601]] [[0.619]
 [1.523]
 [0.382]
 [0.325]
 [0.637]
 [0.389]
 [0.382]] [[0.747]
 [0.485]
 [0.643]
 [0.654]
 [0.704]
 [0.629]
 [0.601]]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.486]
 [0.475]
 [0.504]
 [0.504]
 [0.511]
 [0.697]] [[-1.189]
 [ 0.309]
 [-1.785]
 [-1.265]
 [-1.671]
 [-1.559]
 [-0.338]] [[0.468]
 [0.486]
 [0.475]
 [0.504]
 [0.504]
 [0.511]
 [0.697]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.557]
 [0.511]
 [0.532]
 [0.569]
 [0.619]
 [0.635]] [[-0.47 ]
 [ 0.456]
 [-0.765]
 [-0.866]
 [-0.398]
 [-0.245]
 [-0.404]] [[0.531]
 [0.557]
 [0.511]
 [0.532]
 [0.569]
 [0.619]
 [0.635]]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[4.167]
 [1.78 ]
 [1.78 ]
 [1.78 ]
 [1.78 ]
 [1.78 ]
 [1.78 ]] [[0.339]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]]
siam score:  -0.63176745
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[0.177]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[0.528]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]] [[1.876]
 [2.442]
 [2.442]
 [2.442]
 [2.442]
 [2.442]
 [2.442]] [[1.823]
 [1.952]
 [1.952]
 [1.952]
 [1.952]
 [1.952]
 [1.952]]
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.472]
 [0.495]
 [0.497]
 [0.496]
 [0.493]
 [0.488]] [[0.403]
 [2.279]
 [0.825]
 [0.208]
 [0.254]
 [0.429]
 [0.908]] [[0.645]
 [0.472]
 [0.495]
 [0.497]
 [0.496]
 [0.493]
 [0.488]]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.636]
 [0.49 ]
 [0.636]
 [0.636]
 [0.489]
 [0.636]] [[4.474]
 [4.474]
 [5.389]
 [4.474]
 [4.474]
 [6.214]
 [4.474]] [[0.636]
 [0.636]
 [0.49 ]
 [0.636]
 [0.636]
 [0.489]
 [0.636]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1543747039038662, 0.07011612730802383, 0.146638595501369, 0.220022789788865, 0.2389192767091276, 0.16992850678874843]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.54 ]
 [0.609]
 [0.68 ]
 [0.629]
 [0.622]
 [0.609]] [[-1.137]
 [ 0.757]
 [-0.975]
 [-0.455]
 [-0.82 ]
 [-0.789]
 [-0.84 ]] [[0.605]
 [0.54 ]
 [0.609]
 [0.68 ]
 [0.629]
 [0.622]
 [0.609]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1546001557264689, 0.07007862107064902, 0.14653773003766832, 0.21987513008987472, 0.23873168938748812, 0.1701766736878509]
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.604]
 [0.254]
 [0.549]
 [0.604]
 [0.604]
 [0.558]] [[2.693]
 [2.693]
 [3.351]
 [3.503]
 [2.693]
 [2.693]
 [3.565]] [[1.107]
 [1.107]
 [1.247]
 [1.872]
 [1.107]
 [1.107]
 [1.951]]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.323]
 [0.385]
 [0.366]
 [0.413]
 [0.36 ]
 [0.377]] [[-0.982]
 [ 0.901]
 [-2.497]
 [-0.63 ]
 [ 0.   ]
 [-0.635]
 [-2.32 ]] [[0.433]
 [0.323]
 [0.385]
 [0.366]
 [0.413]
 [0.36 ]
 [0.377]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1546001557264689, 0.07007862107064902, 0.14653773003766832, 0.21987513008987472, 0.23873168938748812, 0.1701766736878509]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.464]
 [0.792]
 [0.792]
 [0.792]
 [0.695]
 [0.664]] [[-0.826]
 [ 1.873]
 [ 0.124]
 [ 0.124]
 [ 0.124]
 [-0.558]
 [-0.629]] [[0.651]
 [0.464]
 [0.792]
 [0.792]
 [0.792]
 [0.695]
 [0.664]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1546001557264689, 0.07007862107064902, 0.14653773003766832, 0.21987513008987472, 0.23873168938748812, 0.1701766736878509]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1546001557264689, 0.07007862107064902, 0.14653773003766832, 0.21987513008987472, 0.23873168938748812, 0.1701766736878509]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1546723821260543, 0.0701113605362987, 0.1466061898176362, 0.21951066984521425, 0.23884322051959062, 0.17025617715520597]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1546723821260543, 0.0701113605362987, 0.1466061898176362, 0.21951066984521425, 0.23884322051959062, 0.17025617715520597]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.0496937206381853
siam score:  -0.6357361
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.092]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.107]] [[2.054]
 [1.686]
 [2.054]
 [2.054]
 [2.054]
 [2.054]
 [1.862]] [[0.019]
 [0.092]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.107]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
rdn probs:  [0.1544617017305149, 0.07030332000577405, 0.14637966753066212, 0.22011167304077842, 0.23949715474006533, 0.16924648295220523]
first move QE:  0.2720410467342757
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.657]] [[2.232]
 [2.232]
 [2.232]
 [2.232]
 [2.232]
 [2.232]
 [2.186]] [[0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.368]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.976]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[4.388]
 [3.866]
 [3.866]
 [3.866]
 [3.866]
 [3.866]
 [3.866]] [[1.798]
 [1.152]
 [1.152]
 [1.152]
 [1.152]
 [1.152]
 [1.152]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.294]
 [0.339]
 [0.329]
 [0.333]
 [0.333]
 [0.334]] [[5.687]
 [5.234]
 [5.651]
 [5.74 ]
 [5.594]
 [5.584]
 [5.5  ]] [[1.229]
 [0.848]
 [1.187]
 [1.234]
 [1.141]
 [1.135]
 [1.079]]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.34 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]] [[-0.64 ]
 [ 0.283]
 [-0.55 ]
 [-0.55 ]
 [-0.55 ]
 [-0.55 ]
 [-0.55 ]] [[0.413]
 [0.557]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0000],
        [-0.4919],
        [-0.0000],
        [-0.5852],
        [-0.4543],
        [-0.5056],
        [-0.4214],
        [-0.4925],
        [-0.0000]], dtype=torch.float64)
-0.965595015 -0.965595015
-0.9514752239519999 -0.9514752239519999
-0.0628797758985 -0.5548172311470266
-0.014701994999999253 -0.014701994999999253
-0.0727797758985 -0.657940069847563
-0.024259925299500003 -0.478541713695994
-0.024259925299500003 -0.5298427032904985
-0.0536639152995 -0.47510364597755855
-0.0727797758985 -0.5653235892628122
-0.9514752239519999 -0.9514752239519999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.577]
 [0.662]
 [0.875]
 [0.678]
 [0.547]
 [0.605]] [[2.39 ]
 [2.515]
 [2.796]
 [2.908]
 [2.547]
 [3.26 ]
 [2.857]] [[0.727]
 [0.76 ]
 [1.072]
 [1.446]
 [0.922]
 [1.235]
 [1.035]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.5222255142374053
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1542874931992444, 0.07009231222054108, 0.14681231764638172, 0.22029442587962023, 0.23913271584083715, 0.16938073521337543]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1542874931992444, 0.07009231222054108, 0.14681231764638172, 0.22029442587962023, 0.23913271584083715, 0.16938073521337543]
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.003]] [[4.557]
 [4.557]
 [4.557]
 [4.557]
 [4.557]
 [4.557]
 [8.713]] [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [1.613]]
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [ 0.029]] [[4.218]
 [4.218]
 [4.218]
 [4.218]
 [4.218]
 [4.218]
 [8.395]] [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [1.466]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.056]] [[5.933]
 [5.68 ]
 [5.68 ]
 [5.68 ]
 [5.68 ]
 [5.68 ]
 [6.596]] [[1.101]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.328]]
Printing some Q and Qe and total Qs values:  [[-0.09 ]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]
 [-0.035]] [[2.913]
 [2.913]
 [2.913]
 [2.913]
 [2.913]
 [2.913]
 [6.265]] [[-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [ 1.047]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1543357739371372, 0.07011424600457275, 0.14654533209874643, 0.2203633619755707, 0.23920754694824387, 0.16943373903572911]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.027]
 [-0.026]
 [-0.025]
 [-0.025]
 [-0.026]
 [-0.023]] [[7.332]
 [8.447]
 [7.559]
 [8.128]
 [7.969]
 [8.078]
 [7.819]] [[-0.163]
 [ 0.209]
 [-0.087]
 [ 0.105]
 [ 0.051]
 [ 0.086]
 [ 0.006]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]] [[-2.305]
 [-2.321]
 [-2.336]
 [-2.321]
 [-2.321]
 [-2.321]
 [-2.321]] [[0.065]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15446444086637107, 0.06990059626430645, 0.14572429656138416, 0.22099599287552316, 0.23936005484623707, 0.169554618586178]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1542717290563744, 0.0699551897848349, 0.14583810964454783, 0.22070092896609073, 0.23954699900360207, 0.16968704354455005]
siam score:  -0.6456148
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15436640818641448, 0.0699981224501465, 0.1453138962634772, 0.22083637680271875, 0.23969401298735724, 0.1697911833098858]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.526]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[4.734]
 [4.961]
 [4.734]
 [4.734]
 [4.734]
 [4.734]
 [4.734]] [[1.476]
 [1.515]
 [1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.476]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.554]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.535]] [[0.687]
 [1.593]
 [0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.927]] [[0.456]
 [0.795]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.535]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.622]
 [0.627]] [[2.831]
 [2.831]
 [2.831]
 [2.831]
 [2.831]
 [3.195]
 [3.088]] [[1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.748]
 [1.63 ]]
1473 2041
siam score:  -0.64265037
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 8.897492671400254
siam score:  -0.6465942
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1547184262752753, 0.07029994722698059, 0.14532939138622863, 0.2194686841404134, 0.2396602453668305, 0.17052330560427165]
siam score:  -0.6438007
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.113]
 [0.552]
 [0.559]
 [0.55 ]
 [0.549]
 [0.539]] [[-2.882]
 [-0.164]
 [-2.938]
 [-2.798]
 [-2.813]
 [-2.764]
 [-2.944]] [[0.276]
 [1.381]
 [0.247]
 [0.319]
 [0.307]
 [0.33 ]
 [0.238]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.64476347
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.118]
 [0.259]
 [0.193]
 [0.185]
 [0.179]
 [0.175]] [[-6.435]
 [-2.628]
 [-5.152]
 [-5.052]
 [-5.362]
 [-5.314]
 [-5.317]] [[0.31 ]
 [0.118]
 [0.259]
 [0.193]
 [0.185]
 [0.179]
 [0.175]]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.261]
 [0.611]
 [0.564]
 [0.611]
 [0.611]
 [0.562]] [[1.472]
 [4.565]
 [1.472]
 [0.217]
 [1.472]
 [1.472]
 [0.903]] [[1.304]
 [1.859]
 [1.304]
 [1.031]
 [1.304]
 [1.304]
 [1.173]]
siam score:  -0.64601517
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.223]
 [0.232]
 [0.218]
 [0.232]
 [0.231]
 [0.216]] [[4.044]
 [2.611]
 [4.044]
 [2.128]
 [4.044]
 [4.92 ]
 [3.363]] [[1.054]
 [0.293]
 [1.054]
 [0.035]
 [1.054]
 [1.516]
 [0.685]]
siam score:  -0.64539915
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15496666806281192, 0.0704166994938327, 0.14555229744880788, 0.21934787195159083, 0.23894145238187864, 0.17077501066107814]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.561937806677435
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.494]
 [0.52 ]
 [0.512]
 [0.506]
 [0.537]
 [0.875]] [[-3.152]
 [ 1.026]
 [-2.628]
 [-2.653]
 [-2.688]
 [-2.537]
 [ 0.79 ]] [[0.473]
 [0.494]
 [0.52 ]
 [0.512]
 [0.506]
 [0.537]
 [0.875]]
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.577]
 [0.651]
 [0.646]
 [0.659]
 [0.643]
 [0.644]] [[0.932]
 [1.441]
 [1.1  ]
 [1.119]
 [0.958]
 [1.142]
 [0.968]] [[0.698]
 [0.577]
 [0.651]
 [0.646]
 [0.659]
 [0.643]
 [0.644]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15548626819834938, 0.07051445291821316, 0.1451309993813383, 0.22008334097483717, 0.23816966432814887, 0.1706152741991131]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.47 ]] [[3.61]
 [3.61]
 [3.61]
 [3.61]
 [3.61]
 [3.61]
 [4.6 ]] [[0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [1.081]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.52]] [[1.862]
 [1.862]
 [1.862]
 [1.862]
 [1.862]
 [1.862]
 [1.862]] [[0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.52]]
from probs:  [0.15569452575014467, 0.07060889963367616, 0.14502444265755543, 0.22037811953189845, 0.2374502169587226, 0.17084379546800268]
1485 2060
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15547626572191467, 0.07065222944199806, 0.1448135823734845, 0.2205133566157108, 0.2375959304939879, 0.17094863535290417]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.543]
 [0.562]
 [0.549]
 [0.562]
 [0.559]
 [0.57 ]] [[2.81 ]
 [2.787]
 [2.616]
 [2.685]
 [2.616]
 [2.792]
 [2.637]] [[0.895]
 [0.873]
 [0.74 ]
 [0.783]
 [0.74 ]
 [0.91 ]
 [0.777]]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.41 ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.441]
 [0.455]] [[3.085]
 [3.994]
 [3.633]
 [3.633]
 [3.633]
 [4.588]
 [4.008]] [[0.586]
 [1.126]
 [0.994]
 [0.994]
 [0.994]
 [1.597]
 [1.201]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.045]
 [-0.046]
 [-0.046]
 [-0.025]
 [-0.025]
 [-0.026]] [[1.479]
 [1.206]
 [2.491]
 [2.491]
 [1.465]
 [1.475]
 [1.456]] [[-0.989]
 [-1.302]
 [-0.05 ]
 [-0.05 ]
 [-1.011]
 [-1.001]
 [-1.02 ]]
siam score:  -0.63141036
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15560303575100365, 0.07070983685326418, 0.14463282428650592, 0.22069315566415645, 0.2372731264221036, 0.1710880210229663]
from probs:  [0.15540659001281515, 0.07062453307846063, 0.14444336607432204, 0.22085847099437256, 0.23745086136421287, 0.17121617847581672]
siam score:  -0.62729615
using explorer policy with actor:  1
siam score:  -0.62761503
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[2.907]
 [2.907]
 [2.907]
 [2.907]
 [2.907]
 [2.907]
 [2.907]]
siam score:  -0.6294043
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15613763879624448, 0.07095675823688549, 0.1445282069035353, 0.22006432872405318, 0.23702344147220075, 0.17128962586708085]
siam score:  -0.6273325
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1562160533211905, 0.07085397136486922, 0.14460079100800244, 0.2201748481345393, 0.23714247798836305, 0.17101185818303546]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1561943676420165, 0.07084840797473708, 0.14487130663363565, 0.21967953623932143, 0.23707459852266183, 0.17133178298762747]
Printing some Q and Qe and total Qs values:  [[1.002]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]] [[-1.685]
 [-1.136]
 [-1.136]
 [-1.136]
 [-1.136]
 [-1.136]
 [-1.136]] [[1.379]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.777]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[0.543]
 [1.268]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[0.598]
 [0.777]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.62867486
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.155988652355387, 0.07090098482943001, 0.14496389150476047, 0.21936813306701422, 0.23771205203676588, 0.1710662862066423]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.508]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[2.296]
 [4.071]
 [2.296]
 [2.296]
 [2.296]
 [2.296]
 [2.296]] [[0.487]
 [1.453]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]] [[0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]] [[1.277]
 [1.277]
 [1.277]
 [1.277]
 [1.277]
 [1.277]
 [1.277]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.46 ]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[5.314]
 [5.697]
 [5.314]
 [5.314]
 [5.314]
 [5.314]
 [5.314]] [[1.235]
 [1.42 ]
 [1.235]
 [1.235]
 [1.235]
 [1.235]
 [1.235]]
1502 2090
Printing some Q and Qe and total Qs values:  [[0.809]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]] [[2.871]
 [3.214]
 [3.214]
 [3.214]
 [3.214]
 [3.214]
 [3.214]] [[1.937]
 [2.024]
 [2.024]
 [2.024]
 [2.024]
 [2.024]
 [2.024]]
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.63 ]
 [0.805]
 [0.665]
 [0.665]
 [0.665]
 [0.764]] [[0.555]
 [0.73 ]
 [1.368]
 [1.242]
 [1.242]
 [1.242]
 [1.308]] [[0.387]
 [0.28 ]
 [0.843]
 [0.521]
 [0.521]
 [0.521]
 [0.74 ]]
from probs:  [0.1558461642468711, 0.07043580425104705, 0.1454099948490877, 0.219145836984767, 0.23793134126287066, 0.1712308584053566]
Printing some Q and Qe and total Qs values:  [[1.043]
 [1.043]
 [1.043]
 [0.69 ]
 [1.043]
 [1.043]
 [0.709]] [[1.85 ]
 [1.85 ]
 [1.85 ]
 [1.465]
 [1.85 ]
 [1.85 ]
 [1.412]] [[1.506]
 [1.506]
 [1.506]
 [0.544]
 [1.506]
 [1.506]
 [0.546]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.015]
 [0.008]
 [0.003]
 [0.005]
 [0.006]
 [0.007]] [[0.244]
 [0.247]
 [0.255]
 [0.335]
 [0.321]
 [0.264]
 [0.36 ]] [[0.013]
 [0.025]
 [0.024]
 [0.178]
 [0.153]
 [0.04 ]
 [0.235]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.531]
 [0.531]
 [0.696]
 [0.121]
 [0.531]
 [0.342]] [[2.249]
 [1.701]
 [1.701]
 [2.263]
 [2.008]
 [1.701]
 [1.733]] [[0.904]
 [1.4  ]
 [1.4  ]
 [2.104]
 [0.783]
 [1.4  ]
 [1.043]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
in main func line 156:  1506
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16711582413093531, 0.0035911957505341437, 0.15589940860148138, 0.23591396054159558, 0.25393205993191587, 0.1835475510435377]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1671959136503601, 0.0035929168152107915, 0.15597412270338307, 0.23554777568736263, 0.2540537557482891, 0.18363551539539438]
1506 2116
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.536]
 [0.408]] [[1.131]
 [1.533]
 [1.533]
 [1.533]
 [1.533]
 [0.613]
 [1.462]] [[1.894]
 [1.97 ]
 [1.97 ]
 [1.97 ]
 [1.97 ]
 [1.658]
 [1.829]]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.192]] [[0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.614]]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.505]
 [0.513]
 [0.494]
 [0.502]
 [0.51 ]
 [0.419]] [[3.629]
 [3.838]
 [3.603]
 [3.297]
 [3.152]
 [3.327]
 [6.609]] [[0.64 ]
 [0.718]
 [0.629]
 [0.496]
 [0.443]
 [0.517]
 [1.771]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16726806760552065, 0.003608677126393138, 0.1560245565323359, 0.2361016378679591, 0.2540885068065955, 0.1829085540611957]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16742132992700004, 0.0036119836405519843, 0.15616751678819496, 0.2363179701640317, 0.2537844690619592, 0.182696730418262]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16742132992700004, 0.0036119836405519843, 0.15616751678819496, 0.2363179701640317, 0.2537844690619592, 0.182696730418262]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[-4.891]
 [-4.891]
 [-4.891]
 [-4.891]
 [-4.891]
 [-4.891]
 [-4.891]] [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]]
siam score:  -0.62496454
UNIT TEST: sample policy line 217 mcts : [0.02  0.347 0.02  0.02  0.02  0.163 0.408]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[-5.613]
 [-5.839]
 [-5.839]
 [-5.839]
 [-5.839]
 [-5.839]
 [-5.839]] [[0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]]
Printing some Q and Qe and total Qs values:  [[0.012]
 [1.462]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[0.303]
 [0.229]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]] [[0.244]
 [2.997]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -4.704737222551398
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.064]
 [0.195]
 [0.156]
 [0.156]
 [0.156]
 [0.146]] [[-3.147]
 [-1.885]
 [-4.848]
 [-3.147]
 [-3.147]
 [-3.147]
 [-2.299]] [[0.156]
 [0.064]
 [0.195]
 [0.156]
 [0.156]
 [0.156]
 [0.146]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.376]
 [0.369]
 [0.369]
 [0.372]
 [0.376]
 [0.379]] [[-4.234]
 [-3.048]
 [-4.234]
 [-4.234]
 [-4.188]
 [-4.113]
 [-4.089]] [[0.369]
 [0.376]
 [0.369]
 [0.369]
 [0.372]
 [0.376]
 [0.379]]
UNIT TEST: sample policy line 217 mcts : [0.061 0.245 0.204 0.061 0.082 0.082 0.265]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[-1.464]
 [-0.384]
 [-0.384]
 [-0.384]
 [-0.384]
 [-0.384]
 [-0.384]] [[0.687]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.5543],
        [-0.4152],
        [-0.5323],
        [-0.4918],
        [-0.6086],
        [-0.0000],
        [-0.5129],
        [-0.3329],
        [-0.4466]], dtype=torch.float64)
-0.7622999999999999 -0.7622999999999999
-0.024259925299500003 -0.5785175970291612
-0.024259925299500003 -0.43943602869994364
-0.0727797758985 -0.6050404016461296
-0.043375785898500004 -0.5351434251261107
-0.024259925299500003 -0.6328988266636085
-0.9605475 -0.9605475
-0.0727797758985 -0.5856974129116126
-0.024259925299500003 -0.35717585694246373
-0.024259925299500003 -0.4708118899147489
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16779758424413627, 0.0036201010317994187, 0.15651847984895684, 0.23589320295551727, 0.2538190536468435, 0.18235157827274667]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16803013049456908, 0.003625118034427466, 0.15673539468581982, 0.2357444667265026, 0.25363660221197315, 0.1822282878467079]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16803013049456908, 0.003625118034427466, 0.15673539468581982, 0.2357444667265026, 0.25363660221197315, 0.1822282878467079]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16803013049456908, 0.003625118034427466, 0.15673539468581982, 0.2357444667265026, 0.25363660221197315, 0.1822282878467079]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -0.7054745370414006
first move QE:  0.2250386352783396
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16803013049456908, 0.003625118034427466, 0.15673539468581982, 0.2357444667265026, 0.25363660221197315, 0.1822282878467079]
siam score:  -0.63549596
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16835164222721338, 0.0036320543974289014, 0.15703529487731407, 0.23572092486407986, 0.2530582636233142, 0.18220182001064958]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.512201881922767
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3692051130766194
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16835164222721338, 0.0036320543974289014, 0.15703529487731407, 0.23572092486407986, 0.2530582636233142, 0.18220182001064958]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.22199931760778488
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.22113690963862132
Printing some Q and Qe and total Qs values:  [[1.25 ]
 [0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]] [[1.76 ]
 [1.712]
 [1.712]
 [1.712]
 [1.712]
 [1.712]
 [1.712]] [[1.524]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.63279927
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.054]
 [0.054]
 [1.455]
 [0.054]
 [0.054]
 [0.054]] [[1.058]
 [1.058]
 [1.058]
 [0.21 ]
 [1.058]
 [1.058]
 [1.058]] [[0.962]
 [0.962]
 [0.962]
 [2.633]
 [0.962]
 [0.962]
 [0.962]]
siam score:  -0.62284285
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.017]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.012]] [[5.498]
 [5.532]
 [5.545]
 [5.545]
 [5.545]
 [5.625]
 [5.537]] [[0.334]
 [0.345]
 [0.361]
 [0.361]
 [0.361]
 [0.416]
 [0.358]]
using explorer policy with actor:  1
1529 2149
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.163 0.184 0.082 0.184 0.061 0.204 0.122]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.503]
 [0.556]
 [0.55 ]
 [0.66 ]
 [0.509]
 [0.451]] [[3.568]
 [4.424]
 [4.635]
 [3.787]
 [3.361]
 [3.6  ]
 [5.509]] [[0.905]
 [1.195]
 [1.31 ]
 [0.951]
 [0.826]
 [0.85 ]
 [1.625]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.43]
 [0.5 ]
 [0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]] [[-1.185]
 [ 0.599]
 [-1.185]
 [-1.185]
 [-1.185]
 [-1.185]
 [-1.185]] [[0.556]
 [1.135]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]]
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.086]
 [-0.059]
 [-0.059]
 [-0.063]
 [-0.059]
 [-0.059]] [[8.542]
 [7.997]
 [7.534]
 [7.534]
 [6.994]
 [7.534]
 [7.534]] [[ 0.142]
 [-0.072]
 [-0.172]
 [-0.172]
 [-0.36 ]
 [-0.172]
 [-0.172]]
Printing some Q and Qe and total Qs values:  [[-0.084]
 [-0.089]
 [-0.086]
 [-0.089]
 [-0.094]
 [-0.087]
 [-0.088]] [[7.646]
 [5.463]
 [7.77 ]
 [8.271]
 [9.995]
 [8.562]
 [8.963]] [[-0.236]
 [-0.974]
 [-0.199]
 [-0.036]
 [ 0.528]
 [ 0.064]
 [ 0.197]]
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]] [[1.607]
 [1.556]
 [1.556]
 [1.556]
 [1.556]
 [1.556]
 [1.556]] [[0.259]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]]
using explorer policy with actor:  1
first move QE:  0.2149593211756521
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1686642292142748, 0.0036602861260308364, 0.156986140934329, 0.23565929917683812, 0.25290851171112, 0.18212153283740717]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.488]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]] [[2.185]
 [2.71 ]
 [2.185]
 [2.185]
 [2.185]
 [2.185]
 [2.185]] [[0.695]
 [0.754]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[1.371]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[ 0.8  ]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.178]
 [0.233]
 [0.21 ]
 [0.219]
 [0.219]
 [0.227]] [[5.216]
 [5.185]
 [5.296]
 [5.177]
 [5.164]
 [5.315]
 [5.354]] [[ 0.002]
 [-0.081]
 [ 0.067]
 [-0.02 ]
 [-0.006]
 [ 0.044]
 [ 0.075]]
start point for exploration sampling:  10749
1537 2158
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10301276267741723, 0.003949342799114106, 0.1693835027951805, 0.25426956369051945, 0.2728809648124658, 0.19650386322530292]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.599]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]] [[0.624]
 [0.389]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[1.139]
 [1.107]
 [1.139]
 [1.139]
 [1.139]
 [1.139]
 [1.139]]
1538 2159
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.061]
 [-0.061]
 [-0.069]
 [-0.068]
 [-0.069]
 [-0.068]] [[6.281]
 [5.84 ]
 [5.84 ]
 [6.368]
 [6.326]
 [6.412]
 [6.424]] [[-0.448]
 [-0.569]
 [-0.569]
 [-0.41 ]
 [-0.422]
 [-0.395]
 [-0.389]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10315845882696727, 0.003954928553967473, 0.16860877568617147, 0.2546291899680273, 0.2732669141339414, 0.19638173283092508]
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.19 ]
 [0.193]
 [0.195]
 [0.19 ]
 [0.19 ]
 [0.192]] [[-4.859]
 [-3.981]
 [-3.887]
 [-3.939]
 [-3.981]
 [-3.981]
 [-3.97 ]] [[0.234]
 [0.19 ]
 [0.193]
 [0.195]
 [0.19 ]
 [0.19 ]
 [0.192]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10315845882696727, 0.003954928553967473, 0.16860877568617147, 0.2546291899680273, 0.2732669141339414, 0.19638173283092508]
from probs:  [0.10315845882696727, 0.003954928553967473, 0.16860877568617147, 0.2546291899680273, 0.2732669141339414, 0.19638173283092508]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.3  ]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]] [[-5.182]
 [-4.388]
 [-5.182]
 [-5.182]
 [-5.182]
 [-5.182]
 [-5.182]] [[0.267]
 [0.3  ]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.592]
 [0.55 ]
 [0.616]
 [0.543]
 [0.616]
 [0.552]] [[0.922]
 [0.764]
 [0.402]
 [0.922]
 [0.147]
 [0.922]
 [0.255]] [[0.733]
 [0.632]
 [0.426]
 [0.733]
 [0.328]
 [0.733]
 [0.383]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10338618655653084, 0.0039636592669930705, 0.16864490426296003, 0.2546837667302117, 0.27330313476075796, 0.1960183484225465]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10338618655653084, 0.0039636592669930705, 0.16864490426296003, 0.2546837667302117, 0.27330313476075796, 0.1960183484225465]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.226]
 [0.252]
 [0.267]
 [0.316]
 [0.302]
 [0.413]] [[2.22 ]
 [4.422]
 [3.997]
 [3.202]
 [1.949]
 [3.519]
 [4.432]] [[-0.585]
 [ 0.038]
 [-0.052]
 [-0.287]
 [-0.609]
 [-0.111]
 [ 0.415]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1034794705194799, 0.003967235623337375, 0.16879707028750152, 0.25440762428183233, 0.2735497324963553, 0.1957988667914935]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.933]] [[1.892]
 [1.892]
 [1.892]
 [1.892]
 [1.892]
 [1.892]
 [2.9  ]] [[1.272]
 [1.272]
 [1.272]
 [1.272]
 [1.272]
 [1.272]
 [2.333]]
siam score:  -0.62692827
Printing some Q and Qe and total Qs values:  [[0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]] [[1.859]
 [1.859]
 [1.859]
 [1.859]
 [1.859]
 [1.859]
 [1.859]] [[1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.511]
 [0.475]
 [0.475]
 [0.475]
 [0.679]
 [0.475]] [[-2.272]
 [ 0.064]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.673]
 [ 0.   ]] [[0.451]
 [0.511]
 [0.475]
 [0.475]
 [0.475]
 [0.679]
 [0.475]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10380487136910374, 0.003979710965895771, 0.16832372981564728, 0.254198685315257, 0.27327842793805474, 0.1964145745960415]
from probs:  [0.10380487136910374, 0.003979710965895771, 0.16832372981564728, 0.254198685315257, 0.27327842793805474, 0.1964145745960415]
Printing some Q and Qe and total Qs values:  [[0.55]
 [0.55]
 [0.55]
 [0.55]
 [0.55]
 [0.55]
 [0.55]] [[0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10387370613652025, 0.003982349979605911, 0.16777223120224977, 0.2543672487665718, 0.27345964348968504, 0.19654482042536733]
using explorer policy with actor:  0
1548 2179
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]] [[2.03]
 [2.03]
 [2.03]
 [2.03]
 [2.03]
 [2.03]
 [2.03]] [[2.085]
 [2.085]
 [2.085]
 [2.085]
 [2.085]
 [2.085]
 [2.085]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.635171
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10387370613652025, 0.003982349979605911, 0.16777223120224977, 0.2543672487665718, 0.27345964348968504, 0.19654482042536733]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10391487919383226, 0.003983928488066284, 0.16783873210943387, 0.2544680738714159, 0.2735680363640004, 0.1962263499732514]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10391487919383226, 0.003983928488066284, 0.16783873210943387, 0.2544680738714159, 0.2735680363640004, 0.1962263499732514]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[-3.983]
 [-3.983]
 [-3.983]
 [-3.983]
 [-3.983]
 [-3.983]
 [-3.983]] [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10432829639323318, 0.003999778235195159, 0.16784579591954746, 0.2544745324440153, 0.2735286694012039, 0.19582292760680503]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10432829639323318, 0.003999778235195159, 0.16784579591954746, 0.2544745324440153, 0.2735286694012039, 0.19582292760680503]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1043625637087823, 0.004001091988679596, 0.16757246939469675, 0.25455811627892816, 0.2736185116927982, 0.1958872469361149]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.852]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]] [[0.871]
 [1.71 ]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]] [[0.341]
 [1.733]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.15 ]
 [0.144]
 [0.147]
 [0.15 ]
 [0.15 ]
 [0.15 ]] [[-4.773]
 [-4.404]
 [-4.216]
 [-4.337]
 [-4.404]
 [-4.404]
 [-4.404]] [[0.444]
 [0.15 ]
 [0.144]
 [0.147]
 [0.15 ]
 [0.15 ]
 [0.15 ]]
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[-0.013]
 [-0.817]
 [-0.817]
 [-0.817]
 [-0.817]
 [-0.817]
 [-0.817]] [[0.45 ]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]]
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.219]
 [0.219]
 [0.215]
 [0.219]
 [0.298]
 [0.219]] [[-0.515]
 [-0.456]
 [-0.456]
 [-1.743]
 [-0.456]
 [-1.082]
 [-0.456]] [[0.137]
 [0.219]
 [0.219]
 [0.215]
 [0.219]
 [0.298]
 [0.219]]
from probs:  [0.10442109540588419, 0.004003335999328968, 0.16766645233908473, 0.2547008850844018, 0.27321112096049827, 0.19599711021080202]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]] [[-3.926]
 [-5.215]
 [-5.215]
 [-5.215]
 [-5.215]
 [-5.215]
 [-5.215]] [[0.494]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]]
line 256 mcts: sample exp_bonus -4.2582654335471215
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.64538544
using explorer policy with actor:  1
siam score:  -0.64370817
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.11361685212639176, 0.004355886446891164, 0.09497540048827635, 0.2771309062082956, 0.2966635223936715, 0.2132574323364736]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.293]
 [0.333]
 [0.338]
 [0.345]
 [0.343]
 [0.344]] [[1.372]
 [6.266]
 [1.615]
 [1.148]
 [1.37 ]
 [1.692]
 [1.713]] [[-0.304]
 [ 1.259]
 [-0.216]
 [-0.361]
 [-0.274]
 [-0.17 ]
 [-0.161]]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]] [[2.091]
 [2.091]
 [2.091]
 [2.091]
 [2.091]
 [2.091]
 [2.091]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.11375425393236639, 0.004361154210019801, 0.09509025837516566, 0.277466052679015, 0.29581294696914345, 0.21351533383428964]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.636]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[-0.244]
 [ 2.137]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]] [[1.119]
 [1.815]
 [1.119]
 [1.119]
 [1.119]
 [1.119]
 [1.119]]
first move QE:  0.19135892269924676
start point for exploration sampling:  10749
using explorer policy with actor:  1
from probs:  [0.11385124971539458, 0.004364872871545716, 0.09517133977432754, 0.2777026419590901, 0.2960651802477179, 0.21284471543192407]
from probs:  [0.11391976606751442, 0.004367499677728106, 0.09522861444670923, 0.2778697650433658, 0.29564154813037075, 0.2129728066343117]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.11396808791296632, 0.0043693522591696215, 0.09526900798462402, 0.2779876302769919, 0.2957669516989617, 0.21263896986728634]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[2.176]
 [2.176]
 [2.176]
 [2.176]
 [2.176]
 [2.176]
 [2.176]] [[2.131]
 [2.131]
 [2.131]
 [2.131]
 [2.131]
 [2.131]
 [2.131]]
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.246]
 [0.319]
 [0.319]
 [0.272]
 [0.319]
 [0.281]] [[-4.203]
 [-3.955]
 [ 0.   ]
 [ 0.   ]
 [-4.765]
 [ 0.   ]
 [-4.654]] [[0.243]
 [0.246]
 [0.319]
 [0.319]
 [0.272]
 [0.319]
 [0.281]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
start point for exploration sampling:  10749
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[4.788]
 [4.788]
 [4.788]
 [4.788]
 [4.788]
 [4.788]
 [4.788]] [[0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]]
siam score:  -0.63191503
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1252402273947522, 0.004801507865290067, 0.10469169433445685, 0.20947135133270442, 0.32305155142093295, 0.23274366765186352]
line 256 mcts: sample exp_bonus 0.5762019508103597
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1252402273947522, 0.004801507865290067, 0.10469169433445685, 0.20947135133270442, 0.32305155142093295, 0.23274366765186352]
using explorer policy with actor:  0
using explorer policy with actor:  0
siam score:  -0.6244845
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1252402273947522, 0.004801507865290067, 0.10469169433445685, 0.20947135133270442, 0.32305155142093295, 0.23274366765186352]
Printing some Q and Qe and total Qs values:  [[1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]] [[0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]] [[2.856]
 [2.856]
 [2.856]
 [2.856]
 [2.856]
 [2.856]
 [2.856]]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.514]
 [0.514]
 [0.559]
 [0.514]
 [0.514]
 [0.61 ]] [[1.715]
 [1.506]
 [1.715]
 [0.729]
 [1.715]
 [1.715]
 [1.68 ]] [[0.514]
 [0.514]
 [0.514]
 [0.559]
 [0.514]
 [0.514]
 [0.61 ]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.8  ]
 [0.726]
 [0.794]
 [0.8  ]
 [0.795]
 [0.79 ]
 [0.792]] [[-0.496]
 [ 0.632]
 [-0.083]
 [-0.076]
 [-0.13 ]
 [-0.051]
 [-0.143]] [[0.862]
 [1.091]
 [0.988]
 [1.002]
 [0.974]
 [0.991]
 [0.963]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1252402273947522, 0.004801507865290067, 0.10469169433445685, 0.20947135133270442, 0.32305155142093295, 0.23274366765186352]
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.533]
 [0.509]
 [0.507]
 [0.028]
 [0.486]
 [0.344]] [[-0.011]
 [-1.716]
 [-4.608]
 [-2.689]
 [ 0.317]
 [-4.701]
 [-0.719]] [[1.659]
 [1.158]
 [0.107]
 [0.798]
 [1.709]
 [0.065]
 [1.45 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.6314936
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]] [[-6.443]
 [-5.392]
 [-5.392]
 [-5.392]
 [-5.392]
 [-5.392]
 [-5.392]] [[0.132]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.506]
 [0.554]
 [0.569]
 [0.586]
 [0.585]
 [0.567]] [[-0.599]
 [ 0.239]
 [-0.198]
 [-0.221]
 [-0.254]
 [-0.257]
 [-0.26 ]] [[0.574]
 [0.506]
 [0.554]
 [0.569]
 [0.586]
 [0.585]
 [0.567]]
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.425]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[-2.745]
 [-0.166]
 [-2.745]
 [-2.745]
 [-2.745]
 [-2.745]
 [-2.745]] [[0.424]
 [0.425]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1254609720834975, 0.0048099708598188995, 0.10487622079181654, 0.20984055928779097, 0.3223196831358596, 0.23269259384121646]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]] [[2.332]
 [2.332]
 [2.332]
 [2.332]
 [2.332]
 [2.332]
 [2.332]] [[2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[0.172]
 [0.175]
 [0.175]
 [0.173]
 [0.175]
 [0.175]
 [0.175]] [[2.937]
 [2.939]
 [2.939]
 [2.937]
 [2.939]
 [2.939]
 [2.939]]
from probs:  [0.12554215508459612, 0.004813083284847275, 0.10494408385876373, 0.20997634244068938, 0.3218811712943295, 0.23284316403677402]
UNIT TEST: sample policy line 217 mcts : [0.347 0.082 0.061 0.122 0.163 0.102 0.122]
using explorer policy with actor:  1
from probs:  [0.12562311256449576, 0.004816187063757441, 0.10501175840643942, 0.21011174839664964, 0.3214438776107256, 0.2329933159579321]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.02  0.224 0.673 0.    0.082 0.    0.   ]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12584232082900507, 0.004824591154264723, 0.10519500052521379, 0.21047838659545565, 0.3207206564241245, 0.23293904447193628]
from probs:  [0.12600282000260074, 0.004830744433130651, 0.10532916612657482, 0.2107468305250159, 0.31985430420997, 0.23323613470270776]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[0.521]
 [1.925]
 [1.925]
 [1.925]
 [1.925]
 [1.925]
 [1.925]] [[0.207]
 [2.737]
 [2.737]
 [2.737]
 [2.737]
 [2.737]
 [2.737]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12617608231115562, 0.004837387029959084, 0.10547400077774274, 0.2110366215183971, 0.3202941252802246, 0.2321817830825206]
using explorer policy with actor:  1
Starting evaluation
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.404]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[-3.91 ]
 [-1.823]
 [-3.91 ]
 [-3.91 ]
 [-3.91 ]
 [-3.91 ]
 [-3.91 ]] [[0.375]
 [0.404]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]]
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.414]
 [0.381]
 [0.375]
 [0.312]
 [0.356]
 [0.312]] [[ 0.203]
 [-2.049]
 [-4.45 ]
 [-4.232]
 [ 0.203]
 [-4.512]
 [ 0.203]] [[0.312]
 [0.414]
 [0.381]
 [0.375]
 [0.312]
 [0.356]
 [0.312]]
using another actor
from probs:  [0.12623347925936332, 0.004839587536170193, 0.10552198043956938, 0.21113262115488407, 0.32043982567750434, 0.23183250593250865]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.363]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[-1.067]
 [-1.346]
 [-1.067]
 [-1.067]
 [-1.067]
 [-1.067]
 [-1.067]] [[0.359]
 [0.363]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]]
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.46 ]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.387]] [[-0.06 ]
 [ 1.103]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]
 [ 0.143]] [[0.357]
 [0.46 ]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.387]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12623347925936332, 0.004839587536170193, 0.10552198043956938, 0.21113262115488407, 0.32043982567750434, 0.23183250593250865]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 4.046116636189811
siam score:  -0.6294931
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[-2.625]
 [-2.625]
 [-2.625]
 [-2.625]
 [-2.625]
 [-2.625]
 [-2.625]] [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.402]
 [0.461]
 [0.458]
 [0.458]
 [0.46 ]
 [0.458]] [[-2.949]
 [-0.368]
 [-2.257]
 [-2.471]
 [-2.471]
 [-2.047]
 [-2.471]] [[0.468]
 [0.402]
 [0.461]
 [0.458]
 [0.458]
 [0.46 ]
 [0.458]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12623347925936332, 0.004839587536170193, 0.10552198043956938, 0.21113262115488407, 0.32043982567750434, 0.23183250593250865]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.426]
 [0.434]
 [0.435]
 [0.429]
 [0.436]
 [0.439]] [[-2.999]
 [-0.475]
 [-2.538]
 [-2.518]
 [-2.458]
 [-2.377]
 [-2.41 ]] [[0.46 ]
 [0.426]
 [0.434]
 [0.435]
 [0.429]
 [0.436]
 [0.439]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12623347925936332, 0.004839587536170193, 0.10552198043956938, 0.21113262115488407, 0.32043982567750434, 0.23183250593250865]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -0.4097528208334387
Printing some Q and Qe and total Qs values:  [[0.995]
 [1.128]
 [0.995]
 [0.995]
 [0.995]
 [0.995]
 [0.995]] [[0.476]
 [1.005]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[0.936]
 [1.484]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]]
siam score:  -0.63287723
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[-3.084]
 [-2.477]
 [-2.477]
 [-2.477]
 [-2.477]
 [-2.477]
 [-2.477]] [[0.466]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[-2.594]
 [-2.594]
 [-2.594]
 [-2.594]
 [-2.594]
 [-2.594]
 [-2.594]] [[0.455]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6284835
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12623347925936332, 0.004839587536170193, 0.10552198043956938, 0.21113262115488407, 0.32043982567750434, 0.23183250593250865]
siam score:  -0.6239796
from probs:  [0.12623347925936332, 0.004839587536170193, 0.10552198043956938, 0.21113262115488407, 0.32043982567750434, 0.23183250593250865]
line 256 mcts: sample exp_bonus -0.5571114456154367
rdn probs:  [0.12623347925936332, 0.004839587536170193, 0.10552198043956938, 0.21113262115488407, 0.32043982567750434, 0.23183250593250865]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13985654192390246, 0.00536187373680883, 0.11690986707985539, 0.2339179626141946, 0.3543180129251949, 0.14963574172004385]
from probs:  [0.13985654192390246, 0.00536187373680883, 0.11690986707985539, 0.2339179626141946, 0.3543180129251949, 0.14963574172004385]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1593 2268
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.463]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[-1.698]
 [-1.034]
 [-1.698]
 [-1.698]
 [-1.698]
 [-1.698]
 [-1.698]] [[0.423]
 [0.463]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[1.108]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[3.054]
 [2.832]
 [2.832]
 [2.832]
 [2.832]
 [2.832]
 [2.832]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14005242229491655, 0.005369383473589454, 0.11707360878134312, 0.2342455835940459, 0.353413683206717, 0.1498453186493879]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.15580971536596847
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
first move QE:  0.15550918861923105
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1600 2287
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.614]
 [0.571]
 [0.648]
 [0.721]
 [0.571]
 [0.95 ]] [[1.631]
 [1.613]
 [1.679]
 [1.694]
 [1.436]
 [1.679]
 [1.456]] [[1.175]
 [1.105]
 [1.085]
 [1.255]
 [1.141]
 [1.085]
 [1.62 ]]
line 256 mcts: sample exp_bonus 0.562022876186302
using another actor
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5747],
        [-0.0000],
        [-0.5396],
        [-0.4522],
        [-0.5985],
        [-0.0000],
        [-0.0000],
        [-0.6104],
        [-0.6159],
        [-0.0000]], dtype=torch.float64)
-0.0727797758985 -0.6474896107976859
-0.9652499999999999 -0.9652499999999999
-0.0628797758985 -0.6024598143200529
-0.0727797758985 -0.5249666936038215
-0.024259925299500003 -0.6227537658861466
-0.965448 -0.965448
-0.9560860847999999 -0.9560860847999999
-0.024259925299500003 -0.6346332272373015
-0.024259925299500003 -0.6401112717291723
-0.96074352 -0.96074352
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.9869019758755226
line 256 mcts: sample exp_bonus -2.628232052700679
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
from probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
1606 2304
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
line 256 mcts: sample exp_bonus 2.8086309142705783
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.599]
 [0.599]
 [1.232]
 [0.599]
 [0.599]
 [0.599]] [[0.256]
 [0.256]
 [0.256]
 [0.138]
 [0.256]
 [0.256]
 [0.256]] [[0.966]
 [0.966]
 [0.966]
 [2.194]
 [0.966]
 [0.966]
 [0.966]]
first move QE:  0.14685715632816407
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]] [[-2.165]
 [-4.929]
 [-4.929]
 [-4.929]
 [-4.929]
 [-4.929]
 [-4.929]] [[0.461]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.546741548940117
siam score:  -0.6276836
siam score:  -0.6291948
from probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
using another actor
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.619]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.499]] [[-0.016]
 [ 0.877]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [ 0.501]] [[0.45 ]
 [0.619]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.499]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.683]
 [0.642]
 [0.646]
 [0.656]
 [0.661]
 [0.658]] [[0.589]
 [1.088]
 [1.015]
 [1.078]
 [1.165]
 [1.317]
 [1.335]] [[0.412]
 [0.984]
 [0.83 ]
 [0.9  ]
 [1.008]
 [1.17 ]
 [1.182]]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.575]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.505]] [[0.741]
 [1.258]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [1.47 ]] [[1.611]
 [1.809]
 [1.611]
 [1.611]
 [1.611]
 [1.611]
 [1.81 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.605]
 [0.552]
 [0.548]
 [0.549]
 [0.545]
 [0.551]] [[1.89 ]
 [1.622]
 [1.858]
 [1.96 ]
 [1.862]
 [1.888]
 [1.788]] [[0.325]
 [0.174]
 [0.224]
 [0.285]
 [0.221]
 [0.232]
 [0.177]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.207033765583867
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
using explorer policy with actor:  1
siam score:  -0.6182166
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.13958222857076835
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[-4.998]
 [-4.663]
 [-4.663]
 [-4.663]
 [-4.663]
 [-4.663]
 [-4.663]] [[0.434]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.541]
 [0.505]
 [0.506]
 [0.509]
 [0.516]
 [0.515]] [[-1.298]
 [ 0.504]
 [-1.405]
 [-1.555]
 [-1.624]
 [-0.486]
 [-1.23 ]] [[0.517]
 [0.541]
 [0.505]
 [0.506]
 [0.509]
 [0.516]
 [0.515]]
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]] [[1.751]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]] [[1.998]
 [1.339]
 [1.339]
 [1.339]
 [1.339]
 [1.339]
 [1.339]]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.26389286296084624
from probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
siam score:  -0.61652726
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
1625 2359
siam score:  -0.61700356
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.13295474078289188
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15702040558158445, 0.0060199085237576415, 0.13125760506331127, 0.26262549364679627, 0.2750768316409956, 0.1679997555435546]
actor:  1 policy actor:  1  step number:  97 total reward:  0.0899999999999993  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
1632 2372
Printing some Q and Qe and total Qs values:  [[1.287]
 [1.287]
 [1.287]
 [1.296]
 [1.287]
 [1.287]
 [1.373]] [[0.351]
 [0.351]
 [0.351]
 [0.145]
 [0.351]
 [0.351]
 [0.33 ]] [[2.017]
 [2.017]
 [2.017]
 [1.898]
 [2.017]
 [2.017]
 [2.175]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
from probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
siam score:  -0.6195243
1633 2377
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6203104
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.686]] [[5.908]
 [5.908]
 [5.908]
 [5.908]
 [5.908]
 [5.908]
 [7.48 ]] [[1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.583]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.4565549626814553
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
in main func line 156:  1638
from probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
from probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
line 256 mcts: sample exp_bonus 1.757884044403521
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]] [[3.19]
 [3.19]
 [3.19]
 [3.19]
 [3.19]
 [3.19]
 [3.19]] [[0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.73 ]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.727]] [[3.437]
 [2.908]
 [3.437]
 [3.437]
 [3.437]
 [3.437]
 [2.416]] [[1.918]
 [1.691]
 [1.918]
 [1.918]
 [1.918]
 [1.918]
 [1.474]]
using another actor
in main func line 156:  1642
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
from probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.406]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.336]
 [0.42 ]] [[-0.6  ]
 [-0.159]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-1.882]
 [ 0.   ]] [[0.401]
 [0.406]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.336]
 [0.42 ]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.819249170199702
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[2.275]
 [1.626]
 [1.626]
 [1.626]
 [1.626]
 [1.626]
 [1.626]] [[0.81 ]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]]
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[0.257]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]] [[0.81 ]
 [0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.052]]
from probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.554]
 [0.539]
 [0.536]
 [0.687]
 [0.687]
 [0.524]] [[-1.384]
 [-0.093]
 [-1.489]
 [-1.569]
 [ 0.753]
 [ 0.753]
 [-1.459]] [[0.412]
 [1.334]
 [0.372]
 [0.315]
 [2.163]
 [2.163]
 [0.364]]
siam score:  -0.6087053
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]] [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
from probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
first move QE:  0.11117372124067675
siam score:  -0.61346906
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
1658 2415
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.373]
 [0.424]
 [0.405]
 [0.429]
 [0.431]
 [0.439]] [[-4.29 ]
 [-2.371]
 [-4.014]
 [-4.268]
 [-4.171]
 [-3.865]
 [-4.178]] [[0.398]
 [0.373]
 [0.424]
 [0.405]
 [0.429]
 [0.431]
 [0.439]]
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.22 ]
 [0.198]
 [0.336]
 [0.228]
 [0.246]
 [0.25 ]] [[3.943]
 [5.218]
 [4.345]
 [4.968]
 [5.31 ]
 [3.902]
 [5.717]] [[0.247]
 [0.616]
 [0.282]
 [0.764]
 [0.664]
 [0.229]
 [0.843]]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.641]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.671]] [[3.051]
 [1.875]
 [3.051]
 [3.051]
 [3.051]
 [3.051]
 [2.205]] [[1.398]
 [0.751]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.03 ]]
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.167]
 [0.237]
 [0.125]
 [0.217]
 [0.033]
 [0.036]] [[0.124]
 [1.185]
 [0.141]
 [0.201]
 [0.444]
 [0.217]
 [0.179]] [[ 0.153]
 [ 0.882]
 [ 0.327]
 [ 0.143]
 [ 0.489]
 [-0.031]
 [-0.05 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.226]
 [0.577]
 [0.653]
 [0.577]
 [0.765]
 [0.672]] [[1.508]
 [1.85 ]
 [2.139]
 [1.145]
 [2.139]
 [1.085]
 [0.956]] [[1.803]
 [1.239]
 [2.107]
 [1.37 ]
 [2.107]
 [1.513]
 [1.238]]
siam score:  -0.60933554
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.959]
 [0.183]
 [0.546]
 [0.546]
 [0.552]
 [0.325]
 [0.546]] [[1.501]
 [1.833]
 [1.347]
 [1.347]
 [2.356]
 [0.609]
 [1.347]] [[ 1.556]
 [ 0.115]
 [ 0.679]
 [ 0.679]
 [ 1.029]
 [-0.01 ]
 [ 0.679]]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.52 ]] [[4.733]
 [4.508]
 [4.508]
 [4.508]
 [4.508]
 [4.508]
 [4.141]] [[1.353]
 [1.105]
 [1.105]
 [1.105]
 [1.105]
 [1.105]
 [1.002]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13737342091999058, 0.13039044201481206, 0.11483415905420945, 0.2297648025390969, 0.24065818221773672, 0.14697899325415412]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.578]
 [0.676]
 [0.664]
 [0.673]
 [0.672]
 [0.676]] [[1.969]
 [1.979]
 [2.19 ]
 [2.138]
 [2.205]
 [2.247]
 [2.057]] [[0.849]
 [0.628]
 [1.034]
 [0.958]
 [1.044]
 [1.083]
 [0.901]]
siam score:  -0.60603124
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.48999999999999966  reward:  1.0 rdn_beta:  0.833
using another actor
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.663]
 [0.682]
 [0.74 ]
 [0.657]
 [0.64 ]
 [0.754]] [[3.235]
 [3.522]
 [2.984]
 [3.645]
 [3.594]
 [3.255]
 [2.898]] [[1.321]
 [1.476]
 [1.229]
 [1.614]
 [1.506]
 [1.321]
 [1.257]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[-1.471]
 [-0.882]
 [-0.882]
 [-0.882]
 [-0.882]
 [-0.882]
 [-0.882]] [[0.272]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
1673 2435
start point for exploration sampling:  10749
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1675 2440
1676 2441
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]] [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
siam score:  -0.60734445
siam score:  -0.6059839
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 3.2696789680829386
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.509]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]] [[-1.111]
 [-0.356]
 [-1.111]
 [-1.111]
 [-1.111]
 [-1.111]
 [-1.111]] [[0.461]
 [0.509]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.847]] [[1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.171]] [[2.115]
 [2.115]
 [2.115]
 [2.115]
 [2.115]
 [2.115]
 [2.213]]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.554]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.493]] [[0.847]
 [1.106]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [1.396]] [[0.466]
 [0.554]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.493]]
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.446]
 [0.48 ]] [[2.325]
 [2.325]
 [2.325]
 [2.325]
 [2.325]
 [0.823]
 [2.325]] [[0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.446]
 [0.48 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
using explorer policy with actor:  1
from probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]] [[-4.5  ]
 [-5.896]
 [-5.896]
 [-5.896]
 [-5.896]
 [-5.896]
 [-5.896]] [[0.324]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[2.064]
 [2.064]
 [2.064]
 [2.064]
 [2.064]
 [2.064]
 [2.064]] [[1.231]
 [1.231]
 [1.231]
 [1.231]
 [1.231]
 [1.231]
 [1.231]]
from probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.521]
 [0.581]
 [0.583]
 [0.602]
 [0.584]
 [0.581]] [[2.017]
 [2.035]
 [2.042]
 [1.974]
 [1.918]
 [2.092]
 [2.072]] [[1.115]
 [1.008]
 [1.137]
 [1.05 ]
 [1.013]
 [1.209]
 [1.176]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.407]
 [0.398]
 [0.399]
 [0.399]
 [0.416]
 [0.411]] [[-3.824]
 [-2.267]
 [-3.825]
 [-3.839]
 [-4.002]
 [-4.394]
 [-4.274]] [[0.419]
 [0.407]
 [0.398]
 [0.399]
 [0.399]
 [0.416]
 [0.411]]
siam score:  -0.5915425
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]] [[-4.61 ]
 [-2.568]
 [-2.568]
 [-2.568]
 [-2.568]
 [-2.568]
 [-2.568]] [[0.513]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.521]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.526]] [[5.924]
 [5.252]
 [5.924]
 [5.924]
 [5.924]
 [5.924]
 [5.045]] [[1.684]
 [1.263]
 [1.684]
 [1.684]
 [1.684]
 [1.684]
 [1.138]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[-1.161]
 [-1.427]
 [-1.427]
 [-1.427]
 [-1.427]
 [-1.427]
 [-1.427]] [[0.591]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]]
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]] [[ 0.409]
 [-1.96 ]
 [-1.96 ]
 [-1.96 ]
 [-1.96 ]
 [-1.96 ]
 [-1.96 ]] [[0.69 ]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]]
siam score:  -0.59957385
using explorer policy with actor:  1
1690 2464
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
line 256 mcts: sample exp_bonus 2.343196351844669
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
using explorer policy with actor:  1
siam score:  -0.6024903
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.507]
 [0.507]
 [0.522]
 [0.507]
 [0.777]
 [0.512]] [[ 0.836]
 [ 0.836]
 [ 0.836]
 [ 0.045]
 [ 0.836]
 [ 0.141]
 [-0.37 ]] [[0.507]
 [0.507]
 [0.507]
 [0.522]
 [0.507]
 [0.777]
 [0.512]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
from probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
1704 2473
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.493]
 [0.125]
 [0.297]
 [0.207]
 [0.163]
 [0.09 ]] [[1.54 ]
 [0.803]
 [1.385]
 [1.572]
 [1.469]
 [1.444]
 [1.459]] [[1.688]
 [1.633]
 [1.651]
 [1.911]
 [1.771]
 [1.718]
 [1.663]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.913]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.958]] [[1.538]
 [2.255]
 [1.538]
 [1.538]
 [1.538]
 [1.538]
 [1.036]] [[1.335]
 [1.698]
 [1.335]
 [1.335]
 [1.335]
 [1.335]
 [1.306]]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.61 ]] [[0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [1.589]] [[0.937]
 [0.937]
 [0.937]
 [0.937]
 [0.937]
 [0.937]
 [1.721]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[6.04]
 [6.04]
 [6.04]
 [6.04]
 [6.04]
 [6.04]
 [6.04]] [[1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]] [[-4.253]
 [-5.625]
 [-5.625]
 [-5.625]
 [-5.625]
 [-5.625]
 [-5.625]] [[0.639]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.6139606
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]] [[-0.33]
 [-1.46]
 [-1.46]
 [-1.46]
 [-1.46]
 [-1.46]
 [-1.46]] [[0.828]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.567]
 [0.573]
 [0.593]
 [0.59 ]
 [0.588]
 [0.56 ]] [[5.008]
 [5.004]
 [5.309]
 [5.167]
 [4.939]
 [5.315]
 [5.838]] [[1.394]
 [1.352]
 [1.542]
 [1.481]
 [1.34 ]
 [1.564]
 [1.846]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.4124907699296949
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.041 0.102 0.041 0.02  0.041 0.02  0.735]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.539]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.591]] [[1.906]
 [1.164]
 [1.906]
 [1.906]
 [1.906]
 [1.906]
 [1.593]] [[2.284]
 [2.077]
 [2.284]
 [2.284]
 [2.284]
 [2.284]
 [2.224]]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[2.301]
 [2.301]
 [2.301]
 [2.301]
 [2.301]
 [2.301]
 [2.301]] [[2.09]
 [2.09]
 [2.09]
 [2.09]
 [2.09]
 [2.09]
 [2.09]]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.049]
 [0.362]
 [0.359]
 [0.354]
 [0.253]
 [0.36 ]] [[-0.6  ]
 [ 0.235]
 [-0.976]
 [-0.852]
 [-0.794]
 [-0.48 ]
 [-0.734]] [[0.062]
 [0.208]
 [0.027]
 [0.103]
 [0.133]
 [0.139]
 [0.184]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.484]
 [0.626]
 [0.498]
 [0.508]
 [0.516]
 [0.524]] [[2.141]
 [2.599]
 [5.117]
 [2.581]
 [2.53 ]
 [2.528]
 [2.51 ]] [[-0.073]
 [-0.174]
 [ 0.95 ]
 [-0.153]
 [-0.15 ]
 [-0.134]
 [-0.123]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.338]
 [-0.03 ]
 [-0.008]
 [-0.01 ]
 [-0.018]
 [-0.016]
 [-0.003]] [[0.866]
 [2.412]
 [2.132]
 [2.175]
 [2.259]
 [2.244]
 [1.969]] [[-0.888]
 [-1.107]
 [-1.157]
 [-1.147]
 [-1.134]
 [-1.135]
 [-1.201]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.60991293
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
start point for exploration sampling:  10749
1726 2495
from probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.495]
 [0.579]
 [0.579]
 [0.496]
 [0.477]
 [0.508]] [[-1.036]
 [-0.002]
 [ 0.   ]
 [ 0.   ]
 [-0.395]
 [-0.351]
 [-0.207]] [[0.517]
 [0.495]
 [0.579]
 [0.579]
 [0.496]
 [0.477]
 [0.508]]
line 256 mcts: sample exp_bonus 1.8243844899995039
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.715]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.796]] [[-0.536]
 [-0.063]
 [ 0.857]
 [ 0.857]
 [ 0.857]
 [ 0.857]
 [-0.675]] [[0.859]
 [0.715]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.796]]
1730 2496
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[1.764]
 [1.87 ]
 [1.87 ]
 [1.87 ]
 [1.87 ]
 [1.87 ]
 [1.87 ]] [[1.002]
 [1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
siam score:  -0.61239594
1735 2502
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
line 256 mcts: sample exp_bonus -0.14921503294246102
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]] [[-2.82 ]
 [-5.225]
 [-5.225]
 [-5.225]
 [-5.225]
 [-5.225]
 [-5.225]] [[0.569]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[-2.459]
 [-4.569]
 [-4.569]
 [-4.569]
 [-4.569]
 [-4.569]
 [-4.569]] [[0.634]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[-3.693]
 [-5.983]
 [-5.983]
 [-5.983]
 [-5.983]
 [-5.983]
 [-5.983]] [[0.607]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.489]
 [0.534]
 [0.529]
 [0.52 ]
 [0.52 ]
 [0.523]] [[2.627]
 [2.669]
 [2.626]
 [2.729]
 [2.657]
 [2.72 ]
 [2.657]] [[0.59 ]
 [0.532]
 [0.595]
 [0.653]
 [0.587]
 [0.629]
 [0.594]]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.883]] [[2.305]
 [2.305]
 [2.305]
 [2.305]
 [2.305]
 [2.305]
 [1.256]] [[0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.466]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
using another actor
from probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]] [[7.768]
 [3.548]
 [3.548]
 [3.548]
 [3.548]
 [3.548]
 [3.548]] [[2.147]
 [1.407]
 [1.407]
 [1.407]
 [1.407]
 [1.407]
 [1.407]]
from probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
using explorer policy with actor:  0
Starting evaluation
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.37 ]
 [0.345]
 [0.441]
 [0.441]
 [0.349]
 [0.441]] [[ 0.   ]
 [-4.276]
 [-4.979]
 [ 0.   ]
 [ 0.   ]
 [-4.209]
 [ 0.   ]] [[0.441]
 [0.37 ]
 [0.345]
 [0.441]
 [0.441]
 [0.349]
 [0.441]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.374]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]] [[-5.132]
 [-3.996]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.377]
 [0.374]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.418]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[-3.769]
 [-2.894]
 [-3.769]
 [-3.769]
 [-3.769]
 [-3.769]
 [-3.769]] [[0.388]
 [0.418]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.4807670488327525
line 256 mcts: sample exp_bonus -3.4178915272201147
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.499]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]] [[-0.746]
 [ 0.105]
 [-0.746]
 [-0.746]
 [-0.746]
 [-0.746]
 [-0.746]] [[0.394]
 [0.499]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.907]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[-1.929]
 [-0.901]
 [-1.929]
 [-1.929]
 [-1.929]
 [-1.929]
 [-1.929]] [[0.61 ]
 [0.907]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]]
using explorer policy with actor:  0
using explorer policy with actor:  0
first move QE:  0.06730404702461121
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8864707967456773
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.805]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[-0.303]
 [-0.142]
 [-0.303]
 [-0.303]
 [-0.303]
 [-0.303]
 [-0.303]] [[0.606]
 [0.805]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.508]
 [0.508]
 [0.443]
 [0.508]
 [0.43 ]
 [0.508]] [[1.003]
 [0.285]
 [0.285]
 [0.578]
 [0.285]
 [1.482]
 [0.285]] [[0.384]
 [0.508]
 [0.508]
 [0.443]
 [0.508]
 [0.43 ]
 [0.508]]
line 256 mcts: sample exp_bonus 0.07835626339935797
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.465]
 [0.433]
 [0.433]
 [0.433]
 [0.491]
 [0.433]] [[-0.372]
 [ 0.606]
 [-0.559]
 [-0.559]
 [-0.559]
 [-0.531]
 [-0.559]] [[0.515]
 [0.465]
 [0.433]
 [0.433]
 [0.433]
 [0.491]
 [0.433]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
line 256 mcts: sample exp_bonus 0.19331549933941156
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.514]
 [0.558]
 [0.569]
 [0.539]
 [0.537]
 [0.541]] [[ 0.012]
 [ 1.234]
 [-0.524]
 [-0.758]
 [-0.675]
 [-0.56 ]
 [-0.444]] [[0.77 ]
 [0.514]
 [0.558]
 [0.569]
 [0.539]
 [0.537]
 [0.541]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.402]
 [0.441]
 [0.441]
 [0.441]
 [0.865]
 [0.441]] [[-0.429]
 [ 0.614]
 [-0.615]
 [-0.615]
 [-0.615]
 [-0.03 ]
 [-0.615]] [[0.49 ]
 [0.402]
 [0.441]
 [0.441]
 [0.441]
 [0.865]
 [0.441]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[1.125]
 [1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.347]] [[0.732]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[-0.618]
 [-0.618]
 [-0.618]
 [-0.618]
 [-0.618]
 [-0.618]
 [-0.618]] [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[-5.397]
 [-6.367]
 [-6.367]
 [-6.367]
 [-6.367]
 [-6.367]
 [-6.367]] [[0.598]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]]
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]] [[-1.322]
 [-2.964]
 [-2.964]
 [-2.964]
 [-2.964]
 [-2.964]
 [-2.964]] [[0.777]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]]
Printing some Q and Qe and total Qs values:  [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]] [[-0.168]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]] [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.554]
 [0.561]
 [0.561]
 [0.561]
 [0.559]
 [0.559]] [[-1.009]
 [ 1.111]
 [-0.819]
 [-0.819]
 [-0.819]
 [-0.579]
 [-0.543]] [[0.591]
 [0.554]
 [0.561]
 [0.561]
 [0.561]
 [0.559]
 [0.559]]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.656]] [[-1.214]
 [-1.21 ]
 [-1.21 ]
 [-1.21 ]
 [-1.21 ]
 [-1.21 ]
 [-0.715]] [[0.689]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.656]]
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]] [[-0.841]
 [-0.841]
 [-0.841]
 [-0.841]
 [-0.841]
 [-0.841]
 [-0.841]] [[0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.783]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[0.081]
 [0.46 ]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]] [[0.598]
 [0.783]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.508]
 [0.513]
 [0.516]
 [0.524]
 [0.522]
 [0.534]] [[-1.783]
 [ 0.196]
 [-0.756]
 [-0.943]
 [-0.708]
 [-0.691]
 [-0.753]] [[0.487]
 [0.508]
 [0.513]
 [0.516]
 [0.524]
 [0.522]
 [0.534]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.362]] [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]] [[-2.215]
 [-5.038]
 [-5.038]
 [-5.038]
 [-5.038]
 [-5.038]
 [-5.038]] [[0.694]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.52 ]
 [0.528]
 [0.528]
 [0.528]
 [0.535]
 [0.547]] [[-0.298]
 [ 1.658]
 [-0.298]
 [-0.298]
 [-0.298]
 [-0.146]
 [ 0.073]] [[0.528]
 [0.52 ]
 [0.528]
 [0.528]
 [0.528]
 [0.535]
 [0.547]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]] [[-8.104]
 [-7.055]
 [-7.055]
 [-7.055]
 [-7.055]
 [-7.055]
 [-7.055]] [[0.665]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]]
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.535]
 [0.546]
 [0.547]
 [0.575]
 [0.553]
 [0.557]] [[-0.374]
 [ 1.971]
 [ 0.007]
 [ 0.183]
 [ 0.147]
 [ 0.695]
 [ 0.385]] [[0.563]
 [0.535]
 [0.546]
 [0.547]
 [0.575]
 [0.553]
 [0.557]]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.247]
 [0.281]
 [0.273]
 [0.27 ]
 [0.277]
 [0.278]] [[-7.394]
 [-0.557]
 [-6.788]
 [-6.403]
 [-6.462]
 [-7.017]
 [-6.828]] [[0.334]
 [0.247]
 [0.281]
 [0.273]
 [0.27 ]
 [0.277]
 [0.278]]
using explorer policy with actor:  1
1743 2516
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.565]
 [0.563]
 [0.565]
 [0.565]
 [0.563]
 [0.6  ]] [[0.718]
 [0.792]
 [0.929]
 [0.792]
 [0.792]
 [0.952]
 [0.884]] [[0.563]
 [0.565]
 [0.563]
 [0.565]
 [0.565]
 [0.563]
 [0.6  ]]
1743 2517
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.4  ]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[ 0.442]
 [-1.627]
 [-2.158]
 [-2.158]
 [-2.158]
 [-2.158]
 [-2.158]] [[0.488]
 [0.4  ]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -1.2408929402743611
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
rdn probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.268]] [[6.29 ]
 [5.976]
 [5.976]
 [5.976]
 [5.976]
 [5.976]
 [7.121]] [[0.311]
 [0.245]
 [0.245]
 [0.245]
 [0.245]
 [0.245]
 [0.581]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.878 0.02  0.02  0.02  0.02  0.02 ]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.747]] [[-3.015]
 [-3.015]
 [-3.015]
 [-3.015]
 [-3.015]
 [-3.015]
 [-2.646]] [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.747]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.505]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.439]] [[3.327]
 [4.397]
 [3.327]
 [3.327]
 [3.327]
 [3.327]
 [3.808]] [[0.696]
 [1.264]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.85 ]]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.47 ]
 [0.464]
 [0.464]
 [0.464]
 [0.546]
 [0.464]] [[5.58 ]
 [5.132]
 [5.58 ]
 [5.58 ]
 [5.58 ]
 [5.704]
 [5.58 ]] [[1.516]
 [1.279]
 [1.516]
 [1.516]
 [1.516]
 [1.671]
 [1.516]]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.498]
 [0.499]
 [0.494]
 [0.512]
 [0.508]] [[5.562]
 [5.562]
 [5.224]
 [5.158]
 [5.562]
 [5.144]
 [5.161]] [[1.729]
 [1.729]
 [1.619]
 [1.597]
 [1.729]
 [1.601]
 [1.604]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6057842
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
from probs:  [0.111339964868291, 0.10568032109752531, 0.09307208883021165, 0.18622237745373182, 0.3845600499084726, 0.11912519784176763]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  93 total reward:  0.16999999999999937  reward:  1.0 rdn_beta:  0.167
siam score:  -0.6070006
line 256 mcts: sample exp_bonus 2.7363503897733548
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.9211983829300054
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
line 256 mcts: sample exp_bonus 1.771127359494958
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.235]
 [0.453]
 [0.453]
 [0.444]
 [0.417]
 [0.419]] [[4.093]
 [3.84 ]
 [4.093]
 [4.093]
 [5.698]
 [4.628]
 [4.44 ]] [[0.658]
 [0.418]
 [0.658]
 [0.658]
 [1.317]
 [0.857]
 [0.78 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[ 0.177]
 [-2.147]
 [-2.147]
 [-2.147]
 [-2.147]
 [-2.147]
 [-2.147]] [[0.498]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]] [[1.595]
 [1.861]
 [1.861]
 [1.861]
 [1.861]
 [1.861]
 [1.861]] [[0.603]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.083]
 [0.169]
 [0.121]
 [0.176]
 [0.178]
 [0.324]] [[5.107]
 [4.128]
 [5.1  ]
 [5.351]
 [4.805]
 [4.48 ]
 [3.327]] [[1.154]
 [0.484]
 [1.182]
 [1.297]
 [1.002]
 [0.798]
 [0.206]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[3.033]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]] [[1.289]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]]
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.052]
 [-0.042]
 [-0.043]
 [-0.039]
 [-0.036]
 [-0.017]] [[5.347]
 [3.486]
 [5.474]
 [4.55 ]
 [4.414]
 [4.921]
 [4.762]] [[ 1.084]
 [-0.125]
 [ 1.166]
 [ 0.569]
 [ 0.486]
 [ 0.815]
 [ 0.731]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.561]
 [0.613]
 [0.631]
 [0.614]
 [0.613]
 [0.623]] [[2.181]
 [2.609]
 [2.323]
 [1.945]
 [2.419]
 [2.348]
 [2.354]] [[0.219]
 [0.415]
 [0.328]
 [0.113]
 [0.394]
 [0.345]
 [0.369]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.072]
 [0.2  ]
 [0.13 ]
 [0.112]
 [0.109]
 [0.162]] [[2.301]
 [3.073]
 [2.68 ]
 [2.936]
 [2.998]
 [2.881]
 [2.78 ]] [[-0.727]
 [-0.768]
 [-0.644]
 [-0.698]
 [-0.714]
 [-0.759]
 [-0.686]]
using explorer policy with actor:  1
using another actor
siam score:  -0.6145394
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[-5.9  ]
 [-6.299]
 [-6.299]
 [-6.299]
 [-6.299]
 [-6.299]
 [-6.299]] [[0.369]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]]
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.289]
 [0.322]
 [0.308]
 [0.3  ]
 [0.304]
 [0.334]] [[-7.947]
 [-3.147]
 [-6.802]
 [-7.146]
 [-7.161]
 [-6.952]
 [-6.856]] [[0.493]
 [0.289]
 [0.322]
 [0.308]
 [0.3  ]
 [0.304]
 [0.334]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[-1.272]
 [-2.175]
 [-2.175]
 [-2.175]
 [-2.175]
 [-2.175]
 [-2.175]] [[0.54 ]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6189776
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
UNIT TEST: sample policy line 217 mcts : [0.061 0.02  0.143 0.429 0.102 0.122 0.122]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0000],
        [-0.5410],
        [-0.4494],
        [-0.0000],
        [-0.0000],
        [-0.5032],
        [-0.5786],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.009850499999999242 -0.009850499999999242
-0.8233334999999999 -0.8233334999999999
-0.0628797758985 -0.6038560644404266
-0.024259925299500003 -0.4736893798110247
-0.3038804999999995 -0.3038804999999995
-0.9801 -0.9801
-0.0727797758985 -0.5759311257350958
-0.024259925299500003 -0.6029045427124347
-0.97515 -0.97515
-0.2960887049999996 -0.2960887049999996
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[ 0.138]
 [-1.741]
 [-1.741]
 [-1.741]
 [-1.741]
 [-1.741]
 [-1.741]] [[0.652]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]]
siam score:  -0.61252743
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
start point for exploration sampling:  10749
1766 2546
first move QE:  0.053425569705297445
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1770 2547
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.259]
 [0.259]
 [0.261]
 [0.259]
 [0.261]
 [0.28 ]] [[7.586]
 [7.586]
 [7.586]
 [7.293]
 [7.586]
 [7.242]
 [7.083]] [[1.301]
 [1.301]
 [1.301]
 [1.185]
 [1.301]
 [1.164]
 [1.123]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.2022102271778624
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.658]
 [0.637]
 [0.634]
 [0.632]
 [0.627]
 [0.682]] [[-2.49 ]
 [ 0.01 ]
 [-1.329]
 [-1.415]
 [-1.766]
 [-2.109]
 [-1.984]] [[1.617]
 [2.443]
 [1.955]
 [1.92 ]
 [1.798]
 [1.673]
 [1.824]]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.574]
 [0.574]
 [0.592]
 [0.574]
 [0.574]
 [0.571]] [[1.978]
 [1.978]
 [1.978]
 [1.356]
 [1.978]
 [1.978]
 [1.434]] [[1.007]
 [1.007]
 [1.007]
 [0.835]
 [1.007]
 [1.007]
 [0.819]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.672]
 [0.653]] [[4.534]
 [4.534]
 [4.534]
 [4.534]
 [4.534]
 [4.802]
 [4.534]] [[1.393]
 [1.393]
 [1.393]
 [1.393]
 [1.393]
 [1.595]
 [1.393]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]] [[1.67]
 [1.67]
 [1.67]
 [1.67]
 [1.67]
 [1.67]
 [1.67]] [[1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.797]
 [0.726]
 [0.726]
 [0.726]
 [0.742]
 [0.775]] [[4.695]
 [4.283]
 [4.695]
 [4.695]
 [4.695]
 [5.578]
 [4.248]] [[1.756]
 [1.761]
 [1.756]
 [1.756]
 [1.756]
 [2.083]
 [1.705]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.172]] [[1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [3.317]] [[0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [1.08 ]]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.68 ]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[1.83 ]
 [3.234]
 [1.83 ]
 [1.83 ]
 [1.83 ]
 [1.83 ]
 [1.83 ]] [[0.883]
 [1.58 ]
 [0.883]
 [0.883]
 [0.883]
 [0.883]
 [0.883]]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.299]] [[3.933]
 [3.933]
 [3.933]
 [3.933]
 [3.933]
 [3.933]
 [5.791]] [[0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [1.47 ]]
siam score:  -0.6284848
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.617]] [[-3.711]
 [-3.711]
 [-3.711]
 [-3.711]
 [-3.711]
 [-3.711]
 [-3.764]] [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.617]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.484]
 [0.604]
 [0.564]
 [0.604]
 [0.604]
 [0.579]] [[ 0.   ]
 [-2.134]
 [ 0.   ]
 [-3.341]
 [ 0.   ]
 [ 0.   ]
 [-2.891]] [[0.604]
 [0.484]
 [0.604]
 [0.564]
 [0.604]
 [0.604]
 [0.579]]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[-3.267]
 [-3.267]
 [-3.267]
 [-3.267]
 [-3.267]
 [-3.267]
 [-3.267]] [[0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
1788 2574
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
Printing some Q and Qe and total Qs values:  [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]] [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[2.615]
 [2.615]
 [2.615]
 [2.615]
 [2.615]
 [2.615]
 [2.615]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]] [[2.887]
 [2.887]
 [2.887]
 [2.887]
 [2.887]
 [2.887]
 [2.887]] [[2.122]
 [2.122]
 [2.122]
 [2.122]
 [2.122]
 [2.122]
 [2.122]]
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.546]
 [0.515]
 [0.546]
 [0.546]
 [0.524]
 [0.546]] [[-0.076]
 [ 0.   ]
 [ 0.104]
 [ 0.   ]
 [ 0.   ]
 [ 0.009]
 [ 0.   ]] [[0.199]
 [0.26 ]
 [0.233]
 [0.26 ]
 [0.26 ]
 [0.22 ]
 [0.26 ]]
using another actor
from probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.883]
 [0.883]
 [0.883]
 [0.883]
 [0.883]
 [0.883]
 [1.19 ]] [[1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [0.993]] [[0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [1.496]]
siam score:  -0.61987907
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.69 ]
 [0.503]
 [0.503]
 [0.506]
 [0.503]
 [0.503]] [[-1.744]
 [-1.761]
 [-1.744]
 [-1.744]
 [-1.851]
 [-1.744]
 [-1.744]] [[0.503]
 [0.69 ]
 [0.503]
 [0.503]
 [0.506]
 [0.503]
 [0.503]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -3.3293064362840386
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.547]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.52 ]] [[-0.612]
 [ 0.497]
 [-0.612]
 [-0.612]
 [-0.612]
 [-0.612]
 [ 0.14 ]] [[0.507]
 [0.547]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.52 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]] [[-7.738]
 [-6.613]
 [-6.613]
 [-6.613]
 [-6.613]
 [-6.613]
 [-6.613]] [[0.443]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -5.038410692146884
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.8917000130839825
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1893983218821067
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[3.684]
 [3.684]
 [3.684]
 [3.684]
 [3.684]
 [3.684]
 [3.684]] [[1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[2.647]
 [2.647]
 [2.647]
 [2.647]
 [2.647]
 [2.647]
 [2.647]] [[1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]]
1806 2601
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.234]
 [0.373]
 [0.404]
 [0.402]
 [0.348]
 [0.642]] [[1.846]
 [2.402]
 [2.061]
 [1.5  ]
 [1.42 ]
 [2.101]
 [3.511]] [[-0.194]
 [-0.076]
 [ 0.088]
 [-0.036]
 [-0.067]
 [ 0.053]
 [ 1.111]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
UNIT TEST: sample policy line 217 mcts : [0.061 0.02  0.061 0.02  0.061 0.592 0.184]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2285647641217106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.462]
 [0.517]
 [0.487]
 [0.517]
 [0.473]
 [0.491]] [[1.95 ]
 [1.888]
 [1.701]
 [1.801]
 [1.701]
 [1.956]
 [1.777]] [[0.627]
 [0.517]
 [0.5  ]
 [0.508]
 [0.5  ]
 [0.583]
 [0.5  ]]
siam score:  -0.64460707
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.379]
 [0.469]
 [0.457]
 [0.403]
 [0.434]
 [0.395]] [[3.147]
 [2.877]
 [3.116]
 [3.409]
 [3.419]
 [2.854]
 [3.399]] [[1.328]
 [1.23 ]
 [1.403]
 [1.514]
 [1.473]
 [1.266]
 [1.459]]
Printing some Q and Qe and total Qs values:  [[0.9  ]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[ 0.113]
 [-1.735]
 [-1.735]
 [-1.735]
 [-1.735]
 [-1.735]
 [-1.735]] [[0.9  ]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
first move QE:  0.022624175108694945
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
in main func line 156:  1814
siam score:  -0.6551299
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
siam score:  -0.6568715
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]] [[-4.07 ]
 [-4.618]
 [-4.618]
 [-4.618]
 [-4.618]
 [-4.618]
 [-4.618]] [[0.254]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.27 ]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]] [[0.302]
 [0.823]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]] [[-0.049]
 [ 0.075]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.5  ]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[0.859]
 [0.689]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]] [[0.498]
 [0.5  ]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]] [[-3.012]
 [-3.012]
 [-3.012]
 [-3.012]
 [-3.012]
 [-3.012]
 [-3.012]] [[0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]]
first move QE:  0.01863213251227106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.597]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.603]] [[-1.091]
 [ 2.411]
 [-1.091]
 [-1.091]
 [-1.091]
 [-1.285]
 [-1.201]] [[0.598]
 [0.597]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.603]]
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.497]
 [0.654]
 [0.853]
 [0.394]
 [0.392]
 [0.394]] [[1.084]
 [1.747]
 [1.34 ]
 [0.888]
 [1.065]
 [0.883]
 [0.967]] [[1.489]
 [2.052]
 [2.095]
 [2.191]
 [1.392]
 [1.268]
 [1.327]]
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.669]
 [0.681]
 [0.69 ]
 [0.681]
 [0.681]
 [0.965]] [[1.668]
 [1.501]
 [1.668]
 [1.258]
 [1.668]
 [1.668]
 [1.788]] [[1.643]
 [1.508]
 [1.643]
 [1.389]
 [1.643]
 [1.643]
 [2.292]]
siam score:  -0.6668985
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.361]] [[1.753]
 [1.753]
 [1.753]
 [1.753]
 [1.753]
 [1.753]
 [1.819]] [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.361]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.316]
 [1.136]
 [1.302]
 [0.695]
 [0.701]
 [0.864]] [[-1.623]
 [ 0.861]
 [-0.467]
 [ 0.122]
 [-1.584]
 [-1.429]
 [-1.232]] [[0.422]
 [1.197]
 [1.953]
 [2.678]
 [0.328]
 [0.443]
 [0.899]]
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.123]
 [0.116]
 [0.229]
 [0.116]
 [0.039]
 [0.207]] [[4.09 ]
 [4.43 ]
 [5.613]
 [5.079]
 [5.613]
 [4.151]
 [4.835]] [[0.475]
 [0.562]
 [1.037]
 [0.957]
 [1.037]
 [0.344]
 [0.831]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.302]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.506]] [[-0.007]
 [-0.022]
 [-1.131]
 [-1.131]
 [-1.131]
 [-1.131]
 [-1.701]] [[1.954]
 [1.698]
 [1.423]
 [1.423]
 [1.423]
 [1.423]
 [1.245]]
1824 2637
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[-0.249]
 [-0.802]
 [-0.802]
 [-0.802]
 [-0.802]
 [-0.802]
 [-0.802]] [[0.421]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]] [[-2.376]
 [-2.83 ]
 [-2.83 ]
 [-2.83 ]
 [-2.83 ]
 [-2.83 ]
 [-2.83 ]] [[0.37 ]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[0.625]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[1.242]
 [1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.236]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.439]
 [0.472]
 [0.472]
 [0.472]
 [0.471]
 [0.472]] [[0.962]
 [1.917]
 [1.314]
 [1.314]
 [1.314]
 [0.94 ]
 [1.314]] [[1.382]
 [1.832]
 [1.561]
 [1.561]
 [1.561]
 [1.371]
 [1.561]]
from probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2043125415761994, 0.09462393128440647, 0.0833347859516709, 0.1667395903494596, 0.34432696039678606, 0.10666219044147758]
siam score:  -0.68012774
actor:  1 policy actor:  1  step number:  76 total reward:  0.4449999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[2.109]
 [2.109]
 [2.109]
 [2.109]
 [2.109]
 [2.109]
 [2.109]] [[2.077]
 [2.077]
 [2.077]
 [2.077]
 [2.077]
 [2.077]
 [2.077]]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.425]
 [0.46 ]
 [0.482]
 [0.748]
 [0.439]
 [0.562]] [[0.817]
 [0.682]
 [0.874]
 [0.913]
 [0.559]
 [0.928]
 [0.654]] [[0.874]
 [0.631]
 [0.846]
 [0.915]
 [1.061]
 [0.856]
 [0.833]]
from probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
using explorer policy with actor:  0
1830 2651
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
1833 2656
using explorer policy with actor:  1
1833 2657
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.579]
 [0.482]
 [0.733]
 [0.474]
 [0.472]
 [0.48 ]] [[-1.463]
 [ 1.6  ]
 [-0.903]
 [ 0.518]
 [-1.499]
 [-1.583]
 [-1.242]] [[0.392]
 [1.536]
 [0.573]
 [1.486]
 [0.37 ]
 [0.341]
 [0.462]]
from probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 0.6744091847056723
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.445]
 [0.578]
 [0.552]
 [0.55 ]
 [0.571]
 [0.556]] [[-7.328]
 [-1.472]
 [-6.727]
 [-7.066]
 [-7.067]
 [-6.844]
 [-6.926]] [[0.069]
 [1.487]
 [0.216]
 [0.124]
 [0.123]
 [0.185]
 [0.16 ]]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.52 ]
 [0.533]
 [0.537]
 [0.532]
 [0.581]
 [0.571]] [[0.34 ]
 [1.735]
 [1.031]
 [1.229]
 [0.937]
 [1.198]
 [1.033]] [[0.544]
 [0.52 ]
 [0.533]
 [0.537]
 [0.532]
 [0.581]
 [0.571]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6660344
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
line 256 mcts: sample exp_bonus -3.4617337869633324
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65702105
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1838 2668
siam score:  -0.6575252
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.167]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.488]] [[1.77 ]
 [1.756]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [5.181]] [[0.498]
 [0.216]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [1.47 ]]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.551]
 [0.527]
 [0.615]
 [0.628]
 [0.628]
 [0.603]] [[4.574]
 [3.815]
 [2.201]
 [4.398]
 [4.254]
 [4.254]
 [4.715]] [[1.674]
 [1.321]
 [0.734]
 [1.643]
 [1.621]
 [1.621]
 [1.726]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5949],
        [-0.4766],
        [-0.6475],
        [-0.0000],
        [-0.0000],
        [-0.5520],
        [-0.0000],
        [-0.6808],
        [-0.0000],
        [-0.5515]], dtype=torch.float64)
-0.024259925299500003 -0.6191788842202798
-0.0727797758985 -0.5493741255234981
-0.024259925299500003 -0.6717997102503745
-0.9752489999999999 -0.9752489999999999
-0.917080065 -0.917080065
-0.024259925299500003 -0.5762887014142664
-0.9409455 -0.9409455
-0.024259925299500003 -0.7050548986690383
-0.004949999999999235 -0.004949999999999235
-0.024259925299500003 -0.5757985777534793
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.091]
 [0.125]
 [0.109]
 [0.125]
 [0.125]
 [0.125]] [[8.471]
 [8.333]
 [8.471]
 [9.104]
 [8.471]
 [8.471]
 [8.471]] [[1.132]
 [1.04 ]
 [1.132]
 [1.401]
 [1.132]
 [1.132]
 [1.132]]
using another actor
1841 2676
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.546]
 [0.459]
 [0.519]
 [0.503]
 [0.672]
 [0.511]] [[3.443]
 [4.045]
 [3.5  ]
 [2.037]
 [2.647]
 [3.67 ]
 [3.849]] [[1.423]
 [1.664]
 [1.328]
 [0.698]
 [0.97 ]
 [1.607]
 [1.54 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
using explorer policy with actor:  1
from probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
start point for exploration sampling:  10749
siam score:  -0.6625693
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
using another actor
from probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
siam score:  -0.65818876
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
from probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.56 ]
 [0.579]
 [0.579]
 [0.579]
 [0.612]
 [0.609]] [[1.649]
 [1.99 ]
 [1.145]
 [1.145]
 [1.145]
 [1.618]
 [1.569]] [[0.597]
 [0.56 ]
 [0.579]
 [0.579]
 [0.579]
 [0.612]
 [0.609]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.606]
 [0.332]
 [0.587]
 [0.558]
 [0.51 ]
 [0.63 ]] [[2.745]
 [0.82 ]
 [0.649]
 [3.179]
 [2.745]
 [0.933]
 [1.999]] [[1.329]
 [0.559]
 [0.129]
 [1.554]
 [1.329]
 [0.483]
 [1.1  ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
siam score:  -0.6540045
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
siam score:  -0.65067077
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
siam score:  -0.64927
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3159301425100631
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
using explorer policy with actor:  1
siam score:  -0.6523731
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
line 256 mcts: sample exp_bonus 3.4758950570139238
siam score:  -0.65253586
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[0.164]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[0.753]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65984863
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]] [[-0.377]
 [ 0.312]
 [ 0.312]
 [ 0.312]
 [ 0.312]
 [ 0.312]
 [ 0.312]] [[0.918]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
siam score:  -0.66581595
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.66151166
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.281]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.112]] [[-0.644]
 [ 0.305]
 [ 0.405]
 [ 0.405]
 [ 0.405]
 [ 0.405]
 [ 0.285]] [[-0.237]
 [-0.254]
 [-0.495]
 [-0.495]
 [-0.495]
 [-0.495]
 [-0.598]]
siam score:  -0.6639122
line 256 mcts: sample exp_bonus 4.044768488950708
start point for exploration sampling:  10749
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9593028888330256
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.242]
 [0.225]
 [0.229]
 [0.229]
 [0.229]
 [0.227]] [[-4.58 ]
 [-4.269]
 [-4.377]
 [-4.291]
 [-4.347]
 [-4.546]
 [-4.551]] [[0.231]
 [0.242]
 [0.225]
 [0.229]
 [0.229]
 [0.229]
 [0.227]]
line 256 mcts: sample exp_bonus 1.5379137911591505
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]] [[-0.427]
 [-0.427]
 [-0.427]
 [-0.427]
 [-0.427]
 [-0.427]
 [-0.427]] [[0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]]
line 256 mcts: sample exp_bonus 2.562308723407591
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[7.624]
 [7.624]
 [7.624]
 [7.624]
 [7.624]
 [7.624]
 [7.624]] [[1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.583]]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.387]] [[7.143]
 [7.143]
 [7.143]
 [7.143]
 [7.143]
 [7.143]
 [7.236]] [[1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.488]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.255]
 [0.297]
 [0.288]
 [0.317]
 [0.294]
 [0.297]] [[1.612]
 [1.607]
 [1.753]
 [1.863]
 [1.571]
 [1.821]
 [1.67 ]] [[-0.183]
 [-0.272]
 [-0.043]
 [ 0.049]
 [-0.184]
 [ 0.02 ]
 [-0.125]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.394]
 [0.408]
 [0.426]
 [0.415]
 [0.407]
 [0.412]] [[2.424]
 [2.268]
 [2.18 ]
 [2.185]
 [1.988]
 [2.05 ]
 [1.985]] [[0.56 ]
 [0.25 ]
 [0.199]
 [0.234]
 [0.049]
 [0.087]
 [0.041]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.313]] [[ 4.683]
 [-1.081]
 [-1.081]
 [-1.081]
 [-1.081]
 [-1.081]
 [-0.852]] [[0.46 ]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.313]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.355]
 [0.355]
 [0.358]
 [0.355]
 [0.355]
 [0.355]] [[-0.554]
 [-1.295]
 [-1.295]
 [-1.328]
 [-1.295]
 [-1.295]
 [-1.295]] [[0.39 ]
 [0.355]
 [0.355]
 [0.358]
 [0.355]
 [0.355]
 [0.355]]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]] [[-2.025]
 [-6.146]
 [-6.146]
 [-6.146]
 [-6.146]
 [-6.146]
 [-6.146]] [[0.587]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]]
first move QE:  -0.013562237746713512
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.113]
 [0.133]
 [0.129]
 [0.123]
 [0.127]
 [0.158]] [[-7.346]
 [-2.497]
 [-6.358]
 [-6.5  ]
 [-6.438]
 [-6.158]
 [-6.603]] [[0.373]
 [0.113]
 [0.133]
 [0.129]
 [0.123]
 [0.127]
 [0.158]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[ 0.066]
 [-0.006]
 [ 0.053]
 [ 0.014]
 [ 1.587]
 [ 0.028]
 [ 0.037]] [[-2.773]
 [-1.183]
 [-2.936]
 [-3.121]
 [ 0.   ]
 [-2.36 ]
 [-2.525]] [[ 0.066]
 [-0.006]
 [ 0.053]
 [ 0.014]
 [ 1.587]
 [ 0.028]
 [ 0.037]]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.377]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[-2.829]
 [-3.102]
 [-2.829]
 [-2.829]
 [-2.829]
 [-2.829]
 [-2.829]] [[0.352]
 [0.377]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[ 0.232]
 [-0.492]
 [-0.492]
 [-0.492]
 [-0.492]
 [-0.492]
 [-0.492]] [[0.703]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.52 ]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[-3.607]
 [-3.14 ]
 [-3.607]
 [-3.607]
 [-3.607]
 [-3.607]
 [-3.607]] [[0.356]
 [0.754]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.224]
 [0.399]
 [0.397]
 [0.36 ]
 [0.403]
 [0.393]] [[-1.733]
 [-0.505]
 [-2.232]
 [-2.3  ]
 [-2.384]
 [-3.306]
 [-1.941]] [[0.477]
 [0.224]
 [0.399]
 [0.397]
 [0.36 ]
 [0.403]
 [0.393]]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.199]
 [0.395]
 [0.426]
 [0.426]
 [0.41 ]
 [0.42 ]] [[-2.179]
 [-0.455]
 [-1.918]
 [-2.947]
 [ 0.   ]
 [-2.62 ]
 [-2.273]] [[0.462]
 [0.199]
 [0.395]
 [0.426]
 [0.426]
 [0.41 ]
 [0.42 ]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.437]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[-0.496]
 [ 0.411]
 [-0.496]
 [-0.496]
 [-0.496]
 [-0.496]
 [-0.496]] [[0.374]
 [0.437]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.471]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.446]] [[-0.643]
 [ 1.131]
 [-0.643]
 [-0.643]
 [-0.643]
 [-0.643]
 [-0.653]] [[0.401]
 [0.471]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.446]]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.293]
 [0.426]
 [0.359]
 [0.359]
 [0.359]
 [0.338]] [[-1.025]
 [-0.437]
 [-2.459]
 [-1.025]
 [-1.025]
 [-1.025]
 [-0.074]] [[0.359]
 [0.293]
 [0.426]
 [0.359]
 [0.359]
 [0.359]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]] [[1.044]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]] [[0.625]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]]
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.438]
 [0.474]
 [0.403]
 [0.377]
 [0.531]
 [0.443]] [[ 0.158]
 [ 1.223]
 [-0.569]
 [-1.026]
 [-1.596]
 [ 0.88 ]
 [ 0.694]] [[0.32 ]
 [0.438]
 [0.474]
 [0.403]
 [0.377]
 [0.531]
 [0.443]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.644]
 [0.704]
 [0.704]
 [0.704]
 [0.601]
 [0.578]] [[0.872]
 [0.835]
 [0.767]
 [0.767]
 [0.767]
 [0.673]
 [0.485]] [[0.57 ]
 [0.644]
 [0.704]
 [0.704]
 [0.704]
 [0.601]
 [0.578]]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.537]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]] [[1.98 ]
 [3.633]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]] [[0.532]
 [0.537]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]]
siam score:  -0.65001464
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 0.4512900441186299
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.626]
 [0.647]
 [0.647]
 [0.647]
 [0.767]
 [0.647]] [[1.539]
 [1.221]
 [1.539]
 [1.539]
 [1.539]
 [2.133]
 [1.539]] [[0.647]
 [0.626]
 [0.647]
 [0.647]
 [0.647]
 [0.767]
 [0.647]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.518]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[-1.033]
 [ 0.821]
 [-1.033]
 [-1.033]
 [-1.033]
 [-1.033]
 [-1.033]] [[0.438]
 [0.518]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.544]
 [0.535]
 [0.571]
 [0.555]
 [0.571]
 [0.552]] [[0.72 ]
 [1.604]
 [0.86 ]
 [0.905]
 [1.148]
 [0.905]
 [1.067]] [[0.556]
 [0.544]
 [0.535]
 [0.571]
 [0.555]
 [0.571]
 [0.552]]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.553]
 [0.567]
 [0.541]
 [0.568]
 [0.56 ]
 [0.567]] [[1.547]
 [1.319]
 [1.497]
 [0.758]
 [1.352]
 [1.686]
 [1.442]] [[0.546]
 [0.553]
 [0.567]
 [0.541]
 [0.568]
 [0.56 ]
 [0.567]]
Printing some Q and Qe and total Qs values:  [[0.855]
 [0.567]
 [0.566]
 [0.574]
 [0.621]
 [0.58 ]
 [0.625]] [[1.032]
 [1.449]
 [1.913]
 [0.779]
 [2.26 ]
 [1.013]
 [0.854]] [[0.855]
 [0.567]
 [0.566]
 [0.574]
 [0.621]
 [0.58 ]
 [0.625]]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]]
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.677]
 [0.628]
 [0.628]
 [0.628]
 [0.631]
 [0.629]] [[0.795]
 [1.095]
 [0.831]
 [0.831]
 [0.831]
 [1.045]
 [0.889]] [[0.629]
 [0.677]
 [0.628]
 [0.628]
 [0.628]
 [0.631]
 [0.629]]
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
rdn probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
using explorer policy with actor:  1
siam score:  -0.6588667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
siam score:  -0.6621805
from probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.168064264126479
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17619424333135025, 0.21922535872247453, 0.07186592384715919, 0.14379222992556953, 0.29693932529868183, 0.09198291887476462]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.676]
 [0.606]
 [0.606]
 [0.606]
 [0.585]
 [0.6  ]] [[-0.313]
 [ 0.45 ]
 [ 0.29 ]
 [ 0.29 ]
 [ 0.29 ]
 [-0.07 ]
 [ 0.006]] [[0.536]
 [0.92 ]
 [0.737]
 [0.737]
 [0.737]
 [0.582]
 [0.633]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65745497
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.499]
 [3.   ]
 [3.   ]
 [0.573]
 [3.   ]
 [3.   ]] [[-4.146]
 [-1.114]
 [ 0.   ]
 [ 0.   ]
 [-3.761]
 [ 0.   ]
 [ 0.   ]] [[1.338]
 [2.136]
 [7.084]
 [7.084]
 [1.457]
 [7.084]
 [7.084]]
line 256 mcts: sample exp_bonus 0.5591283370207755
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.137]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[-0.619]
 [ 0.269]
 [-0.619]
 [-0.619]
 [-0.619]
 [-0.619]
 [-0.619]] [[0.394]
 [0.435]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]]
from probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
siam score:  -0.64660895
line 256 mcts: sample exp_bonus 1.6653414320521713
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]] [[-0.498]
 [-6.066]
 [-6.066]
 [-6.066]
 [-6.066]
 [-6.066]
 [-6.066]] [[0.501]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]] [[-3.812]
 [-6.066]
 [-6.066]
 [-6.066]
 [-6.066]
 [-6.066]
 [-6.066]] [[0.258]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]]
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.218]
 [0.189]
 [0.185]
 [0.18 ]
 [0.181]
 [0.183]] [[-2.86 ]
 [-2.759]
 [-2.813]
 [-2.539]
 [-2.515]
 [-2.039]
 [-3.036]] [[0.187]
 [0.218]
 [0.189]
 [0.185]
 [0.18 ]
 [0.181]
 [0.183]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
line 256 mcts: sample exp_bonus 0.7882814685551226
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.633]
 [0.494]
 [0.494]
 [0.517]
 [0.488]
 [0.517]] [[-4.741]
 [-0.618]
 [-4.066]
 [-4.566]
 [-4.457]
 [-4.787]
 [-4.462]] [[0.059]
 [1.053]
 [0.212]
 [0.097]
 [0.13 ]
 [0.044]
 [0.129]]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.515]
 [0.319]
 [0.23 ]
 [0.23 ]
 [0.281]
 [0.285]] [[-3.689]
 [-2.803]
 [-3.737]
 [-3.748]
 [-3.748]
 [-3.33 ]
 [-3.655]] [[0.353]
 [0.515]
 [0.319]
 [0.23 ]
 [0.23 ]
 [0.281]
 [0.285]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]] [[-6.687]
 [-7.438]
 [-7.438]
 [-7.438]
 [-7.438]
 [-7.438]
 [-7.438]] [[0.552]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[-6.257]
 [-6.386]
 [-6.386]
 [-6.386]
 [-6.386]
 [-6.386]
 [-6.386]] [[0.506]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[-6.466]
 [-6.466]
 [-6.466]
 [-6.466]
 [-6.466]
 [-6.466]
 [-6.466]] [[0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]]
siam score:  -0.65243393
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[-2.638]
 [-3.348]
 [-3.348]
 [-3.348]
 [-3.348]
 [-3.348]
 [-3.348]] [[0.268]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]]
siam score:  -0.6590262
line 256 mcts: sample exp_bonus -2.88954543449879
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.693]
 [0.632]
 [0.632]
 [0.632]
 [0.684]
 [0.632]] [[2.037]
 [3.01 ]
 [2.037]
 [2.037]
 [2.037]
 [3.248]
 [2.037]] [[1.301]
 [1.807]
 [1.301]
 [1.301]
 [1.301]
 [1.914]
 [1.301]]
1888 2743
from probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[-3.159]
 [-7.196]
 [-7.196]
 [-7.196]
 [-7.196]
 [-7.196]
 [-7.196]] [[0.917]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[-5.641]
 [-6.77 ]
 [-6.77 ]
 [-6.77 ]
 [-6.77 ]
 [-6.77 ]
 [-6.77 ]] [[0.666]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]]
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.247]
 [0.276]
 [0.267]
 [0.267]
 [0.267]
 [0.263]] [[-5.477]
 [-4.571]
 [-5.274]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-4.835]] [[0.259]
 [0.247]
 [0.276]
 [0.267]
 [0.267]
 [0.267]
 [0.263]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.369]
 [0.337]
 [0.552]
 [0.277]
 [0.284]
 [0.313]] [[0.13 ]
 [0.263]
 [0.315]
 [0.301]
 [0.175]
 [0.696]
 [0.335]] [[1.658]
 [1.728]
 [1.736]
 [1.811]
 [1.66 ]
 [1.859]
 [1.734]]
siam score:  -0.6545655
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.2137687068067695
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.286 0.306 0.082 0.163 0.122]
siam score:  -0.6520861
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4975],
        [-0.5837],
        [-0.4592],
        [-0.5551],
        [-0.6100],
        [-0.5767],
        [-0.4807],
        [-0.0000],
        [-0.5604],
        [-0.5551]], dtype=torch.float64)
-0.024259925299500003 -0.5217738765931873
-0.0727797758985 -0.6564911655376727
-0.024259925299500003 -0.48342465549555264
-0.024259925299500003 -0.5793575229307909
-0.0727797758985 -0.6827862556870976
-0.034159925299499995 -0.6108232208598061
-0.0727797758985 -0.5534911684749255
-0.04890896504999926 -0.04890896504999926
-0.024259925299500003 -0.5847039803103331
-0.024259925299500003 -0.5793575229307909
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.009]
 [-0.009]] [[3.812]
 [4.989]
 [4.989]
 [4.989]
 [4.989]
 [4.17 ]
 [4.208]] [[-0.143]
 [ 0.897]
 [ 0.897]
 [ 0.897]
 [ 0.897]
 [ 0.173]
 [ 0.208]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
line 256 mcts: sample exp_bonus 0.12797034280953568
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.605]
 [0.588]
 [0.588]
 [0.588]
 [0.591]
 [0.588]] [[3.03 ]
 [0.212]
 [3.03 ]
 [3.03 ]
 [3.03 ]
 [1.842]
 [3.03 ]] [[0.588]
 [0.605]
 [0.588]
 [0.588]
 [0.588]
 [0.591]
 [0.588]]
from probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
in main func line 156:  1892
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.478]
 [0.531]
 [0.478]
 [0.478]
 [0.554]] [[2.141]
 [2.141]
 [2.141]
 [2.564]
 [2.141]
 [2.141]
 [2.476]] [[1.517]
 [1.517]
 [1.517]
 [1.965]
 [1.517]
 [1.517]
 [1.929]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.359]
 [0.365]
 [0.359]
 [0.359]
 [0.355]
 [0.359]] [[0.873]
 [2.446]
 [2.394]
 [2.446]
 [2.446]
 [1.371]
 [2.446]] [[0.345]
 [0.359]
 [0.365]
 [0.359]
 [0.359]
 [0.355]
 [0.359]]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
1895 2760
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[-0.252]
 [-0.46 ]
 [-0.46 ]
 [-0.46 ]
 [-0.46 ]
 [-0.46 ]
 [-0.46 ]] [[0.34 ]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.015]
 [0.038]
 [0.038]
 [0.036]
 [0.038]
 [0.043]] [[-3.891]
 [-1.851]
 [-6.122]
 [-6.014]
 [-5.919]
 [-5.806]
 [-5.478]] [[0.396]
 [0.015]
 [0.038]
 [0.038]
 [0.036]
 [0.038]
 [0.043]]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]] [[-0.249]
 [-0.413]
 [-0.413]
 [-0.413]
 [-0.413]
 [-0.413]
 [-0.413]] [[0.315]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.411]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]] [[-3.034]
 [-1.113]
 [-3.034]
 [-3.034]
 [-3.034]
 [-3.034]
 [-3.034]] [[0.39 ]
 [0.411]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]]
siam score:  -0.65966827
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.153]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]] [[-5.474]
 [-2.502]
 [-5.728]
 [-5.728]
 [-5.728]
 [-5.728]
 [-5.728]] [[0.565]
 [0.153]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.318]
 [0.325]
 [0.321]
 [0.324]
 [0.359]
 [0.341]] [[2.598]
 [3.1  ]
 [3.084]
 [2.643]
 [2.937]
 [3.705]
 [3.489]] [[0.274]
 [0.661]
 [0.657]
 [0.306]
 [0.54 ]
 [1.184]
 [0.994]]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]] [[-1.566]
 [-4.783]
 [-4.783]
 [-4.783]
 [-4.783]
 [-4.783]
 [-4.783]] [[0.591]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[-5.435]
 [-5.435]
 [-5.435]
 [-5.435]
 [-5.435]
 [-5.435]
 [-5.435]] [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.615]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[-0.362]
 [ 0.492]
 [-0.362]
 [-0.362]
 [-0.362]
 [-0.362]
 [-0.362]] [[1.204]
 [1.575]
 [1.204]
 [1.204]
 [1.204]
 [1.204]
 [1.204]]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.368]
 [0.522]
 [0.416]
 [0.414]
 [0.445]
 [0.473]] [[1.454]
 [1.742]
 [1.584]
 [1.797]
 [1.679]
 [1.767]
 [1.765]] [[0.273]
 [0.174]
 [0.325]
 [0.325]
 [0.204]
 [0.354]
 [0.408]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 0.39517469946405437
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[0.751]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[1.825]
 [1.711]
 [1.711]
 [1.711]
 [1.711]
 [1.711]
 [1.711]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.587]
 [0.587]
 [0.632]
 [0.587]
 [0.587]
 [0.587]] [[2.264]
 [2.264]
 [2.264]
 [1.341]
 [2.264]
 [2.264]
 [2.264]] [[2.376]
 [2.376]
 [2.376]
 [2.078]
 [2.376]
 [2.376]
 [2.376]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.708]
 [0.455]] [[1.448]
 [1.448]
 [1.448]
 [1.448]
 [1.448]
 [2.818]
 [1.677]] [[0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.811]
 [0.194]]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.347]
 [0.408]
 [0.413]
 [0.415]
 [0.407]
 [0.421]] [[1.46 ]
 [1.625]
 [1.471]
 [1.543]
 [1.344]
 [1.484]
 [1.358]] [[0.218]
 [0.305]
 [0.22 ]
 [0.327]
 [0.064]
 [0.237]
 [0.095]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5483811187869536
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
using another actor
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4095],
        [-0.4352],
        [-0.0000],
        [-0.3291],
        [-0.5139],
        [-0.5146],
        [-0.0000],
        [-0.5483],
        [-0.4957],
        [-0.0000]], dtype=torch.float64)
-0.0337698257985 -0.4433051685766778
-0.0727797758985 -0.5080015299660743
-0.4018904999999996 -0.4018904999999996
-0.0727797758985 -0.40185168189552817
-0.0727797758985 -0.5867269065183923
-0.024259925299500003 -0.5388579577895338
-0.5000970199499997 -0.5000970199499997
-0.024259925299500003 -0.5725749533339848
-0.024259925299500003 -0.5199711375980558
-0.06326987539949927 -0.06326987539949927
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]] [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
in main func line 156:  1918
line 256 mcts: sample exp_bonus -1.1243211822784693
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.93 ]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[-0.476]
 [-0.076]
 [-0.476]
 [-0.476]
 [-0.476]
 [-0.476]
 [-0.476]] [[0.933]
 [1.536]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.715]
 [0.721]
 [0.744]
 [0.747]
 [0.741]
 [0.986]] [[0.928]
 [1.103]
 [1.   ]
 [0.997]
 [1.206]
 [1.28 ]
 [2.5  ]] [[1.509]
 [1.42 ]
 [1.402]
 [1.437]
 [1.498]
 [1.507]
 [2.211]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6689235035517591
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.618]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]] [[0.459]
 [1.009]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[0.895]
 [1.42 ]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.48 ]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]] [[-0.752]
 [ 0.405]
 [-0.752]
 [-0.752]
 [-0.752]
 [-0.752]
 [-0.752]] [[1.344]
 [1.754]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]]
1922 2789
from probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.71 ]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[-0.975]
 [ 0.641]
 [-0.975]
 [-0.975]
 [-0.975]
 [-0.975]
 [-0.975]] [[0.574]
 [0.71 ]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5220532029165905
siam score:  -0.6438168
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]] [[-3.163]
 [-4.337]
 [-4.337]
 [-4.337]
 [-4.337]
 [-4.337]
 [-4.337]] [[0.328]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]]
siam score:  -0.63980794
line 256 mcts: sample exp_bonus -3.4138386514218824
first move QE:  -0.03508873511821523
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]] [[-4.123]
 [-4.73 ]
 [-4.73 ]
 [-4.73 ]
 [-4.73 ]
 [-4.73 ]
 [-4.73 ]] [[0.326]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.28873072230801045
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  -0.03837592532701011
using another actor
first move QE:  -0.03860001119970305
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5472],
        [-0.3727],
        [-0.4965],
        [-0.4608],
        [-0.0000],
        [-0.2724],
        [-0.6008],
        [-0.5948],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.0628797758985 -0.6100792443125561
-0.024259925299500003 -0.39695855669977587
-0.024259925299500003 -0.5207314830262701
-0.024259925299500003 -0.4850900725406903
-0.039254489999999254 -0.039254489999999254
-0.024259925299500003 -0.2966469901250145
-0.024259925299500003 -0.6250610316681448
-0.024259925299500003 -0.6190931655133465
-0.9311445 -0.9311445
-0.97515 -0.97515
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.314]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.301]] [[5.136]
 [3.825]
 [5.291]
 [5.291]
 [5.291]
 [5.291]
 [4.521]] [[0.962]
 [0.059]
 [1.021]
 [1.021]
 [1.021]
 [1.021]
 [0.498]]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.294]
 [0.294]
 [0.307]
 [0.294]
 [0.294]
 [0.294]] [[6.264]
 [6.264]
 [6.264]
 [6.003]
 [6.264]
 [6.264]
 [6.264]] [[1.369]
 [1.369]
 [1.369]
 [1.241]
 [1.369]
 [1.369]
 [1.369]]
from probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.321]
 [0.321]
 [0.274]
 [0.321]
 [0.296]
 [0.311]] [[5.418]
 [5.418]
 [5.418]
 [6.25 ]
 [5.418]
 [5.679]
 [5.635]] [[0.736]
 [0.736]
 [0.736]
 [1.195]
 [0.736]
 [0.859]
 [0.86 ]]
line 256 mcts: sample exp_bonus 1.2193510018924405
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]] [[-1.609]
 [-1.609]
 [-1.609]
 [-1.609]
 [-1.609]
 [-1.609]
 [-1.609]] [[0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[ 2.389]
 [-0.389]
 [-0.389]
 [-0.389]
 [-0.389]
 [-0.389]
 [-0.389]] [[0.545]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
first move QE:  -0.039438706339731205
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.811]
 [0.676]
 [0.675]
 [0.67 ]
 [0.722]
 [0.706]] [[0.217]
 [0.752]
 [0.343]
 [0.261]
 [0.243]
 [0.486]
 [0.483]] [[0.695]
 [0.811]
 [0.676]
 [0.675]
 [0.67 ]
 [0.722]
 [0.706]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.6389937
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.63677406
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.64 ]
 [0.576]
 [0.585]
 [0.584]
 [0.574]
 [0.574]] [[-2.536]
 [ 0.144]
 [-2.6  ]
 [-2.095]
 [-2.101]
 [-2.56 ]
 [-2.396]] [[0.447]
 [1.508]
 [0.406]
 [0.607]
 [0.604]
 [0.42 ]
 [0.483]]
siam score:  -0.63597566
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.626]
 [0.593]
 [1.053]
 [0.558]
 [0.679]
 [0.568]] [[ 1.133]
 [ 1.202]
 [-0.016]
 [ 1.308]
 [ 1.133]
 [ 0.155]
 [ 1.303]] [[1.124]
 [1.282]
 [0.811]
 [2.172]
 [1.124]
 [1.039]
 [1.2  ]]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.673]
 [0.62 ]
 [0.691]
 [0.675]
 [0.461]
 [0.674]] [[2.91 ]
 [2.882]
 [2.248]
 [2.616]
 [2.91 ]
 [2.162]
 [2.289]] [[2.038]
 [2.026]
 [1.744]
 [1.978]
 [2.038]
 [1.438]
 [1.853]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.408]
 [0.421]
 [0.427]
 [0.42 ]
 [0.451]
 [0.449]] [[ 0.093]
 [-0.437]
 [ 0.131]
 [ 0.546]
 [ 0.295]
 [ 0.844]
 [ 0.379]] [[1.015]
 [0.658]
 [0.982]
 [1.216]
 [1.072]
 [1.4  ]
 [1.142]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.066]
 [-0.048]
 [-0.064]
 [-0.072]
 [-0.073]
 [-0.055]
 [-0.055]] [[3.142]
 [2.715]
 [3.139]
 [3.291]
 [3.363]
 [3.4  ]
 [3.486]] [[ 0.27 ]
 [-0.147]
 [ 0.271]
 [ 0.417]
 [ 0.49 ]
 [ 0.557]
 [ 0.647]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -0.04206705766258527
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
1946 2837
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]] [[-9.072]
 [-6.902]
 [-6.902]
 [-6.902]
 [-6.902]
 [-6.902]
 [-6.902]] [[0.314]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]]
siam score:  -0.621875
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.331]
 [0.386]
 [0.32 ]
 [0.372]
 [0.416]
 [0.661]] [[2.293]
 [2.688]
 [2.337]
 [2.747]
 [2.433]
 [2.185]
 [2.48 ]] [[-0.184]
 [ 0.024]
 [ 0.017]
 [ 0.022]
 [ 0.021]
 [ 0.026]
 [ 0.615]]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[-2.471]
 [-6.524]
 [-6.524]
 [-6.524]
 [-6.524]
 [-6.524]
 [-6.524]] [[0.608]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[-0.321]
 [-0.341]
 [-0.341]
 [-0.341]
 [-0.341]
 [-0.341]
 [-0.341]] [[0.36 ]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.218]
 [0.216]
 [0.218]
 [0.195]
 [0.201]
 [0.209]] [[-4.574]
 [-3.389]
 [-4.133]
 [-4.425]
 [-4.324]
 [-4.23 ]
 [-4.385]] [[0.194]
 [0.218]
 [0.216]
 [0.218]
 [0.195]
 [0.201]
 [0.209]]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.307]
 [0.301]] [[-1.141]
 [-1.044]
 [-1.044]
 [-1.044]
 [-1.044]
 [-1.096]
 [-1.099]] [[0.288]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.307]
 [0.301]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.5510683369993845
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]] [[2.92]
 [2.92]
 [2.92]
 [2.92]
 [2.92]
 [2.92]
 [2.92]] [[0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.502]
 [0.471]] [[2.588]
 [2.571]
 [2.571]
 [2.571]
 [2.571]
 [2.497]
 [2.571]] [[0.584]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.544]
 [0.531]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[1.705]
 [1.844]
 [1.844]
 [1.844]
 [1.844]
 [1.844]
 [1.844]] [[0.621]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.052]
 [0.013]
 [0.017]
 [0.016]
 [0.024]
 [0.02 ]] [[5.558]
 [4.489]
 [5.547]
 [5.301]
 [5.527]
 [5.502]
 [5.419]] [[1.179]
 [0.842]
 [1.171]
 [1.089]
 [1.167]
 [1.167]
 [1.134]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
siam score:  -0.6366562
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.736]] [[0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [2.325]] [[0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.736]]
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.193]
 [0.316]
 [0.301]
 [0.333]
 [0.319]
 [0.299]] [[-0.032]
 [ 0.258]
 [-0.331]
 [-0.235]
 [-0.513]
 [-0.284]
 [ 0.021]] [[0.312]
 [0.193]
 [0.316]
 [0.301]
 [0.333]
 [0.319]
 [0.299]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[-5.935]
 [-5.697]
 [-5.697]
 [-5.697]
 [-5.697]
 [-5.697]
 [-5.697]] [[0.336]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.26 ]
 [0.26 ]
 [0.275]
 [0.26 ]
 [0.235]
 [0.275]] [[1.686]
 [1.686]
 [1.686]
 [1.88 ]
 [1.686]
 [1.486]
 [2.052]] [[0.65 ]
 [0.65 ]
 [0.65 ]
 [0.817]
 [0.65 ]
 [0.467]
 [0.953]]
using explorer policy with actor:  1
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.09 ]
 [0.107]
 [0.466]
 [0.101]
 [0.101]
 [0.103]] [[-7.082]
 [ 0.437]
 [-5.85 ]
 [ 0.   ]
 [-5.894]
 [-5.748]
 [-5.88 ]] [[0.114]
 [0.09 ]
 [0.107]
 [0.466]
 [0.101]
 [0.101]
 [0.103]]
siam score:  -0.63577026
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.189]
 [0.236]
 [0.212]
 [0.198]
 [0.199]
 [0.199]] [[6.406]
 [6.998]
 [6.588]
 [6.749]
 [7.32 ]
 [7.146]
 [7.241]] [[-0.11 ]
 [ 0.004]
 [-0.039]
 [-0.033]
 [ 0.13 ]
 [ 0.073]
 [ 0.106]]
line 256 mcts: sample exp_bonus -4.1637124018566265
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]] [[7.098]
 [7.098]
 [7.098]
 [7.098]
 [7.098]
 [7.098]
 [7.098]] [[-0.14]
 [-0.14]
 [-0.14]
 [-0.14]
 [-0.14]
 [-0.14]
 [-0.14]]
from probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.485]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[0.15 ]
 [2.772]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]] [[0.408]
 [0.485]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 7.521386556964599
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]] [[-3.403]
 [-7.258]
 [-7.258]
 [-7.258]
 [-7.258]
 [-7.258]
 [-7.258]] [[0.572]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6379059
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[4.362]
 [4.362]
 [4.362]
 [4.362]
 [4.362]
 [4.362]
 [4.362]] [[0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]]
line 256 mcts: sample exp_bonus 4.1606728108936455
line 256 mcts: sample exp_bonus -0.7644696523297351
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.349]
 [0.495]
 [0.502]
 [0.495]
 [0.324]
 [0.393]] [[2.159]
 [2.716]
 [2.159]
 [2.654]
 [2.159]
 [1.819]
 [2.717]] [[1.141]
 [1.219]
 [1.141]
 [1.484]
 [1.141]
 [0.572]
 [1.309]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[-0.359]
 [-1.603]
 [-1.603]
 [-1.603]
 [-1.603]
 [-1.603]
 [-1.603]] [[0.758]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.99 ]
 [0.048]] [[ 1.268]
 [ 1.268]
 [ 1.268]
 [ 1.268]
 [ 1.268]
 [-0.046]
 [ 1.268]] [[-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [ 0.444]
 [-0.125]]
from probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]] [[-4.638]
 [-4.791]
 [-4.791]
 [-4.791]
 [-4.791]
 [-4.791]
 [-4.791]] [[0.353]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]]
line 256 mcts: sample exp_bonus 2.3120317012131757
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.342]
 [0.415]
 [0.491]
 [0.415]
 [0.41 ]
 [0.375]] [[-1.519]
 [ 0.314]
 [-1.453]
 [-1.542]
 [-1.245]
 [-1.168]
 [-1.345]] [[0.417]
 [0.342]
 [0.415]
 [0.491]
 [0.415]
 [0.41 ]
 [0.375]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
line 256 mcts: sample exp_bonus 1.4476656464258206
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]] [[-0.719]
 [-5.086]
 [-5.086]
 [-5.086]
 [-5.086]
 [-5.086]
 [-5.086]] [[0.53 ]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.544]
 [0.57 ]
 [0.57 ]
 [0.563]
 [0.571]
 [0.598]] [[-7.388]
 [ 0.309]
 [-7.321]
 [-7.258]
 [-7.139]
 [-6.974]
 [-6.295]] [[0.174]
 [1.753]
 [0.179]
 [0.192]
 [0.215]
 [0.252]
 [0.4  ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[-4.728]
 [-5.176]
 [-5.176]
 [-5.176]
 [-5.176]
 [-5.176]
 [-5.176]] [[0.172]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]] [[-4.121]
 [-7.518]
 [-7.518]
 [-7.518]
 [-7.518]
 [-7.518]
 [-7.518]] [[0.516]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]]
line 256 mcts: sample exp_bonus -6.503488841768728
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.018]
 [0.045]
 [0.463]
 [0.037]
 [0.463]
 [0.056]] [[-4.927]
 [ 0.53 ]
 [-7.753]
 [ 0.   ]
 [-8.047]
 [ 0.   ]
 [-7.521]] [[0.495]
 [0.018]
 [0.045]
 [0.463]
 [0.037]
 [0.463]
 [0.056]]
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]]
Printing some Q and Qe and total Qs values:  [[0.865]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]] [[-1.625]
 [-2.215]
 [-2.215]
 [-2.215]
 [-2.215]
 [-2.215]
 [-2.215]] [[0.865]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
in main func line 156:  1973
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.872617750752673
siam score:  -0.6479704
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.858]
 [0.827]
 [0.817]
 [0.808]
 [0.817]
 [0.813]] [[3.089]
 [2.759]
 [2.975]
 [2.778]
 [3.089]
 [3.205]
 [2.951]] [[2.495]
 [2.484]
 [2.495]
 [2.408]
 [2.495]
 [2.552]
 [2.46 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.275]
 [1.275]
 [1.275]
 [1.275]
 [1.275]
 [1.275]
 [1.275]] [[-0.052]
 [-0.05 ]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]] [[1.772]
 [1.773]
 [1.772]
 [1.772]
 [1.772]
 [1.772]
 [1.772]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.723]
 [0.557]
 [0.42 ]
 [0.557]
 [0.557]
 [0.557]] [[1.049]
 [0.901]
 [1.049]
 [0.097]
 [1.049]
 [1.049]
 [1.049]] [[2.08 ]
 [2.097]
 [2.08 ]
 [1.705]
 [2.08 ]
 [2.08 ]
 [2.08 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
siam score:  -0.63505673
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
using explorer policy with actor:  1
using explorer policy with actor:  0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  116 total reward:  0.12499999999999933  reward:  1.0 rdn_beta:  0.333
from probs:  [0.1019472705649408, 0.23898343774415434, 0.07834296925200097, 0.1567517627922465, 0.32370151507492406, 0.1002730445717333]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.09344958512775088, 0.3024170080860147, 0.07181279041317791, 0.14368591841453934, 0.29671978583974773, 0.0919149121187695]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.6375484
first move QE:  -0.05295703802572436
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.09344958512775088, 0.3024170080860147, 0.07181279041317791, 0.14368591841453934, 0.29671978583974773, 0.0919149121187695]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.695]
 [0.809]] [[2.631]
 [3.348]
 [3.348]
 [3.348]
 [3.348]
 [4.568]
 [3.348]] [[0.565]
 [1.02 ]
 [1.02 ]
 [1.02 ]
 [1.02 ]
 [1.199]
 [1.02 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.09344958512775088, 0.3024170080860147, 0.07181279041317791, 0.14368591841453934, 0.29671978583974773, 0.0919149121187695]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.09344958512775088, 0.3024170080860147, 0.07181279041317791, 0.14368591841453934, 0.29671978583974773, 0.0919149121187695]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.6468573
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.09344958512775088, 0.3024170080860147, 0.07181279041317791, 0.14368591841453934, 0.29671978583974773, 0.0919149121187695]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.064]
 [0.015]
 [0.07 ]
 [0.072]
 [0.016]
 [0.066]] [[5.865]
 [3.673]
 [4.308]
 [6.364]
 [5.71 ]
 [6.777]
 [5.324]] [[-0.529]
 [-1.314]
 [-1.201]
 [-0.403]
 [-0.619]
 [-0.374]
 [-0.758]]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[-3.206]
 [-1.754]
 [-1.754]
 [-1.754]
 [-1.754]
 [-1.754]
 [-1.754]] [[0.67 ]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]]
siam score:  -0.65202177
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1991 2897
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.828]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[0.692]
 [2.734]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[1.207]
 [1.695]
 [1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.287]] [[0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.769]] [[1.999]
 [1.999]
 [1.999]
 [1.999]
 [1.999]
 [1.999]
 [1.938]]
first move QE:  -0.059678836614088585
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.09344958512775088, 0.3024170080860147, 0.07181279041317791, 0.14368591841453934, 0.29671978583974773, 0.0919149121187695]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -4.998866424925001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.122]
 [0.021]
 [0.018]
 [0.021]
 [0.029]
 [0.028]] [[0.6  ]
 [0.881]
 [0.978]
 [1.348]
 [0.562]
 [0.326]
 [0.106]] [[1.299]
 [1.563]
 [1.56 ]
 [1.813]
 [1.274]
 [1.117]
 [0.964]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.24 ]
 [0.238]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]] [[-5.694]
 [-5.694]
 [-5.829]
 [-5.694]
 [-5.694]
 [-5.694]
 [-5.694]] [[0.24 ]
 [0.24 ]
 [0.238]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.09344958512775088, 0.3024170080860147, 0.07181279041317791, 0.14368591841453934, 0.29671978583974773, 0.0919149121187695]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.09344958512775088, 0.3024170080860147, 0.07181279041317791, 0.14368591841453934, 0.29671978583974773, 0.0919149121187695]
line 256 mcts: sample exp_bonus 4.819408551712385
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.09344958512775088, 0.3024170080860147, 0.07181279041317791, 0.14368591841453934, 0.29671978583974773, 0.0919149121187695]
Printing some Q and Qe and total Qs values:  [[0.43]
 [0.49]
 [0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.5 ]] [[5.621]
 [5.873]
 [5.621]
 [5.621]
 [5.621]
 [5.621]
 [6.433]] [[1.103]
 [1.332]
 [1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.645]]
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[-0.872]
 [-1.959]
 [-1.959]
 [-1.959]
 [-1.959]
 [-1.959]
 [-1.959]] [[0.197]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]] [[ 5.168]
 [-3.513]
 [-3.513]
 [-3.513]
 [-3.513]
 [-3.513]
 [-3.513]] [[0.327]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.714]
 [0.765]
 [0.679]
 [0.664]
 [0.569]
 [0.687]] [[5.132]
 [5.05 ]
 [4.865]
 [5.696]
 [5.419]
 [6.527]
 [6.072]] [[0.942]
 [0.906]
 [0.864]
 [1.201]
 [1.044]
 [1.514]
 [1.402]]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.439]
 [0.362]
 [0.392]
 [0.377]
 [0.397]
 [0.422]] [[-3.919]
 [-4.132]
 [-3.896]
 [-4.212]
 [-4.137]
 [-4.196]
 [-3.836]] [[0.42 ]
 [0.439]
 [0.362]
 [0.392]
 [0.377]
 [0.397]
 [0.422]]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.708]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[-3.233]
 [ 0.608]
 [-3.233]
 [-3.233]
 [-3.233]
 [-3.233]
 [-3.233]] [[0.526]
 [0.708]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.597]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.586]] [[1.   ]
 [1.989]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.879]] [[0.584]
 [0.597]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.586]]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.555]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[-3.197]
 [-2.826]
 [-3.197]
 [-3.197]
 [-3.197]
 [-3.197]
 [-3.197]] [[0.496]
 [0.555]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.584]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[-1.272]
 [-0.11 ]
 [-1.272]
 [-1.272]
 [-1.272]
 [-1.272]
 [-1.272]] [[0.575]
 [0.584]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.575]
 [0.511]
 [0.608]
 [0.509]
 [0.652]
 [0.54 ]] [[-4.158]
 [-2.137]
 [-3.764]
 [-2.413]
 [-3.945]
 [ 0.   ]
 [-3.465]] [[0.477]
 [0.575]
 [0.511]
 [0.608]
 [0.509]
 [0.652]
 [0.54 ]]
using explorer policy with actor:  0
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9045293381339196
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6418594596044371
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.545]
 [0.677]
 [0.643]
 [0.536]
 [0.735]
 [0.565]] [[ 0.256]
 [ 0.726]
 [ 0.759]
 [-0.97 ]
 [-0.091]
 [ 1.193]
 [ 0.502]] [[0.447]
 [0.545]
 [0.677]
 [0.643]
 [0.536]
 [0.735]
 [0.565]]
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]] [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]]
line 256 mcts: sample exp_bonus 0.11527854373071739
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.744]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.718]] [[0.176]
 [0.556]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.274]] [[0.72 ]
 [0.744]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.718]]
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.601]
 [0.703]
 [0.629]
 [0.654]
 [0.666]
 [0.655]] [[-0.214]
 [ 0.454]
 [ 0.241]
 [ 0.416]
 [ 0.334]
 [ 0.209]
 [ 0.163]] [[0.828]
 [0.601]
 [0.703]
 [0.629]
 [0.654]
 [0.666]
 [0.655]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]] [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.523140392926604
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]] [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  -0.06421240426926092
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.777]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.714]] [[0.083]
 [0.456]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.371]] [[0.712]
 [0.777]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.714]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.10919072165080647
siam score:  -0.6481607
siam score:  -0.6464514
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.5148],
        [-0.3650],
        [-0.0000],
        [-0.0000],
        [-0.3854],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.9360449999999999 -0.9360449999999999
-0.0337698257985 -0.5485221467082138
-0.024259925299500003 -0.38922671945244336
-0.9272682044999999 -0.9272682044999999
-0.7618322249999999 -0.7618322249999999
-0.024259925299500003 -0.40965797555853856
-0.965595015 -0.965595015
-0.009850499999999242 -0.009850499999999242
-0.6286499999999998 -0.6286499999999998
-0.9608890648499999 -0.9608890648499999
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.54 ]
 [0.683]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[1.147]
 [1.147]
 [2.44 ]
 [1.147]
 [1.147]
 [1.147]
 [1.147]] [[0.745]
 [0.745]
 [2.368]
 [0.745]
 [0.745]
 [0.745]
 [0.745]]
using explorer policy with actor:  1
rdn probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
using explorer policy with actor:  1
from probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]] [[-9.281]
 [-4.684]
 [-4.684]
 [-4.684]
 [-4.684]
 [-4.684]
 [-4.684]] [[0.238]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]]
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.496]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[-2.078]
 [-1.937]
 [-2.078]
 [-2.078]
 [-2.078]
 [-2.078]
 [-2.078]] [[0.463]
 [0.496]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.466256338507254
siam score:  -0.65803605
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 5.316823483160062
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
using another actor
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.603]
 [0.633]
 [0.628]
 [0.632]
 [0.619]
 [0.602]] [[1.558]
 [2.014]
 [1.589]
 [1.571]
 [1.594]
 [1.628]
 [1.757]] [[0.906]
 [1.149]
 [0.927]
 [0.905]
 [0.928]
 [0.925]
 [0.977]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
first move QE:  -0.07133060889099403
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.62 ]
 [0.699]
 [0.687]
 [0.699]
 [0.699]
 [0.632]] [[6.146]
 [6.767]
 [6.146]
 [6.406]
 [6.146]
 [6.146]
 [6.581]] [[1.571]
 [1.761]
 [1.571]
 [1.682]
 [1.571]
 [1.571]
 [1.687]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.88 ]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]] [[1.221]
 [3.295]
 [1.221]
 [1.221]
 [1.221]
 [1.221]
 [1.221]] [[0.999]
 [1.642]
 [0.999]
 [0.999]
 [0.999]
 [0.999]
 [0.999]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
from probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]] [[ 0.551]
 [-6.142]
 [-6.142]
 [-6.142]
 [-6.142]
 [-6.142]
 [-6.142]] [[0.468]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
siam score:  -0.66223025
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.576]
 [0.619]
 [0.593]
 [0.619]
 [0.619]
 [0.543]] [[0.656]
 [1.747]
 [1.361]
 [1.455]
 [1.361]
 [1.361]
 [1.852]] [[0.382]
 [1.919]
 [1.544]
 [1.608]
 [1.544]
 [1.544]
 [1.982]]
siam score:  -0.662959
2015 2928
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
siam score:  -0.6641972
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.572]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.58 ]
 [0.56 ]] [[3.097]
 [3.393]
 [3.097]
 [3.097]
 [3.097]
 [1.988]
 [3.097]] [[0.56 ]
 [0.572]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.58 ]
 [0.56 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6550423
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.11043576745885529
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.251]
 [0.248]
 [0.328]
 [0.289]
 [0.293]
 [0.194]] [[0.648]
 [0.607]
 [0.783]
 [0.734]
 [1.347]
 [1.171]
 [0.866]] [[0.303]
 [0.251]
 [0.248]
 [0.328]
 [0.289]
 [0.293]
 [0.194]]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.388]
 [0.514]
 [0.431]
 [0.378]
 [0.475]
 [0.445]] [[ 0.544]
 [ 1.049]
 [ 1.667]
 [ 0.56 ]
 [-0.024]
 [ 0.63 ]
 [ 1.522]] [[0.34 ]
 [0.388]
 [0.514]
 [0.431]
 [0.378]
 [0.475]
 [0.445]]
start point for exploration sampling:  10749
2024 2938
from probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
line 256 mcts: sample exp_bonus 6.613513307138387
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.506]
 [0.487]
 [0.481]
 [0.481]
 [0.343]
 [0.484]] [[2.387]
 [1.901]
 [1.173]
 [2.387]
 [2.387]
 [2.322]
 [2.417]] [[1.827]
 [1.561]
 [1.113]
 [1.827]
 [1.827]
 [1.666]
 [1.846]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 4.216851309614954
Printing some Q and Qe and total Qs values:  [[0.745]
 [1.037]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]] [[-3.171]
 [-1.31 ]
 [-3.171]
 [-3.171]
 [-3.171]
 [-3.171]
 [-3.171]] [[0.304]
 [0.947]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.43 ]
 [0.443]
 [0.449]
 [0.43 ]
 [0.43 ]
 [0.45 ]] [[0.712]
 [0.771]
 [0.733]
 [0.777]
 [0.771]
 [0.771]
 [0.726]] [[0.377]
 [0.343]
 [0.318]
 [0.389]
 [0.343]
 [0.343]
 [0.323]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.81 ]
 [0.636]
 [0.636]
 [0.636]
 [0.621]
 [0.636]] [[ 0.072]
 [ 0.42 ]
 [ 0.072]
 [ 0.072]
 [ 0.072]
 [-0.006]
 [ 0.072]] [[0.636]
 [0.81 ]
 [0.636]
 [0.636]
 [0.636]
 [0.621]
 [0.636]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.556]
 [0.588]
 [0.587]
 [0.543]
 [0.588]
 [0.537]] [[-0.037]
 [-0.155]
 [-0.28 ]
 [-0.679]
 [-0.522]
 [-0.28 ]
 [ 0.249]] [[1.395]
 [1.681]
 [1.701]
 [1.567]
 [1.532]
 [1.701]
 [1.777]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.479]
 [0.476]
 [0.479]
 [0.488]
 [0.482]
 [0.477]] [[5.579]
 [5.314]
 [5.649]
 [5.624]
 [5.611]
 [5.749]
 [5.743]] [[1.319]
 [1.15 ]
 [1.367]
 [1.357]
 [1.366]
 [1.445]
 [1.431]]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[5.76]
 [5.76]
 [5.76]
 [5.76]
 [5.76]
 [5.76]
 [5.76]] [[1.475]
 [1.475]
 [1.475]
 [1.475]
 [1.475]
 [1.475]
 [1.475]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.525]
 [0.536]
 [0.606]
 [0.536]
 [0.536]
 [0.536]] [[0.956]
 [1.406]
 [0.872]
 [1.108]
 [0.872]
 [0.872]
 [0.872]] [[1.312]
 [1.376]
 [1.041]
 [1.339]
 [1.041]
 [1.041]
 [1.041]]
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.483]
 [0.661]
 [0.662]
 [0.656]
 [0.666]
 [0.682]] [[1.247]
 [1.494]
 [1.264]
 [1.257]
 [1.214]
 [1.281]
 [1.211]] [[0.443]
 [0.254]
 [0.455]
 [0.453]
 [0.413]
 [0.477]
 [0.462]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[ 0.616]
 [-0.653]
 [-0.653]
 [-0.653]
 [-0.653]
 [-0.653]
 [-0.653]] [[0.409]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[ 0.379]
 [-0.12 ]
 [-0.12 ]
 [-0.12 ]
 [-0.12 ]
 [-0.12 ]
 [-0.12 ]] [[0.423]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]]
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[ 1.182]
 [-0.53 ]
 [-0.53 ]
 [-0.53 ]
 [-0.53 ]
 [-0.53 ]
 [-0.53 ]] [[0.447]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[-1.532]
 [-1.532]
 [-1.532]
 [-1.532]
 [-1.532]
 [-1.532]
 [-1.532]] [[0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[1.351]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]] [[0.498]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]] [[-7.612]
 [-6.918]
 [-6.918]
 [-6.918]
 [-6.918]
 [-6.918]
 [-6.918]] [[0.342]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]]
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.559]
 [0.57 ]
 [0.547]] [[-0.489]
 [-0.396]
 [-0.396]
 [-0.396]
 [ 1.375]
 [-0.396]
 [-0.597]] [[1.286]
 [1.289]
 [1.289]
 [1.289]
 [1.86 ]
 [1.289]
 [1.176]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[1.095]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[0.513]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.519]
 [0.584]
 [0.623]
 [0.159]
 [0.595]
 [0.579]] [[-0.409]
 [-1.027]
 [-4.591]
 [-4.968]
 [-0.277]
 [-4.922]
 [-3.546]] [[1.561]
 [1.445]
 [0.249]
 [0.139]
 [1.518]
 [0.141]
 [0.607]]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.021]
 [0.022]
 [0.024]] [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.513]
 [0.567]
 [0.991]] [[0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.019]
 [0.13 ]
 [0.98 ]]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]] [[-6.854]
 [-6.724]
 [-6.724]
 [-6.724]
 [-6.724]
 [-6.724]
 [-6.724]] [[0.381]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]]
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[-1.374]
 [-5.628]
 [-5.628]
 [-5.628]
 [-5.628]
 [-5.628]
 [-5.628]] [[0.631]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[-1.91 ]
 [-4.702]
 [-4.702]
 [-4.702]
 [-4.702]
 [-4.702]
 [-4.702]] [[0.515]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[-3.324]
 [-3.943]
 [-3.943]
 [-3.943]
 [-3.943]
 [-3.943]
 [-3.943]] [[0.434]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
siam score:  -0.6484885
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.44 ]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[-4.21 ]
 [-3.818]
 [-4.21 ]
 [-4.21 ]
 [-4.21 ]
 [-4.21 ]
 [-4.21 ]] [[0.084]
 [0.231]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]] [[-2.203]
 [-3.831]
 [-3.831]
 [-3.831]
 [-3.831]
 [-3.831]
 [-3.831]] [[0.333]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]]
using another actor
from probs:  [0.10871190442679225, 0.3518081844359125, 0.08354135759239784, 0.0038314833674908577, 0.34518048374053034, 0.10692658643687623]
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]] [[0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]] [[1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.781]]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.589]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[0.013]
 [1.061]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[0.567]
 [0.589]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]]
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[-0.09 ]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]] [[2.225]
 [2.225]
 [2.225]
 [2.225]
 [2.225]
 [2.225]
 [2.225]]
using explorer policy with actor:  0
using another actor
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.351]
 [0.351]
 [0.347]
 [0.351]
 [0.351]
 [0.351]] [[7.503]
 [7.503]
 [7.503]
 [7.931]
 [7.503]
 [7.503]
 [7.503]] [[1.61 ]
 [1.61 ]
 [1.61 ]
 [1.735]
 [1.61 ]
 [1.61 ]
 [1.61 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [1.175]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]] [[-1.852]
 [-0.576]
 [-1.852]
 [-1.852]
 [-1.852]
 [-1.852]
 [-1.852]] [[0.819]
 [1.526]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]]
from probs:  [0.12120783909066146, 0.392246921206328, 0.09314405337543906, 0.004271894802451129, 0.3848573967226693, 0.004271894802451129]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.579]
 [0.573]
 [0.558]
 [0.557]
 [0.572]
 [0.574]] [[0.833]
 [0.745]
 [0.909]
 [0.722]
 [0.609]
 [0.835]
 [0.607]] [[1.589]
 [1.603]
 [1.645]
 [1.554]
 [1.513]
 [1.619]
 [1.547]]
siam score:  -0.6548271
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
siam score:  -0.6536579
line 256 mcts: sample exp_bonus -1.7289434948765507
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]] [[-0.295]
 [ 2.047]
 [ 2.047]
 [ 2.047]
 [ 2.047]
 [ 2.047]
 [ 2.047]] [[0.872]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]]
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.733]] [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [1.211]] [[0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.733]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.765]
 [0.749]
 [0.754]
 [0.705]
 [0.749]
 [0.758]] [[3.748]
 [4.244]
 [4.051]
 [4.301]
 [1.086]
 [4.051]
 [4.247]] [[0.751]
 [0.765]
 [0.749]
 [0.754]
 [0.705]
 [0.749]
 [0.758]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
Printing some Q and Qe and total Qs values:  [[1.427]
 [1.429]
 [1.427]
 [1.427]
 [1.427]
 [1.427]
 [1.43 ]] [[0.018]
 [0.029]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.046]] [[2.653]
 [2.663]
 [2.653]
 [2.653]
 [2.653]
 [2.653]
 [2.678]]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.455]
 [0.491]
 [0.468]
 [0.706]
 [0.706]
 [0.488]] [[ 0.   ]
 [-4.637]
 [-4.609]
 [-4.802]
 [ 0.   ]
 [ 0.   ]
 [-4.625]] [[3.592]
 [0.175]
 [0.224]
 [0.071]
 [3.592]
 [3.592]
 [0.21 ]]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.79 ]
 [0.664]] [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [1.   ]
 [0.907]] [[0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.88 ]
 [0.597]]
using explorer policy with actor:  1
siam score:  -0.640822
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]] [[4.199]
 [4.199]
 [4.199]
 [4.199]
 [4.199]
 [4.199]
 [4.199]] [[1.991]
 [1.991]
 [1.991]
 [1.991]
 [1.991]
 [1.991]
 [1.991]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [1.071]
 [0.689]] [[0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.362]
 [0.827]] [[1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.613]
 [1.003]]
Printing some Q and Qe and total Qs values:  [[0.675]
 [1.038]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[1.042]
 [2.03 ]
 [1.042]
 [1.042]
 [1.042]
 [1.042]
 [1.042]] [[1.575]
 [2.631]
 [1.575]
 [1.575]
 [1.575]
 [1.575]
 [1.575]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6536192
using explorer policy with actor:  1
siam score:  -0.6518888
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
line 256 mcts: sample exp_bonus -1.0890381078272595
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
siam score:  -0.6447313
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.743]
 [0.829]
 [0.685]
 [0.685]
 [0.685]
 [0.768]] [[3.631]
 [3.648]
 [3.418]
 [3.631]
 [3.631]
 [3.631]
 [3.792]] [[1.826]
 [1.903]
 [1.915]
 [1.826]
 [1.826]
 [1.826]
 [1.993]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.37 ]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[-2.553]
 [-1.591]
 [-2.553]
 [-2.553]
 [-2.553]
 [-2.553]
 [-2.553]] [[0.331]
 [0.37 ]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[3.625]
 [3.625]
 [3.625]
 [3.625]
 [3.625]
 [3.625]
 [3.625]] [[1.423]
 [1.423]
 [1.423]
 [1.423]
 [1.423]
 [1.423]
 [1.423]]
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.564]
 [0.584]
 [0.599]
 [0.604]
 [0.619]
 [0.581]] [[5.655]
 [4.964]
 [6.113]
 [5.978]
 [5.37 ]
 [5.92 ]
 [6.112]] [[1.377]
 [0.954]
 [1.64 ]
 [1.589]
 [1.253]
 [1.591]
 [1.634]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[ 0.44 ]
 [ 0.399]
 [ 0.199]
 [ 0.44 ]
 [ 0.433]
 [ 0.233]
 [-0.077]] [[1.943]
 [2.922]
 [3.434]
 [2.973]
 [2.613]
 [3.784]
 [6.745]] [[0.315]
 [0.614]
 [0.652]
 [0.659]
 [0.534]
 [0.791]
 [1.571]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.443]
 [0.43 ]
 [0.454]
 [0.43 ]
 [0.43 ]
 [0.45 ]] [[4.932]
 [6.638]
 [4.932]
 [5.937]
 [4.932]
 [4.932]
 [6.238]] [[0.748]
 [1.331]
 [0.748]
 [1.102]
 [0.748]
 [0.748]
 [1.201]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6430989
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 6.597920999001775
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.243]
 [0.24 ]
 [0.231]
 [0.231]
 [0.228]
 [0.224]] [[6.51 ]
 [5.879]
 [6.523]
 [6.587]
 [6.425]
 [6.552]
 [6.565]] [[0.665]
 [0.316]
 [0.701]
 [0.724]
 [0.626]
 [0.697]
 [0.697]]
siam score:  -0.63513684
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.532]
 [0.563]
 [0.547]
 [0.529]
 [0.568]
 [0.509]] [[ 0.633]
 [ 2.069]
 [-2.895]
 [-1.898]
 [-1.262]
 [-3.767]
 [ 1.052]] [[1.14 ]
 [1.544]
 [0.314]
 [0.557]
 [0.709]
 [0.098]
 [1.281]]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]] [[3.645]
 [3.645]
 [3.645]
 [3.645]
 [3.645]
 [3.645]
 [3.645]] [[6.269]
 [6.269]
 [6.269]
 [6.269]
 [6.269]
 [6.269]
 [6.269]]
line 256 mcts: sample exp_bonus 6.070006059130431
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.029]
 [-0.044]
 [-0.045]
 [-0.043]
 [-0.041]
 [-0.044]] [[5.434]
 [3.393]
 [5.812]
 [5.889]
 [5.679]
 [5.482]
 [5.668]] [[ 0.835]
 [-0.333]
 [ 1.049]
 [ 1.093]
 [ 0.972]
 [ 0.861]
 [ 0.966]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6448425
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.406]
 [0.406]
 [0.404]
 [0.406]
 [0.406]
 [0.414]] [[5.635]
 [5.635]
 [5.635]
 [5.417]
 [5.635]
 [5.635]
 [5.481]] [[1.552]
 [1.552]
 [1.552]
 [1.399]
 [1.552]
 [1.552]
 [1.452]]
siam score:  -0.6419108
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.14981136652860338, 0.48481226728746163, 0.11512487991597893, 0.005280008313183015, 0.2396914696415902, 0.005280008313183015]
actor:  1 policy actor:  1  step number:  111 total reward:  0.2099999999999994  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.11053153670812658
from probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]] [[5.104]
 [5.104]
 [5.104]
 [5.104]
 [5.104]
 [5.104]
 [5.104]] [[1.237]
 [1.237]
 [1.237]
 [1.237]
 [1.237]
 [1.237]
 [1.237]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
start point for exploration sampling:  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]] [[-4.034]
 [-3.659]
 [-3.659]
 [-3.659]
 [-3.659]
 [-3.659]
 [-3.659]] [[0.308]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.228]
 [0.22 ]
 [0.217]
 [0.218]
 [0.216]
 [0.22 ]] [[-2.601]
 [-2.097]
 [-2.536]
 [-2.414]
 [-1.943]
 [-2.432]
 [-2.335]] [[0.225]
 [0.228]
 [0.22 ]
 [0.217]
 [0.218]
 [0.216]
 [0.22 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
using another actor
Printing some Q and Qe and total Qs values:  [[ 0.031]
 [-0.017]
 [ 0.138]
 [ 0.138]
 [ 0.138]
 [ 0.138]
 [ 0.041]] [[0.207]
 [0.824]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [0.858]] [[ 0.031]
 [-0.017]
 [ 0.138]
 [ 0.138]
 [ 0.138]
 [ 0.138]
 [ 0.041]]
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.612]
 [0.602]
 [0.597]
 [0.597]
 [0.597]
 [0.597]] [[5.362]
 [5.697]
 [5.365]
 [5.271]
 [5.271]
 [5.271]
 [5.271]] [[1.765]
 [1.866]
 [1.76 ]
 [1.727]
 [1.727]
 [1.727]
 [1.727]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.142]
 [0.132]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[-5.591]
 [-7.573]
 [-7.023]
 [-7.573]
 [-7.573]
 [-7.573]
 [-7.573]] [[0.324]
 [0.142]
 [0.132]
 [0.142]
 [0.142]
 [0.142]
 [0.142]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.531]
 [0.543]
 [0.568]
 [0.535]
 [0.595]
 [0.554]] [[ 1.958]
 [ 0.894]
 [-1.048]
 [ 0.381]
 [ 0.491]
 [-1.088]
 [ 0.884]] [[2.186]
 [1.522]
 [0.899]
 [1.424]
 [1.396]
 [0.989]
 [1.565]]
from probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]] [[-5.097]
 [-8.171]
 [-8.171]
 [-8.171]
 [-8.171]
 [-8.171]
 [-8.171]] [[0.309]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[-2.171]
 [-2.591]
 [-2.591]
 [-2.591]
 [-2.591]
 [-2.591]
 [-2.591]] [[0.552]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]] [[-4.409]
 [-4.107]
 [-4.107]
 [-4.107]
 [-4.107]
 [-4.107]
 [-4.107]] [[0.453]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.838]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[-0.683]
 [ 0.171]
 [-0.683]
 [-0.683]
 [-0.683]
 [-0.683]
 [-0.683]] [[0.671]
 [0.838]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.299]] [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.307]] [[2.172]
 [2.172]
 [2.172]
 [2.172]
 [2.172]
 [2.172]
 [2.514]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [1.042]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[0.267]
 [2.919]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]] [[0.633]
 [2.268]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.220115132500363
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.56 ]
 [0.56 ]
 [0.561]
 [0.56 ]
 [0.56 ]
 [0.554]] [[7.164]
 [7.164]
 [7.164]
 [6.032]
 [7.164]
 [7.164]
 [6.874]] [[1.08 ]
 [1.08 ]
 [1.08 ]
 [0.705]
 [1.08 ]
 [1.08 ]
 [0.971]]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[6.789]
 [6.789]
 [6.789]
 [6.789]
 [6.789]
 [6.789]
 [6.789]] [[0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.504]] [[2.483]
 [2.483]
 [2.483]
 [2.483]
 [2.483]
 [2.483]
 [3.823]] [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [1.669]]
first move QE:  -0.11797562708829189
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.47 ]
 [0.532]
 [0.515]
 [0.515]
 [0.517]
 [0.47 ]] [[ 0.751]
 [ 0.903]
 [-4.192]
 [ 0.   ]
 [ 0.   ]
 [-4.016]
 [ 0.923]] [[1.491]
 [1.658]
 [0.14 ]
 [1.404]
 [1.404]
 [0.187]
 [1.664]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.593]
 [0.589]
 [0.588]
 [0.591]
 [0.585]
 [0.59 ]] [[-7.525]
 [ 0.06 ]
 [-6.622]
 [-6.237]
 [-5.869]
 [-6.252]
 [-5.888]] [[0.146]
 [1.47 ]
 [0.304]
 [0.371]
 [0.435]
 [0.367]
 [0.432]]
first move QE:  -0.1186477101037837
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
line 256 mcts: sample exp_bonus 4.969362752773414
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]] [[6.3]
 [6.3]
 [6.3]
 [6.3]
 [6.3]
 [6.3]
 [6.3]] [[1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.519]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.504]] [[3.904]
 [4.477]
 [3.904]
 [3.904]
 [3.904]
 [3.904]
 [4.715]] [[0.508]
 [0.519]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.504]]
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.847]
 [0.67 ]
 [0.537]
 [0.537]
 [0.537]
 [0.537]] [[-0.01 ]
 [ 0.214]
 [-0.102]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]] [[-0.098]
 [ 0.597]
 [ 0.138]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.548]
 [0.669]
 [0.674]
 [0.638]
 [0.525]
 [0.59 ]] [[1.47 ]
 [3.299]
 [2.088]
 [1.386]
 [2.064]
 [1.593]
 [2.338]] [[0.503]
 [0.548]
 [0.669]
 [0.674]
 [0.638]
 [0.525]
 [0.59 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
UNIT TEST: sample policy line 217 mcts : [0.143 0.204 0.204 0.122 0.122 0.082 0.122]
Sims:  50 1 epoch:  147338 pick best:  False frame count:  147338
siam score:  -0.64595115
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.68 ]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[-1.632]
 [-1.079]
 [-1.632]
 [-1.632]
 [-1.632]
 [-1.632]
 [-1.632]] [[0.496]
 [0.68 ]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[ 2.032]
 [-1.612]
 [-1.612]
 [-1.612]
 [-1.612]
 [-1.612]
 [-1.612]] [[0.636]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[ 1.612]
 [-1.612]
 [-1.612]
 [-1.612]
 [-1.612]
 [-1.612]
 [-1.612]] [[0.639]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]]
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[-1.159]
 [-8.597]
 [-8.597]
 [-8.597]
 [-8.597]
 [-8.597]
 [-8.597]] [[0.528]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.582]
 [0.643]
 [0.631]
 [0.622]
 [0.624]
 [0.684]] [[4.751]
 [3.995]
 [4.788]
 [4.987]
 [4.422]
 [4.324]
 [4.489]] [[1.717]
 [1.057]
 [1.766]
 [1.901]
 [1.45 ]
 [1.376]
 [1.597]]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.586]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.566]] [[4.042]
 [4.576]
 [4.042]
 [4.042]
 [4.042]
 [4.042]
 [4.886]] [[0.558]
 [0.586]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.566]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
2099 3075
first move QE:  -0.12513542929445645
line 256 mcts: sample exp_bonus 5.655177264887579
using another actor
from probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.556]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[5.125]
 [5.248]
 [5.125]
 [5.125]
 [5.125]
 [5.125]
 [5.125]] [[1.291]
 [1.407]
 [1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]]
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.48 ]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[4.643]
 [5.894]
 [4.643]
 [4.643]
 [4.643]
 [4.643]
 [4.643]] [[0.623]
 [1.221]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.418]] [[5.734]
 [5.734]
 [5.734]
 [5.734]
 [5.734]
 [5.734]
 [5.44 ]] [[0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.418]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.371]
 [0.391]
 [0.356]
 [0.315]
 [0.315]
 [0.315]] [[2.161]
 [2.77 ]
 [2.693]
 [2.028]
 [2.161]
 [2.161]
 [2.161]] [[0.315]
 [0.371]
 [0.391]
 [0.356]
 [0.315]
 [0.315]
 [0.315]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7555493650010945
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.474]
 [0.465]
 [0.463]
 [0.457]
 [0.475]
 [0.477]] [[5.049]
 [4.822]
 [4.697]
 [4.871]
 [5.204]
 [4.719]
 [5.176]] [[0.297]
 [0.245]
 [0.185]
 [0.239]
 [0.338]
 [0.213]
 [0.369]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [ 1.462]] [[ 0.61 ]
 [ 0.61 ]
 [ 0.61 ]
 [ 0.61 ]
 [ 0.61 ]
 [ 0.61 ]
 [-0.122]] [[1.191]
 [1.191]
 [1.191]
 [1.191]
 [1.191]
 [1.191]
 [2.899]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.613]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[5.243]
 [6.009]
 [5.656]
 [5.656]
 [5.656]
 [5.656]
 [5.656]] [[0.594]
 [0.613]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]]
siam score:  -0.63873124
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.212]
 [0.435]
 [0.526]
 [0.379]
 [0.483]
 [0.119]] [[-0.737]
 [ 0.201]
 [ 0.568]
 [-1.821]
 [-0.961]
 [-0.516]
 [ 1.055]] [[0.768]
 [1.185]
 [1.553]
 [0.118]
 [0.563]
 [0.906]
 [1.66 ]]
siam score:  -0.63785523
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.47 ]
 [0.428]
 [0.47 ]
 [0.575]
 [0.391]
 [0.45 ]] [[1.714]
 [4.226]
 [1.251]
 [1.705]
 [3.49 ]
 [0.999]
 [4.531]] [[ 0.222]
 [ 1.14 ]
 [ 0.062]
 [ 0.298]
 [ 1.103]
 [-0.097]
 [ 1.201]]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.412]
 [0.443]
 [0.42 ]
 [0.395]
 [0.393]
 [0.395]] [[ 0.   ]
 [-0.295]
 [-1.165]
 [-1.157]
 [-1.501]
 [-1.154]
 [-0.979]] [[0.517]
 [0.412]
 [0.443]
 [0.42 ]
 [0.395]
 [0.393]
 [0.395]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 3.226879677582044
2113 3087
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  -0.12793237869764654
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.2704953703791746
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6446367
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[-3.267]
 [-2.145]
 [-2.145]
 [-2.145]
 [-2.145]
 [-2.145]
 [-2.145]] [[0.745]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
Printing some Q and Qe and total Qs values:  [[0.922]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[-0.717]
 [-1.205]
 [-1.205]
 [-1.205]
 [-1.205]
 [-1.205]
 [-1.205]] [[0.922]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.356]
 [0.37 ]
 [0.374]
 [0.362]
 [0.369]
 [0.363]] [[-2.372]
 [ 0.036]
 [-1.912]
 [-1.99 ]
 [-2.159]
 [-2.058]
 [-2.115]] [[0.401]
 [0.356]
 [0.37 ]
 [0.374]
 [0.362]
 [0.369]
 [0.363]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.413]
 [0.43 ]
 [0.436]
 [0.439]
 [0.41 ]
 [0.431]] [[6.266]
 [5.995]
 [6.147]
 [6.247]
 [6.258]
 [6.269]
 [6.335]] [[1.526]
 [1.328]
 [1.451]
 [1.524]
 [1.535]
 [1.505]
 [1.576]]
2119 3093
from probs:  [0.12975358401005063, 0.41990224580573565, 0.2335980847601174, 0.004573084259982051, 0.20759991690413232, 0.004573084259982051]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.63460684
line 256 mcts: sample exp_bonus 2.6390629114778874
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.494]
 [0.472]
 [0.472]
 [0.492]
 [0.623]
 [0.705]] [[2.377]
 [4.529]
 [3.142]
 [3.05 ]
 [2.906]
 [2.644]
 [2.512]] [[0.505]
 [0.494]
 [0.472]
 [0.472]
 [0.492]
 [0.623]
 [0.705]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.098]
 [0.233]
 [0.248]
 [0.076]
 [0.084]
 [0.101]] [[ 0.193]
 [-0.754]
 [-0.827]
 [ 0.085]
 [ 0.779]
 [ 0.831]
 [ 0.712]] [[0.446]
 [0.098]
 [0.233]
 [0.248]
 [0.076]
 [0.084]
 [0.101]]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.538]
 [0.491]
 [0.486]
 [0.491]
 [0.489]
 [0.493]] [[-2.893]
 [-2.093]
 [-2.507]
 [-2.531]
 [-2.636]
 [-2.396]
 [-2.808]] [[0.481]
 [0.538]
 [0.491]
 [0.486]
 [0.491]
 [0.489]
 [0.493]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.181]
 [0.175]
 [0.182]
 [0.19 ]
 [0.206]
 [0.183]] [[0.369]
 [0.006]
 [0.771]
 [0.388]
 [0.749]
 [1.144]
 [0.461]] [[0.228]
 [0.181]
 [0.175]
 [0.182]
 [0.19 ]
 [0.206]
 [0.183]]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.828]
 [0.572]
 [0.515]
 [0.515]
 [0.582]
 [0.614]] [[-0.407]
 [ 0.253]
 [-0.681]
 [-1.264]
 [-1.264]
 [-0.884]
 [-0.635]] [[0.605]
 [0.828]
 [0.572]
 [0.515]
 [0.515]
 [0.582]
 [0.614]]
actor:  1 policy actor:  1  step number:  106 total reward:  0.24499999999999944  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.662]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]] [[-0.353]
 [-0.28 ]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.353]] [[0.655]
 [0.662]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.434]
 [0.445]] [[4.321]
 [4.321]
 [4.321]
 [4.321]
 [4.321]
 [4.237]
 [4.321]] [[0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.509]
 [0.56 ]]
rdn probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
line 256 mcts: sample exp_bonus 5.333261066697849
UNIT TEST: sample policy line 217 mcts : [0.02  0.776 0.041 0.02  0.02  0.02  0.102]
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.892]
 [0.899]
 [0.902]
 [0.902]
 [0.95 ]
 [0.902]] [[4.332]
 [4.06 ]
 [4.344]
 [4.332]
 [4.332]
 [3.723]
 [4.332]] [[2.682]
 [2.571]
 [2.681]
 [2.682]
 [2.682]
 [2.576]
 [2.682]]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.584]
 [0.546]
 [0.554]
 [0.561]
 [0.577]
 [0.624]] [[-3.492]
 [-3.595]
 [-3.823]
 [-3.897]
 [-3.936]
 [-3.834]
 [-0.936]] [[0.266]
 [0.225]
 [0.152]
 [0.137]
 [0.132]
 [0.164]
 [0.886]]
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.438]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.454]] [[-0.353]
 [ 0.949]
 [-1.148]
 [-1.148]
 [-1.148]
 [-1.148]
 [ 0.888]] [[1.025]
 [1.707]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [1.701]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.705]
 [0.7  ]
 [0.705]
 [0.705]
 [0.753]
 [0.802]] [[4.3  ]
 [4.3  ]
 [4.39 ]
 [4.3  ]
 [4.3  ]
 [5.059]
 [4.754]] [[1.726]
 [1.726]
 [1.744]
 [1.726]
 [1.726]
 [2.074]
 [2.071]]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.721]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[0.583]
 [2.423]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[0.789]
 [1.351]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
line 256 mcts: sample exp_bonus 4.669397571485807
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.4415912670879436
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.025]
 [0.04 ]
 [0.034]
 [0.035]
 [0.041]
 [0.037]] [[4.59 ]
 [3.686]
 [4.156]
 [4.374]
 [4.59 ]
 [4.353]
 [4.44 ]] [[1.036]
 [0.127]
 [0.618]
 [0.822]
 [1.036]
 [0.813]
 [0.893]]
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.123]
 [0.13 ]
 [0.126]
 [0.127]
 [0.13 ]
 [0.138]] [[4.482]
 [4.482]
 [4.562]
 [4.681]
 [4.647]
 [5.027]
 [5.094]] [[-0.659]
 [-0.659]
 [-0.619]
 [-0.588]
 [-0.596]
 [-0.464]
 [-0.426]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
siam score:  -0.6569689
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.764]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[2.209]
 [1.001]
 [2.209]
 [2.209]
 [2.209]
 [2.209]
 [2.209]] [[0.506]
 [0.764]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]]
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[-4.651]
 [-6.509]
 [-6.509]
 [-6.509]
 [-6.509]
 [-6.509]
 [-6.509]] [[0.383]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.558]
 [0.558]
 [0.558]
 [0.56 ]
 [0.56 ]
 [0.563]] [[4.127]
 [4.833]
 [4.288]
 [4.205]
 [4.321]
 [4.423]
 [4.399]] [[0.557]
 [0.558]
 [0.558]
 [0.558]
 [0.56 ]
 [0.56 ]
 [0.563]]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[-3.179]
 [-3.179]
 [-3.179]
 [-3.179]
 [-3.179]
 [-3.179]
 [-3.179]] [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
first move QE:  -0.1339982996509956
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.459]
 [0.493]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[-3.307]
 [-3.307]
 [-3.097]
 [-3.307]
 [-3.307]
 [-3.307]
 [-3.307]] [[0.459]
 [0.459]
 [0.493]
 [0.459]
 [0.459]
 [0.459]
 [0.459]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]] [[1.372]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[0.741]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]] [[-4.555]
 [-6.412]
 [-6.412]
 [-6.412]
 [-6.412]
 [-6.412]
 [-6.412]] [[0.581]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]]
siam score:  -0.64546984
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]] [[-3.178]
 [-6.249]
 [-6.249]
 [-6.249]
 [-6.249]
 [-6.249]
 [-6.249]] [[0.59 ]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.517]
 [0.314]
 [0.35 ]
 [0.296]
 [0.212]
 [0.261]] [[4.194]
 [4.281]
 [4.556]
 [4.821]
 [5.097]
 [4.93 ]
 [3.561]] [[ 0.653]
 [ 0.707]
 [ 0.484]
 [ 0.733]
 [ 0.81 ]
 [ 0.529]
 [-0.284]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.263]
 [0.356]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[-2.532]
 [-2.532]
 [-3.   ]
 [-2.532]
 [-2.532]
 [-2.532]
 [-2.532]] [[0.263]
 [0.263]
 [0.356]
 [0.263]
 [0.263]
 [0.263]
 [0.263]]
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.408]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]] [[-1.628]
 [-0.285]
 [-1.628]
 [-1.628]
 [-1.628]
 [-1.628]
 [-1.628]] [[0.3  ]
 [0.408]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.295]
 [0.281]
 [0.295]
 [0.295]
 [0.295]
 [0.295]] [[-2.225]
 [-2.225]
 [-2.564]
 [-2.225]
 [-2.225]
 [-2.225]
 [-2.225]] [[0.295]
 [0.295]
 [0.281]
 [0.295]
 [0.295]
 [0.295]
 [0.295]]
from probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]] [[-5.728]
 [-5.679]
 [-5.679]
 [-5.679]
 [-5.679]
 [-5.679]
 [-5.679]] [[0.272]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.474]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[1.718]
 [2.628]
 [1.718]
 [1.718]
 [1.718]
 [1.718]
 [1.718]] [[0.382]
 [0.474]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]]
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.76 ]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]] [[4.107]
 [2.1  ]
 [4.107]
 [4.107]
 [4.107]
 [4.107]
 [4.107]] [[1.781]
 [0.953]
 [1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.781]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
first move QE:  -0.13650788702233596
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[-1.328]
 [-5.497]
 [-5.497]
 [-5.497]
 [-5.497]
 [-5.497]
 [-5.497]] [[0.814]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[-3.421]
 [-1.742]
 [-1.742]
 [-1.742]
 [-1.742]
 [-1.742]
 [-1.742]] [[0.019]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[-0.139]
 [-0.139]
 [-0.139]
 [-0.139]
 [-0.139]
 [-0.139]
 [-0.139]] [[2.853]
 [2.853]
 [2.853]
 [2.853]
 [2.853]
 [2.853]
 [2.853]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[-0.136]
 [-0.139]
 [-0.139]
 [-0.139]
 [-0.139]
 [-0.139]
 [-0.139]] [[0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
from probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.551]
 [0.533]
 [0.531]
 [0.526]
 [0.529]
 [0.585]] [[0.88 ]
 [2.522]
 [0.773]
 [0.624]
 [0.698]
 [0.809]
 [1.075]] [[0.656]
 [1.769]
 [0.565]
 [0.461]
 [0.509]
 [0.586]
 [0.812]]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.555]
 [0.519]] [[0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [2.192]
 [0.827]] [[0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [1.781]
 [0.632]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2151 3123
Printing some Q and Qe and total Qs values:  [[1.338]
 [1.338]
 [1.338]
 [1.338]
 [1.338]
 [1.338]
 [1.339]] [[-0.139]
 [-0.139]
 [-0.139]
 [-0.139]
 [-0.139]
 [-0.139]
 [-0.137]] [[2.257]
 [2.257]
 [2.257]
 [2.257]
 [2.257]
 [2.257]
 [2.259]]
siam score:  -0.6494937
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.342]
 [0.35 ]
 [0.352]
 [0.351]
 [0.35 ]
 [0.353]] [[4.281]
 [4.77 ]
 [4.317]
 [4.174]
 [4.314]
 [4.338]
 [3.988]] [[0.357]
 [0.342]
 [0.35 ]
 [0.352]
 [0.351]
 [0.35 ]
 [0.353]]
from probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
from probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
siam score:  -0.65648115
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65612394
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.587]
 [0.6  ]
 [0.604]
 [0.601]
 [0.59 ]
 [0.616]] [[3.597]
 [4.02 ]
 [3.902]
 [3.892]
 [3.941]
 [4.048]
 [3.844]] [[0.602]
 [0.587]
 [0.6  ]
 [0.604]
 [0.601]
 [0.59 ]
 [0.616]]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]] [[-2.068]
 [-2.629]
 [-2.629]
 [-2.629]
 [-2.629]
 [-2.629]
 [-2.629]] [[0.382]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]] [[-8.332]
 [-8.099]
 [-8.099]
 [-8.099]
 [-8.099]
 [-8.099]
 [-8.099]] [[0.041]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.301]
 [0.554]
 [0.301]
 [0.301]
 [0.301]
 [0.301]] [[-2.185]
 [-2.185]
 [-1.103]
 [-2.185]
 [-2.185]
 [-2.185]
 [-2.185]] [[0.301]
 [0.301]
 [0.554]
 [0.301]
 [0.301]
 [0.301]
 [0.301]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.41 ]
 [0.593]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[-2.716]
 [-2.716]
 [-2.129]
 [-2.716]
 [-2.716]
 [-2.716]
 [-2.716]] [[0.41 ]
 [0.41 ]
 [0.593]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]]
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.559]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.562]] [[2.161]
 [3.671]
 [2.161]
 [2.161]
 [2.161]
 [2.161]
 [4.053]] [[1.117]
 [1.654]
 [1.117]
 [1.117]
 [1.117]
 [1.117]
 [1.79 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.628]
 [0.61 ]] [[2.019]
 [2.019]
 [2.019]
 [2.019]
 [2.019]
 [4.562]
 [2.019]] [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [1.513]
 [0.013]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.706]
 [0.713]
 [0.747]
 [0.713]
 [0.723]
 [0.721]] [[3.985]
 [4.135]
 [4.332]
 [4.055]
 [4.202]
 [4.32 ]
 [3.968]] [[1.421]
 [1.447]
 [1.587]
 [1.448]
 [1.501]
 [1.591]
 [1.357]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]] [[3.37]
 [3.37]
 [3.37]
 [3.37]
 [3.37]
 [3.37]
 [3.37]] [[0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.643]
 [0.61 ]
 [0.615]
 [0.58 ]
 [0.616]
 [0.629]] [[4.582]
 [4.428]
 [4.447]
 [4.735]
 [4.755]
 [4.53 ]
 [4.682]] [[1.248]
 [1.166]
 [1.128]
 [1.366]
 [1.325]
 [1.204]
 [1.346]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
using another actor
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.671]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[2.643]
 [3.008]
 [2.643]
 [2.643]
 [2.643]
 [2.643]
 [2.643]] [[0.662]
 [0.671]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]] [[-1.609]
 [-6.027]
 [-6.027]
 [-6.027]
 [-6.027]
 [-6.027]
 [-6.027]] [[0.459]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]] [[-1.332]
 [-5.729]
 [-5.729]
 [-5.729]
 [-5.729]
 [-5.729]
 [-5.729]] [[0.415]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]] [[-5.18 ]
 [-5.357]
 [-5.357]
 [-5.357]
 [-5.357]
 [-5.357]
 [-5.357]] [[0.111]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]] [[-3.609]
 [-6.011]
 [-6.011]
 [-6.011]
 [-6.011]
 [-6.011]
 [-6.011]] [[0.416]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.883]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[1.248]
 [2.98 ]
 [1.248]
 [1.248]
 [1.248]
 [1.248]
 [1.248]] [[0.748]
 [1.666]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]]
siam score:  -0.64229727
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.57 ]
 [0.544]
 [0.555]
 [0.567]
 [0.518]
 [0.534]] [[3.584]
 [3.3  ]
 [3.526]
 [3.607]
 [3.861]
 [3.671]
 [3.15 ]] [[0.232]
 [0.175]
 [0.199]
 [0.248]
 [0.356]
 [0.196]
 [0.053]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.45 ]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.524]] [[-0.041]
 [ 0.728]
 [-0.41 ]
 [-0.41 ]
 [-0.41 ]
 [-0.41 ]
 [ 0.551]] [[1.68 ]
 [1.868]
 [1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.852]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[-5.351]
 [-6.667]
 [-6.667]
 [-6.667]
 [-6.667]
 [-6.667]
 [-6.667]] [[0.216]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]]
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[-8.128]
 [-8.662]
 [-8.662]
 [-8.662]
 [-8.662]
 [-8.662]
 [-8.662]] [[0.149]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.287]
 [0.293]
 [0.291]
 [0.287]
 [0.291]
 [0.293]] [[1.524]
 [1.704]
 [1.667]
 [1.589]
 [1.704]
 [1.6  ]
 [1.733]] [[-0.085]
 [-0.019]
 [-0.019]
 [-0.05 ]
 [-0.019]
 [-0.046]
 [ 0.002]]
line 256 mcts: sample exp_bonus -7.931544796200754
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[-5.01]
 [-6.6 ]
 [-6.6 ]
 [-6.6 ]
 [-6.6 ]
 [-6.6 ]
 [-6.6 ]] [[0.224]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]]
from probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6502087
2172 3150
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.604]
 [0.478]
 [0.604]
 [0.583]
 [0.562]
 [0.609]] [[1.786]
 [5.03 ]
 [2.268]
 [5.991]
 [1.848]
 [2.514]
 [5.558]] [[0.515]
 [0.604]
 [0.478]
 [0.604]
 [0.583]
 [0.562]
 [0.609]]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[4.765]
 [4.765]
 [4.765]
 [4.765]
 [4.765]
 [4.765]
 [4.765]] [[-0.635]
 [-0.635]
 [-0.635]
 [-0.635]
 [-0.635]
 [-0.635]
 [-0.635]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.554]
 [0.615]
 [0.629]
 [0.616]
 [0.598]
 [0.584]] [[-4.615]
 [-0.08 ]
 [-4.171]
 [-4.285]
 [-3.888]
 [-3.844]
 [-0.2  ]] [[1.297]
 [2.115]
 [1.383]
 [1.378]
 [1.44 ]
 [1.428]
 [2.127]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.791]
 [0.805]
 [0.812]
 [0.813]
 [0.809]
 [0.805]] [[3.444]
 [3.69 ]
 [3.895]
 [3.765]
 [3.702]
 [3.921]
 [3.723]] [[1.466]
 [1.596]
 [1.754]
 [1.676]
 [1.634]
 [1.778]
 [1.638]]
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.762]] [[2.941]
 [2.941]
 [2.941]
 [2.941]
 [2.941]
 [2.941]
 [3.486]] [[1.704]
 [1.704]
 [1.704]
 [1.704]
 [1.704]
 [1.704]
 [2.132]]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.732]
 [0.715]
 [0.715]
 [0.715]
 [0.769]
 [0.715]] [[2.51 ]
 [2.752]
 [2.51 ]
 [2.51 ]
 [2.51 ]
 [3.638]
 [2.51 ]] [[1.46 ]
 [1.579]
 [1.46 ]
 [1.46 ]
 [1.46 ]
 [1.994]
 [1.46 ]]
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]] [[-4.74 ]
 [-5.722]
 [-5.722]
 [-5.722]
 [-5.722]
 [-5.722]
 [-5.722]] [[0.183]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.461]
 [0.618]
 [0.618]
 [0.461]
 [0.618]
 [0.618]] [[1.176]
 [2.399]
 [1.176]
 [1.176]
 [0.75 ]
 [1.176]
 [1.176]] [[0.618]
 [0.461]
 [0.618]
 [0.618]
 [0.461]
 [0.618]
 [0.618]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
first move QE:  -0.15130400151461304
start point for exploration sampling:  10749
2181 3157
2182 3159
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[-3.408]
 [-3.388]
 [-3.388]
 [-3.388]
 [-3.388]
 [-3.388]
 [-3.388]] [[0.313]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2377927468131529, 0.36777231310800873, 0.20459740053301606, 0.004005345989765249, 0.18182684756629194, 0.004005345989765249]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.435]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.576]] [[-0.359]
 [-0.206]
 [-1.179]
 [-1.179]
 [-1.179]
 [-1.179]
 [-0.265]] [[0.729]
 [0.435]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.576]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.397738713659035
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]] [[-0.507]
 [-0.596]
 [-0.596]
 [-0.596]
 [-0.596]
 [-0.596]
 [-0.596]] [[0.748]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.709]
 [1.088]
 [0.723]
 [0.677]
 [0.662]
 [0.689]] [[1.274]
 [1.562]
 [1.211]
 [1.02 ]
 [1.17 ]
 [1.604]
 [1.569]] [[1.152]
 [1.076]
 [1.717]
 [0.923]
 [0.88 ]
 [0.996]
 [1.038]]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.618]
 [0.618]
 [0.601]
 [0.618]
 [0.618]
 [0.618]] [[0.188]
 [0.188]
 [0.188]
 [0.031]
 [0.188]
 [0.188]
 [0.188]] [[0.618]
 [0.618]
 [0.618]
 [0.601]
 [0.618]
 [0.618]
 [0.618]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.22499999999999942  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6486563
siam score:  -0.64904207
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21227878530074662, 0.4356071528439517, 0.18264513212829697, 0.003575592581403137, 0.16231774456419845, 0.003575592581403137]
from probs:  [0.21227878530074662, 0.4356071528439517, 0.18264513212829697, 0.003575592581403137, 0.16231774456419845, 0.003575592581403137]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.472]
 [0.571]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[-3.151]
 [-3.151]
 [-2.865]
 [-3.151]
 [-3.151]
 [-3.151]
 [-3.151]] [[0.472]
 [0.472]
 [0.571]
 [0.472]
 [0.472]
 [0.472]
 [0.472]]
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.392]
 [0.428]
 [0.392]
 [0.392]
 [0.392]
 [0.392]] [[-2.669]
 [-2.669]
 [-2.837]
 [-2.669]
 [-2.669]
 [-2.669]
 [-2.669]] [[0.392]
 [0.392]
 [0.428]
 [0.392]
 [0.392]
 [0.392]
 [0.392]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.581]
 [0.546]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[0.404]
 [1.643]
 [0.298]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[0.538]
 [0.581]
 [0.546]
 [0.538]
 [0.538]
 [0.538]
 [0.538]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.52 ]
 [0.457]
 [0.457]
 [0.485]
 [0.466]
 [0.448]] [[4.744]
 [4.014]
 [4.955]
 [4.886]
 [4.92 ]
 [5.39 ]
 [4.82 ]] [[1.307]
 [0.999]
 [1.519]
 [1.475]
 [1.533]
 [1.808]
 [1.422]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.02  0.    0.    0.918 0.02 ]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
from probs:  [0.21227878530074662, 0.4356071528439517, 0.18264513212829697, 0.003575592581403137, 0.16231774456419845, 0.003575592581403137]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21227878530074662, 0.4356071528439517, 0.18264513212829697, 0.003575592581403137, 0.16231774456419845, 0.003575592581403137]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21227878530074662, 0.4356071528439517, 0.18264513212829697, 0.003575592581403137, 0.16231774456419845, 0.003575592581403137]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.033]
 [0.024]
 [0.026]
 [0.033]
 [0.043]
 [0.03 ]] [[ 0.287]
 [-0.159]
 [-0.086]
 [-0.005]
 [ 0.192]
 [ 0.103]
 [-0.031]] [[0.315]
 [0.046]
 [0.078]
 [0.134]
 [0.281]
 [0.241]
 [0.126]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21227878530074662, 0.4356071528439517, 0.18264513212829697, 0.003575592581403137, 0.16231774456419845, 0.003575592581403137]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.658]
 [0.651]] [[4.039]
 [4.039]
 [4.039]
 [4.039]
 [4.039]
 [4.262]
 [4.039]] [[1.854]
 [1.854]
 [1.854]
 [1.854]
 [1.854]
 [1.981]
 [1.854]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.3964830041425036
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21227878530074662, 0.4356071528439517, 0.18264513212829697, 0.003575592581403137, 0.16231774456419845, 0.003575592581403137]
siam score:  -0.6468195
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.21227878530074662, 0.4356071528439517, 0.18264513212829697, 0.003575592581403137, 0.16231774456419845, 0.003575592581403137]
line 256 mcts: sample exp_bonus 5.688954545668755
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]] [[-5.194]
 [-7.638]
 [-7.638]
 [-7.638]
 [-7.638]
 [-7.638]
 [-7.638]] [[0.089]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64766645
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.29 ]
 [0.29 ]
 [0.288]
 [0.29 ]
 [0.29 ]
 [0.293]] [[-0.137]
 [-0.137]
 [-0.137]
 [-0.063]
 [-0.137]
 [-0.137]
 [ 0.188]] [[0.29 ]
 [0.29 ]
 [0.29 ]
 [0.288]
 [0.29 ]
 [0.29 ]
 [0.293]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[-3.417]
 [-3.522]
 [-3.522]
 [-3.522]
 [-3.522]
 [-3.522]
 [-3.522]] [[0.133]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.531]
 [0.56 ]
 [0.588]] [[1.161]
 [0.722]
 [0.722]
 [0.722]
 [0.236]
 [0.722]
 [0.792]] [[2.091]
 [1.877]
 [1.877]
 [1.877]
 [1.658]
 [1.877]
 [1.956]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
actor:  1 policy actor:  1  step number:  90 total reward:  0.04499999999999926  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 3.503588393232979
from probs:  [0.19690488789432234, 0.4764822582710142, 0.16941739710454012, 0.0033166369187551544, 0.15056218289261303, 0.0033166369187551544]
UNIT TEST: sample policy line 217 mcts : [0.122 0.286 0.265 0.041 0.143 0.041 0.102]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19690488789432234, 0.4764822582710142, 0.16941739710454012, 0.0033166369187551544, 0.15056218289261303, 0.0033166369187551544]
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.817]
 [0.422]
 [0.591]
 [0.591]
 [0.358]
 [0.368]] [[-0.49 ]
 [-0.215]
 [-1.41 ]
 [ 0.   ]
 [ 0.   ]
 [-1.412]
 [-1.403]] [[0.768]
 [0.817]
 [0.422]
 [0.591]
 [0.591]
 [0.358]
 [0.368]]
line 256 mcts: sample exp_bonus 0.05534074125495134
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.642896367594338
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.715]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[1.922]
 [2.576]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]] [[1.208]
 [1.451]
 [1.208]
 [1.208]
 [1.208]
 [1.208]
 [1.208]]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.737]
 [0.685]
 [0.726]
 [0.726]
 [0.982]
 [0.8  ]] [[4.636]
 [5.069]
 [3.694]
 [4.636]
 [4.636]
 [2.883]
 [3.968]] [[1.264]
 [1.575]
 [0.556]
 [1.264]
 [1.264]
 [0.608]
 [0.967]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.2286367485929686
actor:  1 policy actor:  1  step number:  77 total reward:  0.3099999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.093]
 [0.381]
 [0.291]
 [0.115]
 [0.264]
 [0.112]] [[-0.487]
 [ 0.032]
 [-0.953]
 [-0.999]
 [-0.113]
 [-0.481]
 [ 0.271]] [[ 0.138]
 [ 0.489]
 [-0.157]
 [-0.367]
 [ 0.357]
 [ 0.188]
 [ 0.8  ]]
2206 3205
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.333]
 [0.423]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[-2.988]
 [-2.988]
 [-3.466]
 [-2.988]
 [-2.988]
 [-2.988]
 [-2.988]] [[0.333]
 [0.333]
 [0.423]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[-1.337]
 [-1.337]
 [-1.337]
 [-1.337]
 [-1.337]
 [-1.337]
 [-1.337]] [[1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
from probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.066]
 [0.073]
 [0.093]
 [0.065]
 [0.103]
 [0.059]] [[1.309]
 [2.33 ]
 [2.065]
 [2.008]
 [0.783]
 [2.485]
 [1.763]] [[-0.548]
 [-0.217]
 [-0.291]
 [-0.27 ]
 [-0.737]
 [-0.092]
 [-0.421]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.23 ]
 [0.293]
 [0.281]
 [0.4  ]
 [0.394]
 [0.387]] [[1.759]
 [2.463]
 [1.967]
 [1.792]
 [1.801]
 [1.209]
 [2.25 ]] [[0.626]
 [1.029]
 [0.8  ]
 [0.691]
 [0.797]
 [0.456]
 [1.041]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.108]
 [0.108]
 [0.09 ]
 [0.108]
 [0.108]
 [0.084]] [[4.027]
 [4.027]
 [4.027]
 [4.33 ]
 [4.027]
 [4.027]
 [4.327]] [[0.599]
 [0.599]
 [0.599]
 [0.86 ]
 [0.599]
 [0.599]
 [0.849]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.484]
 [0.369]
 [0.375]
 [0.37 ]
 [0.337]
 [0.382]] [[-1.948]
 [ 0.297]
 [-2.406]
 [-2.469]
 [-2.63 ]
 [-1.117]
 [-1.356]] [[0.371]
 [0.484]
 [0.369]
 [0.375]
 [0.37 ]
 [0.337]
 [0.382]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.581]
 [0.581]
 [0.558]
 [0.581]
 [0.581]
 [0.562]] [[1.263]
 [1.263]
 [1.263]
 [1.382]
 [1.263]
 [1.263]
 [1.452]] [[0.078]
 [0.078]
 [0.078]
 [0.072]
 [0.078]
 [0.078]
 [0.103]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64832187
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.538]
 [0.529]
 [0.565]
 [0.561]
 [0.507]
 [0.679]] [[4.429]
 [4.974]
 [4.641]
 [4.25 ]
 [4.021]
 [4.44 ]
 [4.435]] [[0.239]
 [0.436]
 [0.306]
 [0.249]
 [0.163]
 [0.195]
 [0.539]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.809]
 [0.639]
 [0.639]
 [0.639]
 [0.632]
 [0.635]] [[2.196]
 [2.055]
 [2.196]
 [2.196]
 [2.196]
 [2.013]
 [2.089]] [[1.794]
 [1.895]
 [1.794]
 [1.794]
 [1.794]
 [1.623]
 [1.694]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[-7.256]
 [-6.995]
 [-6.995]
 [-6.995]
 [-6.995]
 [-6.995]
 [-6.995]] [[0.255]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.119]
 [0.119]
 [0.14 ]
 [0.119]
 [0.119]
 [0.129]] [[3.289]
 [3.469]
 [3.469]
 [3.205]
 [3.469]
 [3.469]
 [3.265]] [[-0.859]
 [-0.807]
 [-0.807]
 [-0.853]
 [-0.807]
 [-0.807]
 [-0.855]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
UNIT TEST: sample policy line 217 mcts : [0.694 0.02  0.041 0.082 0.122 0.02  0.02 ]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.009]
 [ 0.018]
 [-0.012]
 [-0.011]
 [-0.011]
 [-0.006]] [[3.028]
 [2.74 ]
 [2.707]
 [3.013]
 [3.016]
 [2.848]
 [2.775]] [[-1.233]
 [-1.328]
 [-1.284]
 [-1.243]
 [-1.24 ]
 [-1.295]
 [-1.311]]
first move QE:  -0.16314834727178568
using another actor
2220 3225
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.2375690552287715
UNIT TEST: sample policy line 217 mcts : [0.122 0.245 0.163 0.122 0.122 0.102 0.122]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17678399893717658, 0.5299783520504533, 0.152105340146469, 0.0029777236298719548, 0.1351768616061571, 0.0029777236298719548]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]] [[1.071]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[0.096]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.9407129534949985
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.808]] [[0.051]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.175]] [[0.784]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [1.279]]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  112 total reward:  0.12499999999999933  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1643951431056299, 0.49283796922809703, 0.21152499783277717, 0.0027690475676804705, 0.12570379469813478, 0.0027690475676804705]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1643951431056299, 0.49283796922809703, 0.21152499783277717, 0.0027690475676804705, 0.12570379469813478, 0.0027690475676804705]
from probs:  [0.1643951431056299, 0.49283796922809703, 0.21152499783277717, 0.0027690475676804705, 0.12570379469813478, 0.0027690475676804705]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.376]
 [0.385]] [[3.652]
 [3.652]
 [3.652]
 [3.652]
 [3.652]
 [4.684]
 [3.652]] [[0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [1.506]
 [0.685]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.614]
 [0.966]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[-2.875]
 [-2.875]
 [-1.789]
 [-2.875]
 [-2.875]
 [-2.875]
 [-2.875]] [[0.614]
 [0.614]
 [0.966]
 [0.614]
 [0.614]
 [0.614]
 [0.614]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[-1.06]
 [-1.06]
 [-1.06]
 [-1.06]
 [-1.06]
 [-1.06]
 [-1.06]] [[0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1643951431056299, 0.49283796922809703, 0.21152499783277717, 0.0027690475676804705, 0.12570379469813478, 0.0027690475676804705]
siam score:  -0.6528097
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.52 ]
 [0.588]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[1.055]
 [1.055]
 [1.78 ]
 [1.055]
 [1.055]
 [1.055]
 [1.055]] [[1.008]
 [1.008]
 [1.802]
 [1.008]
 [1.008]
 [1.008]
 [1.008]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1643951431056299, 0.49283796922809703, 0.21152499783277717, 0.0027690475676804705, 0.12570379469813478, 0.0027690475676804705]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1643951431056299, 0.49283796922809703, 0.21152499783277717, 0.0027690475676804705, 0.12570379469813478, 0.0027690475676804705]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5205070130607905
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 0.5748623059238192
first move QE:  -0.17023749995806473
line 256 mcts: sample exp_bonus -0.3215612106019241
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[-0.494]
 [-1.632]
 [-1.632]
 [-1.632]
 [-1.632]
 [-1.632]
 [-1.632]] [[0.581]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[-0.624]
 [-0.716]
 [-0.716]
 [-0.716]
 [-0.716]
 [-0.716]
 [-0.716]] [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]] [[-6.62 ]
 [-6.768]
 [-6.768]
 [-6.768]
 [-6.768]
 [-6.768]
 [-6.768]] [[0.182]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]]
siam score:  -0.6528702
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[-3.238]
 [-4.507]
 [-4.507]
 [-4.507]
 [-4.507]
 [-4.507]
 [-4.507]] [[0.275]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]]
siam score:  -0.65130347
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.494]
 [0.494]
 [0.524]
 [0.518]
 [0.5  ]
 [0.494]] [[0.708]
 [3.205]
 [3.205]
 [2.478]
 [2.876]
 [3.788]
 [3.205]] [[-0.179]
 [ 1.307]
 [ 1.307]
 [ 0.917]
 [ 1.15 ]
 [ 1.676]
 [ 1.307]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1643951431056299, 0.49283796922809703, 0.21152499783277717, 0.0027690475676804705, 0.12570379469813478, 0.0027690475676804705]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]] [[-8.019]
 [-6.387]
 [-6.387]
 [-6.387]
 [-6.387]
 [-6.387]
 [-6.387]] [[0.197]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1643951431056299, 0.49283796922809703, 0.21152499783277717, 0.0027690475676804705, 0.12570379469813478, 0.0027690475676804705]
line 256 mcts: sample exp_bonus 2.5688319496430254
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1643951431056299, 0.49283796922809703, 0.21152499783277717, 0.0027690475676804705, 0.12570379469813478, 0.0027690475676804705]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1643951431056299, 0.49283796922809703, 0.21152499783277717, 0.0027690475676804705, 0.12570379469813478, 0.0027690475676804705]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  160 total reward:  0.10499999999999932  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.584]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]] [[-3.194]
 [-1.573]
 [-3.194]
 [-3.194]
 [-3.194]
 [-3.194]
 [-3.194]] [[0.11 ]
 [0.636]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]]
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.568]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]] [[-2.148]
 [-0.963]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.116]
 [1.286]
 [2.54 ]
 [2.54 ]
 [2.54 ]
 [2.54 ]
 [2.54 ]]
first move QE:  -0.17375305785215484
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.414]
 [0.437]
 [0.562]
 [0.44 ]
 [0.441]
 [0.562]] [[-0.975]
 [ 0.266]
 [-1.101]
 [ 2.233]
 [-0.861]
 [-0.789]
 [ 2.233]] [[ 0.263]
 [ 1.202]
 [-0.118]
 [ 3.465]
 [ 0.127]
 [ 0.201]
 [ 3.465]]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.572]
 [0.499]
 [0.499]
 [0.501]
 [0.563]] [[1.764]
 [1.764]
 [1.698]
 [1.764]
 [1.764]
 [1.299]
 [1.979]] [[1.471]
 [1.471]
 [1.534]
 [1.471]
 [1.471]
 [1.112]
 [1.74 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3944366265063357
in main func line 156:  2240
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.428]
 [0.616]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[-2.572]
 [-2.572]
 [-1.278]
 [-2.572]
 [-2.572]
 [-2.572]
 [-2.572]] [[0.428]
 [0.428]
 [0.616]
 [0.428]
 [0.428]
 [0.428]
 [0.428]]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.458]
 [0.465]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[1.737]
 [1.737]
 [1.587]
 [1.737]
 [1.737]
 [1.737]
 [1.737]] [[0.278]
 [0.278]
 [0.142]
 [0.278]
 [0.278]
 [0.278]
 [0.278]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7694078318402138
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.486]
 [0.468]
 [0.605]
 [0.593]
 [0.623]
 [0.792]] [[0.418]
 [0.967]
 [0.359]
 [0.426]
 [0.186]
 [0.281]
 [1.456]] [[0.751]
 [1.112]
 [0.544]
 [0.788]
 [0.556]
 [0.682]
 [1.961]]
from probs:  [0.15398436959284523, 0.5249553964032884, 0.19812959695214194, 0.0025936900326058144, 0.11774325698651278, 0.0025936900326058144]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.135]
 [0.364]
 [0.306]
 [0.305]
 [0.448]
 [0.361]] [[1.148]
 [1.241]
 [1.036]
 [1.339]
 [0.871]
 [1.557]
 [0.62 ]] [[-0.006]
 [-0.309]
 [-0.085]
 [ 0.081]
 [-0.335]
 [ 0.523]
 [-0.459]]
from probs:  [0.15398436959284523, 0.5249553964032884, 0.19812959695214194, 0.0025936900326058144, 0.11774325698651278, 0.0025936900326058144]
from probs:  [0.15398436959284523, 0.5249553964032884, 0.19812959695214194, 0.0025936900326058144, 0.11774325698651278, 0.0025936900326058144]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[-1.571]
 [-2.846]
 [-2.846]
 [-2.846]
 [-2.846]
 [-2.846]
 [-2.846]] [[0.538]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]]
siam score:  -0.6488277
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]] [[-9.056]
 [-8.597]
 [-8.597]
 [-8.597]
 [-8.597]
 [-8.597]
 [-8.597]] [[0.118]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]]
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]] [[-8.963]
 [-8.597]
 [-8.597]
 [-8.597]
 [-8.597]
 [-8.597]
 [-8.597]] [[0.147]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]]
Printing some Q and Qe and total Qs values:  [[0.029]
 [1.485]
 [0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]] [[ 0.007]
 [-0.168]
 [ 0.007]
 [ 0.007]
 [ 0.007]
 [ 0.007]
 [ 0.007]] [[0.464]
 [3.026]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[1.233]
 [1.124]
 [1.124]
 [1.124]
 [1.124]
 [1.124]
 [1.124]] [[0.36 ]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.581]
 [0.604]
 [0.567]
 [0.556]
 [0.569]
 [0.591]] [[ 1.205]
 [-0.344]
 [-2.637]
 [-0.997]
 [-1.032]
 [-3.255]
 [-0.139]] [[1.611]
 [1.082]
 [0.29 ]
 [0.845]
 [0.828]
 [0.055]
 [1.159]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1539843695928452, 0.5249553964032884, 0.1981295969521419, 0.002593690032605814, 0.11774325698651276, 0.002593690032605814]
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[-5.043]
 [-8.002]
 [-8.002]
 [-8.002]
 [-8.002]
 [-8.002]
 [-8.002]] [[0.285]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[1.484]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[-0.119]
 [-0.908]
 [-0.908]
 [-0.908]
 [-0.908]
 [-0.908]
 [-0.908]]
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[-4.09 ]
 [-4.378]
 [-4.378]
 [-4.378]
 [-4.378]
 [-4.378]
 [-4.378]] [[0.493]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.377]
 [0.367]
 [0.361]
 [0.361]
 [0.361]
 [0.361]] [[-3.363]
 [-3.334]
 [-3.635]
 [-3.363]
 [-3.363]
 [-3.363]
 [-3.363]] [[0.361]
 [0.377]
 [0.367]
 [0.361]
 [0.361]
 [0.361]
 [0.361]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1539843695928452, 0.5249553964032884, 0.1981295969521419, 0.002593690032605814, 0.11774325698651276, 0.002593690032605814]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.584]
 [0.583]
 [0.548]
 [0.545]
 [0.606]
 [0.581]] [[2.245]
 [2.618]
 [2.49 ]
 [2.689]
 [2.462]
 [2.873]
 [2.75 ]] [[0.633]
 [0.991]
 [0.88 ]
 [1.006]
 [0.809]
 [1.237]
 [1.101]]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.49 ]
 [0.488]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]] [[-2.874]
 [-2.874]
 [-2.887]
 [-2.874]
 [-2.874]
 [-2.874]
 [-2.874]] [[0.49 ]
 [0.49 ]
 [0.488]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.437]
 [0.563]
 [0.468]
 [0.565]
 [0.494]
 [0.568]] [[0.512]
 [1.08 ]
 [0.385]
 [0.883]
 [0.365]
 [1.032]
 [0.582]] [[0.839]
 [1.207]
 [0.723]
 [1.063]
 [0.707]
 [1.243]
 [0.918]]
siam score:  -0.6563176
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1539843695928452, 0.5249553964032884, 0.1981295969521419, 0.002593690032605814, 0.11774325698651276, 0.002593690032605814]
using explorer policy with actor:  0
from probs:  [0.1539843695928452, 0.5249553964032884, 0.1981295969521419, 0.002593690032605814, 0.11774325698651276, 0.002593690032605814]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.965]] [[-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.175]] [[0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.965]]
siam score:  -0.6567462
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.498]
 [0.548]
 [0.542]
 [0.548]
 [0.548]
 [0.544]] [[3.249]
 [2.664]
 [3.249]
 [2.119]
 [3.249]
 [3.249]
 [3.235]] [[0.548]
 [0.498]
 [0.548]
 [0.542]
 [0.548]
 [0.548]
 [0.544]]
rdn probs:  [0.1539843695928452, 0.5249553964032884, 0.1981295969521419, 0.002593690032605814, 0.11774325698651276, 0.002593690032605814]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1539843695928452, 0.5249553964032884, 0.1981295969521419, 0.002593690032605814, 0.11774325698651276, 0.002593690032605814]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.1797650268679793
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]] [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1539843695928452, 0.5249553964032884, 0.1981295969521419, 0.002593690032605814, 0.11774325698651276, 0.002593690032605814]
line 256 mcts: sample exp_bonus 4.110500232102524
using explorer policy with actor:  1
siam score:  -0.6700672
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.42216309229561244
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15398436959284523, 0.5249553964032884, 0.19812959695214194, 0.0025936900326058144, 0.11774325698651278, 0.0025936900326058144]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
siam score:  -0.66189736
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15398436959284523, 0.5249553964032884, 0.19812959695214194, 0.0025936900326058144, 0.11774325698651278, 0.0025936900326058144]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1539843695928452, 0.5249553964032884, 0.1981295969521419, 0.002593690032605814, 0.11774325698651276, 0.002593690032605814]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.857 0.041 0.02  0.02  0.02 ]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1539843695928452, 0.5249553964032884, 0.1981295969521419, 0.002593690032605814, 0.11774325698651276, 0.002593690032605814]
actor:  1 policy actor:  1  step number:  62 total reward:  0.4349999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 5.42015350902592
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.23562355034804702, 0.47429802441742885, 0.1790104017538845, 0.0023434030145125834, 0.10638121745161436, 0.0023434030145125834]
UNIT TEST: sample policy line 217 mcts : [0.02  0.122 0.184 0.082 0.041 0.327 0.224]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.23562355034804702, 0.47429802441742885, 0.1790104017538845, 0.0023434030145125834, 0.10638121745161436, 0.0023434030145125834]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.5  ]
 [0.492]
 [0.517]
 [0.506]
 [0.499]
 [0.499]] [[4.592]
 [4.548]
 [4.695]
 [3.798]
 [3.814]
 [4.54 ]
 [5.26 ]] [[0.589]
 [0.579]
 [0.612]
 [0.363]
 [0.345]
 [0.574]
 [0.815]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.23562355034804702, 0.47429802441742885, 0.1790104017538845, 0.0023434030145125834, 0.10638121745161436, 0.0023434030145125834]
start point for exploration sampling:  10749
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.212688237975823
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.723]
 [0.707]
 [0.713]
 [0.713]
 [0.768]
 [0.713]] [[4.732]
 [4.159]
 [4.344]
 [4.732]
 [4.732]
 [4.489]
 [4.732]] [[2.085]
 [1.878]
 [1.931]
 [2.085]
 [2.085]
 [2.055]
 [2.085]]
using another actor
line 256 mcts: sample exp_bonus 5.158564634888659
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.23562355034804702, 0.47429802441742885, 0.1790104017538845, 0.0023434030145125834, 0.10638121745161436, 0.0023434030145125834]
line 256 mcts: sample exp_bonus 0.3784903440096047
siam score:  -0.65699285
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.2347667806768245
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.494]
 [0.653]
 [0.622]
 [0.529]
 [0.504]
 [0.509]] [[3.13 ]
 [3.835]
 [3.022]
 [2.903]
 [3.11 ]
 [3.2  ]
 [3.308]] [[0.547]
 [0.779]
 [0.825]
 [0.723]
 [0.606]
 [0.585]
 [0.631]]
siam score:  -0.65521896
first move QE:  -0.1866339433777741
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.23562355034804702, 0.47429802441742885, 0.1790104017538845, 0.0023434030145125834, 0.10638121745161436, 0.0023434030145125834]
actor:  1 policy actor:  1  step number:  88 total reward:  0.2049999999999994  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.472]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[-0.186]
 [ 0.892]
 [-0.186]
 [-0.186]
 [-0.186]
 [-0.186]
 [-0.186]] [[1.737]
 [2.081]
 [1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.964]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[0.121]
 [0.028]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]] [[0.965]
 [1.097]
 [0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.965]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]] [[-0.008]
 [ 0.856]
 [ 0.856]
 [ 0.856]
 [ 0.856]
 [ 0.856]
 [ 0.856]] [[0.031]
 [0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.505]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]] [[1.609]
 [2.968]
 [1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.609]] [[0.483]
 [0.505]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.4440816849588685, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[3.269]
 [3.269]
 [3.269]
 [3.269]
 [3.269]
 [3.269]
 [3.269]] [[1.886]
 [1.886]
 [1.886]
 [1.886]
 [1.886]
 [1.886]
 [1.886]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.65 ]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]] [[2.49 ]
 [3.354]
 [2.49 ]
 [2.49 ]
 [2.49 ]
 [2.49 ]
 [2.49 ]] [[1.709]
 [1.893]
 [1.709]
 [1.709]
 [1.709]
 [1.709]
 [1.709]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.4440816849588685, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.543]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[-0.358]
 [ 0.253]
 [-0.358]
 [-0.358]
 [-0.358]
 [-0.358]
 [-0.358]] [[1.617]
 [1.79 ]
 [1.617]
 [1.617]
 [1.617]
 [1.617]
 [1.617]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[1.592]
 [1.592]
 [1.592]
 [1.592]
 [1.592]
 [1.592]
 [1.592]] [[1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.839]]
line 256 mcts: sample exp_bonus 0.8413083697115643
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.606]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]] [[0.236]
 [1.149]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]] [[-0.197]
 [ 0.725]
 [-0.197]
 [-0.197]
 [-0.197]
 [-0.197]
 [-0.197]]
line 256 mcts: sample exp_bonus 3.316781078076479
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.613]
 [0.517]
 [0.518]
 [0.374]
 [0.484]
 [0.685]] [[0.738]
 [1.977]
 [1.635]
 [1.796]
 [0.581]
 [0.849]
 [4.137]] [[0.325]
 [1.022]
 [0.775]
 [0.85 ]
 [0.154]
 [0.381]
 [2.089]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.794]
 [0.749]
 [0.742]
 [0.743]
 [0.666]
 [0.767]] [[1.897]
 [4.218]
 [1.511]
 [1.591]
 [1.49 ]
 [1.898]
 [1.91 ]] [[0.814]
 [2.023]
 [0.583]
 [0.616]
 [0.565]
 [0.696]
 [0.806]]
from probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]] [[-10.   ]
 [ -9.974]
 [ -9.974]
 [ -9.974]
 [ -9.974]
 [ -9.974]
 [ -9.974]] [[0.126]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]] [[-3.061]
 [-4.557]
 [-4.557]
 [-4.557]
 [-4.557]
 [-4.557]
 [-4.557]] [[0.776]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]] [[ -9.732]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.181]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]] [[-9.08 ]
 [-9.399]
 [-9.399]
 [-9.399]
 [-9.399]
 [-9.399]
 [-9.399]] [[0.072]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]]
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]] [[-1.283]
 [-4.9  ]
 [-4.9  ]
 [-4.9  ]
 [-4.9  ]
 [-4.9  ]
 [-4.9  ]] [[0.271]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]]
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[-3.721]
 [-4.328]
 [-4.328]
 [-4.328]
 [-4.328]
 [-4.328]
 [-4.328]] [[0.486]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]] [[-0.967]
 [-0.967]
 [-0.967]
 [-0.967]
 [-0.967]
 [-0.967]
 [-0.967]] [[0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]] [[ 1.868]
 [-0.724]
 [-0.724]
 [-0.724]
 [-0.724]
 [-0.724]
 [-0.724]] [[0.404]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]]
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]] [[-9.966]
 [-9.297]
 [-9.297]
 [-9.297]
 [-9.297]
 [-9.297]
 [-9.297]] [[0.323]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[-7.959]
 [-8.815]
 [-8.815]
 [-8.815]
 [-8.815]
 [-8.815]
 [-8.815]] [[0.291]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]] [[-0.911]
 [-1.02 ]
 [-1.02 ]
 [-1.02 ]
 [-1.02 ]
 [-1.02 ]
 [-1.02 ]] [[0.134]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]]
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[-9.095]
 [-9.207]
 [-9.207]
 [-9.207]
 [-9.207]
 [-9.207]
 [-9.207]] [[0.4  ]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]]
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[-0.712]
 [-1.362]
 [-1.362]
 [-1.362]
 [-1.362]
 [-1.362]
 [-1.362]] [[0.717]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]]
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]] [[-2.717]
 [-3.75 ]
 [-3.75 ]
 [-3.75 ]
 [-3.75 ]
 [-3.75 ]
 [-3.75 ]] [[0.407]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[-1.15 ]
 [-1.313]
 [-1.313]
 [-1.313]
 [-1.313]
 [-1.313]
 [-1.313]] [[0.77 ]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]]
in main func line 156:  2289
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[-3.467]
 [-3.467]
 [-3.467]
 [-3.467]
 [-3.467]
 [-3.467]
 [-3.467]] [[0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]] [[-0.651]
 [-1.64 ]
 [-1.64 ]
 [-1.64 ]
 [-1.64 ]
 [-1.64 ]
 [-1.64 ]] [[0.372]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]]
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.529]
 [0.516]
 [0.528]
 [0.516]
 [0.516]
 [0.563]] [[3.332]
 [4.391]
 [5.399]
 [4.481]
 [5.399]
 [5.399]
 [4.425]] [[1.031]
 [1.473]
 [1.931]
 [1.514]
 [1.931]
 [1.931]
 [1.521]]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6668718
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[-1.818]
 [-1.469]
 [-1.469]
 [-1.469]
 [-1.469]
 [-1.469]
 [-1.469]] [[0.499]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.429]
 [0.724]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[-3.358]
 [-3.358]
 [-2.585]
 [-3.358]
 [-3.358]
 [-3.358]
 [-3.358]] [[0.429]
 [0.429]
 [0.724]
 [0.429]
 [0.429]
 [0.429]
 [0.429]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.33 ]
 [0.582]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]] [[-3.385]
 [-3.385]
 [-2.77 ]
 [-3.385]
 [-3.385]
 [-3.385]
 [-3.385]] [[0.33 ]
 [0.33 ]
 [0.582]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.352]
 [0.644]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[-2.617]
 [-2.617]
 [-1.854]
 [-2.617]
 [-2.617]
 [-2.617]
 [-2.617]] [[0.352]
 [0.352]
 [0.644]
 [0.352]
 [0.352]
 [0.352]
 [0.352]]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.544]
 [0.53 ]
 [0.53 ]] [[1.093]
 [1.353]
 [1.353]
 [1.353]
 [0.141]
 [1.353]
 [1.353]] [[2.353]
 [2.404]
 [2.404]
 [2.404]
 [1.828]
 [2.404]
 [2.404]]
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]] [[-0.564]
 [-2.568]
 [-2.568]
 [-2.568]
 [-2.568]
 [-2.568]
 [-2.568]] [[0.461]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.627]
 [0.685]
 [0.674]
 [0.674]
 [0.684]
 [0.683]] [[1.179]
 [1.42 ]
 [1.178]
 [1.106]
 [1.139]
 [1.168]
 [1.185]] [[0.742]
 [0.992]
 [0.784]
 [0.667]
 [0.711]
 [0.77 ]
 [0.79 ]]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.587]] [[1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.276]] [[2.1 ]
 [2.1 ]
 [2.1 ]
 [2.1 ]
 [2.1 ]
 [2.1 ]
 [2.08]]
siam score:  -0.6573996
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.696]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]] [[1.519]
 [1.684]
 [1.519]
 [1.519]
 [1.519]
 [1.519]
 [1.519]] [[1.013]
 [1.1  ]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2843200599006257, 0.44408168495886857, 0.16760609731333273, 0.0021941106765111782, 0.09960393647415057, 0.0021941106765111782]
actor:  1 policy actor:  1  step number:  74 total reward:  0.5349999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2576803216985658, 0.4961691753019775, 0.15190202579950166, 0.0019885306199051416, 0.09027141596014467, 0.0019885306199051416]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.341]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[1.436]
 [1.696]
 [1.436]
 [1.436]
 [1.436]
 [1.436]
 [1.436]] [[0.326]
 [0.341]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2576803216985658, 0.4961691753019775, 0.15190202579950166, 0.0019885306199051416, 0.09027141596014467, 0.0019885306199051416]
siam score:  -0.6530876
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  -0.1969520534456947
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2576803216985658, 0.4961691753019775, 0.15190202579950166, 0.0019885306199051416, 0.09027141596014467, 0.0019885306199051416]
first move QE:  -0.19697471195292934
using explorer policy with actor:  1
siam score:  -0.65029246
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.087]
 [0.167]
 [0.162]
 [0.129]
 [0.09 ]
 [0.14 ]] [[4.278]
 [3.179]
 [4.278]
 [4.756]
 [4.457]
 [3.557]
 [4.391]] [[1.339]
 [0.806]
 [1.339]
 [1.538]
 [1.383]
 [0.969]
 [1.364]]
from probs:  [0.2576803216985658, 0.4961691753019775, 0.15190202579950166, 0.0019885306199051416, 0.09027141596014467, 0.0019885306199051416]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2576803216985658, 0.4961691753019775, 0.15190202579950166, 0.0019885306199051416, 0.09027141596014467, 0.0019885306199051416]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]] [[-0.196]
 [-0.196]
 [-0.196]
 [-0.196]
 [-0.196]
 [-0.196]
 [-0.196]] [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
from probs:  [0.2576803216985658, 0.4961691753019775, 0.15190202579950166, 0.0019885306199051416, 0.09027141596014467, 0.0019885306199051416]
siam score:  -0.65171754
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.245]
 [0.247]
 [0.245]
 [0.245]
 [0.245]
 [0.245]] [[-2.527]
 [-2.527]
 [-2.331]
 [-2.527]
 [-2.527]
 [-2.527]
 [-2.527]] [[0.245]
 [0.245]
 [0.247]
 [0.245]
 [0.245]
 [0.245]
 [0.245]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6623464
using explorer policy with actor:  1
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]] [[-7.117]
 [-5.613]
 [-5.613]
 [-5.613]
 [-5.613]
 [-5.613]
 [-5.613]] [[0.177]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]] [[-7.722]
 [-5.464]
 [-5.464]
 [-5.464]
 [-5.464]
 [-5.464]
 [-5.464]] [[0.209]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.586]
 [0.619]
 [0.604]
 [0.61 ]
 [0.619]
 [0.59 ]] [[4.404]
 [4.829]
 [4.404]
 [4.418]
 [2.184]
 [4.404]
 [4.995]] [[1.615]
 [1.874]
 [1.615]
 [1.608]
 [0.074]
 [1.615]
 [1.993]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.457]] [[2.667]
 [2.667]
 [2.667]
 [2.667]
 [2.667]
 [2.667]
 [2.646]] [[1.658]
 [1.658]
 [1.658]
 [1.658]
 [1.658]
 [1.658]
 [1.63 ]]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.406]
 [0.535]
 [0.557]
 [0.386]
 [0.525]
 [0.524]] [[-0.514]
 [-0.173]
 [-3.516]
 [-3.516]
 [-0.566]
 [-3.355]
 [-1.67 ]] [[1.216]
 [1.415]
 [0.105]
 [0.116]
 [1.243]
 [0.166]
 [0.858]]
siam score:  -0.6584838
UNIT TEST: sample policy line 217 mcts : [0.    0.061 0.    0.878 0.02  0.02  0.02 ]
from probs:  [0.2576803216985658, 0.4961691753019775, 0.15190202579950166, 0.0019885306199051416, 0.09027141596014467, 0.0019885306199051416]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2576803216985658, 0.4961691753019775, 0.15190202579950166, 0.0019885306199051416, 0.09027141596014467, 0.0019885306199051416]
first move QE:  -0.19913011930378324
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.588]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.576]] [[-0.382]
 [ 0.396]
 [-0.382]
 [-0.382]
 [-0.382]
 [-0.382]
 [-0.394]] [[2.458]
 [2.806]
 [2.458]
 [2.458]
 [2.458]
 [2.458]
 [2.522]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2576803216985658, 0.4961691753019775, 0.15190202579950166, 0.0019885306199051416, 0.09027141596014467, 0.0019885306199051416]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]] [[-0.925]
 [-4.815]
 [-4.815]
 [-4.815]
 [-4.815]
 [-4.815]
 [-4.815]] [[0.672]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.096]
 [0.133]
 [0.126]
 [0.126]
 [0.127]
 [0.124]] [[-7.694]
 [-0.466]
 [-5.565]
 [-5.544]
 [-5.443]
 [-5.242]
 [-5.367]] [[0.275]
 [0.096]
 [0.133]
 [0.126]
 [0.126]
 [0.127]
 [0.124]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.66963196
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[-2.882]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.454]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]]
actor:  1 policy actor:  1  step number:  80 total reward:  0.26499999999999946  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.443]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[-1.016]
 [ 0.435]
 [-1.016]
 [-1.016]
 [-1.016]
 [-1.016]
 [-1.016]] [[1.774]
 [2.232]
 [1.774]
 [1.774]
 [1.774]
 [1.774]
 [1.774]]
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.722]
 [0.728]
 [0.708]
 [0.728]
 [0.719]
 [0.762]] [[3.689]
 [4.26 ]
 [3.093]
 [3.864]
 [3.093]
 [3.917]
 [3.655]] [[1.545]
 [1.859]
 [1.19 ]
 [1.604]
 [1.19 ]
 [1.653]
 [1.577]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]] [[0.368]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.621]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.118]
 [0.164]
 [0.269]
 [0.197]
 [0.193]
 [0.201]] [[ 0.109]
 [-0.037]
 [ 0.134]
 [ 0.096]
 [-0.062]
 [-0.173]
 [-0.141]] [[ 0.163]
 [-0.137]
 [ 0.069]
 [ 0.254]
 [ 0.005]
 [-0.077]
 [-0.041]]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.576]
 [0.591]
 [0.596]
 [0.596]
 [0.599]
 [0.596]] [[3.916]
 [3.061]
 [2.761]
 [3.916]
 [3.916]
 [3.613]
 [3.685]] [[1.83 ]
 [1.505]
 [1.434]
 [1.83 ]
 [1.83 ]
 [1.735]
 [1.752]]
using another actor
from probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.797]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[2.446]
 [2.586]
 [2.446]
 [2.446]
 [2.446]
 [2.446]
 [2.446]] [[2.069]
 [2.145]
 [2.069]
 [2.069]
 [2.069]
 [2.069]
 [2.069]]
siam score:  -0.670332
using another actor
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[-0.39]
 [-0.39]
 [-0.39]
 [-0.39]
 [-0.39]
 [-0.39]
 [-0.39]] [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
using explorer policy with actor:  1
start point for exploration sampling:  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.547]
 [0.567]
 [0.556]
 [0.552]
 [0.571]
 [0.552]] [[0.713]
 [0.944]
 [0.708]
 [0.778]
 [0.905]
 [0.848]
 [0.905]] [[0.219]
 [0.446]
 [0.25 ]
 [0.298]
 [0.416]
 [0.398]
 [0.416]]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]] [[-5.491]
 [-4.273]
 [-4.273]
 [-4.273]
 [-4.273]
 [-4.273]
 [-4.273]] [[0.103]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.075]
 [0.091]
 [0.092]
 [0.093]
 [0.088]
 [0.092]] [[-5.834]
 [-1.981]
 [-3.321]
 [-3.049]
 [-3.017]
 [-2.808]
 [-3.107]] [[0.5  ]
 [0.075]
 [0.091]
 [0.092]
 [0.093]
 [0.088]
 [0.092]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.075]
 [0.091]
 [0.092]
 [0.093]
 [0.088]
 [0.092]] [[-6.062]
 [-1.977]
 [-3.319]
 [-3.046]
 [-3.015]
 [-2.807]
 [-3.105]] [[0.38 ]
 [0.075]
 [0.091]
 [0.092]
 [0.093]
 [0.088]
 [0.092]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  -0.2072809447269158
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[-0.091]
 [-0.805]
 [-0.805]
 [-0.805]
 [-0.805]
 [-0.805]
 [-0.805]] [[0.567]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.058]
 [0.094]
 [0.093]
 [0.094]
 [0.094]
 [0.116]] [[-4.462]
 [-2.333]
 [-4.532]
 [-4.283]
 [-4.251]
 [-4.052]
 [-4.253]] [[0.592]
 [0.058]
 [0.094]
 [0.093]
 [0.094]
 [0.094]
 [0.116]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]] [[-5.171]
 [-5.035]
 [-5.035]
 [-5.035]
 [-5.035]
 [-5.035]
 [-5.035]] [[0.432]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[-1.154]
 [-1.694]
 [-1.694]
 [-1.694]
 [-1.694]
 [-1.694]
 [-1.694]] [[0.636]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[-1.185]
 [-1.429]
 [-1.429]
 [-1.429]
 [-1.429]
 [-1.429]
 [-1.429]] [[0.534]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.34497308041573793
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]] [[-0.89 ]
 [-3.584]
 [-3.584]
 [-3.584]
 [-3.584]
 [-3.584]
 [-3.584]] [[0.681]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]]
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.036]
 [0.095]
 [0.096]
 [0.096]
 [0.095]
 [0.098]] [[-7.193]
 [-2.393]
 [-5.504]
 [-5.429]
 [-5.333]
 [-5.154]
 [-5.346]] [[0.15 ]
 [0.036]
 [0.095]
 [0.096]
 [0.096]
 [0.095]
 [0.098]]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]] [[ 0.13 ]
 [-2.182]
 [-2.182]
 [-2.182]
 [-2.182]
 [-2.182]
 [-2.182]] [[0.459]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]]
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.28 ]
 [0.344]
 [0.343]
 [0.344]
 [0.343]
 [0.353]] [[-2.377]
 [-1.493]
 [-2.356]
 [-2.242]
 [-2.27 ]
 [-2.272]
 [-2.291]] [[0.757]
 [0.28 ]
 [0.344]
 [0.343]
 [0.344]
 [0.343]
 [0.353]]
siam score:  -0.65950716
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.532]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[2.245]
 [3.087]
 [2.245]
 [2.245]
 [2.245]
 [2.245]
 [2.245]] [[0.501]
 [0.532]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]]
from probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.5656197840020627
siam score:  -0.66415286
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.475]
 [0.54 ]
 [0.54 ]] [[2.074]
 [2.074]
 [2.074]
 [2.074]
 [2.278]
 [2.074]
 [2.074]] [[0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.068]
 [0.062]
 [0.062]]
siam score:  -0.6645674
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.009]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]] [[-4.558]
 [-2.085]
 [-5.65 ]
 [-5.65 ]
 [-5.65 ]
 [-5.65 ]
 [-5.65 ]] [[0.099]
 [0.009]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.21 ]
 [0.229]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]] [[-2.302]
 [-2.302]
 [-2.074]
 [-2.302]
 [-2.302]
 [-2.302]
 [-2.302]] [[0.21 ]
 [0.21 ]
 [0.229]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
using another actor
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]] [[0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]]
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.173]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]] [[-4.664]
 [-1.798]
 [-6.62 ]
 [-6.62 ]
 [-6.62 ]
 [-6.62 ]
 [-6.62 ]] [[0.217]
 [0.173]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]]
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.3  ]
 [0.344]
 [0.324]
 [0.292]
 [0.327]
 [0.286]] [[ 0.02 ]
 [-0.148]
 [-1.896]
 [-1.033]
 [-0.825]
 [-0.682]
 [ 0.08 ]] [[0.22 ]
 [0.3  ]
 [0.344]
 [0.324]
 [0.292]
 [0.327]
 [0.286]]
siam score:  -0.66468763
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -0.7474000616409636
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.622]
 [0.622]
 [0.689]
 [0.622]
 [0.622]
 [0.744]] [[1.9  ]
 [1.9  ]
 [1.9  ]
 [1.702]
 [1.9  ]
 [1.9  ]
 [2.295]] [[1.493]
 [1.493]
 [1.493]
 [1.429]
 [1.493]
 [1.493]
 [2.132]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.287]
 [0.557]
 [0.287]
 [0.287]
 [0.287]
 [0.287]] [[-2.622]
 [-2.622]
 [-2.365]
 [-2.622]
 [-2.622]
 [-2.622]
 [-2.622]] [[0.287]
 [0.287]
 [0.557]
 [0.287]
 [0.287]
 [0.287]
 [0.287]]
first move QE:  -0.21086721177802295
UNIT TEST: sample policy line 217 mcts : [0.286 0.122 0.082 0.041 0.122 0.184 0.163]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.62 ]
 [0.617]] [[4.2  ]
 [4.2  ]
 [4.2  ]
 [4.2  ]
 [4.2  ]
 [4.122]
 [4.2  ]] [[1.895]
 [1.895]
 [1.895]
 [1.895]
 [1.895]
 [1.822]
 [1.895]]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]] [[-4.686]
 [-4.076]
 [-4.076]
 [-4.076]
 [-4.076]
 [-4.076]
 [-4.076]] [[0.378]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[-2.946]
 [-2.946]
 [-2.946]
 [-2.946]
 [-2.946]
 [-2.946]
 [-2.946]] [[0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]] [[1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2345 3415
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]] [[2.076]
 [2.076]
 [2.076]
 [2.076]
 [2.076]
 [2.076]
 [2.076]] [[0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.487]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[-0.235]
 [ 0.64 ]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]] [[1.794]
 [2.042]
 [1.794]
 [1.794]
 [1.794]
 [1.794]
 [1.794]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.637]] [[3.485]
 [3.485]
 [3.485]
 [3.485]
 [3.485]
 [3.485]
 [2.839]] [[1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [0.916]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.662]
 [0.664]] [[0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [2.03 ]
 [0.812]] [[1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.884]
 [1.482]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.549]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[1.626]
 [1.867]
 [1.626]
 [1.626]
 [1.626]
 [1.626]
 [1.626]] [[1.944]
 [2.07 ]
 [1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.698]
 [0.677]
 [0.672]
 [0.666]
 [0.67 ]
 [0.687]] [[2.93 ]
 [3.49 ]
 [3.72 ]
 [3.425]
 [3.544]
 [3.506]
 [3.741]] [[1.395]
 [1.79 ]
 [1.936]
 [1.698]
 [1.783]
 [1.758]
 [1.968]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.205]
 [0.277]
 [0.272]
 [0.272]
 [0.266]
 [0.275]] [[4.245]
 [4.155]
 [4.195]
 [4.119]
 [4.119]
 [4.311]
 [4.404]] [[0.775]
 [0.622]
 [0.744]
 [0.687]
 [0.687]
 [0.807]
 [0.88 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2348 3421
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]] [[-0.445]
 [-4.314]
 [-4.314]
 [-4.314]
 [-4.314]
 [-4.314]
 [-4.314]] [[0.56 ]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]] [[-1.413]
 [-4.854]
 [-4.854]
 [-4.854]
 [-4.854]
 [-4.854]
 [-4.854]] [[0.628]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]]
siam score:  -0.650194
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.055]
 [0.075]
 [0.551]
 [0.551]
 [0.067]
 [0.071]] [[-3.581]
 [-2.046]
 [-8.424]
 [ 0.   ]
 [ 0.   ]
 [-8.582]
 [-8.1  ]] [[0.6  ]
 [0.055]
 [0.075]
 [0.551]
 [0.551]
 [0.067]
 [0.071]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[-3.292]
 [-6.213]
 [-6.213]
 [-6.213]
 [-6.213]
 [-6.213]
 [-6.213]] [[0.566]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.688]
 [0.781]
 [0.781]
 [0.781]
 [0.694]
 [0.781]] [[3.521]
 [5.556]
 [3.521]
 [3.521]
 [3.521]
 [4.558]
 [3.521]] [[0.459]
 [0.954]
 [0.459]
 [0.459]
 [0.459]
 [0.632]
 [0.459]]
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.56 ]
 [0.629]
 [0.629]
 [0.629]
 [0.605]
 [0.629]] [[0.584]
 [0.77 ]
 [0.584]
 [0.584]
 [0.584]
 [0.627]
 [0.584]] [[0.313]
 [0.425]
 [0.313]
 [0.313]
 [0.313]
 [0.323]
 [0.313]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.546]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.675]] [[-1.048]
 [-0.607]
 [-1.048]
 [-1.048]
 [-1.048]
 [-1.048]
 [ 0.24 ]] [[1.784]
 [1.877]
 [1.784]
 [1.784]
 [1.784]
 [1.784]
 [2.133]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.542]
 [0.63 ]
 [0.633]
 [0.603]
 [0.536]
 [0.498]] [[-0.855]
 [ 0.464]
 [-1.13 ]
 [-1.386]
 [-1.483]
 [-1.141]
 [-0.99 ]] [[0.493]
 [0.542]
 [0.63 ]
 [0.633]
 [0.603]
 [0.536]
 [0.498]]
2353 3435
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.53 ]
 [0.52 ]
 [0.527]
 [0.52 ]
 [0.52 ]
 [0.533]] [[5.039]
 [5.069]
 [5.615]
 [4.976]
 [5.615]
 [5.615]
 [4.99 ]] [[1.887]
 [1.781]
 [2.078]
 [1.724]
 [2.078]
 [2.078]
 [1.742]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]] [[-7.216]
 [-8.685]
 [-8.685]
 [-8.685]
 [-8.685]
 [-8.685]
 [-8.685]] [[0.242]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.419]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[-3.246]
 [-1.769]
 [-3.236]
 [-3.236]
 [-3.236]
 [-3.236]
 [-3.236]] [[0.417]
 [0.419]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.549]
 [0.379]
 [0.627]
 [0.228]
 [0.368]
 [0.557]] [[0.478]
 [1.329]
 [0.719]
 [0.975]
 [0.342]
 [0.154]
 [1.257]] [[0.402]
 [1.707]
 [0.827]
 [1.466]
 [0.211]
 [0.234]
 [1.648]]
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]] [[-6.206]
 [-5.041]
 [-5.041]
 [-5.041]
 [-5.041]
 [-5.041]
 [-5.041]] [[0.328]
 [0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
siam score:  -0.6472362
siam score:  -0.6538826
using another actor
from probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
siam score:  -0.6523762
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[-1.535]
 [-2.787]
 [-2.787]
 [-2.787]
 [-2.787]
 [-2.787]
 [-2.787]] [[0.505]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.24226457459226894, 0.46648581229631214, 0.14281447421921994, 0.06169465144330577, 0.0848709208420238, 0.0018695666068694655]
line 256 mcts: sample exp_bonus 2.1820167088119593
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  78 total reward:  0.25499999999999945  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[-1.314]
 [-2.798]
 [-2.798]
 [-2.798]
 [-2.798]
 [-2.798]
 [-2.798]] [[0.544]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
siam score:  -0.6558653
line 256 mcts: sample exp_bonus 0.5728402025083115
siam score:  -0.65528977
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.18 ]
 [0.139]
 [0.151]
 [0.137]
 [0.141]
 [0.144]] [[1.177]
 [0.427]
 [1.057]
 [1.173]
 [1.244]
 [1.14 ]
 [1.21 ]] [[-0.423]
 [-0.642]
 [-0.514]
 [-0.452]
 [-0.456]
 [-0.482]
 [-0.454]]
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.562]
 [0.555]
 [0.52 ]
 [0.493]
 [0.493]
 [0.493]] [[1.06 ]
 [1.034]
 [0.661]
 [0.288]
 [1.06 ]
 [1.06 ]
 [1.06 ]] [[1.759]
 [1.888]
 [1.75 ]
 [1.554]
 [1.759]
 [1.759]
 [1.759]]
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[-1.486]
 [-2.643]
 [-2.643]
 [-2.643]
 [-2.643]
 [-2.643]
 [-2.643]] [[0.725]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.718]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]] [[5.416]
 [4.983]
 [5.416]
 [5.416]
 [5.416]
 [5.416]
 [5.416]] [[0.948]
 [0.835]
 [0.948]
 [0.948]
 [0.948]
 [0.948]
 [0.948]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.561]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[0.689]
 [0.797]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[0.02 ]
 [0.083]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.31 ]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]] [[0.916]
 [1.388]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]] [[-0.422]
 [-0.432]
 [-0.507]
 [-0.507]
 [-0.507]
 [-0.507]
 [-0.507]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
siam score:  -0.66136014
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
using another actor
from probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.536]
 [0.529]
 [0.521]
 [0.526]
 [0.526]
 [0.528]] [[4.166]
 [4.03 ]
 [4.132]
 [4.18 ]
 [4.16 ]
 [4.129]
 [4.294]] [[1.147]
 [1.07 ]
 [1.124]
 [1.14 ]
 [1.137]
 [1.115]
 [1.23 ]]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.501]
 [0.489]
 [0.759]
 [0.759]
 [0.475]
 [0.759]] [[-2.203]
 [-0.636]
 [-2.289]
 [ 0.   ]
 [ 0.   ]
 [-2.363]
 [ 0.   ]] [[0.471]
 [0.501]
 [0.489]
 [0.759]
 [0.759]
 [0.475]
 [0.759]]
siam score:  -0.6591889
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.105]
 [0.373]
 [0.263]
 [0.119]
 [0.287]
 [0.287]] [[ 0.394]
 [ 0.683]
 [ 0.631]
 [-0.585]
 [ 0.53 ]
 [ 0.347]
 [ 0.765]] [[0.137]
 [0.105]
 [0.373]
 [0.263]
 [0.119]
 [0.287]
 [0.287]]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.342]
 [0.342]
 [0.396]
 [0.342]
 [0.342]
 [0.402]] [[1.851]
 [1.851]
 [1.851]
 [1.605]
 [1.851]
 [1.851]
 [2.194]] [[0.342]
 [0.342]
 [0.342]
 [0.396]
 [0.342]
 [0.342]
 [0.402]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.414]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]] [[-3.85 ]
 [-3.494]
 [-3.85 ]
 [-3.85 ]
 [-3.85 ]
 [-3.85 ]
 [-3.85 ]] [[0.393]
 [0.414]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[-3.644]
 [-3.644]
 [-3.644]
 [-3.644]
 [-3.644]
 [-3.644]
 [-3.644]] [[0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
siam score:  -0.6616315
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.409]
 [0.515]
 [0.498]
 [0.517]
 [0.503]
 [0.479]] [[ 0.   ]
 [-3.551]
 [-3.462]
 [-3.466]
 [-3.498]
 [-3.444]
 [-3.656]] [[0.656]
 [0.409]
 [0.515]
 [0.498]
 [0.517]
 [0.503]
 [0.479]]
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]] [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]]
Printing some Q and Qe and total Qs values:  [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[-2.74]
 [-2.74]
 [-2.74]
 [-2.74]
 [-2.74]
 [-2.74]
 [-2.74]] [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.4560032393435836
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.566]] [[-3.066]
 [-3.066]
 [-3.066]
 [-3.066]
 [-3.066]
 [-3.066]
 [-3.275]] [[0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.566]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[-0.718]
 [-3.383]
 [-3.383]
 [-3.383]
 [-3.383]
 [-3.383]
 [-3.383]] [[0.31 ]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.565]
 [0.865]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[-2.806]
 [-2.806]
 [ 0.465]
 [-2.806]
 [-2.806]
 [-2.806]
 [-2.806]] [[0.565]
 [0.565]
 [0.865]
 [0.565]
 [0.565]
 [0.565]
 [0.565]]
siam score:  -0.6580118
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.646]
 [0.619]
 [0.487]
 [0.47 ]
 [0.47 ]
 [0.47 ]] [[-3.467]
 [-3.102]
 [-3.365]
 [-3.33 ]
 [-3.467]
 [-3.467]
 [-3.467]] [[0.47 ]
 [0.646]
 [0.619]
 [0.487]
 [0.47 ]
 [0.47 ]
 [0.47 ]]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.668]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[-2.865]
 [-3.035]
 [-2.865]
 [-2.865]
 [-2.865]
 [-2.865]
 [-2.865]] [[0.522]
 [0.668]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]]
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.567]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]] [[-2.611]
 [-2.544]
 [-2.611]
 [-2.611]
 [-2.611]
 [-2.611]
 [-2.611]] [[0.563]
 [0.567]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]]
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.709]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[-2.801]
 [-2.495]
 [-2.801]
 [-2.801]
 [-2.801]
 [-2.801]
 [-2.801]] [[0.541]
 [0.709]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.398]
 [0.601]
 [0.487]
 [0.446]
 [0.413]
 [0.422]] [[-3.759]
 [-3.694]
 [-3.187]
 [-3.303]
 [-3.733]
 [-3.836]
 [-3.796]] [[0.448]
 [0.398]
 [0.601]
 [0.487]
 [0.446]
 [0.413]
 [0.422]]
line 256 mcts: sample exp_bonus -3.5208666510038378
Printing some Q and Qe and total Qs values:  [[0.7  ]
 [0.695]
 [0.699]
 [0.699]
 [0.697]
 [0.695]
 [0.696]] [[2.927]
 [3.082]
 [3.045]
 [3.011]
 [3.113]
 [3.082]
 [3.113]] [[0.7  ]
 [0.695]
 [0.699]
 [0.699]
 [0.697]
 [0.695]
 [0.696]]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.565]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[-2.917]
 [-2.826]
 [-2.917]
 [-2.917]
 [-2.917]
 [-2.917]
 [-2.917]] [[0.517]
 [0.565]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.792]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[-2.99 ]
 [-2.994]
 [-2.914]
 [-2.99 ]
 [-2.99 ]
 [-2.99 ]
 [-2.99 ]] [[0.506]
 [0.792]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.752]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[-3.371]
 [-3.371]
 [-1.91 ]
 [-3.371]
 [-3.371]
 [-3.371]
 [-3.371]] [[0.508]
 [0.508]
 [0.752]
 [0.508]
 [0.508]
 [0.508]
 [0.508]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.034380085835764
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.555]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.546]] [[-3.141]
 [-2.582]
 [-3.141]
 [-3.141]
 [-3.141]
 [-3.141]
 [-3.13 ]] [[0.533]
 [0.555]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.546]]
using explorer policy with actor:  0
using explorer policy with actor:  0
rdn probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
using another actor
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.418]
 [0.453]
 [0.541]
 [0.364]
 [0.444]
 [0.617]] [[2.186]
 [2.929]
 [2.415]
 [2.675]
 [1.453]
 [1.758]
 [4.768]] [[0.73 ]
 [1.163]
 [0.927]
 [1.159]
 [0.32 ]
 [0.568]
 [2.353]]
from probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]] [[-0.883]
 [-3.572]
 [-3.572]
 [-3.572]
 [-3.572]
 [-3.572]
 [-3.572]] [[0.475]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]]
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.633]
 [0.748]
 [0.767]
 [0.756]
 [0.754]
 [0.767]] [[-0.888]
 [ 0.979]
 [-0.843]
 [-0.855]
 [-0.856]
 [-0.816]
 [-0.724]] [[0.633]
 [0.633]
 [0.748]
 [0.767]
 [0.756]
 [0.754]
 [0.767]]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]] [[-1.944]
 [-3.305]
 [-3.305]
 [-3.305]
 [-3.305]
 [-3.305]
 [-3.305]] [[0.073]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]]
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.291]
 [0.295]
 [0.284]
 [0.283]
 [0.293]
 [0.296]] [[2.978]
 [2.943]
 [2.834]
 [2.814]
 [2.941]
 [3.351]
 [3.148]] [[0.383]
 [0.35 ]
 [0.251]
 [0.207]
 [0.334]
 [0.763]
 [0.567]]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.572]
 [0.51 ]
 [0.624]
 [0.606]
 [0.659]
 [0.658]] [[1.176]
 [1.122]
 [1.474]
 [0.967]
 [1.1  ]
 [0.732]
 [1.53 ]] [[1.553]
 [1.346]
 [1.458]
 [1.348]
 [1.4  ]
 [1.261]
 [1.789]]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.658]
 [0.569]
 [0.569]
 [0.569]
 [0.587]
 [0.569]] [[3.766]
 [2.51 ]
 [3.766]
 [3.766]
 [3.766]
 [2.245]
 [3.766]] [[1.719]
 [1.368]
 [1.719]
 [1.719]
 [1.719]
 [1.198]
 [1.719]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
siam score:  -0.67041886
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2287926030712072, 0.44054523229689596, 0.19048128234013592, 0.05826390392834256, 0.0801513755660612, 0.0017656027973572297]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5649999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.483]] [[2.197]
 [2.197]
 [2.197]
 [2.197]
 [2.197]
 [2.197]
 [2.245]] [[-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.051]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.342]
 [0.641]
 [0.463]
 [0.531]
 [0.464]
 [0.46 ]] [[-0.557]
 [ 0.168]
 [ 0.112]
 [-0.513]
 [-0.596]
 [-0.485]
 [-0.501]] [[0.16 ]
 [0.223]
 [0.802]
 [0.237]
 [0.346]
 [0.249]
 [0.235]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.474]
 [0.454]
 [0.44 ]
 [0.454]
 [0.454]
 [0.576]] [[1.31 ]
 [1.611]
 [1.31 ]
 [1.15 ]
 [1.31 ]
 [1.31 ]
 [1.612]] [[0.934]
 [1.072]
 [0.934]
 [0.851]
 [0.934]
 [0.934]
 [1.278]]
siam score:  -0.65849185
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.403]] [[4.308]
 [4.308]
 [4.308]
 [4.308]
 [4.308]
 [4.308]
 [4.181]] [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.078]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
from probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.421]
 [0.314]
 [0.583]
 [0.509]
 [0.552]
 [0.798]] [[0.939]
 [1.257]
 [0.506]
 [1.03 ]
 [1.501]
 [0.832]
 [2.361]] [[0.387]
 [0.46 ]
 [0.103]
 [0.495]
 [0.614]
 [0.399]
 [1.144]]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]] [[ 0.044]
 [-0.506]
 [-0.506]
 [-0.506]
 [-0.506]
 [-0.506]
 [-0.506]] [[0.576]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.509]
 [0.54 ]
 [0.53 ]
 [0.536]
 [0.52 ]
 [0.537]] [[1.174]
 [1.232]
 [1.242]
 [1.13 ]
 [1.102]
 [1.132]
 [1.207]] [[0.384]
 [0.38 ]
 [0.453]
 [0.321]
 [0.306]
 [0.303]
 [0.412]]
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[-3.501]
 [-3.359]
 [-3.359]
 [-3.359]
 [-3.359]
 [-3.359]
 [-3.359]] [[0.357]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.325]
 [0.51 ]
 [0.325]
 [0.325]
 [0.325]
 [0.325]] [[1.842]
 [1.842]
 [0.672]
 [1.842]
 [1.842]
 [1.842]
 [1.842]] [[0.85]
 [0.85]
 [0.05]
 [0.85]
 [0.85]
 [0.85]
 [0.85]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1920934535930314
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.586]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]] [[2.449]
 [2.866]
 [2.449]
 [2.449]
 [2.449]
 [2.449]
 [2.449]] [[0.6  ]
 [0.586]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
start point for exploration sampling:  10749
from probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[-2.105]
 [-1.937]
 [-1.937]
 [-1.937]
 [-1.937]
 [-1.937]
 [-1.937]] [[0.555]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]] [[-1.865]
 [-4.188]
 [-4.188]
 [-4.188]
 [-4.188]
 [-4.188]
 [-4.188]] [[0.702]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.21058226619100254, 0.4850740310970013, 0.17532026631853107, 0.0536264929970076, 0.07377187058697972, 0.0016250728094777697]
line 256 mcts: sample exp_bonus -0.4375876404054454
Printing some Q and Qe and total Qs values:  [[1.234]
 [1.225]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]] [[1.012]
 [1.111]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]] [[2.739]
 [2.753]
 [2.739]
 [2.739]
 [2.739]
 [2.739]
 [2.739]]
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[-1.492]
 [-2.228]
 [-2.228]
 [-2.228]
 [-2.228]
 [-2.228]
 [-2.228]] [[0.463]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  58 total reward:  0.6249999999999998  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.229]
 [1.229]
 [1.229]
 [1.229]
 [1.229]
 [1.229]
 [1.229]] [[-0.224]
 [-0.218]
 [-0.224]
 [-0.218]
 [-0.224]
 [-0.224]
 [-0.218]] [[2.128]
 [2.136]
 [2.128]
 [2.137]
 [2.128]
 [2.128]
 [2.137]]
using explorer policy with actor:  1
start point for exploration sampling:  10749
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5867],
        [-0.4857],
        [-0.4796],
        [-0.4320],
        [-0.0000],
        [-0.0000],
        [-0.4425],
        [-0.2068],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.0628797758985 -0.6495640682046223
-0.043375785898500004 -0.5290977755705414
-0.024259925299500003 -0.5038940356983146
-0.024259925299500003 -0.4562741296421999
-0.7969499999999999 -0.7969499999999999
-0.945846 -0.945846
-0.0536639152995 -0.4961648713073171
-0.024259925299500003 -0.23110454470890385
-0.9320711845499999 -0.9320711845499999
-0.9467202737024999 -0.9467202737024999
from probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
line 256 mcts: sample exp_bonus 1.7475437209162838
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.07979971155470605
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[-0.225]
 [-0.225]
 [-0.225]
 [-0.225]
 [-0.225]
 [-0.225]
 [-0.225]] [[2.276]
 [2.276]
 [2.276]
 [2.276]
 [2.276]
 [2.276]
 [2.276]]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[1.784]
 [1.784]
 [1.784]
 [1.784]
 [1.784]
 [1.784]
 [1.784]] [[1.971]
 [1.971]
 [1.971]
 [1.971]
 [1.971]
 [1.971]
 [1.971]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.65546423
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.61 ]
 [0.644]
 [0.605]
 [0.602]
 [0.594]
 [0.77 ]] [[-3.798]
 [-2.555]
 [-4.435]
 [-3.981]
 [-3.964]
 [-3.987]
 [ 0.   ]] [[0.589]
 [0.61 ]
 [0.644]
 [0.605]
 [0.602]
 [0.594]
 [0.77 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.701]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]] [[4.136]
 [3.575]
 [4.136]
 [4.136]
 [4.136]
 [4.136]
 [4.136]] [[1.59 ]
 [1.189]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.59 ]]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.65 ]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[-3.288]
 [-0.92 ]
 [-3.288]
 [-3.288]
 [-3.288]
 [-3.288]
 [-3.288]] [[0.089]
 [1.2  ]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.589]
 [0.615]
 [0.395]
 [0.511]
 [0.604]
 [0.539]] [[-0.098]
 [-0.64 ]
 [-1.88 ]
 [-0.127]
 [-0.407]
 [-1.712]
 [-0.61 ]] [[1.089]
 [1.023]
 [0.661]
 [0.807]
 [0.946]
 [0.694]
 [0.933]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.725]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[-1.476]
 [ 0.013]
 [-1.476]
 [-1.476]
 [-1.476]
 [-1.476]
 [-1.476]] [[0.799]
 [1.472]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]]
siam score:  -0.6664121
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6655649
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.535]] [[-0.931]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [ 0.256]] [[2.468]
 [2.701]
 [2.701]
 [2.701]
 [2.701]
 [2.701]
 [2.718]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.407]
 [0.385]
 [0.361]
 [0.361]
 [0.381]
 [0.385]] [[3.79 ]
 [3.073]
 [3.685]
 [3.79 ]
 [3.79 ]
 [3.615]
 [3.795]] [[1.378]
 [0.686]
 [1.304]
 [1.378]
 [1.378]
 [1.223]
 [1.422]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.9717140238131807
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.19393961094322362, 0.4467378501107283, 0.2404960322828031, 0.04938830498982221, 0.06794156098223238, 0.001496640691190339]
actor:  1 policy actor:  1  step number:  118 total reward:  0.2949999999999995  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  10749
line 256 mcts: sample exp_bonus -0.22309416026404588
2419 3528
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1846709176067935, 0.4253875126819056, 0.22900233091351416, 0.09481960311515858, 0.06469452191434936, 0.0014251137682786132]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1846709176067935, 0.4253875126819056, 0.22900233091351416, 0.09481960311515858, 0.06469452191434936, 0.0014251137682786132]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[2.767]
 [2.767]
 [2.767]
 [2.767]
 [2.767]
 [2.767]
 [2.767]] [[1.901]
 [1.901]
 [1.901]
 [1.901]
 [1.901]
 [1.901]
 [1.901]]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.605]
 [0.578]
 [0.55 ]
 [0.576]
 [0.578]
 [0.544]] [[2.692]
 [4.679]
 [3.523]
 [2.873]
 [1.944]
 [3.523]
 [3.547]] [[0.652]
 [1.895]
 [1.173]
 [0.755]
 [0.217]
 [1.173]
 [1.156]]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.605]
 [0.605]
 [0.545]
 [0.605]
 [0.605]
 [0.605]] [[3.265]
 [3.265]
 [3.265]
 [4.197]
 [3.265]
 [3.265]
 [3.265]] [[1.192]
 [1.192]
 [1.192]
 [1.95 ]
 [1.192]
 [1.192]
 [1.192]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  108 total reward:  0.37499999999999956  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  82 total reward:  0.4449999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  64 total reward:  0.5649999999999997  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.967]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.791]] [[0.327]
 [0.685]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.265]] [[2.037]
 [2.455]
 [2.037]
 [2.037]
 [2.037]
 [2.037]
 [2.113]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.61 ]
 [0.674]
 [0.634]
 [0.634]
 [0.69 ]
 [0.634]] [[2.527]
 [2.482]
 [3.013]
 [2.527]
 [2.527]
 [2.472]
 [2.527]] [[0.793]
 [0.731]
 [1.036]
 [0.793]
 [0.793]
 [0.886]
 [0.793]]
Printing some Q and Qe and total Qs values:  [[1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]] [[-0.231]
 [-0.225]
 [-0.231]
 [-0.228]
 [-0.231]
 [-0.231]
 [-0.227]] [[2.907]
 [2.911]
 [2.907]
 [2.909]
 [2.907]
 [2.907]
 [2.909]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.5381065852796382
siam score:  -0.65138507
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -8.344287464737825
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]] [[-0.207]
 [-0.204]
 [-0.204]
 [-0.204]
 [-0.204]
 [-0.204]
 [-0.204]] [[0.688]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.081]
 [0.069]
 [0.065]
 [0.06 ]
 [0.052]
 [0.048]] [[ 0.232]
 [ 0.033]
 [-0.013]
 [ 0.019]
 [ 0.071]
 [ 0.118]
 [ 0.029]] [[ 0.004]
 [-0.031]
 [-0.069]
 [-0.067]
 [-0.058]
 [-0.059]
 [-0.098]]
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[-0.745]
 [-1.968]
 [-1.968]
 [-1.968]
 [-1.968]
 [-1.968]
 [-1.968]] [[0.612]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
from probs:  [0.15607961826047467, 0.41852074473158557, 0.1935475106364302, 0.17596932884927216, 0.0546783240955971, 0.0012044734266403513]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]] [[-1.015]
 [-3.619]
 [-3.619]
 [-3.619]
 [-3.619]
 [-3.619]
 [-3.619]] [[0.619]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[-1.335]
 [-4.62 ]
 [-4.62 ]
 [-4.62 ]
 [-4.62 ]
 [-4.62 ]
 [-4.62 ]] [[0.587]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[-0.203]
 [-0.633]
 [-0.633]
 [-0.633]
 [-0.633]
 [-0.633]
 [-0.633]] [[0.56 ]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]] [[ -5.058]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.414]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]] [[-2.647]
 [-4.693]
 [-4.693]
 [-4.693]
 [-4.693]
 [-4.693]
 [-4.693]] [[0.472]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]]
line 256 mcts: sample exp_bonus -1.251234866417769
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.482]
 [0.482]
 [0.482]
 [0.096]
 [0.482]
 [0.482]] [[-8.99 ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-9.115]
 [ 0.   ]
 [ 0.   ]] [[0.11 ]
 [0.482]
 [0.482]
 [0.482]
 [0.096]
 [0.482]
 [0.482]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[-4.182]
 [-5.763]
 [-5.763]
 [-5.763]
 [-5.763]
 [-5.763]
 [-5.763]] [[0.285]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15607961826047467, 0.41852074473158557, 0.1935475106364302, 0.17596932884927216, 0.0546783240955971, 0.0012044734266403513]
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.083]
 [0.269]
 [0.128]
 [0.269]
 [0.102]
 [0.269]] [[-6.936]
 [-2.061]
 [ 0.   ]
 [-7.659]
 [ 0.   ]
 [-7.935]
 [ 0.   ]] [[0.267]
 [0.083]
 [0.269]
 [0.128]
 [0.269]
 [0.102]
 [0.269]]
Printing some Q and Qe and total Qs values:  [[0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.5 ]] [[0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [1.056]] [[1.982]
 [1.982]
 [1.982]
 [1.982]
 [1.982]
 [1.982]
 [2.28 ]]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]] [[-3.111]
 [-3.694]
 [-3.694]
 [-3.694]
 [-3.694]
 [-3.694]
 [-3.694]] [[0.311]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]] [[-3.723]
 [-5.89 ]
 [-5.89 ]
 [-5.89 ]
 [-5.89 ]
 [-5.89 ]
 [-5.89 ]] [[0.398]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]]
2429 3544
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.014]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]] [[-7.635]
 [-2.   ]
 [-9.532]
 [-9.532]
 [-9.532]
 [-9.532]
 [-9.532]] [[0.211]
 [0.014]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]] [[-4.637]
 [-7.809]
 [-7.809]
 [-7.809]
 [-7.809]
 [-7.809]
 [-7.809]] [[0.292]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.664101
2432 3549
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]] [[ -7.828]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.11 ]
 [0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[-0.003]
 [-0.29 ]
 [-0.29 ]
 [-0.29 ]
 [-0.29 ]
 [-0.29 ]
 [-0.29 ]] [[0.409]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]]
Printing some Q and Qe and total Qs values:  [[0.932]
 [0.934]
 [0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]] [[-0.755]
 [-0.601]
 [-0.755]
 [-0.755]
 [-0.755]
 [-0.755]
 [-0.755]] [[1.384]
 [1.438]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.384]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15607961826047467, 0.41852074473158557, 0.1935475106364302, 0.17596932884927216, 0.0546783240955971, 0.0012044734266403513]
using another actor
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.888]
 [0.889]
 [0.963]
 [0.889]
 [0.889]
 [0.98 ]] [[1.604]
 [1.131]
 [1.604]
 [0.505]
 [1.604]
 [1.604]
 [0.854]] [[0.872]
 [0.711]
 [0.872]
 [0.653]
 [0.872]
 [0.872]
 [0.804]]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.741]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[0.406]
 [1.26 ]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[1.081]
 [1.751]
 [1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]]
siam score:  -0.65191215
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.422]] [[3.856]
 [3.856]
 [3.856]
 [3.856]
 [3.856]
 [3.856]
 [3.789]] [[1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.221]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.295]
 [0.523]
 [0.531]
 [0.316]
 [0.295]
 [0.634]] [[-0.557]
 [ 0.   ]
 [-2.524]
 [-1.331]
 [-0.174]
 [ 0.   ]
 [ 1.629]] [[0.902]
 [1.087]
 [0.038]
 [0.587]
 [1.017]
 [1.087]
 [1.986]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.6199999999999998  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.612]
 [0.968]
 [0.595]
 [0.968]
 [0.562]
 [0.634]] [[ 0.472]
 [ 0.646]
 [ 0.   ]
 [-2.973]
 [ 0.   ]
 [-3.779]
 [ 1.048]] [[1.249]
 [1.495]
 [1.629]
 [0.33 ]
 [1.629]
 [0.042]
 [1.644]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.626]
 [0.61 ]
 [0.595]
 [0.56 ]
 [0.562]
 [0.577]] [[ 0.472]
 [ 0.695]
 [-3.433]
 [-2.973]
 [-1.724]
 [-3.779]
 [-0.055]] [[1.33 ]
 [1.623]
 [0.211]
 [0.351]
 [0.738]
 [0.045]
 [1.32 ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.652548
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.681]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.676]] [[1.005]
 [1.353]
 [1.053]
 [1.053]
 [1.053]
 [1.053]
 [0.893]] [[1.011]
 [1.463]
 [1.066]
 [1.066]
 [1.066]
 [1.066]
 [0.839]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1468000085576415, 0.39363787272733175, 0.24149460602644546, 0.16550718965653252, 0.05142746077041012, 0.0011328622616386297]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1468000085576415, 0.39363787272733175, 0.24149460602644546, 0.16550718965653252, 0.05142746077041012, 0.0011328622616386297]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.368]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.413]] [[1.825]
 [2.563]
 [2.308]
 [2.308]
 [2.308]
 [2.308]
 [2.33 ]] [[0.021]
 [0.764]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.581]]
siam score:  -0.6509795
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1468000085576415, 0.39363787272733175, 0.24149460602644546, 0.16550718965653252, 0.05142746077041012, 0.0011328622616386297]
from probs:  [0.1468000085576415, 0.39363787272733175, 0.24149460602644546, 0.16550718965653252, 0.05142746077041012, 0.0011328622616386297]
actor:  1 policy actor:  1  step number:  88 total reward:  0.4049999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[ 0.82 ]
 [-3.729]
 [-3.729]
 [-3.729]
 [-3.729]
 [-3.729]
 [-3.729]] [[0.132]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1405161808661362, 0.3767880605960334, 0.23115734169241592, 0.20122795753332293, 0.04922608963107524, 0.001084369681016259]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.143]
 [0.13 ]
 [0.13 ]
 [0.13 ]
 [0.13 ]
 [0.13 ]] [[3.619]
 [3.877]
 [3.619]
 [3.619]
 [3.619]
 [3.619]
 [3.619]] [[0.475]
 [0.741]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]]
siam score:  -0.655426
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.521]
 [0.527]] [[3.519]
 [3.519]
 [3.519]
 [3.519]
 [3.519]
 [2.302]
 [3.519]] [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.521]
 [0.527]]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.377]
 [0.372]
 [0.365]
 [0.394]
 [0.375]
 [0.389]] [[2.033]
 [3.85 ]
 [3.154]
 [3.116]
 [2.923]
 [3.063]
 [2.973]] [[0.416]
 [0.377]
 [0.372]
 [0.365]
 [0.394]
 [0.375]
 [0.389]]
siam score:  -0.6562729
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.24451540676185618
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1405161808661362, 0.3767880605960334, 0.23115734169241592, 0.20122795753332293, 0.04922608963107524, 0.001084369681016259]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.618]
 [0.625]
 [0.637]
 [0.632]
 [0.627]
 [0.629]] [[-4.329]
 [-0.203]
 [-4.592]
 [-4.107]
 [-3.933]
 [-4.131]
 [-3.814]] [[0.57 ]
 [1.623]
 [0.464]
 [0.602]
 [0.645]
 [0.588]
 [0.674]]
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]] [[-6.57 ]
 [-3.799]
 [-3.799]
 [-3.799]
 [-3.799]
 [-3.799]
 [-3.799]] [[0.207]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1405161808661362, 0.3767880605960334, 0.23115734169241592, 0.2012279575333229, 0.04922608963107524, 0.001084369681016259]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]] [[0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]] [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]] [[-3.672]
 [-4.278]
 [-4.278]
 [-4.278]
 [-4.278]
 [-4.278]
 [-4.278]] [[0.47 ]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1405161808661362, 0.3767880605960334, 0.23115734169241592, 0.2012279575333229, 0.04922608963107524, 0.001084369681016259]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1405161808661362, 0.3767880605960334, 0.23115734169241592, 0.2012279575333229, 0.04922608963107524, 0.001084369681016259]
siam score:  -0.654947
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.679]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.659]] [[0.016]
 [0.579]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.284]] [[1.979]
 [2.198]
 [1.979]
 [1.979]
 [1.979]
 [1.979]
 [2.088]]
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]] [[-6.721]
 [-4.668]
 [-4.668]
 [-4.668]
 [-4.668]
 [-4.668]
 [-4.668]] [[0.205]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -0.14093824471156435
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.599]
 [0.576]
 [0.563]
 [0.588]
 [0.582]
 [0.57 ]] [[ 0.467]
 [ 0.993]
 [-0.595]
 [-1.007]
 [-0.711]
 [-0.321]
 [ 0.278]] [[0.77 ]
 [0.835]
 [0.26 ]
 [0.095]
 [0.244]
 [0.362]
 [0.538]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1405161808661362, 0.3767880605960334, 0.23115734169241592, 0.20122795753332293, 0.04922608963107524, 0.001084369681016259]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
first move QE:  -0.2510497650140039
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1405161808661362, 0.3767880605960334, 0.23115734169241592, 0.20122795753332293, 0.04922608963107524, 0.001084369681016259]
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.562]
 [0.661]
 [0.646]
 [0.652]
 [0.641]
 [0.67 ]] [[0.747]
 [1.017]
 [0.553]
 [0.763]
 [0.647]
 [0.539]
 [0.599]] [[0.57 ]
 [0.555]
 [0.444]
 [0.554]
 [0.488]
 [0.395]
 [0.493]]
first move QE:  -0.2511651579581959
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.626]
 [1.227]
 [0.626]
 [0.626]
 [0.799]
 [0.727]] [[1.191]
 [1.191]
 [0.175]
 [1.191]
 [1.191]
 [0.305]
 [0.669]] [[0.607]
 [0.607]
 [1.133]
 [0.607]
 [0.607]
 [0.363]
 [0.462]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1405161808661362, 0.3767880605960334, 0.23115734169241592, 0.20122795753332293, 0.04922608963107524, 0.001084369681016259]
actor:  1 policy actor:  1  step number:  56 total reward:  0.6149999999999998  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.25217578122426937
start point for exploration sampling:  10749
actor:  1 policy actor:  1  step number:  42 total reward:  0.7449999999999999  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.25 ]
 [0.231]
 [0.229]
 [0.222]
 [0.224]
 [0.225]] [[3.202]
 [2.947]
 [3.188]
 [3.177]
 [3.05 ]
 [3.127]
 [3.173]] [[0.328]
 [0.119]
 [0.321]
 [0.305]
 [0.166]
 [0.246]
 [0.293]]
line 256 mcts: sample exp_bonus 5.032278740372624
from probs:  [0.12517145205200816, 0.3860240869817615, 0.2647346870133025, 0.17925334635945878, 0.043850473874136826, 0.0009659537193320411]
line 256 mcts: sample exp_bonus 2.0082794804137007
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12517145205200816, 0.3860240869817615, 0.2647346870133025, 0.17925334635945878, 0.043850473874136826, 0.0009659537193320411]
using explorer policy with actor:  0
siam score:  -0.65065926
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.585]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[ 0.079]
 [-0.294]
 [ 0.079]
 [ 0.079]
 [ 0.079]
 [ 0.079]
 [ 0.079]] [[0.514]
 [0.585]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]]
actor:  1 policy actor:  1  step number:  93 total reward:  0.2899999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.472]] [[0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.878]] [[1.122]
 [1.122]
 [1.122]
 [1.122]
 [1.122]
 [1.122]
 [1.807]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646134, 0.0424848457687915, 0.0009358711813104688]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646134, 0.0424848457687915, 0.0009358711813104688]
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]] [[-9.961]
 [-9.62 ]
 [-9.62 ]
 [-9.62 ]
 [-9.62 ]
 [-9.62 ]
 [-9.62 ]] [[0.15 ]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6465029
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]] [[-6.82 ]
 [-9.759]
 [-9.759]
 [-9.759]
 [-9.759]
 [-9.759]
 [-9.759]] [[0.41 ]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]]
siam score:  -0.6444244
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]] [[-7.178]
 [-8.831]
 [-8.831]
 [-8.831]
 [-8.831]
 [-8.831]
 [-8.831]] [[0.16 ]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
UNIT TEST: sample policy line 217 mcts : [0.02  0.857 0.02  0.02  0.02  0.02  0.041]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.4  ]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]] [[4.374]
 [4.795]
 [4.374]
 [4.374]
 [4.374]
 [4.374]
 [4.374]] [[1.043]
 [1.225]
 [1.043]
 [1.043]
 [1.043]
 [1.043]
 [1.043]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]] [[-1.873]
 [-6.509]
 [-6.509]
 [-6.509]
 [-6.509]
 [-6.509]
 [-6.509]] [[0.368]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.753]
 [0.733]
 [0.741]
 [0.741]
 [0.824]
 [0.745]] [[0.578]
 [0.696]
 [0.937]
 [0.578]
 [0.578]
 [1.079]
 [0.713]] [[0.741]
 [0.753]
 [0.733]
 [0.741]
 [0.741]
 [0.824]
 [0.745]]
Printing some Q and Qe and total Qs values:  [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]] [[3.288]
 [3.288]
 [3.288]
 [3.288]
 [3.288]
 [3.288]
 [3.288]] [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]]
first move QE:  -0.26007148545180014
siam score:  -0.65987605
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
2479 3637
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.564]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[4.114]
 [4.777]
 [4.114]
 [4.114]
 [4.114]
 [4.114]
 [4.114]] [[0.786]
 [1.339]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.589]
 [0.615]
 [0.633]
 [0.609]
 [0.593]
 [0.701]] [[2.523]
 [3.789]
 [2.765]
 [2.749]
 [2.54 ]
 [2.425]
 [3.969]] [[0.655]
 [1.25 ]
 [0.784]
 [0.789]
 [0.673]
 [0.608]
 [1.414]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.47 ]
 [0.604]
 [0.627]
 [0.624]
 [0.617]
 [0.626]] [[-4.438]
 [-1.246]
 [-4.152]
 [-3.888]
 [-3.617]
 [-3.71 ]
 [-3.564]] [[0.239]
 [1.365]
 [0.334]
 [0.444]
 [0.544]
 [0.506]
 [0.565]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5644],
        [-0.3268],
        [-0.0000],
        [-0.4596],
        [-0.4398],
        [ 0.4306],
        [-0.4855],
        [-0.5873],
        [ 0.4487],
        [-0.0000]], dtype=torch.float64)
-0.024259925299500003 -0.5886448094605389
-0.024259925299500003 -0.35107308754366534
-0.6017328899999997 -0.6017328899999997
-0.024259925299500003 -0.48387713466323073
-0.0727797758985 -0.5125397941590294
-0.024259925299500003 0.4063403406381349
-0.043375785898500004 -0.5288872971180927
-0.024259925299500003 -0.6115768166308689
-0.024259925299500003 0.42447744182041103
-0.0890999999999993 -0.0890999999999993
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646134, 0.0424848457687915, 0.0009358711813104688]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646134, 0.0424848457687915, 0.0009358711813104688]
line 256 mcts: sample exp_bonus -6.544432313830408
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[-1.3  ]
 [-2.443]
 [-2.443]
 [-2.443]
 [-2.443]
 [-2.443]
 [-2.443]] [[0.75 ]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]]
start point for exploration sampling:  10749
siam score:  -0.64331573
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.0570887078041153
2482 3654
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[ 0.473]
 [-2.151]
 [-2.151]
 [-2.151]
 [-2.151]
 [-2.151]
 [-2.151]] [[0.753]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.636324
line 256 mcts: sample exp_bonus 2.173680744402448
siam score:  -0.6389579
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.502]
 [0.537]
 [0.555]
 [0.559]
 [0.57 ]
 [0.591]] [[-1.996]
 [-0.008]
 [-2.168]
 [-2.275]
 [-2.232]
 [-2.123]
 [-2.428]] [[0.532]
 [0.502]
 [0.537]
 [0.555]
 [0.559]
 [0.57 ]
 [0.591]]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.512]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[-2.161]
 [-0.86 ]
 [-2.161]
 [-2.161]
 [-2.161]
 [-2.161]
 [-2.161]] [[1.493]
 [1.954]
 [1.493]
 [1.493]
 [1.493]
 [1.493]
 [1.493]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.837 0.02  0.    0.    0.    0.102]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.423]
 [0.577]
 [0.437]
 [0.347]
 [0.573]
 [0.419]] [[-0.009]
 [-1.008]
 [-4.079]
 [-3.622]
 [-3.063]
 [-4.226]
 [-1.921]] [[1.635]
 [1.405]
 [0.174]
 [0.297]
 [0.489]
 [0.109]
 [1.013]]
siam score:  -0.6469037
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.501]
 [0.499]
 [0.499]
 [0.499]
 [0.5  ]] [[0.276]
 [0.276]
 [0.214]
 [0.276]
 [0.276]
 [0.273]
 [0.107]] [[0.499]
 [0.499]
 [0.501]
 [0.499]
 [0.499]
 [0.499]
 [0.5  ]]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.753]
 [0.617]
 [0.618]
 [0.618]
 [0.618]
 [0.615]] [[-0.611]
 [ 0.065]
 [-1.751]
 [-0.611]
 [-0.611]
 [-0.611]
 [-0.976]] [[0.618]
 [0.753]
 [0.617]
 [0.618]
 [0.618]
 [0.618]
 [0.615]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.566]
 [0.559]
 [0.559]
 [0.578]
 [0.559]] [[1.749]
 [1.749]
 [2.66 ]
 [1.749]
 [1.749]
 [2.968]
 [1.749]] [[1.332]
 [1.332]
 [1.598]
 [1.332]
 [1.332]
 [1.694]
 [1.332]]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.394]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[4.804]
 [4.708]
 [4.804]
 [4.804]
 [4.804]
 [4.804]
 [4.804]] [[1.32 ]
 [1.223]
 [1.32 ]
 [1.32 ]
 [1.32 ]
 [1.32 ]
 [1.32 ]]
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.676]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[-0.975]
 [ 0.484]
 [-0.975]
 [-0.975]
 [-0.975]
 [-0.975]
 [-0.975]] [[1.297]
 [1.795]
 [1.297]
 [1.297]
 [1.297]
 [1.297]
 [1.297]]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.376]] [[4.586]
 [4.586]
 [4.586]
 [4.586]
 [4.586]
 [4.586]
 [4.827]] [[1.323]
 [1.323]
 [1.323]
 [1.323]
 [1.323]
 [1.323]
 [1.509]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.288]
 [0.26 ]
 [0.273]
 [0.278]
 [0.27 ]
 [0.251]] [[4.917]
 [4.192]
 [4.53 ]
 [4.426]
 [4.341]
 [4.456]
 [4.544]] [[1.327]
 [0.826]
 [1.049]
 [0.988]
 [0.928]
 [1.007]
 [1.045]]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.522]
 [0.602]
 [0.56 ]
 [0.602]
 [0.598]
 [0.743]] [[ 0.26 ]
 [ 0.68 ]
 [-0.932]
 [-2.523]
 [-1.03 ]
 [-2.322]
 [-0.311]] [[0.555]
 [0.522]
 [0.602]
 [0.56 ]
 [0.602]
 [0.598]
 [0.743]]
using another actor
2495 3672
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.31 ]
 [0.342]
 [0.345]
 [0.36 ]
 [0.351]
 [0.359]] [[0.733]
 [0.705]
 [0.462]
 [0.175]
 [0.351]
 [0.475]
 [0.578]] [[0.345]
 [0.31 ]
 [0.342]
 [0.345]
 [0.36 ]
 [0.351]
 [0.359]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.737]] [[1.607]
 [1.607]
 [1.607]
 [1.607]
 [1.607]
 [1.607]
 [2.04 ]] [[1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]
 [2.148]]
siam score:  -0.6430075
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
using explorer policy with actor:  0
UNIT TEST: sample policy line 217 mcts : [0.265 0.082 0.204 0.122 0.041 0.163 0.122]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.473]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.491]] [[2.727]
 [2.159]
 [2.727]
 [2.727]
 [2.727]
 [2.727]
 [2.699]] [[0.581]
 [0.493]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.71 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[-3.877]
 [-8.02 ]
 [-8.02 ]
 [-8.02 ]
 [-8.02 ]
 [-8.02 ]
 [-8.02 ]] [[0.447]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]]
first move QE:  -0.27967393670211005
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.586]
 [0.554]
 [0.551]
 [0.534]
 [0.919]
 [0.569]] [[-3.13 ]
 [-2.655]
 [-3.196]
 [-3.132]
 [-3.452]
 [ 0.   ]
 [-3.181]] [[0.48 ]
 [0.68 ]
 [0.436]
 [0.452]
 [0.311]
 [2.234]
 [0.471]]
siam score:  -0.6408162
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]] [[-9.946]
 [-9.827]
 [-9.827]
 [-9.827]
 [-9.827]
 [-9.827]
 [-9.827]] [[0.085]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
line 256 mcts: sample exp_bonus -9.544138546797843
Printing some Q and Qe and total Qs values:  [[-0.017]
 [ 0.12 ]
 [ 0.118]
 [ 0.123]
 [ 0.12 ]
 [ 0.216]
 [ 0.146]] [[-0.804]
 [-0.485]
 [-1.329]
 [-2.878]
 [-0.485]
 [-2.072]
 [-0.058]] [[-0.017]
 [ 0.12 ]
 [ 0.118]
 [ 0.123]
 [ 0.12 ]
 [ 0.216]
 [ 0.146]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646134, 0.0424848457687915, 0.0009358711813104688]
siam score:  -0.63691366
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646134, 0.0424848457687915, 0.0009358711813104688]
line 256 mcts: sample exp_bonus -2.3690531697809574
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.467]
 [0.71 ]
 [0.443]
 [0.421]
 [0.421]
 [0.421]] [[-3.678]
 [-2.962]
 [-4.016]
 [-3.78 ]
 [-4.056]
 [-4.056]
 [-4.056]] [[0.442]
 [0.467]
 [0.71 ]
 [0.443]
 [0.421]
 [0.421]
 [0.421]]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.618]
 [0.603]
 [0.603]
 [0.603]
 [0.619]
 [0.715]] [[1.696]
 [1.446]
 [1.696]
 [1.696]
 [1.696]
 [1.438]
 [1.268]] [[1.816]
 [1.596]
 [1.816]
 [1.816]
 [1.816]
 [1.59 ]
 [1.613]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646134, 0.0424848457687915, 0.0009358711813104688]
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]] [[-7.231]
 [-9.751]
 [-9.751]
 [-9.751]
 [-9.751]
 [-9.751]
 [-9.751]] [[0.328]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.479]
 [0.466]
 [0.479]
 [0.479]
 [0.456]
 [0.461]] [[-0.067]
 [ 0.   ]
 [ 0.18 ]
 [ 0.   ]
 [ 0.   ]
 [ 0.159]
 [ 0.203]] [[0.47 ]
 [0.479]
 [0.466]
 [0.479]
 [0.479]
 [0.456]
 [0.461]]
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.076]
 [0.1  ]
 [0.094]
 [0.092]
 [0.091]
 [0.092]] [[-10.   ]
 [ -4.041]
 [ -9.986]
 [ -9.63 ]
 [ -9.696]
 [ -9.802]
 [ -9.855]] [[0.201]
 [0.076]
 [0.1  ]
 [0.094]
 [0.092]
 [0.091]
 [0.092]]
using explorer policy with actor:  0
using explorer policy with actor:  0
siam score:  -0.63305676
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646134, 0.0424848457687915, 0.0009358711813104688]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.773]] [[0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.66 ]] [[1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.61 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12127325808038208, 0.40514503799427265, 0.25649009813878193, 0.17367088883646137, 0.0424848457687915, 0.0009358711813104688]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.547]
 [0.566]
 [0.564]
 [0.064]
 [0.535]
 [0.552]] [[-0.668]
 [-1.4  ]
 [-3.899]
 [-3.216]
 [-0.389]
 [-4.048]
 [-2.036]] [[1.512]
 [1.301]
 [0.174]
 [0.484]
 [1.541]
 [0.092]
 [1.014]]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.598]] [[1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.209]] [[1.869]
 [1.869]
 [1.869]
 [1.869]
 [1.869]
 [1.869]
 [2.089]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5699999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]] [[-4.554]
 [-8.325]
 [-8.325]
 [-8.325]
 [-8.325]
 [-8.325]
 [-8.325]] [[0.291]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.11592553208941725, 0.38727956060801866, 0.24517978302106044, 0.21010910889261888, 0.040611412849315486, 0.0008946025395694206]
actor:  1 policy actor:  1  step number:  94 total reward:  0.37499999999999956  reward:  1.0 rdn_beta:  0.333
from probs:  [0.1121329448955168, 0.40732515065186387, 0.2371585500059579, 0.2032352382159388, 0.03928278125692637, 0.0008653349737963796]
Starting evaluation
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -0.4471511813698323
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.403]
 [0.373]
 [0.352]
 [0.372]
 [0.376]
 [0.411]] [[-4.025]
 [-3.889]
 [-4.232]
 [ 0.   ]
 [-3.857]
 [-3.606]
 [-3.718]] [[0.401]
 [0.403]
 [0.373]
 [0.352]
 [0.372]
 [0.376]
 [0.411]]
using explorer policy with actor:  0
siam score:  -0.63494563
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.568]
 [0.592]
 [0.563]
 [0.563]
 [0.584]
 [0.563]] [[-1.594]
 [-0.941]
 [-0.943]
 [-1.594]
 [-1.594]
 [ 0.005]
 [-1.594]] [[0.563]
 [0.568]
 [0.592]
 [0.563]
 [0.563]
 [0.584]
 [0.563]]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.533]
 [0.535]
 [0.578]
 [0.509]
 [0.509]
 [0.535]] [[-2.354]
 [-0.447]
 [ 0.016]
 [-1.135]
 [ 0.   ]
 [-0.666]
 [-1.415]] [[0.509]
 [0.533]
 [0.535]
 [0.578]
 [0.509]
 [0.509]
 [0.535]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.167088018107386
from probs:  [0.1121329448955168, 0.4073251506518638, 0.2371585500059579, 0.2032352382159388, 0.03928278125692637, 0.0008653349737963796]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.041]
 [0.038]
 [0.098]
 [0.097]
 [0.093]
 [0.095]] [[-6.848]
 [-0.171]
 [-2.147]
 [-4.585]
 [-4.927]
 [-5.107]
 [-4.548]] [[0.103]
 [0.041]
 [0.038]
 [0.098]
 [0.097]
 [0.093]
 [0.095]]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[-2.532]
 [-2.532]
 [-2.532]
 [-2.532]
 [-2.532]
 [-2.532]
 [-2.532]] [[0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]]
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.887]
 [0.748]
 [0.733]
 [0.733]
 [0.748]
 [0.831]] [[-0.291]
 [-0.436]
 [-0.049]
 [-0.291]
 [-0.291]
 [-0.477]
 [-0.406]] [[0.733]
 [0.887]
 [0.748]
 [0.733]
 [0.733]
 [0.748]
 [0.831]]
first move QE:  -0.28911165638245256
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.564]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.554]] [[0.395]
 [0.179]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.304]] [[0.569]
 [0.564]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.554]]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.401]
 [0.393]
 [0.401]
 [0.401]
 [0.387]
 [0.393]] [[3.129]
 [3.129]
 [2.854]
 [3.129]
 [3.129]
 [2.063]
 [2.068]] [[0.401]
 [0.401]
 [0.393]
 [0.401]
 [0.401]
 [0.387]
 [0.393]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1121329448955168, 0.4073251506518638, 0.2371585500059579, 0.2032352382159388, 0.03928278125692637, 0.0008653349737963796]
line 256 mcts: sample exp_bonus 0.8845200465342776
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.574]
 [0.604]
 [0.597]
 [0.595]
 [0.624]
 [0.619]] [[-1.396]
 [-0.214]
 [-1.365]
 [-1.413]
 [-1.402]
 [-1.064]
 [-0.976]] [[0.593]
 [0.574]
 [0.604]
 [0.597]
 [0.595]
 [0.624]
 [0.619]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.670568743436201
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1121329448955168, 0.4073251506518638, 0.2371585500059579, 0.2032352382159388, 0.03928278125692637, 0.0008653349737963796]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1121329448955168, 0.4073251506518638, 0.2371585500059579, 0.2032352382159388, 0.03928278125692637, 0.0008653349737963796]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.588]
 [0.586]
 [0.585]
 [0.586]
 [0.586]
 [0.587]] [[-1.936]
 [-0.542]
 [-1.742]
 [-1.998]
 [-2.022]
 [-1.853]
 [-1.767]] [[0.582]
 [0.588]
 [0.586]
 [0.585]
 [0.586]
 [0.586]
 [0.587]]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.316]
 [0.672]
 [0.602]
 [0.515]
 [0.642]
 [0.602]] [[ 1.027]
 [ 0.885]
 [-3.304]
 [-2.34 ]
 [-0.212]
 [-3.436]
 [ 0.019]] [[1.616]
 [1.304]
 [0.24 ]
 [0.492]
 [1.09 ]
 [0.181]
 [1.214]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn probs:  [0.1121329448955168, 0.4073251506518638, 0.2371585500059579, 0.2032352382159388, 0.03928278125692637, 0.0008653349737963796]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.5471725355330388
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64800644
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.11213294489551677, 0.4073251506518637, 0.23715855000595784, 0.2032352382159388, 0.03928278125692636, 0.0008653349737963794]
siam score:  -0.64490247
line 256 mcts: sample exp_bonus -2.17379916294683
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.618]
 [0.578]
 [0.585]
 [0.573]
 [0.631]
 [0.613]] [[-5.046]
 [-4.713]
 [-5.085]
 [-5.2  ]
 [-5.125]
 [-3.934]
 [-4.976]] [[0.182]
 [0.336]
 [0.164]
 [0.126]
 [0.145]
 [0.64 ]
 [0.233]]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]] [[-10.]
 [-10.]
 [-10.]
 [-10.]
 [-10.]
 [-10.]
 [-10.]] [[0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [1.485]
 [0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[-0.295]
 [-0.286]
 [-0.295]
 [-0.295]
 [-0.295]
 [-0.295]
 [-0.295]] [[0.018]
 [2.987]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]]
siam score:  -0.6442072
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10749
siam score:  -0.64537466
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.795]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]] [[-1.299]
 [-0.947]
 [-1.299]
 [-1.299]
 [-1.299]
 [-1.299]
 [-1.299]] [[1.381]
 [1.5  ]
 [1.381]
 [1.381]
 [1.381]
 [1.381]
 [1.381]]
siam score:  -0.6457432
2530 3737
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.568]
 [0.593]
 [0.604]
 [0.339]
 [0.581]
 [0.572]] [[-2.222]
 [-0.612]
 [-5.085]
 [-5.586]
 [-0.663]
 [-5.06 ]
 [-3.528]] [[1.315]
 [1.871]
 [0.439]
 [0.284]
 [1.706]
 [0.439]
 [0.929]]
using explorer policy with actor:  1
from probs:  [0.1121329448955168, 0.4073251506518638, 0.2371585500059579, 0.2032352382159388, 0.03928278125692637, 0.0008653349737963796]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]] [[-1.512]
 [-8.541]
 [-8.541]
 [-8.541]
 [-8.541]
 [-8.541]
 [-8.541]] [[0.418]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]]
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]] [[-5.213]
 [-9.758]
 [-9.758]
 [-9.758]
 [-9.758]
 [-9.758]
 [-9.758]] [[0.236]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1121329448955168, 0.4073251506518638, 0.2371585500059579, 0.2032352382159388, 0.03928278125692637, 0.0008653349737963796]
siam score:  -0.63975143
siam score:  -0.63898367
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  44 total reward:  0.6649999999999998  reward:  1.0 rdn_beta:  0.167
in main func line 156:  2535
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[ 0.47 ]
 [-0.319]
 [-0.319]
 [-0.319]
 [-0.319]
 [-0.319]
 [-0.319]] [[0.237]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]]
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]] [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]] [[0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]]
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.012]
 [0.029]
 [0.03 ]
 [0.032]
 [0.03 ]
 [0.031]] [[-9.248]
 [-2.44 ]
 [-8.408]
 [-8.289]
 [-8.586]
 [-8.459]
 [-8.391]] [[0.136]
 [0.012]
 [0.029]
 [0.03 ]
 [0.032]
 [0.03 ]
 [0.031]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -2.579859847146731
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.55 ]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.759]] [[0.442]
 [0.589]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.844]] [[0.994]
 [0.95 ]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [1.476]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15277621993181395, 0.3886793094395253, 0.226302307374039, 0.19393187952457966, 0.037484560595807154, 0.0008257231342351128]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  -0.30096465130317496
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.188]
 [0.202]
 [0.208]
 [0.208]
 [0.193]
 [0.208]] [[-4.929]
 [-4.452]
 [-5.34 ]
 [ 0.   ]
 [ 0.   ]
 [-5.211]
 [ 0.   ]] [[0.188]
 [0.188]
 [0.202]
 [0.208]
 [0.208]
 [0.193]
 [0.208]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.251]
 [0.317]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[-3.43 ]
 [-3.43 ]
 [-3.505]
 [-3.43 ]
 [-3.43 ]
 [-3.43 ]
 [-3.43 ]] [[0.251]
 [0.251]
 [0.317]
 [0.251]
 [0.251]
 [0.251]
 [0.251]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15277621993181395, 0.3886793094395253, 0.226302307374039, 0.19393187952457966, 0.037484560595807154, 0.0008257231342351128]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15277621993181395, 0.3886793094395253, 0.226302307374039, 0.19393187952457966, 0.037484560595807154, 0.0008257231342351128]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15277621993181395, 0.3886793094395253, 0.226302307374039, 0.19393187952457966, 0.037484560595807154, 0.0008257231342351128]
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.05 ]
 [-0.045]
 [-0.057]
 [-0.048]
 [-0.045]
 [-0.05 ]] [[3.186]
 [3.737]
 [3.186]
 [2.045]
 [1.939]
 [3.186]
 [3.796]] [[-0.783]
 [-0.608]
 [-0.783]
 [-1.189]
 [-1.205]
 [-0.783]
 [-0.589]]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.673]
 [0.699]
 [0.71 ]
 [0.699]
 [0.699]
 [0.653]] [[0.87 ]
 [0.992]
 [0.87 ]
 [0.631]
 [0.87 ]
 [0.87 ]
 [0.994]] [[1.597]
 [1.627]
 [1.597]
 [1.461]
 [1.597]
 [1.597]
 [1.587]]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.018]
 [0.023]
 [0.02 ]
 [0.018]
 [0.016]
 [0.016]] [[-0.277]
 [-0.24 ]
 [-0.291]
 [-0.231]
 [-0.296]
 [-0.27 ]
 [-0.192]] [[1.652]
 [1.66 ]
 [1.645]
 [1.663]
 [1.642]
 [1.649]
 [1.674]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15277621993181395, 0.3886793094395253, 0.226302307374039, 0.19393187952457966, 0.037484560595807154, 0.0008257231342351128]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15277621993181395, 0.3886793094395253, 0.226302307374039, 0.19393187952457966, 0.037484560595807154, 0.0008257231342351128]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.697]
 [0.667]
 [0.683]
 [0.682]
 [0.634]
 [0.733]] [[2.671]
 [3.589]
 [2.71 ]
 [3.256]
 [2.691]
 [1.789]
 [2.854]] [[1.358]
 [2.042]
 [1.334]
 [1.77 ]
 [1.342]
 [0.591]
 [1.542]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15277621993181395, 0.3886793094395253, 0.226302307374039, 0.19393187952457966, 0.037484560595807154, 0.0008257231342351128]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]] [[2.586]
 [2.586]
 [2.586]
 [2.586]
 [2.586]
 [2.586]
 [2.586]] [[1.9]
 [1.9]
 [1.9]
 [1.9]
 [1.9]
 [1.9]
 [1.9]]
siam score:  -0.6430101
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  73 total reward:  0.34999999999999953  reward:  1.0 rdn_beta:  0.167
from probs:  [0.1775270222145624, 0.3773244289868559, 0.2196911099575857, 0.18826635204605638, 0.036389486342948175, 0.0008016004519915858]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1775270222145624, 0.3773244289868559, 0.2196911099575857, 0.18826635204605638, 0.036389486342948175, 0.0008016004519915858]
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.345]
 [0.405]] [[ 0.049]
 [ 0.049]
 [ 0.049]
 [ 0.049]
 [ 0.049]
 [-1.321]
 [ 0.049]] [[1.825]
 [1.825]
 [1.825]
 [1.825]
 [1.825]
 [0.82 ]
 [1.825]]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]] [[ 3.167]
 [-6.148]
 [-6.148]
 [-6.148]
 [-6.148]
 [-6.148]
 [-6.148]] [[0.496]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]]
Printing some Q and Qe and total Qs values:  [[0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]] [[1.564]
 [1.564]
 [1.564]
 [1.564]
 [1.564]
 [1.564]
 [1.564]] [[0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]]
2549 3771
from probs:  [0.1775270222145624, 0.3773244289868559, 0.2196911099575857, 0.18826635204605638, 0.036389486342948175, 0.0008016004519915858]
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.235]
 [0.192]
 [0.219]
 [0.178]
 [0.18 ]
 [0.202]] [[-2.309]
 [-2.449]
 [-3.394]
 [ 0.   ]
 [-4.204]
 [-3.896]
 [-2.734]] [[0.229]
 [0.235]
 [0.192]
 [0.219]
 [0.178]
 [0.18 ]
 [0.202]]
siam score:  -0.65083617
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1775270222145624, 0.3773244289868559, 0.2196911099575857, 0.18826635204605638, 0.036389486342948175, 0.0008016004519915858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1775270222145624, 0.3773244289868559, 0.2196911099575857, 0.18826635204605638, 0.036389486342948175, 0.0008016004519915858]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]] [[-0.394]
 [-3.065]
 [-3.065]
 [-3.065]
 [-3.065]
 [-3.065]
 [-3.065]] [[0.368]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]] [[-5.806]
 [-5.712]
 [-5.712]
 [-5.712]
 [-5.712]
 [-5.712]
 [-5.712]] [[0.247]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[-1.488]
 [-1.245]
 [-1.245]
 [-1.245]
 [-1.245]
 [-1.245]
 [-1.245]] [[0.518]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]] [[-4.883]
 [-5.049]
 [-5.049]
 [-5.049]
 [-5.049]
 [-5.049]
 [-5.049]] [[0.454]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]]
siam score:  -0.6499655
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]] [[-3.579]
 [-4.554]
 [-4.554]
 [-4.554]
 [-4.554]
 [-4.554]
 [-4.554]] [[0.51 ]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]]
siam score:  -0.65011925
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1775270222145624, 0.3773244289868559, 0.2196911099575857, 0.18826635204605638, 0.036389486342948175, 0.0008016004519915858]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.351]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[-2.598]
 [-2.272]
 [-2.747]
 [-2.747]
 [-2.747]
 [-2.747]
 [-2.747]] [[0.337]
 [0.351]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.572]
 [0.645]
 [0.631]
 [0.526]
 [0.553]
 [0.573]] [[ 0.114]
 [ 0.214]
 [ 0.144]
 [ 0.58 ]
 [-0.043]
 [ 0.344]
 [ 1.003]] [[0.924]
 [1.083]
 [1.123]
 [1.537]
 [0.756]
 [1.185]
 [1.875]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.664]
 [0.663]
 [0.655]
 [0.663]
 [0.663]
 [0.681]] [[3.772]
 [4.073]
 [3.772]
 [3.783]
 [3.772]
 [3.772]
 [3.995]] [[0.752]
 [0.854]
 [0.752]
 [0.739]
 [0.752]
 [0.752]
 [0.863]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.018]
 [0.015]
 [0.017]
 [0.017]
 [0.018]] [[-0.416]
 [-0.449]
 [-0.386]
 [-0.288]
 [-0.366]
 [-0.403]
 [-0.45 ]] [[0.09 ]
 [0.034]
 [0.151]
 [0.319]
 [0.184]
 [0.118]
 [0.037]]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.51 ]
 [0.565]
 [0.563]
 [0.465]
 [0.559]
 [0.506]] [[-0.657]
 [-1.501]
 [-3.215]
 [-3.309]
 [-1.003]
 [-3.218]
 [-1.387]] [[1.581]
 [1.49 ]
 [1.027]
 [0.993]
 [1.566]
 [1.016]
 [1.52 ]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1775270222145624, 0.3773244289868559, 0.2196911099575857, 0.18826635204605638, 0.036389486342948175, 0.0008016004519915858]
Printing some Q and Qe and total Qs values:  [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]] [[-0.175]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]] [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]]
siam score:  -0.6414577
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1775270222145624, 0.3773244289868559, 0.2196911099575857, 0.18826635204605638, 0.036389486342948175, 0.0008016004519915858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]] [[-4.549]
 [-8.82 ]
 [-8.82 ]
 [-8.82 ]
 [-8.82 ]
 [-8.82 ]
 [-8.82 ]] [[0.42 ]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]]
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.612]
 [0.612]
 [0.606]
 [0.612]
 [0.612]
 [0.588]] [[3.429]
 [3.429]
 [3.429]
 [3.679]
 [3.429]
 [3.429]
 [4.615]] [[0.612]
 [0.612]
 [0.612]
 [0.606]
 [0.612]
 [0.612]
 [0.588]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1775270222145624, 0.3773244289868559, 0.2196911099575857, 0.18826635204605638, 0.036389486342948175, 0.0008016004519915858]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  1  step number:  73 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9969800000000001 -1.0 -0.9969800000000001
probs:  [0.17751614579672775, 0.3772884073618908, 0.21971394485731185, 0.18827386825973655, 0.03640266032528144, 0.00080497339905149]
maxi score, test score, baseline:  -0.9969800000000001 -1.0 -0.9969800000000001
probs:  [0.17751614579672775, 0.3772884073618908, 0.21971394485731185, 0.18827386825973655, 0.03640266032528144, 0.00080497339905149]
maxi score, test score, baseline:  -0.9969800000000001 -1.0 -0.9969800000000001
actor:  1 policy actor:  1  step number:  96 total reward:  0.4149999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9969800000000001 -1.0 -0.9969800000000001
line 256 mcts: sample exp_bonus 1.3976143678997146
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1719955515358498, 0.3966541840864296, 0.2128810365741236, 0.18241871839760418, 0.03527056996375139, 0.0007799394422414334]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.244]
 [0.158]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[-0.05 ]
 [-1.139]
 [-1.275]
 [-1.139]
 [-1.139]
 [-1.139]
 [-1.139]] [[0.237]
 [0.244]
 [0.158]
 [0.244]
 [0.244]
 [0.244]
 [0.244]]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.471]
 [0.613]
 [0.622]
 [0.634]
 [0.624]
 [0.636]] [[-2.727]
 [-0.882]
 [-2.477]
 [-2.62 ]
 [-2.422]
 [-2.521]
 [-2.471]] [[0.498]
 [1.713]
 [0.675]
 [0.579]
 [0.734]
 [0.653]
 [0.7  ]]
siam score:  -0.6452623
from probs:  [0.1719955515358498, 0.3966541840864296, 0.2128810365741236, 0.18241871839760418, 0.03527056996375139, 0.0007799394422414334]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.358]
 [0.454]
 [0.452]
 [0.452]
 [0.455]
 [0.489]] [[-3.904]
 [-0.505]
 [-2.369]
 [-2.271]
 [-2.562]
 [-2.594]
 [-2.824]] [[0.484]
 [0.358]
 [0.454]
 [0.452]
 [0.452]
 [0.455]
 [0.489]]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.737]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[-4.519]
 [-4.689]
 [-4.519]
 [-4.519]
 [-4.519]
 [-4.519]
 [-4.519]] [[0.578]
 [0.737]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
actor:  1 policy actor:  1  step number:  80 total reward:  0.3949999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16694960927248328, 0.4143549218366679, 0.20663561098069047, 0.1770669850965265, 0.03423581494803214, 0.000757057865599887]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.4793037010250425
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16694960927248328, 0.4143549218366679, 0.20663561098069047, 0.1770669850965265, 0.03423581494803214, 0.000757057865599887]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16694960927248328, 0.4143549218366679, 0.20663561098069047, 0.1770669850965265, 0.03423581494803214, 0.000757057865599887]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
start point for exploration sampling:  10749
2567 3793
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16694960927248328, 0.4143549218366679, 0.20663561098069047, 0.1770669850965265, 0.03423581494803214, 0.000757057865599887]
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
siam score:  -0.6497324
Printing some Q and Qe and total Qs values:  [[1.425]
 [1.425]
 [1.425]
 [1.425]
 [1.425]
 [1.425]
 [1.425]] [[-0.318]
 [-0.318]
 [-0.318]
 [-0.318]
 [-0.318]
 [-0.318]
 [-0.318]] [[2.798]
 [2.798]
 [2.798]
 [2.798]
 [2.798]
 [2.798]
 [2.798]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16694960927248328, 0.4143549218366679, 0.20663561098069047, 0.1770669850965265, 0.03423581494803214, 0.000757057865599887]
actor:  1 policy actor:  1  step number:  63 total reward:  0.6199999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.66]
 [1.14]
 [0.66]
 [0.66]
 [0.66]
 [0.66]
 [0.66]] [[-2.367]
 [-1.342]
 [-2.367]
 [-2.367]
 [-2.367]
 [-2.367]
 [-2.367]] [[0.621]
 [1.912]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]]
2570 3805
from probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.615]] [[3.05 ]
 [3.05 ]
 [3.05 ]
 [3.05 ]
 [3.05 ]
 [3.05 ]
 [3.467]] [[1.361]
 [1.361]
 [1.361]
 [1.361]
 [1.361]
 [1.361]
 [1.424]]
Printing some Q and Qe and total Qs values:  [[0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]] [[2.269]
 [2.269]
 [2.269]
 [2.269]
 [2.269]
 [2.269]
 [2.269]] [[1.918]
 [1.918]
 [1.918]
 [1.918]
 [1.918]
 [1.918]
 [1.918]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
from probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
line 256 mcts: sample exp_bonus 0.5715547528130009
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]] [[-0.425]
 [-1.421]
 [-1.421]
 [-1.421]
 [-1.421]
 [-1.421]
 [-1.421]] [[0.515]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.479]
 [0.635]
 [0.466]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[-3.89 ]
 [-2.68 ]
 [-2.708]
 [-3.197]
 [-3.89 ]
 [-3.89 ]
 [-3.89 ]] [[0.46 ]
 [0.479]
 [0.635]
 [0.466]
 [0.46 ]
 [0.46 ]
 [0.46 ]]
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[-6.94 ]
 [-6.984]
 [-6.984]
 [-6.984]
 [-6.984]
 [-6.984]
 [-6.984]] [[0.137]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.442]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.526]] [[ 0.354]
 [-0.446]
 [-0.719]
 [-0.719]
 [-0.719]
 [-0.719]
 [-0.504]] [[0.611]
 [0.442]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.526]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
from probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]] [[-2.108]
 [-6.096]
 [-6.096]
 [-6.096]
 [-6.096]
 [-6.096]
 [-6.096]] [[0.471]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]]
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]] [[-6.562]
 [-6.38 ]
 [-6.38 ]
 [-6.38 ]
 [-6.38 ]
 [-6.38 ]
 [-6.38 ]] [[0.064]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]]
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]] [[-0.424]
 [-0.734]
 [-0.734]
 [-0.734]
 [-0.734]
 [-0.734]
 [-0.734]] [[0.614]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.482]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[-2.078]
 [-1.613]
 [-2.078]
 [-2.078]
 [-2.078]
 [-2.078]
 [-2.078]] [[0.478]
 [0.482]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[-0.581]
 [-0.599]
 [-0.599]
 [-0.599]
 [-0.599]
 [-0.599]
 [-0.599]] [[0.503]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]] [[-3.643]
 [-9.721]
 [-9.721]
 [-9.721]
 [-9.721]
 [-9.721]
 [-9.721]] [[0.33 ]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]]
from probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]] [[-9.842]
 [-9.762]
 [-9.762]
 [-9.762]
 [-9.762]
 [-9.762]
 [-9.762]] [[0.145]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.129]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[ -9.228]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.129]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.564]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[0.92 ]
 [0.92 ]
 [0.949]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]] [[1.561]
 [1.561]
 [1.605]
 [1.561]
 [1.561]
 [1.561]
 [1.561]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
2577 3820
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]] [[-9.575]
 [-9.433]
 [-9.433]
 [-9.433]
 [-9.433]
 [-9.433]
 [-9.433]] [[0.049]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.033]
 [0.068]
 [0.083]
 [0.069]
 [0.073]
 [0.08 ]] [[-6.446]
 [-1.776]
 [-9.841]
 [-9.844]
 [-9.84 ]
 [-9.649]
 [-9.334]] [[0.42 ]
 [0.033]
 [0.068]
 [0.083]
 [0.069]
 [0.073]
 [0.08 ]]
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[-3.84]
 [-3.84]
 [-3.84]
 [-3.84]
 [-3.84]
 [-3.84]
 [-3.84]] [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]] [[-7.704]
 [-9.233]
 [-9.233]
 [-9.233]
 [-9.233]
 [-9.233]
 [-9.233]] [[0.153]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]]
siam score:  -0.63077945
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]] [[-2.175]
 [-2.175]
 [-2.175]
 [-2.175]
 [-2.175]
 [-2.175]
 [-2.175]] [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]]
siam score:  -0.6306108
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[3.   ]
 [0.608]
 [3.   ]
 [3.   ]
 [0.638]
 [3.   ]
 [3.   ]] [[ 0.   ]
 [-0.45 ]
 [ 0.   ]
 [ 0.   ]
 [-9.054]
 [ 0.   ]
 [ 0.   ]] [[3.728]
 [2.147]
 [3.728]
 [3.728]
 [0.384]
 [3.728]
 [3.728]]
using explorer policy with actor:  1
siam score:  -0.63073397
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.576]
 [0.626]
 [0.665]
 [0.661]
 [0.64 ]
 [0.613]] [[-2.797]
 [-0.321]
 [-1.996]
 [-2.381]
 [-2.168]
 [-2.039]
 [-1.087]] [[0.448]
 [1.858]
 [0.896]
 [0.695]
 [0.818]
 [0.881]
 [1.428]]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.572]
 [0.605]
 [0.648]
 [0.646]
 [0.893]
 [0.644]] [[-1.379]
 [-0.104]
 [-1.328]
 [-1.325]
 [-1.322]
 [-0.658]
 [-1.431]] [[0.464]
 [0.84 ]
 [0.496]
 [0.584]
 [0.58 ]
 [1.296]
 [0.539]]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]] [[-1.605]
 [-6.363]
 [-6.363]
 [-6.363]
 [-6.363]
 [-6.363]
 [-6.363]] [[0.214]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.246]
 [0.413]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[-3.875]
 [-3.875]
 [-3.381]
 [-3.875]
 [-3.875]
 [-3.875]
 [-3.875]] [[0.246]
 [0.246]
 [0.413]
 [0.246]
 [0.246]
 [0.246]
 [0.246]]
siam score:  -0.63180506
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[-3.789]
 [-3.843]
 [-3.843]
 [-3.843]
 [-3.843]
 [-3.843]
 [-3.843]] [[0.596]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]]
from probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.243]] [[ 0.458]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.115]
 [-1.353]] [[0.311]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.243]]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[-2.583]
 [-3.291]
 [-3.291]
 [-3.291]
 [-3.291]
 [-3.291]
 [-3.291]] [[0.468]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.649]
 [0.693]
 [0.591]
 [0.628]
 [0.628]
 [0.556]] [[-0.091]
 [ 0.364]
 [-3.818]
 [-2.136]
 [ 0.   ]
 [ 0.   ]
 [-0.533]] [[1.534]
 [2.064]
 [0.544]
 [1.061]
 [1.904]
 [1.904]
 [1.623]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.43665679929334894, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
using another actor
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.601]
 [0.613]
 [0.601]
 [0.601]
 [0.601]
 [1.026]] [[-1.461]
 [-1.461]
 [-1.49 ]
 [-1.461]
 [-1.461]
 [-1.461]
 [ 1.689]] [[0.35 ]
 [0.35 ]
 [0.36 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [1.563]]
line 256 mcts: sample exp_bonus 2.996692122470965
from probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[-3.09 ]
 [-9.951]
 [-9.951]
 [-9.951]
 [-9.951]
 [-9.951]
 [-9.951]] [[0.337]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[-0.363]
 [-1.639]
 [-1.639]
 [-1.639]
 [-1.639]
 [-1.639]
 [-1.639]] [[0.611]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]]
line 256 mcts: sample exp_bonus -4.689388987247851
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]] [[-5.956]
 [-8.867]
 [-8.867]
 [-8.867]
 [-8.867]
 [-8.867]
 [-8.867]] [[0.324]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.417]
 [0.497]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[-3.239]
 [-3.239]
 [-1.543]
 [-3.239]
 [-3.239]
 [-3.239]
 [-3.239]] [[0.417]
 [0.417]
 [0.497]
 [0.417]
 [0.417]
 [0.417]
 [0.417]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
siam score:  -0.63642305
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]] [[-5.453]
 [-9.854]
 [-9.854]
 [-9.854]
 [-9.854]
 [-9.854]
 [-9.854]] [[0.414]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]] [[-6.086]
 [-9.808]
 [-9.808]
 [-9.808]
 [-9.808]
 [-9.808]
 [-9.808]] [[0.271]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.453]
 [0.489]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[0.582]
 [0.582]
 [0.084]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[0.453]
 [0.453]
 [0.489]
 [0.453]
 [0.453]
 [0.453]
 [0.453]]
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]] [[-8.624]
 [-9.07 ]
 [-9.07 ]
 [-9.07 ]
 [-9.07 ]
 [-9.07 ]
 [-9.07 ]] [[0.109]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]] [[0.482]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[0.348]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[-5.171]
 [-7.006]
 [-7.006]
 [-7.006]
 [-7.006]
 [-7.006]
 [-7.006]] [[0.428]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
line 256 mcts: sample exp_bonus 0.512948861846637
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[-1.74 ]
 [-6.271]
 [-6.271]
 [-6.271]
 [-6.271]
 [-6.271]
 [-6.271]] [[0.752]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.616]] [[-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [ 0.715]] [[1.684]
 [1.684]
 [1.684]
 [1.684]
 [1.684]
 [1.684]
 [2.056]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
2596 3886
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.63431627
line 256 mcts: sample exp_bonus -3.118772212284798
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[1.016]
 [1.016]
 [1.016]
 [1.016]
 [1.016]
 [1.016]
 [1.016]] [[0.987]
 [0.987]
 [0.987]
 [0.987]
 [0.987]
 [0.987]
 [0.987]] [[2.689]
 [2.689]
 [2.689]
 [2.689]
 [2.689]
 [2.689]
 [2.689]]
in main func line 156:  2599
2600 3899
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
line 256 mcts: sample exp_bonus 1.202428608991982
siam score:  -0.63167644
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]] [[-7.089]
 [-8.547]
 [-8.547]
 [-8.547]
 [-8.547]
 [-8.547]
 [-8.547]] [[0.147]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.18 ]
 [0.108]
 [0.107]
 [0.18 ]
 [0.18 ]
 [0.109]] [[-9.853]
 [ 0.   ]
 [-9.544]
 [-9.417]
 [ 0.   ]
 [ 0.   ]
 [-9.572]] [[0.133]
 [0.18 ]
 [0.108]
 [0.107]
 [0.18 ]
 [0.18 ]
 [0.109]]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.072]
 [0.103]
 [0.117]
 [0.106]
 [0.109]
 [0.115]] [[-8.526]
 [-2.167]
 [-9.378]
 [-9.745]
 [-9.354]
 [-9.353]
 [-9.722]] [[0.144]
 [0.072]
 [0.103]
 [0.117]
 [0.106]
 [0.109]
 [0.115]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]] [[-8.943]
 [-9.333]
 [-9.333]
 [-9.333]
 [-9.333]
 [-9.333]
 [-9.333]] [[0.153]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]]
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
line 256 mcts: sample exp_bonus 0.2956912821602973
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]] [[-3.371]
 [-3.371]
 [-3.371]
 [-3.371]
 [-3.371]
 [-3.371]
 [-3.371]] [[0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
actor:  1 policy actor:  1  step number:  56 total reward:  0.5749999999999997  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16059202194482664, 0.4366567992933489, 0.19876674595286464, 0.17032411923717172, 0.032932085132705874, 0.0007282284390824216]
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.717]
 [0.719]
 [0.717]
 [0.717]
 [0.711]
 [0.717]] [[0.182]
 [0.182]
 [1.179]
 [0.182]
 [0.182]
 [1.025]
 [0.182]] [[1.063]
 [1.063]
 [1.732]
 [1.063]
 [1.063]
 [1.612]
 [1.063]]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.582]
 [0.77 ]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[-3.478]
 [-3.478]
 [-3.236]
 [-3.478]
 [-3.478]
 [-3.478]
 [-3.478]] [[0.582]
 [0.582]
 [0.77 ]
 [0.582]
 [0.582]
 [0.582]
 [0.582]]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[-3.401]
 [-3.401]
 [-3.401]
 [-3.401]
 [-3.401]
 [-3.401]
 [-3.401]] [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.5  ]
 [0.632]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[-4.116]
 [-4.116]
 [-4.244]
 [-4.116]
 [-4.116]
 [-4.116]
 [-4.116]] [[0.5  ]
 [0.5  ]
 [0.632]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.7299999999999999  reward:  1.0 rdn_beta:  0.333
2604 3908
using another actor
from probs:  [0.1487749667531529, 0.4447675733723057, 0.1841406295446575, 0.1911333881395615, 0.03050879994781802, 0.0006746422425044874]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1487749667531529, 0.4447675733723057, 0.1841406295446575, 0.1911333881395615, 0.03050879994781802, 0.0006746422425044874]
siam score:  -0.628539
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1487749667531529, 0.4447675733723057, 0.1841406295446575, 0.1911333881395615, 0.03050879994781802, 0.0006746422425044874]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1487749667531529, 0.4447675733723057, 0.1841406295446575, 0.1911333881395615, 0.03050879994781802, 0.0006746422425044874]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[-10.]
 [-10.]
 [-10.]
 [-10.]
 [-10.]
 [-10.]
 [-10.]] [[0.055]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[-4.357]
 [-5.469]
 [-5.469]
 [-5.469]
 [-5.469]
 [-5.469]
 [-5.469]] [[0.144]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]]
siam score:  -0.62931436
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[-0.208]
 [-0.208]
 [-0.208]
 [-0.208]
 [-0.208]
 [-0.208]
 [-0.208]] [[2.121]
 [2.121]
 [2.121]
 [2.121]
 [2.121]
 [2.121]
 [2.121]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1487749667531529, 0.4447675733723057, 0.1841406295446575, 0.1911333881395615, 0.03050879994781802, 0.0006746422425044874]
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.003]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]] [[-10.   ]
 [ -2.855]
 [  0.   ]
 [  0.   ]
 [  0.   ]
 [  0.   ]
 [  0.   ]] [[0.067]
 [0.003]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[-7.982]
 [-8.626]
 [-8.626]
 [-8.626]
 [-8.626]
 [-8.626]
 [-8.626]] [[0.143]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]] [[-4.063]
 [-8.862]
 [-8.862]
 [-8.862]
 [-8.862]
 [-8.862]
 [-8.862]] [[0.349]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]] [[-5.369]
 [-5.869]
 [-5.869]
 [-5.869]
 [-5.869]
 [-5.869]
 [-5.869]] [[0.073]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]] [[ 0.881]
 [-5.819]
 [-5.819]
 [-5.819]
 [-5.819]
 [-5.819]
 [-5.819]] [[0.291]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]] [[-1.601]
 [-6.416]
 [-6.416]
 [-6.416]
 [-6.416]
 [-6.416]
 [-6.416]] [[0.359]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]] [[-1.674]
 [-4.111]
 [-4.111]
 [-4.111]
 [-4.111]
 [-4.111]
 [-4.111]] [[0.324]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]]
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[-1.608]
 [-1.608]
 [-1.608]
 [-1.608]
 [-1.608]
 [-1.608]
 [-1.608]] [[0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1487749667531529, 0.4447675733723057, 0.1841406295446575, 0.1911333881395615, 0.03050879994781802, 0.0006746422425044874]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]] [[-5.083]
 [-9.275]
 [-9.275]
 [-9.275]
 [-9.275]
 [-9.275]
 [-9.275]] [[0.197]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1487749667531529, 0.4447675733723057, 0.1841406295446575, 0.1911333881395615, 0.03050879994781802, 0.0006746422425044874]
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]] [[-6.245]
 [-6.15 ]
 [-6.15 ]
 [-6.15 ]
 [-6.15 ]
 [-6.15 ]
 [-6.15 ]] [[0.193]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.378]
 [0.374]
 [0.361]
 [0.313]
 [0.313]
 [0.313]] [[ 0.279]
 [ 0.508]
 [ 0.295]
 [-0.33 ]
 [ 0.279]
 [ 0.279]
 [ 0.279]] [[0.313]
 [0.378]
 [0.374]
 [0.361]
 [0.313]
 [0.313]
 [0.313]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.625]
 [0.751]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[0.385]
 [0.385]
 [0.648]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[0.958]
 [0.958]
 [1.297]
 [0.958]
 [0.958]
 [0.958]
 [0.958]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1487749667531529, 0.4447675733723057, 0.1841406295446575, 0.1911333881395615, 0.03050879994781802, 0.0006746422425044874]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.647]
 [0.672]
 [0.626]
 [0.672]
 [0.672]
 [0.624]] [[2.846]
 [3.802]
 [2.846]
 [2.071]
 [2.846]
 [2.846]
 [3.045]] [[1.121]
 [1.684]
 [1.121]
 [0.581]
 [1.121]
 [1.121]
 [1.185]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
line 256 mcts: sample exp_bonus -0.3563170096952799
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[2.364]
 [2.364]
 [2.364]
 [2.364]
 [2.364]
 [2.364]
 [2.364]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.624]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[-1.85 ]
 [-0.418]
 [-1.85 ]
 [-1.85 ]
 [-1.85 ]
 [-1.85 ]
 [-1.85 ]] [[0.467]
 [1.076]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
siam score:  -0.620426
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]] [[-0.352]
 [-0.352]
 [-0.349]
 [-0.349]
 [-0.351]
 [-0.349]
 [-0.349]] [[-0.508]
 [-0.509]
 [-0.506]
 [-0.506]
 [-0.507]
 [-0.506]
 [-0.506]]
actor:  1 policy actor:  1  step number:  120 total reward:  0.23499999999999943  reward:  1.0 rdn_beta:  0.5
from probs:  [0.1487749667531529, 0.4447675733723057, 0.1841406295446575, 0.1911333881395615, 0.03050879994781802, 0.0006746422425044874]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.313]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]] [[0.151]
 [0.017]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]] [[0.291]
 [0.313]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]]
Printing some Q and Qe and total Qs values:  [[1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.016]
 [1.122]] [[ 0.424]
 [ 0.424]
 [ 0.424]
 [ 0.424]
 [ 0.424]
 [ 0.1  ]
 [-0.137]] [[0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.622]
 [0.756]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.145789332603893, 0.43584192355013446, 0.2005133947388632, 0.18729770003189722, 0.029896545635382, 0.0006611034398301325]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]] [[-1.656]
 [-9.081]
 [-9.081]
 [-9.081]
 [-9.081]
 [-9.081]
 [-9.081]] [[0.553]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[-0.692]
 [-1.057]
 [-1.057]
 [-1.057]
 [-1.057]
 [-1.057]
 [-1.057]] [[0.481]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.145789332603893, 0.43584192355013446, 0.2005133947388632, 0.18729770003189722, 0.029896545635382, 0.0006611034398301325]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
actor:  1 policy actor:  1  step number:  77 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.889]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.501]] [[-2.031]
 [-0.451]
 [-2.031]
 [-2.031]
 [-2.031]
 [-2.031]
 [-2.038]] [[1.069]
 [2.071]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.051]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.   ]
 [ 0.159]
 [ 0.159]
 [ 0.159]
 [-0.001]
 [-0.001]] [[-0.15 ]
 [-0.351]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.268]
 [-0.264]] [[ 0.396]
 [-0.005]
 [ 1.016]
 [ 1.016]
 [ 1.016]
 [ 0.158]
 [ 0.166]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
siam score:  -0.6208306
line 256 mcts: sample exp_bonus -0.6475861000777088
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.441]
 [0.85 ]
 [0.498]
 [0.506]
 [0.583]
 [0.522]] [[-4.424]
 [-3.22 ]
 [-3.63 ]
 [-4.099]
 [-3.963]
 [-4.086]
 [-3.874]] [[0.439]
 [0.441]
 [0.85 ]
 [0.498]
 [0.506]
 [0.583]
 [0.522]]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.716]
 [0.559]
 [0.559]
 [0.559]
 [0.559]] [[-1.953]
 [-1.953]
 [-1.099]
 [-1.953]
 [-1.953]
 [-1.953]
 [-1.953]] [[0.559]
 [0.559]
 [0.716]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.592]
 [0.765]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[-1.284]
 [-0.667]
 [-0.511]
 [-1.974]
 [-1.974]
 [-1.974]
 [-1.974]] [[0.6  ]
 [0.592]
 [0.765]
 [0.583]
 [0.583]
 [0.583]
 [0.583]]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.531]
 [0.686]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[-3.8  ]
 [-3.8  ]
 [-4.129]
 [-3.8  ]
 [-3.8  ]
 [-3.8  ]
 [-3.8  ]] [[0.531]
 [0.531]
 [0.686]
 [0.531]
 [0.531]
 [0.531]
 [0.531]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.794]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[-1.045]
 [-0.408]
 [-1.516]
 [-1.516]
 [-1.516]
 [-1.516]
 [-1.516]] [[0.75 ]
 [0.794]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]]
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.402]
 [0.513]
 [0.402]
 [0.444]
 [0.455]
 [0.313]] [[3.737]
 [4.476]
 [1.457]
 [2.714]
 [2.113]
 [1.795]
 [2.839]] [[0.887]
 [1.173]
 [0.385]
 [0.584]
 [0.466]
 [0.383]
 [0.447]]
from probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]] [[-5.778]
 [-8.598]
 [-8.598]
 [-8.598]
 [-8.598]
 [-8.598]
 [-8.598]] [[0.384]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.432]
 [0.712]
 [0.5  ]
 [0.503]
 [0.501]
 [0.494]] [[-3.995]
 [-3.392]
 [-5.549]
 [-3.707]
 [-3.757]
 [-3.805]
 [-3.788]] [[0.457]
 [0.432]
 [0.712]
 [0.5  ]
 [0.503]
 [0.501]
 [0.494]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.485]
 [0.784]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[-4.026]
 [-4.026]
 [-4.779]
 [-4.026]
 [-4.026]
 [-4.026]
 [-4.026]] [[0.485]
 [0.485]
 [0.784]
 [0.485]
 [0.485]
 [0.485]
 [0.485]]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.483]
 [0.78 ]
 [0.483]
 [0.483]
 [0.483]
 [0.483]] [[-4.13 ]
 [-4.13 ]
 [-2.608]
 [-4.13 ]
 [-4.13 ]
 [-4.13 ]
 [-4.13 ]] [[0.483]
 [0.483]
 [0.78 ]
 [0.483]
 [0.483]
 [0.483]
 [0.483]]
Printing some Q and Qe and total Qs values:  [[ 0.062]
 [ 0.062]
 [ 0.104]
 [ 0.12 ]
 [ 0.062]
 [ 0.098]
 [-0.015]] [[-0.6  ]
 [-0.6  ]
 [-2.009]
 [-3.565]
 [-0.6  ]
 [-2.339]
 [ 0.116]] [[ 0.062]
 [ 0.062]
 [ 0.104]
 [ 0.12 ]
 [ 0.062]
 [ 0.098]
 [-0.015]]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[-5.418]
 [-9.834]
 [-9.834]
 [-9.834]
 [-9.834]
 [-9.834]
 [-9.834]] [[0.457]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]]
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]] [[-9.842]
 [-9.063]
 [-9.063]
 [-9.063]
 [-9.063]
 [-9.063]
 [-9.063]] [[0.376]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.6167529
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]] [[0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]] [[1.855]
 [1.855]
 [1.855]
 [1.855]
 [1.855]
 [1.855]
 [1.855]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
Printing some Q and Qe and total Qs values:  [[1.257]
 [1.258]
 [1.257]
 [1.257]
 [1.257]
 [1.257]
 [1.258]] [[-0.369]
 [-0.366]
 [-0.369]
 [-0.369]
 [-0.369]
 [-0.369]
 [-0.366]] [[2.044]
 [2.048]
 [2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.048]]
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[-2.773]
 [-2.488]
 [-2.488]
 [-2.488]
 [-2.488]
 [-2.488]
 [-2.488]] [[0.367]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.89 ]
 [0.602]
 [0.602]
 [0.602]
 [0.58 ]
 [0.609]] [[-5.008]
 [-4.078]
 [-5.008]
 [-5.008]
 [-5.008]
 [-4.64 ]
 [-4.783]] [[0.117]
 [0.635]
 [0.117]
 [0.117]
 [0.117]
 [0.265]
 [0.216]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.705]
 [0.63 ]
 [0.663]
 [0.663]
 [0.663]
 [0.663]] [[-4.711]
 [-3.092]
 [-4.9  ]
 [-4.711]
 [-4.711]
 [-4.711]
 [-4.711]] [[0.251]
 [0.76 ]
 [0.175]
 [0.251]
 [0.251]
 [0.251]
 [0.251]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.16727656961256276, 0.4248785405497984, 0.19546958177724283, 0.1825863211819561, 0.029144513161043824, 0.0006444737173962289]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.473]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[-0.404]
 [ 2.163]
 [-0.404]
 [-0.404]
 [-0.404]
 [-0.404]
 [-0.404]] [[0.345]
 [1.459]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]]
Printing some Q and Qe and total Qs values:  [[0.72]
 [0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[-0.363]
 [ 0.789]
 [ 0.789]
 [ 0.789]
 [ 0.789]
 [ 0.789]
 [ 0.789]] [[0.313]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
actor:  1 policy actor:  1  step number:  47 total reward:  0.6699999999999998  reward:  1.0 rdn_beta:  0.333
using another actor
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.535]
 [0.562]
 [0.665]
 [0.591]
 [0.645]
 [0.6  ]] [[-1.043]
 [ 0.948]
 [-1.49 ]
 [-0.509]
 [-1.482]
 [-1.184]
 [-1.62 ]] [[0.611]
 [0.535]
 [0.562]
 [0.665]
 [0.591]
 [0.645]
 [0.6  ]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.13 ]
 [0.16 ]
 [0.141]
 [0.145]
 [0.15 ]
 [0.174]] [[-8.732]
 [-2.323]
 [-7.515]
 [-7.306]
 [-7.402]
 [-6.913]
 [-6.116]] [[0.231]
 [0.13 ]
 [0.16 ]
 [0.141]
 [0.145]
 [0.15 ]
 [0.174]]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.621357
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1614923092937651, 0.44476567862353955, 0.18871043464728418, 0.17627266461418825, 0.02813672437521113, 0.0006221884460119747]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]] [[-4.158]
 [-4.158]
 [-4.158]
 [-4.158]
 [-4.158]
 [-4.158]
 [-4.158]] [[0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1614923092937651, 0.44476567862353955, 0.18871043464728418, 0.17627266461418825, 0.02813672437521113, 0.0006221884460119747]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[0.036]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[0.478]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1614923092937651, 0.44476567862353955, 0.18871043464728418, 0.17627266461418825, 0.02813672437521113, 0.0006221884460119747]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.409]
 [0.644]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[-2.993]
 [-2.993]
 [-3.234]
 [-2.993]
 [-2.993]
 [-2.993]
 [-2.993]] [[0.409]
 [0.409]
 [0.644]
 [0.409]
 [0.409]
 [0.409]
 [0.409]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1614923092937651, 0.4447656786235396, 0.18871043464728418, 0.17627266461418825, 0.02813672437521113, 0.0006221884460119747]
actor:  1 policy actor:  1  step number:  110 total reward:  0.3349999999999995  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]] [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]] [[-7.892]
 [-9.793]
 [-9.793]
 [-9.793]
 [-9.793]
 [-9.793]
 [-9.793]] [[0.302]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]] [[-7.562]
 [-9.448]
 [-9.448]
 [-9.448]
 [-9.448]
 [-9.448]
 [-9.448]] [[0.244]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.15800399652674887, 0.4351585227047344, 0.20623468556017988, 0.17246508895229648, 0.027528957384391, 0.0006087488716495482]
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]] [[-9.981]
 [-9.757]
 [-9.757]
 [-9.757]
 [-9.757]
 [-9.757]
 [-9.757]] [[0.245]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.131]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.138]] [[-0.071]
 [-0.198]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [ 0.459]] [[0.171]
 [0.131]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.138]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[-3.901]
 [-3.901]
 [-3.901]
 [-3.901]
 [-3.901]
 [-3.901]
 [-3.901]] [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.528]
 [0.645]
 [0.505]
 [0.568]
 [0.526]
 [0.631]] [[-4.472]
 [-3.794]
 [ 0.   ]
 [-5.197]
 [-4.41 ]
 [-4.903]
 [-4.463]] [[0.543]
 [0.528]
 [0.645]
 [0.505]
 [0.568]
 [0.526]
 [0.631]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.643]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[-2.325]
 [-1.701]
 [-2.325]
 [-2.325]
 [-2.325]
 [-2.325]
 [-2.325]] [[0.646]
 [0.643]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.526]
 [0.521]
 [0.521]
 [0.523]
 [0.527]
 [0.521]] [[-2.876]
 [-3.017]
 [-3.328]
 [-3.328]
 [-3.372]
 [-3.163]
 [-3.328]] [[0.567]
 [0.526]
 [0.521]
 [0.521]
 [0.523]
 [0.527]
 [0.521]]
UNIT TEST: sample policy line 217 mcts : [0.061 0.041 0.041 0.02  0.02  0.02  0.796]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.54 ]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.896]] [[-1.61 ]
 [-1.085]
 [-1.61 ]
 [-1.61 ]
 [-1.61 ]
 [-1.61 ]
 [-0.418]] [[1.567]
 [1.683]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [2.171]]
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.136]
 [0.348]
 [0.348]
 [0.058]
 [0.03 ]
 [0.059]] [[-0.198]
 [ 0.008]
 [-1.344]
 [-1.344]
 [ 0.445]
 [ 0.383]
 [ 0.002]] [[0.136]
 [0.136]
 [0.348]
 [0.348]
 [0.058]
 [0.03 ]
 [0.059]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.15800399652674887, 0.4351585227047344, 0.20623468556017988, 0.17246508895229648, 0.027528957384391, 0.0006087488716495482]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.564]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[-2.629]
 [-2.039]
 [-2.629]
 [-2.629]
 [-2.629]
 [-2.629]
 [-2.629]] [[0.572]
 [0.564]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[-2.802]
 [-2.802]
 [-2.802]
 [-2.802]
 [-2.802]
 [-2.802]
 [-2.802]] [[0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]]
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.74 ]
 [0.656]
 [0.656]
 [0.656]
 [0.671]
 [0.669]] [[-2.767]
 [-1.23 ]
 [-2.767]
 [-2.767]
 [-2.767]
 [-2.733]
 [-2.677]] [[0.656]
 [0.74 ]
 [0.656]
 [0.656]
 [0.656]
 [0.671]
 [0.669]]
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[-7.732]
 [-8.376]
 [-8.376]
 [-8.376]
 [-8.376]
 [-8.376]
 [-8.376]] [[0.223]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]]
line 256 mcts: sample exp_bonus -1.583222116160859
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.401]
 [0.649]
 [0.69 ]
 [0.662]
 [0.644]
 [0.648]] [[-5.482]
 [-0.354]
 [-4.798]
 [-5.153]
 [-5.057]
 [-5.216]
 [-4.819]] [[0.249]
 [1.861]
 [0.462]
 [0.359]
 [0.38 ]
 [0.319]
 [0.455]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.7  ]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[-3.684]
 [-3.684]
 [-3.987]
 [-3.684]
 [-3.684]
 [-3.684]
 [-3.684]] [[0.465]
 [0.465]
 [0.7  ]
 [0.465]
 [0.465]
 [0.465]
 [0.465]]
line 256 mcts: sample exp_bonus 0.5658626822724165
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[-2.971]
 [-2.971]
 [-2.971]
 [-2.971]
 [-2.971]
 [-2.971]
 [-2.971]] [[0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]] [[ -7.729]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.416]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]]
Printing some Q and Qe and total Qs values:  [[1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]] [[-0.375]
 [-0.375]
 [-0.381]
 [-0.382]
 [-0.381]
 [-0.381]
 [-0.377]] [[2.767]
 [2.767]
 [2.765]
 [2.765]
 [2.765]
 [2.765]
 [2.767]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.598]
 [0.594]
 [0.6  ]
 [0.6  ]
 [0.601]
 [0.601]] [[-0.426]
 [-0.228]
 [-0.578]
 [-0.682]
 [-0.682]
 [-0.587]
 [-0.601]] [[0.672]
 [0.598]
 [0.594]
 [0.6  ]
 [0.6  ]
 [0.601]
 [0.601]]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.577]
 [0.608]
 [0.618]
 [0.634]
 [0.682]
 [0.691]] [[-2.739]
 [-1.629]
 [-2.239]
 [-2.525]
 [-2.443]
 [-1.881]
 [-2.022]] [[0.597]
 [0.577]
 [0.608]
 [0.618]
 [0.634]
 [0.682]
 [0.691]]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]]
line 256 mcts: sample exp_bonus -0.774949818072844
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.079]
 [0.083]
 [0.052]
 [0.033]
 [0.342]
 [0.045]] [[-0.393]
 [ 0.63 ]
 [ 0.884]
 [ 0.63 ]
 [ 0.875]
 [-0.393]
 [ 0.44 ]] [[0.342]
 [0.079]
 [0.083]
 [0.052]
 [0.033]
 [0.342]
 [0.045]]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.654]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.488]] [[-1.795]
 [-0.87 ]
 [-1.795]
 [-1.795]
 [-1.795]
 [-1.795]
 [-1.521]] [[1.89 ]
 [2.324]
 [1.89 ]
 [1.89 ]
 [1.89 ]
 [1.89 ]
 [1.881]]
Printing some Q and Qe and total Qs values:  [[0.88 ]
 [1.331]
 [0.881]
 [0.88 ]
 [0.877]
 [0.864]
 [0.856]] [[-1.684]
 [-0.205]
 [-1.841]
 [-1.781]
 [-1.672]
 [-1.457]
 [-1.499]] [[0.864]
 [2.261]
 [0.815]
 [0.832]
 [0.862]
 [0.908]
 [0.878]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[ 1.987]
 [-4.378]
 [-4.378]
 [-4.378]
 [-4.378]
 [-4.378]
 [-4.378]] [[0.398]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]] [[-4.422]
 [-8.798]
 [-8.798]
 [-8.798]
 [-8.798]
 [-8.798]
 [-8.798]] [[0.441]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]]
line 256 mcts: sample exp_bonus 2.734473319844761
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]] [[-4.673]
 [-8.517]
 [-8.517]
 [-8.517]
 [-8.517]
 [-8.517]
 [-8.517]] [[0.421]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.152]
 [0.227]
 [0.192]
 [0.192]
 [0.177]
 [0.189]] [[ 1.159]
 [ 0.525]
 [-1.36 ]
 [ 0.   ]
 [ 0.   ]
 [-2.277]
 [ 0.996]] [[0.109]
 [0.152]
 [0.227]
 [0.192]
 [0.192]
 [0.177]
 [0.189]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [ 0.1712],
        [-0.4336],
        [-0.0000],
        [-0.0000],
        [ 0.3081],
        [-0.0000],
        [-0.5485],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.004949999999999235 -0.004949999999999235
-0.0631738157985 0.1080303380020926
-0.024259925299500003 -0.45788952749973827
-0.9703485 -0.9703485
-0.903925721457 -0.903925721457
-0.024259925299500003 0.2838780426478462
-0.9608890648499999 -0.9608890648499999
-0.024259925299500003 -0.5727118010526145
-0.9514752239519999 -0.9514752239519999
-0.9608890648499999 -0.9608890648499999
2645 4047
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.708]
 [0.695]
 [0.75 ]
 [0.724]
 [0.729]
 [0.74 ]] [[3.248]
 [3.334]
 [3.48 ]
 [3.98 ]
 [4.01 ]
 [3.631]
 [4.836]] [[0.655]
 [0.726]
 [0.819]
 [1.295]
 [1.278]
 [0.99 ]
 [1.948]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7399999999999999  reward:  1.0 rdn_beta:  0.5
rdn probs:  [0.15240566129891345, 0.4197401577209201, 0.23435906481732124, 0.16635437400661496, 0.02655356223427829, 0.0005871799219522297]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.196]
 [0.371]
 [0.52 ]
 [0.28 ]
 [0.288]
 [0.155]] [[ 0.649]
 [ 0.86 ]
 [ 1.87 ]
 [-1.235]
 [ 0.134]
 [ 0.769]
 [ 1.042]] [[1.207]
 [1.291]
 [2.108]
 [0.149]
 [0.873]
 [1.304]
 [1.38 ]]
2647 4058
siam score:  -0.6482364
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.355]
 [0.43 ]
 [0.355]
 [0.354]
 [0.359]
 [0.356]] [[2.023]
 [2.41 ]
 [2.542]
 [2.277]
 [2.598]
 [2.087]
 [1.829]] [[-0.453]
 [-0.341]
 [-0.146]
 [-0.386]
 [-0.28 ]
 [-0.441]
 [-0.533]]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]] [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[-0.443]
 [-0.443]
 [-0.443]
 [-0.443]
 [-0.443]
 [-0.443]
 [-0.443]]
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.593]
 [0.651]
 [0.636]
 [0.507]
 [0.663]
 [0.604]] [[ 0.295]
 [-0.364]
 [-1.496]
 [-0.64 ]
 [ 0.097]
 [-1.364]
 [ 0.165]] [[1.795]
 [1.237]
 [0.974]
 [1.23 ]
 [1.218]
 [1.044]
 [1.436]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.858]
 [0.695]
 [0.661]
 [0.678]
 [0.67 ]
 [0.664]
 [0.662]] [[-1.312]
 [-1.23 ]
 [-1.794]
 [-1.734]
 [-1.704]
 [-1.71 ]
 [-1.721]] [[0.875]
 [0.577]
 [0.323]
 [0.376]
 [0.369]
 [0.356]
 [0.347]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.4099999999999996  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.519]
 [0.541]
 [0.529]
 [0.522]
 [0.521]
 [0.52 ]] [[2.892]
 [3.665]
 [2.909]
 [2.792]
 [2.887]
 [2.884]
 [2.874]] [[0.833]
 [1.302]
 [0.848]
 [0.763]
 [0.815]
 [0.813]
 [0.805]]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]] [[-4.431]
 [-4.429]
 [-4.429]
 [-4.429]
 [-4.429]
 [-4.429]
 [-4.429]] [[0.209]
 [0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]]
using explorer policy with actor:  1
siam score:  -0.64600235
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.59 ]
 [0.606]
 [0.598]
 [0.598]
 [0.604]
 [0.586]] [[2.24 ]
 [0.943]
 [2.636]
 [2.24 ]
 [2.24 ]
 [2.252]
 [1.578]] [[0.598]
 [0.59 ]
 [0.606]
 [0.598]
 [0.598]
 [0.604]
 [0.586]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
Printing some Q and Qe and total Qs values:  [[0.773]
 [0.773]
 [0.704]
 [0.773]
 [0.773]
 [0.708]
 [0.798]] [[3.433]
 [3.433]
 [3.965]
 [3.433]
 [3.433]
 [4.365]
 [2.457]] [[1.408]
 [1.408]
 [1.579]
 [1.408]
 [1.408]
 [1.794]
 [0.935]]
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.4  ]
 [0.4  ]
 [0.37 ]
 [0.372]
 [0.388]
 [0.379]] [[4.363]
 [4.513]
 [4.513]
 [4.966]
 [4.697]
 [4.493]
 [4.369]] [[1.179]
 [1.294]
 [1.294]
 [1.536]
 [1.359]
 [1.257]
 [1.156]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[3.969]
 [3.969]
 [3.969]
 [3.969]
 [3.969]
 [3.969]
 [3.969]] [[0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
siam score:  -0.65138
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
siam score:  -0.6485331
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]] [[-4.479]
 [-9.45 ]
 [-9.45 ]
 [-9.45 ]
 [-9.45 ]
 [-9.45 ]
 [-9.45 ]] [[0.464]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]]
siam score:  -0.64909685
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
siam score:  -0.64949846
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[-0.125]
 [-2.763]
 [-2.763]
 [-2.763]
 [-2.763]
 [-2.763]
 [-2.763]] [[0.547]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
using another actor
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
2667 4112
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
using another actor
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.573026185834504
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus -0.19783542073627164
line 256 mcts: sample exp_bonus 0.11125957281424517
siam score:  -0.651367
line 256 mcts: sample exp_bonus -2.0078841275724253
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.468]
 [0.536]
 [0.538]
 [0.526]
 [0.571]
 [0.612]] [[-2.149]
 [-1.953]
 [-2.722]
 [-2.545]
 [-2.592]
 [-2.405]
 [-2.649]] [[0.739]
 [0.468]
 [0.536]
 [0.538]
 [0.526]
 [0.571]
 [0.612]]
using another actor
siam score:  -0.648991
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [ 1.238]] [[-0.397]
 [-0.397]
 [-0.397]
 [-0.397]
 [-0.397]
 [-0.397]
 [-0.399]] [[-0.282]
 [-0.282]
 [-0.282]
 [-0.282]
 [-0.282]
 [-0.282]
 [ 2.194]]
first move QE:  -0.406199009832201
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.549]] [[0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.748]] [[1.981]
 [1.981]
 [1.981]
 [1.981]
 [1.981]
 [1.981]
 [2.   ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[-7.913]
 [-7.619]
 [-7.619]
 [-7.619]
 [-7.619]
 [-7.619]
 [-7.619]] [[0.314]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.251]
 [0.243]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[-6.075]
 [-8.136]
 [-6.848]
 [-8.136]
 [-8.136]
 [-8.136]
 [-8.136]] [[0.313]
 [0.251]
 [0.243]
 [0.251]
 [0.251]
 [0.251]
 [0.251]]
using another actor
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.171662472738555, 0.41020392475921347, 0.229034573944395, 0.1625749070303322, 0.025950281964875015, 0.0005738395626295076]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]] [[-7.245]
 [-7.267]
 [-7.267]
 [-7.267]
 [-7.267]
 [-7.267]
 [-7.267]] [[0.263]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]]
from probs:  [0.171662472738555, 0.41020392475921347, 0.229034573944395, 0.1625749070303322, 0.025950281964875015, 0.0005738395626295076]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.171662472738555, 0.41020392475921347, 0.229034573944395, 0.1625749070303322, 0.025950281964875015, 0.0005738395626295076]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[-5.495]
 [-6.84 ]
 [-6.84 ]
 [-6.84 ]
 [-6.84 ]
 [-6.84 ]
 [-6.84 ]] [[0.381]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]]
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[-7.168]
 [-6.637]
 [-6.637]
 [-6.637]
 [-6.637]
 [-6.637]
 [-6.637]] [[0.281]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.832]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[-3.982]
 [-3.982]
 [-2.713]
 [-3.982]
 [-3.982]
 [-3.982]
 [-3.982]] [[0.478]
 [0.478]
 [0.832]
 [0.478]
 [0.478]
 [0.478]
 [0.478]]
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.554]
 [0.571]
 [0.554]
 [0.554]
 [0.554]
 [0.554]] [[-3.063]
 [-3.063]
 [-2.897]
 [-3.063]
 [-3.063]
 [-3.063]
 [-3.063]] [[0.554]
 [0.554]
 [0.571]
 [0.554]
 [0.554]
 [0.554]
 [0.554]]
using explorer policy with actor:  0
UNIT TEST: sample policy line 217 mcts : [0.143 0.163 0.102 0.306 0.082 0.102 0.102]
from probs:  [0.17166247273855498, 0.4102039247592135, 0.22903457394439497, 0.16257490703033217, 0.025950281964875012, 0.0005738395626295075]
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[-1.208]
 [-1.617]
 [-1.617]
 [-1.617]
 [-1.617]
 [-1.617]
 [-1.617]] [[0.132]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]]
first move QE:  -0.4099783424344297
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.641]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.62 ]] [[4.487]
 [4.082]
 [4.487]
 [4.487]
 [4.487]
 [4.487]
 [4.195]] [[1.477]
 [1.22 ]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.252]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.5410659609196329
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]] [[-2.449]
 [-4.638]
 [-4.638]
 [-4.638]
 [-4.638]
 [-4.638]
 [-4.638]] [[0.171]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]]
2682 4149
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.555]
 [0.584]
 [0.336]
 [0.555]
 [0.555]
 [0.442]] [[ 0.528]
 [ 0.339]
 [-3.419]
 [ 0.475]
 [ 0.339]
 [ 0.339]
 [-0.169]] [[1.732]
 [1.636]
 [0.157]
 [1.516]
 [1.636]
 [1.636]
 [1.343]]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.526]
 [0.487]
 [0.586]
 [0.487]
 [0.487]
 [0.596]] [[0.867]
 [0.816]
 [0.867]
 [0.375]
 [0.867]
 [0.867]
 [0.815]] [[1.231]
 [1.257]
 [1.231]
 [0.936]
 [1.231]
 [1.231]
 [1.397]]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.211]
 [0.237]
 [0.251]
 [0.403]
 [0.223]
 [0.298]] [[-2.201]
 [-1.947]
 [-3.293]
 [-3.032]
 [ 0.   ]
 [-3.323]
 [-2.933]] [[0.46 ]
 [0.211]
 [0.237]
 [0.251]
 [0.403]
 [0.223]
 [0.298]]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.889]
 [0.566]
 [0.595]
 [0.569]
 [0.58 ]
 [0.571]] [[-2.822]
 [-2.201]
 [-2.978]
 [-2.848]
 [-3.138]
 [-2.991]
 [-2.919]] [[ 0.018]
 [ 0.888]
 [-0.016]
 [ 0.084]
 [-0.065]
 [ 0.008]
 [ 0.013]]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.874]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.664]] [[ 0.055]
 [ 1.015]
 [ 0.055]
 [ 0.055]
 [ 0.055]
 [ 0.055]
 [-0.163]] [[0.968]
 [1.697]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.883]]
siam score:  -0.6320378
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[4.88]
 [4.88]
 [4.88]
 [4.88]
 [4.88]
 [4.88]
 [4.88]] [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.621]
 [1.205]
 [0.645]
 [0.592]
 [0.579]
 [0.591]] [[-2.939]
 [-2.202]
 [-0.85 ]
 [-2.626]
 [-3.044]
 [-2.932]
 [-2.627]] [[0.258]
 [0.556]
 [2.133]
 [0.464]
 [0.225]
 [0.236]
 [0.358]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.171662472738555, 0.41020392475921347, 0.229034573944395, 0.1625749070303322, 0.025950281964875015, 0.0005738395626295076]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.171662472738555, 0.41020392475921347, 0.229034573944395, 0.1625749070303322, 0.025950281964875015, 0.0005738395626295076]
actor:  1 policy actor:  1  step number:  61 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  0.167
using another actor
siam score:  -0.62124777
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19083843539287618, 0.40070772920732955, 0.22373243768717838, 0.15881130796214774, 0.025349534538312405, 0.0005605552121559327]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19083843539287618, 0.40070772920732955, 0.22373243768717838, 0.15881130796214774, 0.025349534538312405, 0.0005605552121559327]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.345]
 [0.62 ]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[0.406]
 [0.878]
 [1.047]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[1.534]
 [1.588]
 [1.818]
 [1.534]
 [1.534]
 [1.534]
 [1.534]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
actor:  1 policy actor:  1  step number:  43 total reward:  0.7199999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.489]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.482]] [[3.547]
 [3.864]
 [3.547]
 [3.547]
 [3.547]
 [3.547]
 [3.797]] [[1.296]
 [1.608]
 [1.296]
 [1.296]
 [1.296]
 [1.296]
 [1.544]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.18473290873196516, 0.4198809891927713, 0.21657452759219406, 0.15373043065972758, 0.024538522549837842, 0.0005426212735041886]
siam score:  -0.6366284
Printing some Q and Qe and total Qs values:  [[0.173]
 [0.098]
 [0.104]
 [0.098]
 [0.098]
 [0.098]
 [0.098]] [[ 1.567]
 [-3.121]
 [-2.637]
 [-3.121]
 [-3.121]
 [-3.121]
 [-3.121]] [[0.173]
 [0.098]
 [0.104]
 [0.098]
 [0.098]
 [0.098]
 [0.098]]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.298]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[-1.063]
 [-0.321]
 [-1.629]
 [-1.629]
 [-1.629]
 [-1.629]
 [-1.629]] [[0.36 ]
 [0.298]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]] [[-4.813]
 [-6.903]
 [-6.903]
 [-6.903]
 [-6.903]
 [-6.903]
 [-6.903]] [[0.082]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.386]
 [0.683]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[-4.234]
 [-4.234]
 [-2.159]
 [-4.234]
 [-4.234]
 [-4.234]
 [-4.234]] [[0.386]
 [0.386]
 [0.683]
 [0.386]
 [0.386]
 [0.386]
 [0.386]]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.568]
 [0.582]
 [0.586]
 [0.601]
 [0.585]
 [0.581]] [[-0.703]
 [-0.301]
 [-0.751]
 [-0.314]
 [-0.844]
 [-0.85 ]
 [-0.149]] [[1.258]
 [1.48 ]
 [1.19 ]
 [1.486]
 [1.143]
 [1.126]
 [1.593]]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.381]
 [0.655]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[-4.308]
 [-4.308]
 [-5.794]
 [-4.308]
 [-4.308]
 [-4.308]
 [-4.308]] [[0.381]
 [0.381]
 [0.655]
 [0.381]
 [0.381]
 [0.381]
 [0.381]]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.454]
 [0.527]
 [0.55 ]
 [0.397]
 [0.527]
 [0.527]] [[ 0.585]
 [ 0.496]
 [ 0.503]
 [-1.66 ]
 [-0.204]
 [ 0.503]
 [ 0.503]] [[2.002]
 [1.687]
 [1.838]
 [0.445]
 [1.108]
 [1.838]
 [1.838]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]] [[-7.03 ]
 [-8.012]
 [-8.012]
 [-8.012]
 [-8.012]
 [-8.012]
 [-8.012]] [[0.425]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.566]
 [0.43 ]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[0.875]
 [0.875]
 [0.826]
 [0.875]
 [0.875]
 [0.875]
 [0.875]] [[0.517]
 [0.517]
 [0.196]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[-6.535]
 [-7.171]
 [-7.171]
 [-7.171]
 [-7.171]
 [-7.171]
 [-7.171]] [[0.519]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.18473290873196516, 0.4198809891927713, 0.21657452759219406, 0.15373043065972758, 0.024538522549837842, 0.0005426212735041886]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.18473290873196516, 0.4198809891927713, 0.21657452759219406, 0.15373043065972758, 0.024538522549837842, 0.0005426212735041886]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.824]
 [0.835]
 [0.824]
 [0.824]
 [0.824]
 [0.831]
 [0.779]] [[-0.374]
 [-0.194]
 [-0.374]
 [-0.374]
 [-0.374]
 [-0.542]
 [-1.153]] [[1.899]
 [1.969]
 [1.899]
 [1.899]
 [1.899]
 [1.843]
 [1.598]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
line 256 mcts: sample exp_bonus 3.043144111531493
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.18473290873196516, 0.4198809891927713, 0.21657452759219406, 0.15373043065972758, 0.024538522549837842, 0.0005426212735041886]
actor:  1 policy actor:  1  step number:  77 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.333
2703 4188
siam score:  -0.6466396
actor:  1 policy actor:  1  step number:  98 total reward:  0.0849999999999993  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1788854433591368, 0.4259412814454273, 0.2220217344184764, 0.1488643059600021, 0.02376178947127881, 0.000525445345678684]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1788854433591368, 0.4259412814454273, 0.2220217344184764, 0.1488643059600021, 0.02376178947127881, 0.000525445345678684]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[-7.846]
 [-7.846]
 [-7.846]
 [-7.846]
 [-7.846]
 [-7.846]
 [-7.846]] [[0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
actor:  1 policy actor:  1  step number:  57 total reward:  0.4499999999999996  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17503416049036732, 0.4167710531164563, 0.23877108143210574, 0.14565935791867937, 0.023250214180391013, 0.0005141328620003768]
Printing some Q and Qe and total Qs values:  [[0.806]
 [0.783]
 [0.806]
 [0.771]
 [0.774]
 [0.777]
 [0.779]] [[2.72 ]
 [3.265]
 [2.72 ]
 [2.627]
 [2.808]
 [2.742]
 [2.81 ]] [[1.377]
 [1.872]
 [1.377]
 [1.214]
 [1.401]
 [1.34 ]
 [1.412]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17503416049036732, 0.4167710531164563, 0.23877108143210574, 0.14565935791867937, 0.023250214180391013, 0.0005141328620003768]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.645]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[-3.422]
 [-3.422]
 [-2.85 ]
 [-3.422]
 [-3.422]
 [-3.422]
 [-3.422]] [[0.572]
 [0.572]
 [0.645]
 [0.572]
 [0.572]
 [0.572]
 [0.572]]
siam score:  -0.6423109
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.646]
 [0.599]
 [0.578]
 [0.572]
 [0.574]
 [0.582]] [[-2.668]
 [-1.633]
 [-2.676]
 [-2.858]
 [-2.94 ]
 [-3.034]
 [-2.82 ]] [[0.612]
 [0.646]
 [0.599]
 [0.578]
 [0.572]
 [0.574]
 [0.582]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.17503416049036732, 0.4167710531164563, 0.23877108143210574, 0.14565935791867937, 0.023250214180391013, 0.0005141328620003768]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1769156265067832, 0.42125098189645926, 0.23058852816943967, 0.14722507018381192, 0.023500133897389357, 0.0005196593461167706]
start point for exploration sampling:  10749
actor:  1 policy actor:  1  step number:  45 total reward:  0.5899999999999997  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.76 ]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[-4.214]
 [-1.359]
 [-4.214]
 [-4.214]
 [-4.214]
 [-4.214]
 [-4.214]] [[0.347]
 [1.354]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]]
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.688]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]] [[1.635]
 [2.104]
 [1.635]
 [1.635]
 [1.635]
 [1.635]
 [1.635]] [[1.375]
 [1.675]
 [1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.375]]
from probs:  [0.19791055720664777, 0.4105058682033988, 0.22470676157902947, 0.14346970773819345, 0.022900701204335933, 0.0005064040683948099]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19791055720664777, 0.4105058682033988, 0.22470676157902947, 0.14346970773819345, 0.022900701204335933, 0.0005064040683948099]
2713 4209
siam score:  -0.6483384
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19791055720664777, 0.4105058682033988, 0.22470676157902947, 0.14346970773819345, 0.022900701204335933, 0.0005064040683948099]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]] [[-2.735]
 [-5.659]
 [-5.659]
 [-5.659]
 [-5.659]
 [-5.659]
 [-5.659]] [[0.546]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.594]] [[0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.106]] [[2.056]
 [2.056]
 [2.056]
 [2.056]
 [2.056]
 [2.056]
 [1.928]]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.632]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[0.421]
 [0.676]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[1.185]
 [1.276]
 [1.162]
 [1.162]
 [1.162]
 [1.162]
 [1.162]]
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]] [[-2.376]
 [-3.063]
 [-3.063]
 [-3.063]
 [-3.063]
 [-3.063]
 [-3.063]] [[0.259]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]]
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]] [[-5.668]
 [-5.986]
 [-5.986]
 [-5.986]
 [-5.986]
 [-5.986]
 [-5.986]] [[0.109]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
from probs:  [0.19791055720664777, 0.4105058682033988, 0.22470676157902947, 0.14346970773819345, 0.022900701204335933, 0.0005064040683948099]
line 256 mcts: sample exp_bonus 0.24473604619530112
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19791055720664777, 0.4105058682033988, 0.22470676157902947, 0.14346970773819345, 0.022900701204335933, 0.0005064040683948099]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.831]
 [0.792]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.814]] [[1.853]
 [2.226]
 [1.559]
 [1.853]
 [1.853]
 [1.853]
 [2.147]] [[1.877]
 [1.993]
 [1.81 ]
 [1.877]
 [1.877]
 [1.877]
 [1.963]]
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.865]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]] [[-1.859]
 [-0.578]
 [-1.859]
 [-1.859]
 [-1.859]
 [-1.859]
 [-1.859]] [[0.968]
 [1.394]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]] [[2.738]
 [2.738]
 [2.738]
 [2.738]
 [2.738]
 [2.738]
 [2.738]] [[0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]]
siam score:  -0.64209455
actor:  1 policy actor:  1  step number:  59 total reward:  0.4299999999999996  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 1.9398492691936577
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19391141967227712, 0.42241765371920365, 0.22016615870699935, 0.1405706451446709, 0.022437951490312136, 0.0004961712665369429]
line 256 mcts: sample exp_bonus -1.9242456130490535
actor:  1 policy actor:  1  step number:  118 total reward:  0.06499999999999928  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.771]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.785]] [[0.563]
 [0.509]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [1.007]] [[2.088]
 [2.11 ]
 [2.088]
 [2.088]
 [2.088]
 [2.088]
 [2.272]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.751]
 [0.537]
 [0.528]
 [0.588]
 [0.524]
 [0.533]] [[0.338]
 [2.164]
 [1.392]
 [1.371]
 [1.86 ]
 [1.776]
 [1.703]] [[0.443]
 [1.994]
 [1.053]
 [1.02 ]
 [1.467]
 [1.283]
 [1.251]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19176524931900432, 0.4177424249645311, 0.2177294064857558, 0.15008262646707135, 0.022189613015156484, 0.0004906797484809817]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[-4.026]
 [-4.3  ]
 [-4.3  ]
 [-4.3  ]
 [-4.3  ]
 [-4.3  ]
 [-4.3  ]] [[0.64 ]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]]
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[-6.898]
 [-9.399]
 [-9.399]
 [-9.399]
 [-9.399]
 [-9.399]
 [-9.399]] [[0.14 ]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]]
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[-6.057]
 [-8.123]
 [-8.123]
 [-8.123]
 [-8.123]
 [-8.123]
 [-8.123]] [[0.07 ]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[-1.687]
 [-4.412]
 [-4.412]
 [-3.974]
 [-4.412]
 [-4.412]
 [-4.412]] [[0.721]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19176524931900432, 0.4177424249645311, 0.2177294064857558, 0.15008262646707135, 0.022189613015156484, 0.0004906797484809817]
from probs:  [0.19176524931900432, 0.4177424249645311, 0.2177294064857558, 0.15008262646707135, 0.022189613015156484, 0.0004906797484809817]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19176524931900432, 0.4177424249645311, 0.2177294064857558, 0.15008262646707135, 0.022189613015156484, 0.0004906797484809817]
line 256 mcts: sample exp_bonus 0.9149484470815041
from probs:  [0.19176524931900432, 0.41774242496453107, 0.21772940648575584, 0.15008262646707135, 0.02218961301515648, 0.0004906797484809816]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.57 ]
 [0.253]
 [0.579]
 [0.414]
 [0.572]
 [0.253]] [[-1.089]
 [-1.61 ]
 [ 0.   ]
 [-4.554]
 [-1.555]
 [-4.47 ]
 [ 0.   ]] [[1.786]
 [1.565]
 [2.205]
 [0.113]
 [1.515]
 [0.151]
 [2.205]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
siam score:  -0.6341748
2730 4248
first move QE:  -0.430555776581334
using explorer policy with actor:  1
first move QE:  -0.4306696282702511
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[-1.209]
 [-1.659]
 [-1.659]
 [-1.659]
 [-1.659]
 [-1.659]
 [-1.659]] [[2.043]
 [1.91 ]
 [1.91 ]
 [1.91 ]
 [1.91 ]
 [1.91 ]
 [1.91 ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]] [[-3.308]
 [-6.236]
 [-6.236]
 [-6.236]
 [-6.236]
 [-6.236]
 [-6.236]] [[0.166]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]]
siam score:  -0.6375561
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -7.132768749862471
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19176524931900432, 0.41774242496453107, 0.21772940648575584, 0.15008262646707135, 0.02218961301515648, 0.0004906797484809816]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.398]
 [0.219]
 [0.226]
 [0.232]
 [0.224]
 [0.224]] [[-7.073]
 [ 0.   ]
 [-9.214]
 [-9.194]
 [-9.372]
 [-8.974]
 [-8.872]] [[0.398]
 [0.398]
 [0.219]
 [0.226]
 [0.232]
 [0.224]
 [0.224]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19176524931900432, 0.41774242496453107, 0.21772940648575584, 0.15008262646707135, 0.02218961301515648, 0.0004906797484809816]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19176524931900432, 0.41774242496453107, 0.21772940648575584, 0.15008262646707135, 0.02218961301515648, 0.0004906797484809816]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.559]
 [0.566]
 [0.566]
 [0.566]
 [0.57 ]
 [0.563]] [[-1.769]
 [-0.104]
 [-1.769]
 [-1.769]
 [-1.769]
 [-2.146]
 [-2.095]] [[0.566]
 [0.559]
 [0.566]
 [0.566]
 [0.566]
 [0.57 ]
 [0.563]]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.239]] [[ 0.364]
 [-0.449]
 [-0.449]
 [-0.449]
 [-0.449]
 [-0.449]
 [-0.167]] [[1.476]
 [1.224]
 [1.224]
 [1.224]
 [1.224]
 [1.224]
 [1.217]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[1.104]
 [1.145]
 [1.108]
 [1.11 ]
 [1.108]
 [1.111]
 [1.128]] [[-0.356]
 [-0.347]
 [-0.383]
 [-0.399]
 [-0.383]
 [-0.347]
 [-0.269]] [[0.956]
 [1.033]
 [0.942]
 [0.931]
 [0.942]
 [0.976]
 [1.069]]
from probs:  [0.19176524931900432, 0.41774242496453107, 0.21772940648575584, 0.15008262646707135, 0.02218961301515648, 0.0004906797484809816]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[-2.962]
 [-2.962]
 [-2.962]
 [-2.962]
 [-2.962]
 [-2.962]
 [-2.962]] [[0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19176524931900432, 0.41774242496453107, 0.21772940648575584, 0.15008262646707135, 0.02218961301515648, 0.0004906797484809816]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[-0.442]
 [-0.448]
 [-0.448]
 [-0.448]
 [-0.448]
 [-0.448]
 [-0.448]] [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.19176524931900432, 0.41774242496453107, 0.21772940648575584, 0.15008262646707135, 0.02218961301515648, 0.0004906797484809816]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.383]
 [0.492]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[-2.4  ]
 [-2.523]
 [-1.397]
 [-2.4  ]
 [-2.4  ]
 [-2.4  ]
 [-2.4  ]] [[0.388]
 [0.383]
 [0.492]
 [0.388]
 [0.388]
 [0.388]
 [0.388]]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.284]
 [0.613]
 [0.342]
 [0.336]
 [0.441]
 [0.328]] [[-3.829]
 [-3.33 ]
 [-5.103]
 [-3.772]
 [-3.722]
 [-4.223]
 [-3.568]] [[0.309]
 [0.284]
 [0.613]
 [0.342]
 [0.336]
 [0.441]
 [0.328]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.5449999999999997  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.382]
 [0.642]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[-4.187]
 [-4.187]
 [-4.784]
 [-4.187]
 [-4.187]
 [-4.187]
 [-4.187]] [[0.382]
 [0.382]
 [0.642]
 [0.382]
 [0.382]
 [0.382]
 [0.382]]
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.187]
 [0.324]
 [0.223]
 [0.223]
 [0.223]
 [0.223]] [[1.202]
 [1.094]
 [0.441]
 [1.202]
 [1.202]
 [1.202]
 [1.202]] [[0.223]
 [0.187]
 [0.324]
 [0.223]
 [0.223]
 [0.223]
 [0.223]]
Printing some Q and Qe and total Qs values:  [[0.946]
 [0.666]
 [0.641]
 [0.613]
 [0.739]
 [0.689]
 [0.641]] [[2.886]
 [2.561]
 [1.401]
 [2.087]
 [1.746]
 [2.496]
 [2.614]] [[1.27 ]
 [0.6  ]
 [0.162]
 [0.336]
 [0.474]
 [0.625]
 [0.568]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.1873926649425639, 0.4082171642237926, 0.23556654949000808, 0.14666047907594845, 0.021683650878982746, 0.0004794913887043125]
actor:  1 policy actor:  1  step number:  44 total reward:  0.7049999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.20935611834013731, 0.3971837188243794, 0.22919956914348527, 0.14269648507973046, 0.02109757709522084, 0.0004665315170467418]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]] [[-2.97 ]
 [-4.847]
 [-4.847]
 [-4.847]
 [-4.847]
 [-4.847]
 [-4.847]] [[0.347]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.648]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[-1.071]
 [ 0.246]
 [-1.071]
 [-1.071]
 [-1.071]
 [-1.071]
 [-1.071]] [[0.59 ]
 [0.648]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.505]
 [0.517]
 [0.477]
 [0.517]
 [0.517]
 [0.496]] [[5.364]
 [5.168]
 [5.364]
 [4.669]
 [5.364]
 [5.364]
 [4.81 ]] [[0.517]
 [0.505]
 [0.517]
 [0.477]
 [0.517]
 [0.517]
 [0.496]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[-9.336]
 [-8.328]
 [-8.328]
 [-8.328]
 [-8.328]
 [-8.328]
 [-8.328]] [[0.4  ]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]]
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]] [[-3.022]
 [-3.407]
 [-3.407]
 [-3.407]
 [-3.407]
 [-3.407]
 [-3.407]] [[0.102]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.866]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[-2.11 ]
 [-0.454]
 [-2.11 ]
 [-2.11 ]
 [-2.11 ]
 [-2.11 ]
 [-2.11 ]] [[1.096]
 [1.863]
 [1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.096]]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.861]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]] [[-2.424]
 [ 0.523]
 [-2.424]
 [-2.424]
 [-2.424]
 [-2.424]
 [-2.424]] [[0.373]
 [1.386]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]]
using explorer policy with actor:  1
first move QE:  -0.44417962603039507
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.20935611834013731, 0.3971837188243794, 0.22919956914348527, 0.14269648507973046, 0.02109757709522084, 0.0004665315170467418]
in main func line 156:  2747
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.695]
 [0.695]
 [0.696]
 [0.695]
 [0.695]
 [0.695]] [[2.995]
 [2.995]
 [2.995]
 [1.713]
 [2.995]
 [2.995]
 [2.995]] [[1.384]
 [1.384]
 [1.384]
 [0.783]
 [1.384]
 [1.384]
 [1.384]]
Printing some Q and Qe and total Qs values:  [[0.944]
 [0.934]
 [0.742]
 [0.706]
 [0.699]
 [0.678]
 [0.73 ]] [[1.825]
 [2.212]
 [1.945]
 [3.299]
 [3.513]
 [3.815]
 [2.931]] [[1.01 ]
 [1.184]
 [0.925]
 [1.535]
 [1.63 ]
 [1.758]
 [1.379]]
actor:  1 policy actor:  1  step number:  75 total reward:  0.3299999999999995  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
2750 4298
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.20597198173096495, 0.39076344329530693, 0.2254946731084679, 0.15655436624898406, 0.020756545347121774, 0.00045899026915445176]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
actor:  1 policy actor:  1  step number:  54 total reward:  0.49499999999999966  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -1.131549013463855
using explorer policy with actor:  1
siam score:  -0.6281785
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
actor:  1 policy actor:  1  step number:  71 total reward:  0.34999999999999953  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2146655258146843, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
using another actor
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2146655258146843, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.417]
 [0.599]
 [0.632]
 [0.618]
 [0.638]
 [0.647]] [[-0.84 ]
 [-0.582]
 [-1.091]
 [-1.167]
 [-1.195]
 [-1.07 ]
 [-0.95 ]] [[0.572]
 [0.277]
 [0.472]
 [0.513]
 [0.475]
 [0.557]
 [0.615]]
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.787]
 [1.058]
 [0.787]
 [0.787]
 [0.787]
 [0.787]] [[-0.704]
 [-0.704]
 [-0.369]
 [-0.704]
 [-0.704]
 [-0.704]
 [-0.704]] [[1.081]
 [1.081]
 [1.734]
 [1.081]
 [1.081]
 [1.081]
 [1.081]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2146655258146843, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[-5.13]
 [-5.13]
 [-5.13]
 [-5.13]
 [-5.13]
 [-5.13]
 [-5.13]] [[0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]]
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.564]
 [0.749]
 [0.714]
 [0.561]
 [0.561]
 [0.978]] [[-0.112]
 [-0.406]
 [-3.754]
 [-0.952]
 [ 0.346]
 [ 0.346]
 [-0.064]] [[1.613]
 [1.777]
 [1.254]
 [1.759]
 [1.916]
 [1.916]
 [2.074]]
line 256 mcts: sample exp_bonus -0.5544354407264652
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.21466552581468426, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
line 256 mcts: sample exp_bonus 2.311803154542887
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.332]
 [0.281]
 [0.282]
 [0.281]
 [0.284]
 [0.281]] [[3.469]
 [3.038]
 [3.173]
 [3.864]
 [3.173]
 [3.421]
 [3.173]] [[1.088]
 [0.753]
 [0.806]
 [1.445]
 [0.806]
 [1.039]
 [0.806]]
from probs:  [0.21466552581468426, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
using explorer policy with actor:  1
from probs:  [0.21466552581468426, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.21466552581468426, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.616]
 [0.565]
 [0.583]
 [0.552]
 [0.552]
 [0.548]] [[0.753]
 [0.464]
 [0.129]
 [0.388]
 [0.283]
 [0.283]
 [0.362]] [[1.52 ]
 [1.823]
 [1.336]
 [1.675]
 [1.494]
 [1.494]
 [1.582]]
from probs:  [0.21466552581468426, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
siam score:  -0.6271668
first move QE:  -0.4501479444870835
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.709]
 [0.778]
 [0.778]
 [0.772]
 [0.736]
 [0.772]] [[ 1.285]
 [-0.402]
 [ 3.008]
 [-2.193]
 [ 1.285]
 [ 0.561]
 [ 1.285]] [[1.364]
 [0.737]
 [1.964]
 [0.166]
 [1.364]
 [1.089]
 [1.364]]
from probs:  [0.21466552581468426, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
from probs:  [0.21466552581468426, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
using another actor
from probs:  [0.21466552581468426, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.251]
 [0.264]
 [0.266]
 [0.256]
 [0.256]
 [0.247]] [[-0.784]
 [-0.315]
 [-4.651]
 [-4.097]
 [ 0.   ]
 [ 0.   ]
 [-2.13 ]] [[0.071]
 [0.251]
 [0.264]
 [0.266]
 [0.256]
 [0.256]
 [0.247]]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.41 ]
 [0.407]
 [0.388]
 [0.381]
 [0.395]
 [0.394]] [[-5.32 ]
 [-4.894]
 [-5.54 ]
 [-5.128]
 [-5.233]
 [-5.519]
 [-5.418]] [[0.408]
 [0.41 ]
 [0.407]
 [0.388]
 [0.381]
 [0.395]
 [0.394]]
siam score:  -0.6327706
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.21466552581468426, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.21466552581468426, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.634]
 [0.634]
 [0.632]
 [0.633]
 [0.631]
 [0.63 ]] [[2.607]
 [2.607]
 [2.479]
 [2.133]
 [2.223]
 [2.699]
 [2.4  ]] [[1.435]
 [1.435]
 [1.306]
 [0.957]
 [1.05 ]
 [1.522]
 [1.22 ]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.21466552581468426, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
first move QE:  -0.45202486840085276
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.21466552581468426, 0.3767884670606021, 0.2371337372141277, 0.15095547109820243, 0.020014223533457675, 0.0004425752789258321]
actor:  1 policy actor:  1  step number:  145 total reward:  0.1899999999999994  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[-8.284]
 [-8.284]
 [-8.284]
 [-8.284]
 [-8.284]
 [-8.284]
 [-8.284]] [[0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]]
rdn beta is 0 so we're just using the maxi policy
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.375]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]] [[-5.194]
 [-4.53 ]
 [-5.194]
 [-5.194]
 [-5.194]
 [-5.194]
 [-5.194]] [[0.36 ]
 [0.375]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.395]
 [0.387]
 [0.385]
 [0.286]
 [0.379]
 [0.386]] [[-5.463]
 [-5.456]
 [-5.226]
 [-5.47 ]
 [ 0.   ]
 [-5.268]
 [-5.273]] [[0.362]
 [0.395]
 [0.387]
 [0.385]
 [0.286]
 [0.379]
 [0.386]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.675]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[-3.701]
 [-2.904]
 [-3.701]
 [-3.701]
 [-3.701]
 [-3.701]
 [-3.701]] [[0.552]
 [0.675]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.722]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.619]] [[-2.367]
 [-1.997]
 [-2.367]
 [-2.367]
 [-2.367]
 [-2.367]
 [-2.255]] [[0.586]
 [0.722]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.619]]
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.48 ]
 [0.475]
 [0.572]
 [0.638]
 [0.532]
 [0.491]] [[ 0.554]
 [-0.939]
 [-0.593]
 [-3.233]
 [ 0.31 ]
 [ 0.228]
 [-1.274]] [[0.493]
 [0.48 ]
 [0.475]
 [0.572]
 [0.638]
 [0.532]
 [0.491]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.459]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]] [[-3.309]
 [-2.729]
 [-3.309]
 [-3.309]
 [-3.309]
 [-3.309]
 [-3.309]] [[0.49 ]
 [0.459]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]]
line 256 mcts: sample exp_bonus -2.852496286107097
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.495]
 [0.496]
 [0.538]
 [0.582]
 [0.617]
 [0.593]] [[-4.285]
 [-3.235]
 [-4.217]
 [-4.071]
 [-3.696]
 [-3.165]
 [-3.546]] [[0.486]
 [0.495]
 [0.496]
 [0.538]
 [0.582]
 [0.617]
 [0.593]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.21202139078864576, 0.37214738843662976, 0.246530315439532, 0.14909608241905567, 0.019767699042564504, 0.0004371238735721993]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.495]
 [0.795]
 [0.495]
 [0.495]
 [0.495]
 [0.495]] [[-4.766]
 [-4.766]
 [-0.946]
 [-4.766]
 [-4.766]
 [-4.766]
 [-4.766]] [[0.495]
 [0.495]
 [0.795]
 [0.495]
 [0.495]
 [0.495]
 [0.495]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.21202139078864576, 0.37214738843662976, 0.246530315439532, 0.14909608241905567, 0.019767699042564504, 0.0004371238735721993]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.512]
 [0.812]
 [0.512]
 [0.512]
 [0.512]
 [0.512]] [[-4.423]
 [-4.423]
 [-0.754]
 [-4.423]
 [-4.423]
 [-4.423]
 [-4.423]] [[0.512]
 [0.512]
 [0.812]
 [0.512]
 [0.512]
 [0.512]
 [0.512]]
Printing some Q and Qe and total Qs values:  [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]] [[-3.315]
 [-3.315]
 [-3.315]
 [-3.315]
 [-3.315]
 [-3.315]
 [-3.315]] [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]]
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.719]
 [0.874]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[-1.286]
 [-1.009]
 [-0.372]
 [-1.397]
 [-1.397]
 [-1.397]
 [-1.397]] [[0.714]
 [0.719]
 [0.874]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.778]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[-4.696]
 [-4.696]
 [-1.186]
 [-4.696]
 [-4.696]
 [-4.696]
 [-4.696]] [[0.499]
 [0.499]
 [0.778]
 [0.499]
 [0.499]
 [0.499]
 [0.499]]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.497]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[-2.931]
 [-2.565]
 [-2.931]
 [-2.931]
 [-2.931]
 [-2.931]
 [-2.931]] [[0.543]
 [0.497]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.555]
 [0.558]
 [0.585]
 [0.59 ]
 [0.583]
 [0.66 ]] [[-3.638]
 [-1.759]
 [-3.721]
 [-3.609]
 [-3.63 ]
 [-2.817]
 [-2.893]] [[0.567]
 [0.555]
 [0.558]
 [0.585]
 [0.59 ]
 [0.583]
 [0.66 ]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.354]
 [0.339]
 [0.331]
 [0.337]
 [0.324]
 [0.317]] [[0.797]
 [0.742]
 [0.734]
 [0.663]
 [0.737]
 [0.645]
 [1.018]] [[0.333]
 [0.354]
 [0.339]
 [0.331]
 [0.337]
 [0.324]
 [0.317]]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.429]
 [0.699]
 [0.454]
 [0.67 ]
 [0.483]
 [0.477]] [[-5.025]
 [-3.4  ]
 [-4.517]
 [-4.848]
 [ 0.   ]
 [-4.352]
 [-4.365]] [[0.589]
 [0.429]
 [0.699]
 [0.454]
 [0.67 ]
 [0.483]
 [0.477]]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
2769 4342
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.579]
 [0.516]
 [0.579]
 [0.579]
 [0.579]
 [0.655]] [[0.907]
 [0.964]
 [0.851]
 [0.964]
 [0.964]
 [0.964]
 [1.041]] [[0.604]
 [0.579]
 [0.516]
 [0.579]
 [0.579]
 [0.579]
 [0.655]]
using explorer policy with actor:  0
from probs:  [0.21202139078864576, 0.37214738843662976, 0.246530315439532, 0.14909608241905567, 0.019767699042564504, 0.0004371238735721993]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.496]
 [0.487]
 [0.496]
 [0.496]
 [0.496]
 [0.516]] [[-2.77 ]
 [-2.77 ]
 [-2.648]
 [-2.77 ]
 [-2.77 ]
 [-2.77 ]
 [-2.597]] [[0.496]
 [0.496]
 [0.487]
 [0.496]
 [0.496]
 [0.496]
 [0.516]]
main train batch thing paused
add a thread
Adding thread: now have 6 threads
from probs:  [0.21202139078864576, 0.37214738843662976, 0.246530315439532, 0.14909608241905567, 0.019767699042564504, 0.0004371238735721993]
line 256 mcts: sample exp_bonus -1.5740823789664615
siam score:  -0.6275621
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  1
rdn probs:  [0.21202139078864574, 0.37214738843662976, 0.246530315439532, 0.14909608241905567, 0.019767699042564504, 0.0004371238735721993]
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.627]
 [0.525]
 [0.584]
 [0.584]
 [0.817]
 [0.608]] [[ 0.641]
 [ 0.243]
 [ 0.404]
 [ 0.641]
 [ 0.641]
 [ 1.616]
 [-0.077]] [[1.443]
 [1.349]
 [1.351]
 [1.443]
 [1.443]
 [1.822]
 [1.249]]
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]] [[-1.456]
 [-3.368]
 [-3.368]
 [-3.368]
 [-3.368]
 [-3.368]
 [-3.368]] [[0.413]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[-5.55 ]
 [-7.773]
 [-7.773]
 [-7.773]
 [-7.773]
 [-7.773]
 [-7.773]] [[0.279]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.21202139078864574, 0.37214738843662976, 0.246530315439532, 0.14909608241905567, 0.019767699042564504, 0.0004371238735721993]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.21202139078864574, 0.37214738843662976, 0.246530315439532, 0.14909608241905567, 0.019767699042564504, 0.0004371238735721993]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  76 total reward:  0.4349999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.20828741534593956, 0.38320469838302845, 0.2421885924637995, 0.14647030438653905, 0.019419563873234715, 0.0004294255474586504]
from probs:  [0.20828741534593956, 0.38320469838302845, 0.2421885924637995, 0.14647030438653905, 0.019419563873234715, 0.0004294255474586504]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[-5.762]
 [-7.927]
 [-7.927]
 [-7.927]
 [-7.927]
 [-7.927]
 [-7.927]] [[0.209]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]]
line 256 mcts: sample exp_bonus -0.1316664116412339
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]] [[ -7.246]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.23 ]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]] [[-5.688]
 [-9.072]
 [-9.072]
 [-9.072]
 [-9.072]
 [-9.072]
 [-9.072]] [[0.225]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]] [[-5.762]
 [-6.209]
 [-6.209]
 [-6.209]
 [-6.209]
 [-6.209]
 [-6.209]] [[0.199]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]]
Printing some Q and Qe and total Qs values:  [[1.326]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]] [[-1.951]
 [-1.886]
 [-1.886]
 [-1.886]
 [-1.886]
 [-1.886]
 [-1.886]] [[1.838]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]]
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.197]] [[-3.029]
 [-6.227]
 [-6.227]
 [-6.227]
 [-6.227]
 [-6.227]
 [-5.765]] [[0.198]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.197]]
line 256 mcts: sample exp_bonus 0.9706057111871584
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]] [[-4.987]
 [-4.987]
 [-4.987]
 [-4.987]
 [-4.987]
 [-4.987]
 [-4.987]] [[0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.717]
 [0.369]
 [0.381]
 [0.381]
 [0.374]
 [0.357]] [[2.085]
 [0.206]
 [2.335]
 [2.085]
 [2.085]
 [2.656]
 [1.629]] [[0.381]
 [0.717]
 [0.369]
 [0.381]
 [0.381]
 [0.374]
 [0.357]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.20828741534593956, 0.38320469838302845, 0.2421885924637995, 0.14647030438653905, 0.019419563873234715, 0.0004294255474586504]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.20828741534593956, 0.38320469838302845, 0.2421885924637995, 0.14647030438653905, 0.019419563873234715, 0.0004294255474586504]
Printing some Q and Qe and total Qs values:  [[1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]] [[-0.464]
 [-0.464]
 [-0.464]
 [-0.464]
 [-0.464]
 [-0.464]
 [-0.464]] [[1.803]
 [1.803]
 [1.803]
 [1.803]
 [1.803]
 [1.803]
 [1.803]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.492]
 [0.674]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[-2.776]
 [-2.776]
 [-2.619]
 [-2.776]
 [-2.776]
 [-2.776]
 [-2.776]] [[0.492]
 [0.492]
 [0.674]
 [0.492]
 [0.492]
 [0.492]
 [0.492]]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.536]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.752]] [[1.242]
 [1.148]
 [1.242]
 [1.242]
 [1.242]
 [1.242]
 [3.067]] [[1.021]
 [1.049]
 [1.021]
 [1.021]
 [1.021]
 [1.021]
 [1.821]]
Printing some Q and Qe and total Qs values:  [[0.751]
 [1.013]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[-2.683]
 [-1.933]
 [-2.683]
 [-2.683]
 [-2.683]
 [-2.683]
 [-2.683]] [[1.414]
 [1.914]
 [1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[ 1.655]
 [-4.095]
 [-4.095]
 [-4.095]
 [-4.095]
 [-4.095]
 [-4.095]] [[0.635]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]]
siam score:  -0.63083136
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.527]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[2.69 ]
 [1.671]
 [2.69 ]
 [2.69 ]
 [2.69 ]
 [2.69 ]
 [2.69 ]] [[0.524]
 [0.527]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]]
line 256 mcts: sample exp_bonus 4.224556780856712
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.855]
 [0.938]
 [0.904]
 [0.855]
 [0.855]
 [0.857]
 [0.856]] [[1.052]
 [0.85 ]
 [1.033]
 [1.052]
 [1.052]
 [0.743]
 [0.971]] [[0.855]
 [0.938]
 [0.904]
 [0.855]
 [0.855]
 [0.857]
 [0.856]]
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]] [[1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]] [[0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]]
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]] [[-10.]
 [-10.]
 [-10.]
 [-10.]
 [-10.]
 [-10.]
 [-10.]] [[0.069]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
2798 4398
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.767]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[-1.212]
 [ 0.425]
 [-1.212]
 [-1.212]
 [-1.212]
 [-1.212]
 [-1.212]] [[0.919]
 [1.686]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[-4.428]
 [-7.052]
 [-7.052]
 [-7.052]
 [-7.052]
 [-7.052]
 [-7.052]] [[0.396]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.20828741534593956, 0.38320469838302845, 0.2421885924637995, 0.14647030438653905, 0.019419563873234715, 0.0004294255474586504]
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.213]
 [0.152]
 [0.135]
 [0.122]
 [0.125]
 [0.133]] [[-4.813]
 [-2.769]
 [-4.439]
 [-5.152]
 [-4.937]
 [-4.848]
 [-4.372]] [[0.127]
 [0.213]
 [0.152]
 [0.135]
 [0.122]
 [0.125]
 [0.133]]
2801 4405
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.263]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[-3.671]
 [-3.633]
 [-3.671]
 [-3.671]
 [-3.671]
 [-3.671]
 [-3.671]] [[0.191]
 [0.263]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.078]
 [0.134]
 [0.148]
 [0.086]
 [0.122]
 [0.142]] [[-2.955]
 [-0.819]
 [-5.939]
 [-4.957]
 [-1.73 ]
 [-5.45 ]
 [-4.781]] [[0.105]
 [0.078]
 [0.134]
 [0.148]
 [0.086]
 [0.122]
 [0.142]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
2804 4417
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.504]
 [0.598]
 [0.604]
 [0.606]
 [0.57 ]
 [0.57 ]] [[-1.348]
 [ 0.225]
 [-1.431]
 [-1.338]
 [-1.289]
 [ 0.   ]
 [ 0.   ]] [[0.418]
 [0.762]
 [0.398]
 [0.441]
 [0.46 ]
 [0.819]
 [0.819]]
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.457]] [[-0.012]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.337]] [[1.902]
 [1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.722]]
using explorer policy with actor:  1
siam score:  -0.6302661
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[-9.99 ]
 [-9.878]
 [-9.878]
 [-9.878]
 [-9.878]
 [-9.878]
 [-9.878]] [[0.016]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]]
using another actor
line 256 mcts: sample exp_bonus -8.791213566135195
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]] [[1.62]
 [1.62]
 [1.62]
 [1.62]
 [1.62]
 [1.62]
 [1.62]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.602]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]] [[-3.608]
 [-4.156]
 [-3.608]
 [-3.608]
 [-3.608]
 [-3.608]
 [-3.608]] [[0.355]
 [0.602]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.522]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[-2.712]
 [-2.284]
 [-2.712]
 [-2.712]
 [-2.712]
 [-2.712]
 [-2.712]] [[0.418]
 [0.522]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.477]
 [0.452]
 [0.512]
 [0.539]
 [0.51 ]
 [0.446]] [[1.72 ]
 [2.78 ]
 [2.377]
 [2.292]
 [2.74 ]
 [2.266]
 [2.082]] [[0.774]
 [0.993]
 [0.808]
 [0.901]
 [1.105]
 [0.889]
 [0.698]]
Printing some Q and Qe and total Qs values:  [[0.904]
 [1.144]
 [1.038]
 [1.046]
 [1.03 ]
 [0.929]
 [0.989]] [[ 1.091]
 [-0.03 ]
 [ 0.285]
 [ 0.109]
 [ 0.215]
 [ 1.204]
 [ 0.61 ]] [[1.226]
 [1.332]
 [1.225]
 [1.183]
 [1.186]
 [1.313]
 [1.235]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.6099999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]] [[-4.037]
 [-8.577]
 [-8.577]
 [-8.577]
 [-8.577]
 [-8.577]
 [-8.577]] [[0.175]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]]
start point for exploration sampling:  10749
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.381]
 [0.316]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[-4.817]
 [-3.798]
 [-5.522]
 [-5.184]
 [-5.184]
 [-5.184]
 [-5.184]] [[0.316]
 [0.381]
 [0.316]
 [0.299]
 [0.299]
 [0.299]
 [0.299]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2254656875082154, 0.3748900716734343, 0.2369336784500737, 0.14329224861072642, 0.01899820571883214, 0.00042010803871789655]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2254656875082154, 0.3748900716734343, 0.2369336784500737, 0.14329224861072642, 0.01899820571883214, 0.00042010803871789655]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.05 ]
 [0.045]
 [0.043]
 [0.041]
 [0.041]
 [0.041]] [[0.622]
 [0.547]
 [0.973]
 [0.99 ]
 [0.674]
 [0.994]
 [1.015]] [[0.05 ]
 [0.05 ]
 [0.045]
 [0.043]
 [0.041]
 [0.041]
 [0.041]]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.568]] [[0.299]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.73 ]] [[1.846]
 [1.886]
 [1.886]
 [1.886]
 [1.886]
 [1.886]
 [2.043]]
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[ 0.018]
 [-0.285]
 [-0.285]
 [-0.285]
 [-0.285]
 [-0.285]
 [-0.285]] [[1.947]
 [1.87 ]
 [1.87 ]
 [1.87 ]
 [1.87 ]
 [1.87 ]
 [1.87 ]]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.696]
 [0.654]
 [0.696]
 [0.652]
 [0.657]
 [0.696]] [[3.506]
 [3.179]
 [3.521]
 [3.179]
 [3.409]
 [3.518]
 [3.179]] [[1.526]
 [1.294]
 [1.57 ]
 [1.294]
 [1.455]
 [1.571]
 [1.294]]
siam score:  -0.627409
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]] [[ 0.817]
 [-2.732]
 [-2.732]
 [-2.732]
 [-2.732]
 [-2.732]
 [-2.732]] [[0.171]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[-8.708]
 [-9.314]
 [-9.314]
 [-9.314]
 [-9.314]
 [-9.314]
 [-9.314]] [[0.342]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]]
actor:  1 policy actor:  1  step number:  77 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.897]
 [0.949]
 [0.888]
 [0.897]
 [0.897]
 [0.892]
 [0.897]] [[2.061]
 [1.811]
 [1.761]
 [2.061]
 [2.061]
 [2.136]
 [2.061]] [[2.179]
 [2.133]
 [2.015]
 [2.179]
 [2.179]
 [2.211]
 [2.179]]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]] [[-7.202]
 [-8.482]
 [-8.482]
 [-8.482]
 [-8.482]
 [-8.482]
 [-8.482]] [[0.504]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.115]
 [0.191]
 [0.115]
 [0.115]
 [0.115]
 [0.115]] [[-4.34 ]
 [-4.34 ]
 [-4.549]
 [-4.34 ]
 [-4.34 ]
 [-4.34 ]
 [-4.34 ]] [[0.115]
 [0.115]
 [0.191]
 [0.115]
 [0.115]
 [0.115]
 [0.115]]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[-3.208]
 [-3.241]
 [-3.241]
 [-3.241]
 [-3.241]
 [-3.241]
 [-3.241]] [[0.345]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]]
using another actor
from probs:  [0.22193009302637812, 0.3690113089962713, 0.23321825099258542, 0.15672653757874874, 0.018700289206361187, 0.0004135201996551905]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.22193009302637812, 0.3690113089962713, 0.23321825099258542, 0.15672653757874874, 0.018700289206361187, 0.0004135201996551905]
using explorer policy with actor:  1
siam score:  -0.621615
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.609]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.495]] [[-0.888]
 [ 0.154]
 [-0.888]
 [-0.888]
 [-0.888]
 [-0.888]
 [-0.572]] [[1.782]
 [2.148]
 [1.782]
 [1.782]
 [1.782]
 [1.782]
 [1.855]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.492]
 [0.64 ]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[-4.996]
 [-4.996]
 [-4.394]
 [-4.996]
 [-4.996]
 [-4.996]
 [-4.996]] [[0.492]
 [0.492]
 [0.64 ]
 [0.492]
 [0.492]
 [0.492]
 [0.492]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.534]
 [0.644]
 [0.534]
 [0.534]
 [0.534]
 [0.534]] [[-4.233]
 [-4.233]
 [-4.297]
 [-4.233]
 [-4.233]
 [-4.233]
 [-4.233]] [[0.534]
 [0.534]
 [0.644]
 [0.534]
 [0.534]
 [0.534]
 [0.534]]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.687]
 [0.683]
 [0.686]
 [0.686]
 [0.679]
 [0.686]] [[3.206]
 [2.881]
 [3.271]
 [3.206]
 [3.206]
 [3.271]
 [3.206]] [[1.464]
 [1.249]
 [1.502]
 [1.464]
 [1.464]
 [1.494]
 [1.464]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.22193009302637812, 0.3690113089962713, 0.23321825099258542, 0.15672653757874874, 0.018700289206361187, 0.0004135201996551905]
actor:  1 policy actor:  1  step number:  58 total reward:  0.6249999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.21720417820976937, 0.3824479671946377, 0.22825195925251737, 0.15338910705727746, 0.01830207383759321, 0.00040471444820495706]
line 256 mcts: sample exp_bonus 4.177831875800174
actor:  1 policy actor:  1  step number:  106 total reward:  0.2949999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.506]
 [0.696]
 [0.694]
 [0.506]
 [0.506]
 [0.506]] [[0.346]
 [1.256]
 [0.97 ]
 [0.484]
 [1.256]
 [1.256]
 [1.256]] [[0.826]
 [1.354]
 [1.307]
 [0.914]
 [1.354]
 [1.354]
 [1.354]]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]] [[-4.55 ]
 [-6.192]
 [-6.192]
 [-6.192]
 [-6.192]
 [-6.192]
 [-6.192]] [[0.315]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]]
2831 4460
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.896]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[ 0.261]
 [-0.486]
 [ 0.261]
 [ 0.261]
 [ 0.261]
 [ 0.261]
 [ 0.261]] [[0.294]
 [0.896]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]] [[-7.938]
 [-7.904]
 [-7.904]
 [-7.904]
 [-7.904]
 [-7.904]
 [-7.904]] [[0.439]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]] [[-8.041]
 [-7.942]
 [-7.942]
 [-7.942]
 [-7.942]
 [-7.942]
 [-7.942]] [[0.189]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[-0.321]
 [-3.888]
 [-3.888]
 [-3.888]
 [-3.888]
 [-3.888]
 [-3.888]] [[0.727]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[-0.423]
 [-0.423]
 [-0.423]
 [-0.423]
 [-0.423]
 [-0.423]
 [-0.423]] [[1.688]
 [1.688]
 [1.688]
 [1.688]
 [1.688]
 [1.688]
 [1.688]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[-7.113]
 [-7.659]
 [-7.659]
 [-7.659]
 [-7.659]
 [-7.659]
 [-7.659]] [[0.437]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[-0.232]
 [-5.667]
 [-5.667]
 [-5.667]
 [-5.667]
 [-5.667]
 [-5.667]] [[0.472]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]]
UNIT TEST: sample policy line 217 mcts : [0.837 0.061 0.02  0.02  0.02  0.02  0.02 ]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.5487],
        [-0.3782],
        [-0.4550],
        [-0.4756],
        [-0.4096],
        [-0.0000],
        [-0.5545],
        [-0.5582],
        [-0.0000]], dtype=torch.float64)
0.9604485 0.9604485
-0.024259925299500003 -0.5729626663544432
-0.043375785898500004 -0.42161584612441
-0.0727797758985 -0.5277790640965907
-0.024259925299500003 -0.49989691783786777
-0.0530787758985 -0.46271298337961453
-0.9608890648499999 -0.9608890648499999
-0.024259925299500003 -0.5787704270032409
-0.0727797758985 -0.6309678366888131
-0.945846 -0.945846
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2143086356182786, 0.3906805354085083, 0.22520913901280454, 0.1513442813259504, 0.018058089422808243, 0.000399319211649989]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.611]
 [0.611]
 [0.611]
 [0.126]
 [0.611]
 [0.611]] [[ 1.145]
 [ 0.241]
 [ 0.241]
 [ 0.241]
 [-0.473]
 [ 0.241]
 [ 0.241]] [[1.933]
 [1.354]
 [1.354]
 [1.354]
 [0.176]
 [1.354]
 [1.354]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.45]
 [0.45]
 [0.87]
 [0.45]
 [0.45]
 [0.45]
 [0.45]] [[-3.971]
 [-3.971]
 [-1.988]
 [-3.971]
 [-3.971]
 [-3.971]
 [-3.971]] [[0.45]
 [0.45]
 [0.87]
 [0.45]
 [0.45]
 [0.45]
 [0.45]]
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.664]
 [0.472]
 [0.671]
 [0.653]
 [0.623]
 [0.638]] [[-1.301]
 [-0.11 ]
 [-0.147]
 [ 0.82 ]
 [-1.223]
 [-1.889]
 [ 0.426]] [[0.867]
 [1.464]
 [1.33 ]
 [1.935]
 [0.899]
 [0.547]
 [1.717]]
line 256 mcts: sample exp_bonus 0.09883498645176891
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]] [[-5.491]
 [-6.414]
 [-6.414]
 [-6.414]
 [-6.414]
 [-6.414]
 [-6.414]] [[0.16 ]
 [0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.865]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[-3.306]
 [-3.306]
 [-1.701]
 [-3.306]
 [-3.306]
 [-3.306]
 [-3.306]] [[0.508]
 [0.508]
 [0.865]
 [0.508]
 [0.508]
 [0.508]
 [0.508]]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[-1.083]
 [-1.219]
 [-1.219]
 [-1.219]
 [-1.219]
 [-1.219]
 [-1.219]] [[0.706]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[-3.002]
 [-3.109]
 [-3.109]
 [-3.109]
 [-3.109]
 [-3.109]
 [-3.109]] [[0.778]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]]
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]] [[2.657]
 [2.657]
 [2.657]
 [2.657]
 [2.657]
 [2.657]
 [2.657]] [[1.815]
 [1.815]
 [1.815]
 [1.815]
 [1.815]
 [1.815]
 [1.815]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.692]
 [0.434]
 [0.71 ]
 [0.243]
 [0.444]
 [0.641]] [[ 0.268]
 [ 0.573]
 [ 0.438]
 [ 0.545]
 [-0.289]
 [-0.488]
 [ 0.759]] [[0.917]
 [1.523]
 [1.08 ]
 [1.516]
 [0.129]
 [0.17 ]
 [1.647]]
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[-3.864]
 [-3.511]
 [-3.511]
 [-3.511]
 [-3.511]
 [-3.511]
 [-3.511]] [[0.761]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[-3.146]
 [-2.833]
 [-2.833]
 [-2.833]
 [-2.833]
 [-2.833]
 [-2.833]] [[0.703]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]]
from probs:  [0.2143086356182786, 0.3906805354085083, 0.22520913901280454, 0.1513442813259504, 0.018058089422808243, 0.000399319211649989]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.842]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[1.588]
 [1.822]
 [1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]] [[1.286]
 [1.547]
 [1.286]
 [1.286]
 [1.286]
 [1.286]
 [1.286]]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.723]
 [0.725]
 [0.725]
 [0.725]
 [0.732]
 [0.753]] [[2.839]
 [3.045]
 [2.926]
 [2.926]
 [2.926]
 [2.755]
 [2.556]] [[0.351]
 [0.414]
 [0.377]
 [0.377]
 [0.377]
 [0.334]
 [0.31 ]]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]] [[0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[-3.098]
 [-2.847]
 [-2.847]
 [-2.847]
 [-2.847]
 [-2.847]
 [-2.847]] [[0.67 ]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2143086356182786, 0.3906805354085082, 0.22520913901280454, 0.1513442813259504, 0.018058089422808243, 0.000399319211649989]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.403]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[2.359]
 [2.416]
 [2.359]
 [2.359]
 [2.359]
 [2.359]
 [2.359]] [[0.318]
 [0.339]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2143086356182786, 0.3906805354085082, 0.22520913901280454, 0.1513442813259504, 0.018058089422808243, 0.000399319211649989]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]] [[-5.301]
 [-7.005]
 [-7.005]
 [-7.005]
 [-7.005]
 [-7.005]
 [-7.005]] [[0.398]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.771]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[3.949]
 [4.144]
 [4.187]
 [4.187]
 [4.187]
 [4.187]
 [4.187]] [[1.698]
 [1.777]
 [1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[-1.876]
 [-3.213]
 [-3.213]
 [-3.213]
 [-3.213]
 [-3.213]
 [-3.213]] [[0.758]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.6331306772231375
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
from probs:  [0.2143086356182786, 0.3906805354085082, 0.22520913901280454, 0.1513442813259504, 0.018058089422808243, 0.000399319211649989]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]] [[-3.482]
 [-3.695]
 [-3.695]
 [-3.695]
 [-3.695]
 [-3.695]
 [-3.695]] [[0.342]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[-5.945]
 [-6.666]
 [-6.666]
 [-6.666]
 [-6.666]
 [-6.666]
 [-6.666]] [[0.096]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]]
2847 4486
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2143086356182786, 0.3906805354085082, 0.22520913901280454, 0.1513442813259504, 0.018058089422808243, 0.000399319211649989]
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[-1.432]
 [-1.593]
 [-1.593]
 [-1.593]
 [-1.593]
 [-1.593]
 [-1.593]] [[0.669]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[-1.991]
 [-2.35 ]
 [-2.35 ]
 [-2.35 ]
 [-2.35 ]
 [-2.35 ]
 [-2.35 ]] [[0.63 ]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2143086356182786, 0.3906805354085082, 0.22520913901280454, 0.1513442813259504, 0.018058089422808243, 0.000399319211649989]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.68 ]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[-0.003]
 [ 0.482]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[1.34 ]
 [1.736]
 [1.34 ]
 [1.34 ]
 [1.34 ]
 [1.34 ]
 [1.34 ]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2143086356182786, 0.3906805354085082, 0.22520913901280454, 0.1513442813259504, 0.018058089422808243, 0.000399319211649989]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]] [[-2.597]
 [-5.325]
 [-5.325]
 [-5.325]
 [-5.325]
 [-5.325]
 [-5.325]] [[0.305]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]] [[-7.355]
 [-6.93 ]
 [-6.93 ]
 [-6.93 ]
 [-6.93 ]
 [-6.93 ]
 [-6.93 ]] [[0.425]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.667]
 [0.653]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[0.16 ]
 [0.163]
 [0.263]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]] [[0.671]
 [0.667]
 [0.653]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2143086356182786, 0.3906805354085082, 0.22520913901280454, 0.1513442813259504, 0.018058089422808243, 0.000399319211649989]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2143086356182786, 0.3906805354085082, 0.22520913901280454, 0.1513442813259504, 0.018058089422808243, 0.000399319211649989]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.018]
 [0.039]
 [0.057]
 [0.065]
 [0.048]
 [0.059]] [[-7.85 ]
 [-1.658]
 [-6.224]
 [-7.138]
 [-7.042]
 [-7.044]
 [-7.077]] [[0.334]
 [0.018]
 [0.039]
 [0.057]
 [0.065]
 [0.048]
 [0.059]]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[-4.369]
 [-6.638]
 [-6.638]
 [-6.638]
 [-6.638]
 [-6.638]
 [-6.638]] [[0.423]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]]
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]] [[-2.836]
 [-4.286]
 [-4.286]
 [-4.286]
 [-4.286]
 [-4.286]
 [-4.286]] [[0.176]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.652]
 [0.852]
 [0.606]
 [0.585]
 [0.585]
 [0.585]] [[-4.224]
 [-3.071]
 [-4.275]
 [-4.225]
 [-4.224]
 [-4.224]
 [-4.224]] [[0.585]
 [0.652]
 [0.852]
 [0.606]
 [0.585]
 [0.585]
 [0.585]]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.567]
 [0.769]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[1.195]
 [1.195]
 [2.449]
 [1.195]
 [1.195]
 [1.195]
 [1.195]] [[0.779]
 [0.779]
 [1.602]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2143086356182786, 0.3906805354085082, 0.22520913901280454, 0.1513442813259504, 0.018058089422808243, 0.000399319211649989]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2143086356182786, 0.3906805354085082, 0.22520913901280454, 0.1513442813259504, 0.018058089422808243, 0.000399319211649989]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.617]
 [0.899]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[-3.939]
 [-3.939]
 [-3.583]
 [-3.939]
 [-3.939]
 [-3.939]
 [-3.939]] [[0.617]
 [0.617]
 [0.899]
 [0.617]
 [0.617]
 [0.617]
 [0.617]]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.609]
 [0.66 ]
 [0.65 ]
 [0.634]
 [0.558]
 [0.631]] [[1.337]
 [1.239]
 [1.348]
 [1.338]
 [1.286]
 [1.366]
 [1.382]] [[0.321]
 [0.229]
 [0.439]
 [0.409]
 [0.325]
 [0.253]
 [0.415]]
actor:  1 policy actor:  1  step number:  81 total reward:  0.5299999999999997  reward:  1.0 rdn_beta:  0.667
2853 4494
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  88 total reward:  0.4049999999999996  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.753]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.702]] [[1.08 ]
 [1.032]
 [1.08 ]
 [1.08 ]
 [1.08 ]
 [1.08 ]
 [0.868]] [[2.092]
 [2.173]
 [2.092]
 [2.092]
 [2.092]
 [2.092]
 [2.083]]
actor:  1 policy actor:  1  step number:  94 total reward:  0.3349999999999995  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  49 total reward:  0.5399999999999997  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.864]
 [0.79 ]
 [0.864]
 [0.864]
 [0.776]
 [0.772]] [[3.08 ]
 [3.08 ]
 [3.467]
 [3.08 ]
 [3.08 ]
 [2.837]
 [2.212]] [[2.047]
 [2.047]
 [2.028]
 [2.047]
 [2.047]
 [1.789]
 [1.572]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
from probs:  [0.2154918621888171, 0.3837026460994052, 0.22427253402512368, 0.1592402917493449, 0.016918545503277954, 0.0003741204340310991]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2154918621888171, 0.3837026460994052, 0.22427253402512368, 0.1592402917493449, 0.016918545503277954, 0.0003741204340310991]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.886]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[-1.219]
 [ 0.644]
 [-1.219]
 [-1.219]
 [-1.219]
 [-1.219]
 [-1.219]] [[1.446]
 [2.055]
 [1.446]
 [1.446]
 [1.446]
 [1.446]
 [1.446]]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.766]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[-1.541]
 [ 0.921]
 [-1.541]
 [-1.541]
 [-1.541]
 [-1.541]
 [-1.541]] [[1.265]
 [1.911]
 [1.265]
 [1.265]
 [1.265]
 [1.265]
 [1.265]]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]] [[ 1.524]
 [-3.801]
 [-3.801]
 [-3.801]
 [-3.801]
 [-3.801]
 [-3.801]] [[0.476]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]]
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]] [[-5.211]
 [-5.211]
 [-5.211]
 [-5.211]
 [-5.211]
 [-5.211]
 [-5.211]] [[0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]] [[ 0.845]
 [-2.382]
 [-2.382]
 [-2.382]
 [-2.382]
 [-2.382]
 [-2.382]] [[0.429]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
line 256 mcts: sample exp_bonus 1.5889778725611292
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[-3.394]
 [-3.394]
 [-3.394]
 [-3.394]
 [-3.394]
 [-3.394]
 [-3.394]] [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2154918621888171, 0.3837026460994052, 0.22427253402512368, 0.1592402917493449, 0.016918545503277954, 0.0003741204340310991]
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[-4.092]
 [-4.092]
 [-4.092]
 [-4.092]
 [-4.092]
 [-4.092]
 [-4.092]] [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
actor:  1 policy actor:  1  step number:  86 total reward:  0.5249999999999997  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[-2.802]
 [-2.802]
 [-2.802]
 [-2.802]
 [-2.802]
 [-2.802]
 [-2.802]] [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.962]
 [0.964]
 [0.893]
 [0.893]
 [0.921]
 [0.874]] [[2.092]
 [2.411]
 [2.606]
 [2.818]
 [2.818]
 [2.679]
 [2.6  ]] [[1.796]
 [1.96 ]
 [2.017]
 [2.017]
 [2.017]
 [2.001]
 [1.938]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.22885762908376991, 0.3771654543514012, 0.22045157377468758, 0.15652729424527273, 0.01663030204919553, 0.00036774649567292015]
line 256 mcts: sample exp_bonus 0.4531280107176271
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.432]
 [0.562]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[-1.951]
 [-0.596]
 [-2.14 ]
 [-2.028]
 [-2.028]
 [-2.028]
 [-2.028]] [[0.51 ]
 [0.432]
 [0.562]
 [0.511]
 [0.511]
 [0.511]
 [0.511]]
Printing some Q and Qe and total Qs values:  [[0.854]
 [0.844]
 [0.845]
 [0.85 ]
 [0.849]
 [0.842]
 [0.869]] [[2.665]
 [2.64 ]
 [2.739]
 [3.04 ]
 [2.939]
 [2.335]
 [2.839]] [[1.929]
 [1.896]
 [1.967]
 [2.186]
 [2.113]
 [1.68 ]
 [2.071]]
from probs:  [0.22885762908376991, 0.3771654543514012, 0.22045157377468758, 0.15652729424527273, 0.01663030204919553, 0.00036774649567292015]
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.818]
 [0.822]
 [0.818]
 [0.818]
 [0.818]
 [0.82 ]] [[4.364]
 [4.364]
 [3.975]
 [4.364]
 [4.364]
 [4.364]
 [4.048]] [[2.394]
 [2.394]
 [2.026]
 [2.394]
 [2.394]
 [2.394]
 [2.092]]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.841]
 [0.869]
 [0.815]
 [0.815]
 [0.905]
 [0.815]] [[2.061]
 [1.54 ]
 [2.392]
 [2.061]
 [2.061]
 [2.159]
 [2.061]] [[2.164]
 [2.049]
 [2.373]
 [2.164]
 [2.164]
 [2.367]
 [2.164]]
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]] [[-3.881]
 [-6.988]
 [-6.988]
 [-6.988]
 [-6.988]
 [-6.988]
 [-6.988]] [[0.116]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.22885762908376991, 0.3771654543514012, 0.22045157377468758, 0.15652729424527273, 0.01663030204919553, 0.00036774649567292015]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.039]
 [0.047]
 [0.047]
 [0.056]
 [0.047]
 [0.048]] [[-5.72 ]
 [-1.556]
 [-6.617]
 [-6.287]
 [-7.963]
 [-6.245]
 [-6.271]] [[0.22 ]
 [0.039]
 [0.047]
 [0.047]
 [0.056]
 [0.047]
 [0.048]]
line 256 mcts: sample exp_bonus -6.267085446080474
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]] [[ 0.169]
 [-1.515]
 [-1.515]
 [-1.515]
 [-1.515]
 [-1.515]
 [-1.515]] [[0.328]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]]
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]] [[ 0.437]
 [-0.561]
 [-0.561]
 [-0.561]
 [-0.561]
 [-0.561]
 [-0.561]] [[0.217]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]] [[-1.756]
 [-2.174]
 [-2.174]
 [-2.174]
 [-2.174]
 [-2.174]
 [-2.174]] [[0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]]
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.021]
 [0.032]
 [0.033]
 [0.033]
 [0.033]
 [0.035]] [[-7.324]
 [-2.177]
 [-5.82 ]
 [-5.547]
 [-5.673]
 [-5.549]
 [-5.585]] [[0.056]
 [0.021]
 [0.032]
 [0.033]
 [0.033]
 [0.033]
 [0.035]]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]] [[ 1.429]
 [-3.506]
 [-3.506]
 [-3.506]
 [-3.506]
 [-3.506]
 [-3.506]] [[0.314]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]]
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.039]
 [0.048]
 [0.198]
 [0.198]
 [0.047]
 [0.048]] [[-5.34 ]
 [-1.561]
 [-6.944]
 [ 0.   ]
 [ 0.   ]
 [-6.246]
 [-6.28 ]] [[0.224]
 [0.039]
 [0.048]
 [0.198]
 [0.198]
 [0.047]
 [0.048]]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[3.335]
 [3.335]
 [3.335]
 [3.335]
 [3.335]
 [3.335]
 [3.335]] [[0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.721]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[ 1.257]
 [-0.053]
 [ 1.257]
 [ 1.257]
 [ 1.257]
 [ 1.257]
 [ 1.257]] [[1.818]
 [1.313]
 [1.818]
 [1.818]
 [1.818]
 [1.818]
 [1.818]]
2880 4521
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]] [[-5.117]
 [-6.05 ]
 [-6.05 ]
 [-6.05 ]
 [-6.05 ]
 [-6.05 ]
 [-6.05 ]] [[0.128]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[-2.345]
 [-4.771]
 [-4.771]
 [-4.771]
 [-4.771]
 [-4.771]
 [-4.771]] [[0.307]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]] [[-0.39 ]
 [-2.433]
 [-2.433]
 [-2.433]
 [-2.433]
 [-2.433]
 [-2.433]] [[0.589]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]] [[-5.664]
 [-6.293]
 [-6.293]
 [-6.293]
 [-6.293]
 [-6.293]
 [-6.293]] [[0.278]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]]
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]] [[-7.048]
 [-6.408]
 [-6.408]
 [-6.408]
 [-6.408]
 [-6.408]
 [-6.408]] [[0.246]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]] [[-6.911]
 [-7.517]
 [-7.517]
 [-7.517]
 [-7.517]
 [-7.517]
 [-7.517]] [[0.186]
 [0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]]
line 256 mcts: sample exp_bonus -5.695192210752413
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[-4.97]
 [-6.14]
 [-6.14]
 [-6.14]
 [-6.14]
 [-6.14]
 [-6.14]] [[0.272]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]]
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.807]
 [0.803]
 [0.802]
 [0.796]
 [0.793]
 [0.8  ]] [[2.003]
 [2.499]
 [3.088]
 [2.968]
 [2.431]
 [2.628]
 [2.856]] [[1.094]
 [1.335]
 [1.523]
 [1.481]
 [1.289]
 [1.35 ]
 [1.439]]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.488]
 [0.555]
 [0.559]
 [0.566]
 [0.569]
 [0.581]] [[1.305]
 [1.47 ]
 [2.212]
 [2.172]
 [1.76 ]
 [1.855]
 [2.001]] [[0.423]
 [0.358]
 [1.234]
 [1.202]
 [0.803]
 [0.903]
 [1.074]]
2883 4529
actor:  1 policy actor:  1  step number:  57 total reward:  0.47999999999999965  reward:  1.0 rdn_beta:  0.5
from probs:  [0.22885762908376991, 0.3771654543514012, 0.2204515737746876, 0.15652729424527273, 0.01663030204919553, 0.00036774649567292015]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.22524369349536502, 0.37120956088324636, 0.23276157546307122, 0.1540555411230613, 0.01636768969643227, 0.0003619393388237286]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]] [[-8.834]
 [-9.309]
 [-9.309]
 [-9.309]
 [-9.309]
 [-9.309]
 [-9.309]] [[0.158]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
actor:  1 policy actor:  1  step number:  98 total reward:  0.3949999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2359600667806454, 0.36607501704269735, 0.22954203415932153, 0.1519246560028242, 0.01614129299448615, 0.0003569330200252905]
Printing some Q and Qe and total Qs values:  [[0.1 ]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]] [[-5.458]
 [-7.456]
 [-7.456]
 [-7.456]
 [-7.456]
 [-7.456]
 [-7.456]] [[0.1 ]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2359600667806454, 0.36607501704269735, 0.22954203415932153, 0.1519246560028242, 0.01614129299448615, 0.0003569330200252905]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2359600667806454, 0.36607501704269735, 0.22954203415932153, 0.1519246560028242, 0.01614129299448615, 0.0003569330200252905]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2359600667806454, 0.36607501704269735, 0.22954203415932153, 0.1519246560028242, 0.01614129299448615, 0.0003569330200252905]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -6.019059899534787
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -6.614062769455832
Printing some Q and Qe and total Qs values:  [[0.818]
 [1.018]
 [0.831]
 [0.818]
 [0.827]
 [0.841]
 [0.815]] [[3.785]
 [2.722]
 [4.03 ]
 [3.867]
 [3.849]
 [4.122]
 [4.056]] [[1.448]
 [1.143]
 [1.573]
 [1.485]
 [1.486]
 [1.625]
 [1.57 ]]
actor:  1 policy actor:  1  step number:  71 total reward:  0.5199999999999997  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  72 total reward:  0.5749999999999997  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.564]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]] [[-1.073]
 [ 1.7  ]
 [-1.073]
 [-1.073]
 [-1.073]
 [-1.073]
 [-1.073]] [[0.519]
 [0.564]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]]
line 256 mcts: sample exp_bonus 0.8662850136964356
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.15 ]
 [0.016]
 [0.024]
 [0.15 ]
 [0.019]
 [0.021]] [[ 0.04 ]
 [-0.484]
 [ 0.302]
 [-0.096]
 [-0.484]
 [-0.085]
 [-0.053]] [[1.623]
 [1.409]
 [1.774]
 [1.547]
 [1.409]
 [1.55 ]
 [1.57 ]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.228186178527439, 0.35401439037137966, 0.2549253721620442, 0.1469193798356043, 0.015609505519963825, 0.0003451735835688854]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.581]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.566]] [[ 0.673]
 [ 0.346]
 [-0.657]
 [-0.657]
 [-0.657]
 [-0.657]
 [ 0.071]] [[2.013]
 [1.936]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.874]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.717]
 [0.702]] [[4.75 ]
 [4.959]
 [4.959]
 [4.959]
 [4.959]
 [4.893]
 [4.886]] [[1.595]
 [1.635]
 [1.635]
 [1.635]
 [1.635]
 [1.62 ]
 [1.608]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
2895 4538
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.4488297016723607
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.228186178527439, 0.35401439037137966, 0.2549253721620442, 0.1469193798356043, 0.015609505519963825, 0.0003451735835688854]
using another actor
from probs:  [0.228186178527439, 0.35401439037137966, 0.2549253721620442, 0.1469193798356043, 0.015609505519963825, 0.0003451735835688854]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.8316613866213705
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  144 total reward:  0.11499999999999932  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[-1.59]
 [-1.59]
 [-1.59]
 [-1.59]
 [-1.59]
 [-1.59]
 [-1.59]] [[1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.22625483570748278, 0.35101805134937036, 0.2612316021979416, 0.1456758702979581, 0.015477388375079768, 0.0003422520721672465]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.22625483570748278, 0.35101805134937036, 0.2612316021979416, 0.1456758702979581, 0.015477388375079768, 0.0003422520721672465]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.22625483570748278, 0.35101805134937036, 0.2612316021979416, 0.1456758702979581, 0.015477388375079768, 0.0003422520721672465]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
using explorer policy with actor:  1
first move QE:  -0.5107262179812222
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.22625483570748278, 0.35101805134937036, 0.2612316021979416, 0.1456758702979581, 0.015477388375079768, 0.0003422520721672465]
siam score:  -0.6381358
using explorer policy with actor:  1
siam score:  -0.64317966
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]] [[-8.521]
 [-8.314]
 [-8.314]
 [-8.314]
 [-8.314]
 [-8.314]
 [-8.314]] [[0.134]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]] [[-3.723]
 [-7.693]
 [-7.693]
 [-7.693]
 [-7.693]
 [-7.693]
 [-7.693]] [[0.178]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]]
actor:  1 policy actor:  1  step number:  121 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  0.5
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]] [[-6.814]
 [-7.939]
 [-7.939]
 [-7.939]
 [-7.939]
 [-7.939]
 [-7.939]] [[0.287]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]]
line 256 mcts: sample exp_bonus -9.61358960956444
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[-8.048]
 [-8.624]
 [-8.624]
 [-8.624]
 [-8.624]
 [-8.624]
 [-8.624]] [[0.476]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2246016192289445, 0.34845320527684076, 0.2666296927415122, 0.14461143448800548, 0.015364296986661889, 0.0003397512780350394]
using explorer policy with actor:  1
from probs:  [0.2246016192289445, 0.34845320527684076, 0.2666296927415122, 0.14461143448800548, 0.015364296986661889, 0.0003397512780350394]
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.622]
 [0.633]
 [0.622]
 [0.622]
 [0.625]
 [0.622]] [[2.274]
 [2.274]
 [3.559]
 [2.274]
 [2.274]
 [3.308]
 [2.274]] [[1.559]
 [1.559]
 [1.891]
 [1.559]
 [1.559]
 [1.822]
 [1.559]]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.434]
 [0.469]
 [0.585]
 [0.459]
 [0.585]
 [0.585]] [[-9.741]
 [-3.496]
 [-9.889]
 [ 0.   ]
 [-9.799]
 [ 0.   ]
 [ 0.   ]] [[0.589]
 [0.434]
 [0.469]
 [0.585]
 [0.459]
 [0.585]
 [0.585]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
probs:  [0.2246016192289445, 0.34845320527684076, 0.2666296927415122, 0.14461143448800548, 0.015364296986661889, 0.0003397512780350394]
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.26 ]
 [0.651]
 [0.656]
 [0.652]
 [0.65 ]
 [0.654]] [[-6.525]
 [-1.287]
 [-6.26 ]
 [-6.443]
 [-6.099]
 [-5.981]
 [-5.579]] [[0.183]
 [1.576]
 [0.256]
 [0.205]
 [0.302]
 [0.336]
 [0.453]]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[ 2.28 ]
 [-0.495]
 [-0.495]
 [-0.495]
 [-0.495]
 [-0.495]
 [-0.495]] [[0.497]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]]
from probs:  [0.2246016192289445, 0.34845320527684076, 0.2666296927415122, 0.14461143448800548, 0.015364296986661889, 0.0003397512780350394]
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.656]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[-0.68 ]
 [-0.143]
 [-0.68 ]
 [-0.68 ]
 [-0.68 ]
 [-0.68 ]
 [-0.68 ]] [[0.617]
 [0.656]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [ 0.5596],
        [-0.4087],
        [-0.7506],
        [-0.5272],
        [ 0.2239],
        [-0.5503],
        [ 0.3853],
        [-0.0000],
        [ 0.3942]], dtype=torch.float64)
-0.009850499999999242 -0.009850499999999242
-0.024259925299500003 0.5353516213282338
-0.0337698257985 -0.4424293278625795
-0.024259925299500003 -0.7748268404402289
-0.024259925299500003 -0.5514762607093081
-0.0439609252995 0.1799562764155518
-0.024259925299500003 -0.5745418166046049
-0.024259925299500003 0.36106694323333505
-0.9419653234529999 -0.9419653234529999
-0.0438629152995 0.3503371406522938
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[ 3.433]
 [-0.954]
 [-0.954]
 [-0.954]
 [-0.954]
 [-0.954]
 [-0.954]] [[0.423]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]]
maxi score, test score, baseline:  -0.99698 -1.0 -0.99698
Printing some Q and Qe and total Qs values:  [[0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]] [[-2.67]
 [-2.67]
 [-2.67]
 [-2.67]
 [-2.67]
 [-2.67]
 [-2.67]] [[0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]] [[-5.254]
 [-5.254]
 [-5.254]
 [-5.254]
 [-5.254]
 [-5.254]
 [-5.254]] [[0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]]
line 256 mcts: sample exp_bonus -1.8936383501783776
from probs:  [0.2246016192289445, 0.34845320527684076, 0.2666296927415122, 0.14461143448800545, 0.015364296986661889, 0.0003397512780350394]
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.105]
 [0.13 ]
 [0.13 ]
 [0.13 ]
 [0.13 ]
 [0.102]] [[0.266]
 [0.376]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.569]] [[0.13 ]
 [0.105]
 [0.13 ]
 [0.13 ]
 [0.13 ]
 [0.13 ]
 [0.102]]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.515]] [[-0.297]
 [ 0.372]
 [ 0.372]
 [ 0.372]
 [ 0.372]
 [ 0.372]
 [ 0.13 ]] [[1.631]
 [1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.744]]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.459]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[-4.811]
 [-4.625]
 [-4.811]
 [-4.811]
 [-4.811]
 [-4.811]
 [-4.811]] [[0.435]
 [0.459]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]]
using explorer policy with actor:  1
2914 4576
line 256 mcts: sample exp_bonus 0.781156027026841
using another actor
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[-6.149]
 [-7.957]
 [-7.957]
 [-7.957]
 [-7.957]
 [-7.957]
 [-7.957]] [[0.606]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]]
Printing some Q and Qe and total Qs values:  [[0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]] [[2.807]
 [2.807]
 [2.807]
 [2.807]
 [2.807]
 [2.807]
 [2.807]] [[0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]]
line 256 mcts: sample exp_bonus -0.7463883421590621
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[-1.823]
 [-3.003]
 [-3.003]
 [-3.003]
 [-3.003]
 [-3.003]
 [-3.003]] [[0.486]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]]
line 256 mcts: sample exp_bonus 0.16782227993723187
Starting evaluation
from probs:  [0.22641942671595555, 0.3431799278817276, 0.26878765337148847, 0.1457818434513164, 0.01548864753315793, 0.0003425010463539584]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.488]
 [0.394]
 [0.379]
 [0.376]
 [0.383]
 [0.41 ]] [[-5.377]
 [-4.618]
 [-5.671]
 [-5.826]
 [-5.888]
 [-5.745]
 [-5.438]] [[0.428]
 [0.488]
 [0.394]
 [0.379]
 [0.376]
 [0.383]
 [0.41 ]]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.744]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[-4.576]
 [-4.236]
 [-4.576]
 [-4.576]
 [-4.576]
 [-4.576]
 [-4.576]] [[0.497]
 [0.744]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]]
actor:  0 policy actor:  1  step number:  73 total reward:  0.2899999999999995  reward:  1.0 rdn_beta:  0.333
from probs:  [0.22641942671595555, 0.3431799278817276, 0.26878765337148847, 0.1457818434513164, 0.01548864753315793, 0.0003425010463539584]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.754]
 [0.656]
 [0.654]
 [0.654]
 [0.654]
 [0.683]] [[-2.089]
 [-0.513]
 [-1.495]
 [-2.089]
 [-2.089]
 [-2.089]
 [-1.755]] [[0.654]
 [0.754]
 [0.656]
 [0.654]
 [0.654]
 [0.654]
 [0.683]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.22642452305560515, 0.34319733810468794, 0.2687748828706359, 0.14576782605667588, 0.015491696044495826, 0.0003437338678992986]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
siam score:  -0.6365261
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[-3.915]
 [-3.915]
 [-3.915]
 [-3.915]
 [-3.915]
 [-3.915]
 [-3.915]] [[0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]]
line 256 mcts: sample exp_bonus -1.6693415884958551
from probs:  [0.22642452305560515, 0.34319733810468794, 0.2687748828706359, 0.14576782605667588, 0.015491696044495826, 0.0003437338678992986]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.594]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[-2.767]
 [-2.514]
 [-2.767]
 [-2.767]
 [-2.767]
 [-2.767]
 [-2.767]] [[0.591]
 [0.594]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]]
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[-5.046]
 [-6.994]
 [-6.994]
 [-6.994]
 [-6.994]
 [-6.994]
 [-6.994]] [[0.147]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.22642452305560515, 0.34319733810468794, 0.2687748828706359, 0.14576782605667588, 0.015491696044495826, 0.0003437338678992986]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.22642452305560515, 0.34319733810468794, 0.2687748828706359, 0.14576782605667588, 0.015491696044495826, 0.0003437338678992986]
using explorer policy with actor:  0
siam score:  -0.6384243
2920 4585
siam score:  -0.63964933
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.592]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.618]] [[-0.482]
 [-0.469]
 [-0.482]
 [-0.482]
 [-0.482]
 [-0.482]
 [-0.205]] [[1.779]
 [1.794]
 [1.779]
 [1.779]
 [1.779]
 [1.779]
 [1.909]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]] [[-2.972]
 [-3.15 ]
 [-3.15 ]
 [-3.15 ]
 [-3.15 ]
 [-3.15 ]
 [-3.15 ]] [[0.647]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]]
using explorer policy with actor:  1
from probs:  [0.22642452305560515, 0.34319733810468794, 0.2687748828706359, 0.14576782605667588, 0.015491696044495826, 0.0003437338678992986]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.579]
 [0.573]
 [0.554]
 [0.554]
 [0.571]
 [0.554]] [[-1.493]
 [-1.38 ]
 [-0.954]
 [-1.493]
 [-1.493]
 [-1.74 ]
 [-1.493]] [[0.554]
 [0.579]
 [0.573]
 [0.554]
 [0.554]
 [0.571]
 [0.554]]
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[-1.085]
 [-1.085]
 [-1.085]
 [-1.085]
 [-1.085]
 [-1.085]
 [-1.085]] [[0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]]
from probs:  [0.22642452305560515, 0.34319733810468794, 0.2687748828706359, 0.14576782605667588, 0.015491696044495826, 0.0003437338678992986]
using explorer policy with actor:  0
using explorer policy with actor:  1
using explorer policy with actor:  0
2926 4593
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.773]
 [1.109]
 [0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]] [[-3.459]
 [ 0.277]
 [-3.459]
 [-3.459]
 [-3.459]
 [-3.459]
 [-3.459]] [[0.809]
 [1.757]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.676]] [[0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.842]] [[1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.783]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.22642452305560515, 0.34319733810468794, 0.2687748828706359, 0.14576782605667588, 0.015491696044495826, 0.0003437338678992986]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.7  ]
 [0.918]
 [0.688]
 [0.688]
 [0.688]
 [0.688]] [[-4.806]
 [-2.981]
 [-5.086]
 [-4.806]
 [-4.806]
 [-4.806]
 [-4.806]] [[0.688]
 [0.7  ]
 [0.918]
 [0.688]
 [0.688]
 [0.688]
 [0.688]]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[-0.378]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.378]] [[1.862]
 [1.862]
 [1.862]
 [1.862]
 [1.862]
 [1.862]
 [1.862]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.22642452305560515, 0.34319733810468794, 0.2687748828706359, 0.14576782605667588, 0.015491696044495826, 0.0003437338678992986]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.4040501002253194
rdn probs:  [0.22642452305560515, 0.34319733810468794, 0.2687748828706359, 0.14576782605667588, 0.015491696044495826, 0.0003437338678992986]
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.927]
 [0.802]
 [0.791]
 [0.799]
 [0.781]
 [0.792]] [[3.454]
 [2.815]
 [3.382]
 [3.406]
 [3.382]
 [3.513]
 [3.454]] [[1.269]
 [1.15 ]
 [1.276]
 [1.27 ]
 [1.27 ]
 [1.322]
 [1.305]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.49499999999999966  reward:  1.0 rdn_beta:  0.167
from probs:  [0.2380444831758141, 0.33804213411878786, 0.26473758655844287, 0.14357823192388533, 0.015258993618428127, 0.0003385706046418698]
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]] [[-9.596]
 [-8.584]
 [-8.584]
 [-8.584]
 [-8.584]
 [-8.584]
 [-8.584]] [[0.166]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.4049999999999996  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.275]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]] [[-2.523]
 [-2.105]
 [-2.523]
 [-2.523]
 [-2.523]
 [-2.523]
 [-2.523]] [[0.257]
 [0.275]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.24801623470779258, 0.3336181591040494, 0.26127295197539063, 0.1416992161250193, 0.01505929837424221, 0.00033413971350587256]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.827]
 [0.854]
 [0.803]
 [0.803]
 [0.848]
 [0.836]] [[1.271]
 [1.637]
 [1.664]
 [1.271]
 [1.271]
 [1.388]
 [0.942]] [[1.944]
 [2.115]
 [2.179]
 [1.944]
 [1.944]
 [2.073]
 [1.9  ]]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.557]
 [0.561]
 [0.569]
 [0.572]
 [0.564]
 [0.565]] [[3.791]
 [3.708]
 [3.989]
 [4.063]
 [4.099]
 [4.138]
 [4.039]] [[0.952]
 [0.932]
 [1.126]
 [1.191]
 [1.222]
 [1.232]
 [1.167]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
from probs:  [0.24801623470779258, 0.3336181591040494, 0.26127295197539063, 0.1416992161250193, 0.01505929837424221, 0.00033413971350587256]
actor:  1 policy actor:  1  step number:  123 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
2936 4613
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]] [[-9.677]
 [-9.063]
 [-9.063]
 [-9.063]
 [-9.063]
 [-9.063]
 [-9.063]] [[0.306]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.805]
 [0.558]
 [0.673]
 [0.545]
 [0.536]
 [0.629]] [[-0.85 ]
 [ 1.202]
 [-0.406]
 [-0.956]
 [-0.886]
 [-0.886]
 [-0.413]] [[0.36 ]
 [1.643]
 [0.611]
 [0.658]
 [0.425]
 [0.408]
 [0.752]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]] [[-6.684]
 [-8.218]
 [-8.218]
 [-8.218]
 [-8.218]
 [-8.218]
 [-8.218]] [[0.436]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]] [[-5.617]
 [-7.938]
 [-7.938]
 [-7.938]
 [-7.938]
 [-7.938]
 [-7.938]] [[0.469]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[-6.059]
 [-8.307]
 [-8.307]
 [-8.307]
 [-8.307]
 [-8.307]
 [-8.307]] [[0.682]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.24567407230215604, 0.33991119269269426, 0.2588055986327862, 0.14036106752637129, 0.014917084609288601, 0.00033098423670361535]
line 256 mcts: sample exp_bonus 0.09429506137708546
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
from probs:  [0.24567407230215604, 0.33991119269269426, 0.2588055986327862, 0.14036106752637129, 0.014917084609288601, 0.00033098423670361535]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.574]
 [0.613]
 [0.611]
 [0.619]
 [0.583]
 [0.597]] [[2.182]
 [2.492]
 [1.922]
 [1.937]
 [2.169]
 [2.68 ]
 [2.441]] [[0.873]
 [1.246]
 [0.665]
 [0.678]
 [0.957]
 [1.475]
 [1.229]]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.04 ]
 [0.08 ]
 [0.052]
 [0.057]
 [0.056]
 [0.057]] [[-9.413]
 [-2.78 ]
 [ 0.   ]
 [-6.486]
 [-6.186]
 [-6.358]
 [-6.385]] [[0.065]
 [0.04 ]
 [0.08 ]
 [0.052]
 [0.057]
 [0.056]
 [0.057]]
2939 4617
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using another actor
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]] [[-3.19 ]
 [-2.763]
 [-2.763]
 [-2.763]
 [-2.763]
 [-2.763]
 [-2.763]] [[0.274]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.419]
 [0.417]
 [0.44 ]
 [0.451]
 [0.442]
 [0.459]] [[-2.091]
 [-1.305]
 [-2.6  ]
 [-2.166]
 [-2.371]
 [-2.616]
 [-2.247]] [[0.564]
 [0.419]
 [0.417]
 [0.44 ]
 [0.451]
 [0.442]
 [0.459]]
actor:  1 policy actor:  1  step number:  123 total reward:  0.06999999999999929  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
from probs:  [0.24384529251050852, 0.3373809186823921, 0.2568790687213889, 0.14676015674685594, 0.01480604292455513, 0.0003285204142995441]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]] [[-8.613]
 [-8.813]
 [-8.813]
 [-8.813]
 [-8.813]
 [-8.813]
 [-8.813]] [[0.073]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
UNIT TEST: sample policy line 217 mcts : [0.02  0.878 0.02  0.02  0.02  0.02  0.02 ]
from probs:  [0.24384529251050846, 0.3373809186823921, 0.2568790687213889, 0.1467601567468559, 0.014806042924555129, 0.00032852041429954406]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.24384529251050846, 0.3373809186823921, 0.2568790687213889, 0.1467601567468559, 0.014806042924555129, 0.00032852041429954406]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]] [[-0.372]
 [-1.677]
 [-1.677]
 [-1.677]
 [-1.677]
 [-1.677]
 [-1.677]] [[0.596]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.24384529251050846, 0.3373809186823921, 0.2568790687213889, 0.1467601567468559, 0.014806042924555129, 0.00032852041429954406]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.784663803212417
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]] [[1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  0.5149999999999997  reward:  1.0 rdn_beta:  0.667
siam score:  -0.6322818
siam score:  -0.63031614
using another actor
2951 4664
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.769]
 [0.427]
 [0.589]
 [0.601]
 [0.601]
 [0.769]] [[-0.312]
 [-4.64 ]
 [-2.698]
 [-3.263]
 [-2.462]
 [-2.413]
 [-4.64 ]] [[0.756]
 [0.769]
 [0.427]
 [0.589]
 [0.601]
 [0.601]
 [0.769]]
from probs:  [0.24024893775791217, 0.3324050528051108, 0.25309048518907046, 0.15934417316889007, 0.01458767585135615, 0.000323675227660396]
Printing some Q and Qe and total Qs values:  [[1.098]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[-0.463]
 [-1.091]
 [-1.091]
 [-1.091]
 [-1.091]
 [-1.091]
 [-1.091]] [[2.777]
 [1.981]
 [1.981]
 [1.981]
 [1.981]
 [1.981]
 [1.981]]
first move QE:  -0.5324132266309529
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
siam score:  -0.62302953
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.24024893775791217, 0.3324050528051108, 0.25309048518907046, 0.15934417316889007, 0.01458767585135615, 0.000323675227660396]
Printing some Q and Qe and total Qs values:  [[0.97 ]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]] [[ 0.124]
 [-2.494]
 [-2.494]
 [-2.494]
 [-2.494]
 [-2.494]
 [-2.494]] [[0.97 ]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
siam score:  -0.62235594
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.545]
 [0.227]
 [0.327]
 [0.375]
 [0.407]
 [0.545]] [[-2.344]
 [-4.829]
 [-3.399]
 [-4.065]
 [-3.483]
 [-3.064]
 [-4.829]] [[0.539]
 [0.545]
 [0.227]
 [0.327]
 [0.375]
 [0.407]
 [0.545]]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.017]
 [0.027]
 [0.028]
 [0.03 ]
 [0.032]
 [0.035]] [[-9.293]
 [-3.12 ]
 [-7.238]
 [-7.085]
 [-7.153]
 [-7.216]
 [-7.099]] [[0.325]
 [0.017]
 [0.027]
 [0.028]
 [0.03 ]
 [0.032]
 [0.035]]
line 256 mcts: sample exp_bonus -5.274620266736939
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[-8.627]
 [-9.804]
 [-9.804]
 [-9.804]
 [-9.804]
 [-9.804]
 [-9.804]] [[0.532]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.7149999999999999  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.25435350730042694, 0.32623404440962794, 0.24839192993024203, 0.1563859924129164, 0.014316859660718759, 0.0003176662860680635]
line 256 mcts: sample exp_bonus -1.2815496258063173
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.768]
 [0.647]
 [0.652]
 [0.768]
 [0.575]
 [0.538]] [[-1.528]
 [-4.844]
 [-3.108]
 [-3.906]
 [-4.844]
 [-2.442]
 [-1.893]] [[0.486]
 [0.768]
 [0.647]
 [0.652]
 [0.768]
 [0.575]
 [0.538]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.443]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[0.429]
 [0.845]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[0.049]
 [0.018]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.25435350730042694, 0.32623404440962794, 0.24839192993024203, 0.1563859924129164, 0.014316859660718759, 0.0003176662860680635]
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[-0.254]
 [-0.254]
 [-0.254]
 [-0.254]
 [-0.254]
 [-0.254]
 [-0.254]] [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.42 ]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]] [[-3.216]
 [-2.909]
 [-3.216]
 [-3.216]
 [-3.216]
 [-3.216]
 [-3.216]] [[0.313]
 [0.63 ]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[ 1.627]
 [-3.15 ]
 [-3.15 ]
 [-3.15 ]
 [-3.15 ]
 [-3.15 ]
 [-3.15 ]] [[0.569]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[-2.225]
 [-3.403]
 [-3.403]
 [-3.403]
 [-3.403]
 [-3.403]
 [-3.403]] [[0.504]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]]
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]] [[-1.051]
 [-1.051]
 [-1.051]
 [-1.051]
 [-1.051]
 [-1.051]
 [-1.051]] [[0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.601]] [[0.125]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.362]] [[1.754]
 [1.833]
 [1.833]
 [1.833]
 [1.833]
 [1.833]
 [1.847]]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.516]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[-1.951]
 [-0.882]
 [-1.951]
 [-1.951]
 [-1.951]
 [-1.951]
 [-1.951]] [[0.564]
 [0.84 ]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.995]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[-2.232]
 [ 0.773]
 [-2.232]
 [-2.232]
 [-2.232]
 [-2.232]
 [-2.232]] [[0.538]
 [2.094]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]]
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.68 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.641]] [[-3.436]
 [ 1.162]
 [-3.436]
 [-3.436]
 [-3.436]
 [-3.436]
 [-3.072]] [[0.65 ]
 [0.68 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.641]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.25435350730042694, 0.3262340444096279, 0.2483919299302421, 0.1563859924129164, 0.014316859660718759, 0.0003176662860680635]
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]]
from probs:  [0.25435350730042694, 0.3262340444096279, 0.2483919299302421, 0.1563859924129164, 0.014316859660718759, 0.0003176662860680635]
siam score:  -0.6166695
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.43 ]
 [0.647]
 [0.605]
 [0.187]
 [0.597]
 [0.559]] [[-0.9  ]
 [-1.338]
 [-4.621]
 [-4.138]
 [-0.996]
 [-4.586]
 [-2.443]] [[1.507]
 [1.39 ]
 [0.12 ]
 [0.302]
 [1.412]
 [0.11 ]
 [0.991]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5149999999999997  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -3.700260104670779
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]] [[-5.648]
 [-7.233]
 [-7.233]
 [-7.233]
 [-7.233]
 [-7.233]
 [-7.233]] [[0.115]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.2649925258940538, 0.32157927824586274, 0.2448478290903559, 0.1541546488776707, 0.014112584125831874, 0.00031313376622503345]
from probs:  [0.2649925258940538, 0.32157927824586274, 0.2448478290903559, 0.1541546488776707, 0.014112584125831874, 0.00031313376622503345]
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]] [[-3.849]
 [-6.217]
 [-6.217]
 [-6.217]
 [-6.217]
 [-6.217]
 [-6.217]] [[0.155]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.648]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[-3.444]
 [-3.804]
 [-3.444]
 [-3.444]
 [-3.444]
 [-3.444]
 [-3.444]] [[0.536]
 [0.648]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]]
siam score:  -0.6203551
line 256 mcts: sample exp_bonus -3.38645776218465
Printing some Q and Qe and total Qs values:  [[0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]] [[-0.531]
 [-0.538]
 [-0.538]
 [-0.538]
 [-0.538]
 [-0.538]
 [-0.538]] [[0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.544]
 [0.606]
 [0.572]
 [0.569]
 [0.601]
 [0.662]] [[-2.008]
 [-1.099]
 [-2.236]
 [-2.215]
 [-2.282]
 [-1.748]
 [-1.746]] [[0.676]
 [0.544]
 [0.606]
 [0.572]
 [0.569]
 [0.601]
 [0.662]]
Printing some Q and Qe and total Qs values:  [[1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]] [[-0.539]
 [-0.532]
 [-0.539]
 [-0.539]
 [-0.539]
 [-0.539]
 [-0.539]] [[2.605]
 [2.607]
 [2.605]
 [2.605]
 [2.605]
 [2.605]
 [2.605]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
siam score:  -0.6111872
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.544]
 [0.527]
 [0.527]
 [0.527]
 [0.554]
 [0.553]] [[-3.028]
 [-2.462]
 [-3.227]
 [-3.227]
 [-3.227]
 [-3.033]
 [-3.062]] [[0.555]
 [0.544]
 [0.527]
 [0.527]
 [0.527]
 [0.554]
 [0.553]]
from probs:  [0.2649925258940538, 0.3215792782458628, 0.24484782909035593, 0.15415464887767072, 0.014112584125831875, 0.0003131337662250335]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[-5.419]
 [-6.389]
 [-6.389]
 [-6.389]
 [-6.389]
 [-6.389]
 [-6.389]] [[0.268]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]]
siam score:  -0.61192423
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]] [[-7.605]
 [-6.868]
 [-6.868]
 [-6.868]
 [-6.868]
 [-6.868]
 [-6.868]] [[0.1  ]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.565]
 [0.806]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[-3.851]
 [-3.851]
 [-0.621]
 [-3.851]
 [-3.851]
 [-3.851]
 [-3.851]] [[0.565]
 [0.565]
 [0.806]
 [0.565]
 [0.565]
 [0.565]
 [0.565]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.2649925258940538, 0.3215792782458628, 0.24484782909035593, 0.15415464887767072, 0.014112584125831875, 0.0003131337662250335]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.23486669129149973
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.26499252589405387, 0.32157927824586274, 0.2448478290903559, 0.1541546488776707, 0.014112584125831874, 0.00031313376622503345]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.31 ]
 [0.368]
 [0.382]
 [0.378]
 [0.375]
 [0.385]] [[-0.635]
 [-0.097]
 [-2.801]
 [-2.616]
 [-2.735]
 [-2.76 ]
 [-2.712]] [[0.364]
 [0.31 ]
 [0.368]
 [0.382]
 [0.378]
 [0.375]
 [0.385]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]] [[-3.617]
 [-6.206]
 [-6.206]
 [-6.206]
 [-6.206]
 [-6.206]
 [-6.206]] [[0.278]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.6316477
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.2649925258940539, 0.32157927824586274, 0.24484782909035593, 0.15415464887767072, 0.014112584125831875, 0.0003131337662250335]
line 256 mcts: sample exp_bonus 1.9262452502204814
from probs:  [0.2649925258940539, 0.32157927824586274, 0.24484782909035593, 0.15415464887767072, 0.014112584125831875, 0.0003131337662250335]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.425]
 [0.47 ]
 [0.463]
 [0.47 ]
 [0.47 ]
 [0.47 ]] [[-7.007]
 [-2.604]
 [-7.241]
 [-6.807]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.474]
 [0.425]
 [0.47 ]
 [0.463]
 [0.47 ]
 [0.47 ]
 [0.47 ]]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[-7.036]
 [-7.138]
 [-7.138]
 [-7.138]
 [-7.138]
 [-7.138]
 [-7.138]] [[0.487]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[2.571]
 [2.571]
 [2.571]
 [2.571]
 [2.571]
 [2.571]
 [2.571]] [[2.438]
 [2.438]
 [2.438]
 [2.438]
 [2.438]
 [2.438]
 [2.438]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.612]] [[0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [3.256]] [[1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.994]]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.884]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.829]] [[3.847]
 [3.981]
 [3.847]
 [3.847]
 [3.847]
 [3.847]
 [4.169]] [[1.395]
 [1.623]
 [1.395]
 [1.395]
 [1.395]
 [1.395]
 [1.638]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[-2.335]
 [-2.335]
 [-2.335]
 [-2.335]
 [-2.335]
 [-2.335]
 [-2.335]] [[0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
siam score:  -0.6353313
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.2649925258940538, 0.32157927824586274, 0.2448478290903559, 0.1541546488776707, 0.014112584125831874, 0.00031313376622503345]
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.392]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]] [[-0.552]
 [ 0.633]
 [-0.552]
 [-0.552]
 [-0.552]
 [-0.552]
 [-0.552]] [[0.272]
 [0.392]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.6399999999999998  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.2770863330651098, 0.31628801534266093, 0.2408191048452422, 0.15161818950313696, 0.013880375778140277, 0.0003079814657098877]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.8029145184172428
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.41 ]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[-3.069]
 [-2.36 ]
 [-3.069]
 [-3.069]
 [-3.069]
 [-3.069]
 [-3.069]] [[0.409]
 [0.41 ]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]]
Printing some Q and Qe and total Qs values:  [[0.978]
 [0.883]
 [1.019]
 [1.04 ]
 [1.05 ]
 [1.006]
 [0.994]] [[2.387]
 [2.001]
 [2.401]
 [2.199]
 [2.245]
 [2.032]
 [2.455]] [[1.903]
 [1.593]
 [1.966]
 [1.902]
 [1.936]
 [1.777]
 [1.956]]
Printing some Q and Qe and total Qs values:  [[0.88 ]
 [0.555]
 [0.88 ]
 [0.548]
 [0.88 ]
 [0.88 ]
 [0.88 ]] [[ 0.   ]
 [-2.562]
 [ 0.   ]
 [-3.161]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[2.103]
 [0.597]
 [2.103]
 [0.384]
 [2.103]
 [2.103]
 [2.103]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.35499999999999954  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
probs:  [0.2851045631411699, 0.31277989232145614, 0.2381480487044072, 0.14993650940389863, 0.013726420953903536, 0.0003045654751647173]
from probs:  [0.2851045631411699, 0.31277989232145614, 0.2381480487044072, 0.14993650940389863, 0.013726420953903536, 0.0003045654751647173]
actor:  1 policy actor:  1  step number:  67 total reward:  0.5499999999999997  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.619]
 [0.687]
 [0.687]
 [0.615]
 [0.687]
 [0.687]] [[2.99 ]
 [3.408]
 [4.199]
 [4.199]
 [2.662]
 [4.199]
 [4.199]] [[0.478]
 [0.741]
 [1.404]
 [1.404]
 [0.237]
 [1.404]
 [1.404]]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.507]
 [0.497]
 [0.507]
 [0.507]
 [0.492]
 [0.493]] [[3.693]
 [3.693]
 [4.307]
 [3.693]
 [3.693]
 [3.304]
 [3.697]] [[0.507]
 [0.507]
 [0.497]
 [0.507]
 [0.507]
 [0.492]
 [0.493]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.5349999999999997  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
from probs:  [0.2909790863888056, 0.304030862489467, 0.2456090282294343, 0.14574250900335228, 0.01334246767122666, 0.0002960462177142687]
maxi score, test score, baseline:  -0.9944000000000001 -1.0 -0.9944000000000001
actor:  0 policy actor:  1  step number:  70 total reward:  0.2849999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.29484765208402114, 0.3080555284401558, 0.2488455627387151, 0.14764918912195815, 0.00030103380757496906, 0.00030103380757496906]
from probs:  [0.29484765208402114, 0.3080555284401558, 0.2488455627387151, 0.14764918912195815, 0.00030103380757496906, 0.00030103380757496906]
using explorer policy with actor:  1
siam score:  -0.6128632
siam score:  -0.6122986
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.079]
 [0.075]] [[-8.923]
 [-6.342]
 [-6.342]
 [-6.342]
 [-6.342]
 [-5.767]
 [-6.342]] [[0.088]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.079]
 [0.075]]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.2948476520840211, 0.30805552844015577, 0.2488455627387151, 0.14764918912195815, 0.00030103380757496906, 0.00030103380757496906]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.578]
 [0.544]
 [0.501]
 [0.513]
 [0.517]
 [0.52 ]] [[-4.252]
 [-3.069]
 [-5.091]
 [-4.741]
 [-4.65 ]
 [-4.535]
 [-4.478]] [[0.578]
 [0.578]
 [0.544]
 [0.501]
 [0.513]
 [0.517]
 [0.52 ]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.49  0.041 0.184 0.02  0.02  0.204]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.616]
 [0.553]
 [0.561]
 [0.553]
 [0.553]
 [0.664]] [[-0.382]
 [ 0.35 ]
 [-0.382]
 [-1.233]
 [-0.382]
 [-0.382]
 [ 0.324]] [[1.16 ]
 [1.676]
 [1.16 ]
 [0.623]
 [1.16 ]
 [1.16 ]
 [1.696]]
siam score:  -0.61777693
line 256 mcts: sample exp_bonus 0.024407360023700742
using explorer policy with actor:  1
siam score:  -0.6153294
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.2948476520840211, 0.3080555284401558, 0.24884556273871516, 0.14764918912195815, 0.00030103380757496906, 0.00030103380757496906]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.2948476520840211, 0.3080555284401558, 0.24884556273871516, 0.14764918912195815, 0.00030103380757496906, 0.00030103380757496906]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[-4.821]
 [-4.821]
 [-4.811]
 [-4.821]
 [-4.821]
 [-4.821]
 [-4.821]] [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.29484765208402114, 0.3080555284401558, 0.24884556273871516, 0.14764918912195815, 0.00030103380757496906, 0.00030103380757496906]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.29484765208402114, 0.3080555284401558, 0.24884556273871516, 0.14764918912195815, 0.00030103380757496906, 0.00030103380757496906]
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.845]] [[1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.103]]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
3009 4757
actor:  1 policy actor:  1  step number:  44 total reward:  0.6549999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.29011061883996186, 0.3031062969678245, 0.24484760075733908, 0.16134308866021696, 0.0002961973873288211, 0.0002961973873288211]
from probs:  [0.29011061883996186, 0.3031062969678245, 0.24484760075733908, 0.16134308866021696, 0.0002961973873288211, 0.0002961973873288211]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.2901106188399619, 0.30310629696782443, 0.24484760075733913, 0.161343088660217, 0.00029619738732882114, 0.00029619738732882114]
siam score:  -0.61233026
line 256 mcts: sample exp_bonus -2.972076747459128
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.2901106188399619, 0.30310629696782443, 0.24484760075733913, 0.161343088660217, 0.00029619738732882114, 0.00029619738732882114]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.266]
 [0.303]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[-5.254]
 [-5.254]
 [-5.069]
 [-5.254]
 [-5.254]
 [-5.254]
 [-5.254]] [[0.266]
 [0.266]
 [0.303]
 [0.266]
 [0.266]
 [0.266]
 [0.266]]
siam score:  -0.6085522
using another actor
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.2901106188399619, 0.30310629696782443, 0.24484760075733913, 0.161343088660217, 0.00029619738732882114, 0.00029619738732882114]
actor:  1 policy actor:  1  step number:  42 total reward:  0.7249999999999999  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.3022931847990721, 0.2979046239277262, 0.2406457178649907, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
first move QE:  -0.5603514201515417
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.3022931847990721, 0.2979046239277262, 0.2406457178649907, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
using explorer policy with actor:  1
siam score:  -0.6082336
siam score:  -0.6095603
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.734]
 [0.69 ]
 [0.697]
 [0.693]
 [0.693]
 [0.71 ]] [[-2.409]
 [ 1.496]
 [-1.198]
 [-2.175]
 [-2.092]
 [-1.472]
 [-1.198]] [[0.084]
 [1.502]
 [0.502]
 [0.156]
 [0.183]
 [0.406]
 [0.516]]
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.836]] [[4.862]
 [4.862]
 [4.862]
 [4.862]
 [4.862]
 [4.862]
 [4.674]] [[1.799]
 [1.799]
 [1.799]
 [1.799]
 [1.799]
 [1.799]
 [1.698]]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.778]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]] [[4.285]
 [5.005]
 [4.153]
 [4.153]
 [4.153]
 [4.153]
 [4.153]] [[1.209]
 [1.807]
 [1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.236]]
line 256 mcts: sample exp_bonus -0.5624571235298064
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.999]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]] [[-1.007]
 [-0.7  ]
 [-1.007]
 [-1.007]
 [-1.007]
 [-1.007]
 [-1.007]] [[0.94 ]
 [1.299]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.574]
 [0.84 ]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[0.454]
 [0.454]
 [0.323]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[0.62 ]
 [0.62 ]
 [1.108]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
using another actor
from probs:  [0.3022931847990721, 0.2979046239277262, 0.2406457178649907, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
3030 4789
from probs:  [0.3022931847990721, 0.2979046239277262, 0.2406457178649907, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
using another actor
from probs:  [0.3022931847990721, 0.2979046239277262, 0.2406457178649907, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
line 256 mcts: sample exp_bonus 0.7256261487115462
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.41 ]] [[0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.755]] [[0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.41 ]]
from probs:  [0.3022931847990721, 0.2979046239277262, 0.2406457178649907, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
using explorer policy with actor:  0
from probs:  [0.3022931847990721, 0.2979046239277262, 0.2406457178649907, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.489]
 [0.361]
 [0.418]
 [0.418]
 [0.418]
 [0.446]] [[1.402]
 [2.363]
 [2.197]
 [1.402]
 [1.402]
 [1.402]
 [0.792]] [[0.418]
 [0.489]
 [0.361]
 [0.418]
 [0.418]
 [0.418]
 [0.446]]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.3022931847990721, 0.29790462392772626, 0.2406457178649907, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
line 256 mcts: sample exp_bonus -2.8810372947564584
UNIT TEST: sample policy line 217 mcts : [0.469 0.265 0.    0.082 0.02  0.    0.163]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.3022931847990721, 0.29790462392772626, 0.2406457178649907, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[1.081]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]] [[-2.687]
 [-2.165]
 [-2.165]
 [-2.165]
 [-2.165]
 [-2.165]
 [-2.165]] [[0.941]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]]
Printing some Q and Qe and total Qs values:  [[1.122]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]] [[-0.615]
 [-1.641]
 [-1.641]
 [-1.641]
 [-1.641]
 [-1.641]
 [-1.641]] [[1.676]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.3022931847990721, 0.29790462392772626, 0.2406457178649907, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
using another actor
using another actor
from probs:  [0.3022931847990721, 0.29790462392772626, 0.2406457178649907, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
siam score:  -0.60348207
first move QE:  -0.5674923625012206
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.3022931847990721, 0.29790462392772626, 0.24064571786499064, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
start point for exploration sampling:  10749
siam score:  -0.60637885
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[-0.242]
 [-0.242]
 [-0.242]
 [-0.242]
 [-0.242]
 [-0.242]
 [-0.242]] [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]] [[ -9.988]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.143]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.3022931847990721, 0.29790462392772626, 0.2406457178649907, 0.15857424484903376, 0.00029111427958863645, 0.00029111427958863645]
using another actor
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.583]
 [0.5  ]
 [0.477]
 [0.477]
 [0.477]
 [0.496]] [[ 0.391]
 [ 1.791]
 [-0.33 ]
 [ 0.391]
 [ 0.391]
 [ 0.391]
 [ 0.38 ]] [[0.477]
 [0.583]
 [0.5  ]
 [0.477]
 [0.477]
 [0.477]
 [0.496]]
using explorer policy with actor:  0
using explorer policy with actor:  0
siam score:  -0.61054444
Printing some Q and Qe and total Qs values:  [[0.53]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]] [[0.397]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]] [[1.954]
 [1.888]
 [1.888]
 [1.888]
 [1.888]
 [1.888]
 [1.888]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.65]
 [0.82]
 [0.65]
 [0.65]
 [0.65]
 [0.65]
 [0.65]] [[-4.146]
 [-3.381]
 [-4.146]
 [-4.146]
 [-4.146]
 [-4.146]
 [-4.146]] [[0.622]
 [1.049]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.605]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[1.136]
 [1.344]
 [1.136]
 [1.136]
 [1.136]
 [1.136]
 [1.136]] [[0.445]
 [0.86 ]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [0.696]
 [0.546]
 [0.546]
 [0.512]
 [0.546]] [[2.279]
 [2.279]
 [4.061]
 [2.279]
 [2.279]
 [2.819]
 [2.279]] [[1.166]
 [1.166]
 [2.138]
 [1.166]
 [1.166]
 [1.355]
 [1.166]]
line 256 mcts: sample exp_bonus 2.5698547168400117
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
using another actor
line 256 mcts: sample exp_bonus 3.4125734213630783
3046 4810
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.3022931847990721, 0.29790462392772626, 0.24064571786499064, 0.15857424484903374, 0.00029111427958863645, 0.00029111427958863645]
actor:  1 policy actor:  1  step number:  51 total reward:  0.48999999999999966  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.29847724514637075, 0.3067673893496898, 0.23760797310847503, 0.15657251348608953, 0.0002874394546874789, 0.0002874394546874789]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.29847724514637075, 0.3067673893496898, 0.23760797310847503, 0.15657251348608953, 0.0002874394546874789, 0.0002874394546874789]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.29847724514637075, 0.3067673893496898, 0.23760797310847506, 0.15657251348608953, 0.0002874394546874789, 0.0002874394546874789]
actor:  1 policy actor:  1  step number:  53 total reward:  0.6399999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.421]
 [0.365]
 [0.353]
 [0.353]
 [0.378]
 [0.355]] [[2.335]
 [2.511]
 [3.047]
 [2.866]
 [2.866]
 [2.605]
 [1.974]] [[0.377]
 [0.421]
 [0.365]
 [0.353]
 [0.353]
 [0.378]
 [0.355]]
from probs:  [0.30905442866431526, 0.30214211532682583, 0.23402544763869246, 0.1542117971763901, 0.0002831055968881525, 0.0002831055968881525]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]] [[-9.451]
 [-8.944]
 [-8.944]
 [-8.944]
 [-8.944]
 [-8.944]
 [-8.944]] [[0.361]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]]
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]] [[-9.68 ]
 [-8.651]
 [-8.651]
 [-8.651]
 [-8.651]
 [-8.651]
 [-8.651]] [[0.116]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]]
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.072]
 [0.091]
 [0.09 ]
 [0.091]
 [0.089]
 [0.091]] [[-9.866]
 [-3.056]
 [-7.774]
 [-7.83 ]
 [-8.152]
 [-7.739]
 [-7.836]] [[0.393]
 [0.072]
 [0.091]
 [0.09 ]
 [0.091]
 [0.089]
 [0.091]]
siam score:  -0.6076534
first move QE:  -0.5733247770628924
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.30905442866431526, 0.3021421153268259, 0.23402544763869243, 0.15421179717639016, 0.00028310559688815253, 0.00028310559688815253]
Printing some Q and Qe and total Qs values:  [[0.732]
 [1.117]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.723]] [[0.617]
 [1.072]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.601]] [[0.561]
 [1.568]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.536]]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[ -9.293]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.345]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
siam score:  -0.6105343
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.30905442866431526, 0.3021421153268259, 0.23402544763869243, 0.15421179717639016, 0.00028310559688815253, 0.00028310559688815253]
3054 4823
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.828]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.818]] [[4.437]
 [4.41 ]
 [4.437]
 [4.437]
 [4.437]
 [4.437]
 [4.414]] [[1.265]
 [1.487]
 [1.265]
 [1.265]
 [1.265]
 [1.265]
 [1.468]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -5.175664277878824
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.30905442866431526, 0.30214211532682583, 0.23402544763869246, 0.15421179717639014, 0.0002831055968881525, 0.0002831055968881525]
3056 4823
from probs:  [0.30905442866431526, 0.30214211532682583, 0.2340254476386925, 0.15421179717639016, 0.00028310559688815253, 0.00028310559688815253]
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.523]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[1.218]
 [2.198]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]] [[0.082]
 [0.939]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]]
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.113]
 [0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]] [[-9.197]
 [-2.23 ]
 [-8.138]
 [-8.138]
 [-8.138]
 [-8.138]
 [-8.138]] [[0.15 ]
 [0.113]
 [0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]]
rdn beta is 0 so we're just using the maxi policy
3058 4827
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.359]
 [0.344]
 [1.029]
 [0.425]
 [0.524]
 [0.224]] [[ 0.162]
 [-0.023]
 [ 0.359]
 [-0.24 ]
 [-0.336]
 [-0.317]
 [ 0.619]] [[1.686]
 [1.104]
 [1.445]
 [2.195]
 [0.928]
 [1.138]
 [1.464]]
UNIT TEST: sample policy line 217 mcts : [0.    0.02  0.694 0.02  0.02  0.224 0.02 ]
using another actor
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.76 ]
 [0.754]
 [0.759]
 [0.759]
 [0.733]
 [0.755]] [[1.975]
 [2.581]
 [2.698]
 [1.975]
 [1.975]
 [3.311]
 [2.612]] [[0.78 ]
 [1.185]
 [1.251]
 [0.78 ]
 [0.78 ]
 [1.617]
 [1.195]]
from probs:  [0.30905442866431526, 0.30214211532682583, 0.23402544763869243, 0.15421179717639016, 0.00028310559688815253, 0.00028310559688815253]
line 256 mcts: sample exp_bonus 4.525004858353906
Starting evaluation
line 256 mcts: sample exp_bonus -1.596451283459751
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.908]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.742]] [[0.474]
 [0.497]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.2  ]] [[1.611]
 [2.128]
 [1.611]
 [1.611]
 [1.611]
 [1.611]
 [1.698]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.5765114587760996
from probs:  [0.30905442866431526, 0.30214211532682583, 0.2340254476386925, 0.15421179717639016, 0.00028310559688815253, 0.00028310559688815253]
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.509]
 [0.442]
 [0.446]
 [0.445]
 [0.435]
 [0.473]] [[-5.148]
 [-5.225]
 [-4.951]
 [-5.485]
 [-5.378]
 [-5.1  ]
 [-4.978]] [[0.559]
 [0.509]
 [0.442]
 [0.446]
 [0.445]
 [0.435]
 [0.473]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.717]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.637]
 [0.599]] [[-4.711]
 [-3.187]
 [-4.393]
 [-4.711]
 [-4.711]
 [-4.651]
 [-4.622]] [[0.56 ]
 [0.717]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.637]
 [0.599]]
line 256 mcts: sample exp_bonus -2.5192879598263267
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.653]
 [0.527]
 [0.527]
 [0.527]
 [0.547]
 [0.564]] [[-4.999]
 [-3.835]
 [-4.999]
 [-4.999]
 [-4.999]
 [-4.599]
 [-4.384]] [[0.527]
 [0.653]
 [0.527]
 [0.527]
 [0.527]
 [0.547]
 [0.564]]
using explorer policy with actor:  1
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -4.258056988975311
maxi score, test score, baseline:  -0.9918300000000001 -1.0 -0.9918300000000001
probs:  [0.30905442866431526, 0.30214211532682583, 0.2340254476386925, 0.15421179717639016, 0.00028310559688815253, 0.00028310559688815253]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.653]] [[-0.797]
 [-0.797]
 [-0.797]
 [-0.797]
 [-0.797]
 [-0.797]
 [-0.038]] [[1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [2.123]]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.66 ]
 [0.657]
 [0.685]
 [0.685]
 [0.647]
 [0.651]] [[-2.642]
 [-1.627]
 [-0.406]
 [ 0.   ]
 [ 0.   ]
 [-2.217]
 [-1.564]] [[0.674]
 [0.66 ]
 [0.657]
 [0.685]
 [0.685]
 [0.647]
 [0.651]]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.61 ]
 [0.596]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[-1.945]
 [-0.092]
 [-1.211]
 [-1.945]
 [-1.945]
 [-1.945]
 [-1.945]] [[0.59 ]
 [0.61 ]
 [0.596]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]]
actor:  0 policy actor:  1  step number:  53 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  1
maxi score, test score, baseline:  -0.98895 -1.0 -0.98895
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[-2.491]
 [-2.491]
 [-2.491]
 [-2.491]
 [-2.491]
 [-2.491]
 [-2.491]] [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]]
actor:  0 policy actor:  1  step number:  59 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1
maxi score, test score, baseline:  -0.98611 -1.0 -0.98611
probs:  [0.309120235997332, 0.3021405950578146, 0.23399128752376627, 0.15417716582691113, 0.00028535779708795643, 0.00028535779708795643]
actor:  0 policy actor:  1  step number:  63 total reward:  0.34999999999999953  reward:  1.0 rdn_beta:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.98341 -1.0 -0.98341
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.98341 -1.0 -0.98341
probs:  [0.30915145734472116, 0.3021398710930939, 0.23397508782166362, 0.15416072336683936, 0.0002864301868409904, 0.0002864301868409904]
maxi score, test score, baseline:  -0.98341 -1.0 -0.98341
actor:  1 policy actor:  1  step number:  58 total reward:  0.46499999999999964  reward:  1.0 rdn_beta:  0.333
from probs:  [0.30915145734472116, 0.3021398710930939, 0.23397508782166362, 0.15416072336683936, 0.0002864301868409904, 0.0002864301868409904]
actor:  0 policy actor:  1  step number:  82 total reward:  0.004999999999999227  reward:  1.0 rdn_beta:  1
rdn probs:  [0.30915145734472116, 0.3021398710930939, 0.23397508782166362, 0.15416072336683936, 0.0002864301868409904, 0.0002864301868409904]
from probs:  [0.3087470079780836, 0.3103577706715661, 0.22951906000220393, 0.15055563566137006, 0.000410262843388186, 0.000410262843388186]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.6199999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9814 -0.7392500000000001 -0.7392500000000001
maxi score, test score, baseline:  -0.9814 -0.7392500000000001 -0.7392500000000001
probs:  [0.31906971297463166, 0.30572309748099086, 0.2260915774815747, 0.14830733955207695, 0.0004041362553629338, 0.0004041362553629338]
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]] [[-8.996]
 [-9.881]
 [-9.881]
 [-9.881]
 [-9.881]
 [-9.881]
 [-9.881]] [[0.257]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]] [[ 0.177]
 [-3.348]
 [-3.348]
 [-3.348]
 [-3.348]
 [-3.348]
 [-3.348]] [[0.481]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]]
maxi score, test score, baseline:  -0.9814 -0.7392500000000001 -0.7392500000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9814 -0.7392500000000001 -0.7392500000000001
maxi score, test score, baseline:  -0.9814 -0.7392500000000001 -0.7392500000000001
probs:  [0.31906971297463166, 0.30572309748099086, 0.22609157748157474, 0.14830733955207695, 0.0004041362553629338, 0.0004041362553629338]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.717]
 [0.692]
 [0.682]
 [0.684]
 [0.695]
 [0.699]] [[3.468]
 [4.461]
 [3.582]
 [4.049]
 [3.724]
 [3.865]
 [4.556]] [[0.67 ]
 [0.717]
 [0.692]
 [0.682]
 [0.684]
 [0.695]
 [0.699]]
maxi score, test score, baseline:  -0.9814 -0.7392500000000001 -0.7392500000000001
probs:  [0.31906971297463166, 0.30572309748099086, 0.2260915774815747, 0.14830733955207695, 0.0004041362553629338, 0.0004041362553629338]
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.013]
 [0.235]
 [0.239]
 [0.229]
 [0.226]
 [0.226]] [[-5.565]
 [-1.19 ]
 [-3.925]
 [-3.767]
 [-3.807]
 [-4.136]
 [-3.588]] [[0.08 ]
 [1.431]
 [0.62 ]
 [0.674]
 [0.657]
 [0.547]
 [0.728]]
actor:  1 policy actor:  1  step number:  160 total reward:  0.07499999999999929  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.088]
 [0.235]
 [0.239]
 [0.229]
 [0.226]
 [0.226]] [[-5.461]
 [-1.14 ]
 [-4.128]
 [-3.984]
 [-3.986]
 [-4.459]
 [-4.105]] [[0.111]
 [1.613]
 [0.601]
 [0.656]
 [0.65 ]
 [0.475]
 [0.604]]
Printing some Q and Qe and total Qs values:  [[ 1.094]
 [-0.076]
 [ 1.094]
 [ 1.094]
 [-0.076]
 [ 1.094]
 [ 1.094]] [[-0.576]
 [ 0.465]
 [-0.576]
 [-0.576]
 [ 0.465]
 [-0.569]
 [-0.565]] [[1.993]
 [1.833]
 [1.993]
 [1.993]
 [1.833]
 [1.997]
 [1.998]]
from probs:  [0.31736897342811543, 0.3094238065622089, 0.22488643994792318, 0.1475168158917041, 0.0004019820850242048, 0.0004019820850242048]
using another actor
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.492]
 [0.537]
 [0.526]
 [0.402]
 [0.567]
 [0.285]] [[ 0.217]
 [ 0.572]
 [-2.305]
 [-0.112]
 [ 0.612]
 [-3.604]
 [ 1.306]] [[0.226]
 [0.492]
 [0.537]
 [0.526]
 [0.402]
 [0.567]
 [0.285]]
maxi score, test score, baseline:  -0.9814 -0.7392500000000001 -0.7392500000000001
probs:  [0.31736897342811543, 0.3094238065622089, 0.22488643994792318, 0.1475168158917041, 0.0004019820850242048, 0.0004019820850242048]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[-2.475]
 [-2.154]
 [-2.154]
 [-2.154]
 [-2.154]
 [-2.154]
 [-2.154]] [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.448]
 [0.433]] [[3.88 ]
 [3.88 ]
 [3.88 ]
 [3.88 ]
 [3.88 ]
 [3.836]
 [3.907]] [[1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.165]
 [1.205]]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.389]
 [0.291]
 [0.348]
 [0.291]
 [0.291]
 [0.419]] [[4.052]
 [3.731]
 [4.052]
 [2.894]
 [4.052]
 [4.052]
 [3.929]] [[1.042]
 [1.031]
 [1.042]
 [0.617]
 [1.042]
 [1.042]
 [1.155]]
maxi score, test score, baseline:  -0.9814 -0.7392500000000001 -0.7392500000000001
maxi score, test score, baseline:  -0.9814 -0.7392500000000001 -0.7392500000000001
actor:  1 policy actor:  1  step number:  49 total reward:  0.6699999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]] [[0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]] [[1.788]
 [1.788]
 [1.788]
 [1.788]
 [1.788]
 [1.788]
 [1.788]]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]] [[-2.907]
 [-2.907]
 [-2.907]
 [-2.907]
 [-2.907]
 [-2.907]
 [-2.907]] [[0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]]
maxi score, test score, baseline:  -0.9814 -0.7392500000000001 -0.7392500000000001
actor:  1 policy actor:  1  step number:  80 total reward:  0.5149999999999997  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.426]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[-2.829]
 [-2.495]
 [-2.829]
 [-2.829]
 [-2.829]
 [-2.829]
 [-2.829]] [[1.732]
 [1.92 ]
 [1.732]
 [1.732]
 [1.732]
 [1.732]
 [1.732]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.602]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[-2.926]
 [-0.915]
 [-2.926]
 [-2.926]
 [-2.926]
 [-2.926]
 [-2.926]] [[0.913]
 [1.706]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]]
using explorer policy with actor:  1
first move QE:  -0.5887775561801358
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.222]
 [0.325]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[-4.835]
 [-4.835]
 [-2.667]
 [-4.835]
 [-4.835]
 [-4.835]
 [-4.835]] [[0.222]
 [0.222]
 [0.325]
 [0.222]
 [0.222]
 [0.222]
 [0.222]]
maxi score, test score, baseline:  -0.9814 -0.7392500000000001 -0.7392500000000001
probs:  [0.30854036279818464, 0.3163386666824641, 0.23092619685749471, 0.14341317426975447, 0.00039079969605104243, 0.00039079969605104243]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.236]
 [0.247]
 [0.247]
 [0.247]
 [0.242]
 [0.243]] [[-3.53 ]
 [-2.623]
 [-3.936]
 [-3.936]
 [-3.936]
 [-3.364]
 [-3.336]] [[0.249]
 [0.236]
 [0.247]
 [0.247]
 [0.247]
 [0.242]
 [0.243]]
Printing some Q and Qe and total Qs values:  [[ 0.088]
 [-0.001]
 [ 0.379]
 [ 0.291]
 [-0.013]
 [ 0.247]
 [ 0.281]] [[-0.157]
 [ 0.149]
 [ 0.36 ]
 [-0.325]
 [-0.065]
 [ 0.152]
 [ 0.193]] [[-0.08 ]
 [-0.155]
 [ 0.675]
 [ 0.27 ]
 [-0.252]
 [ 0.341]
 [ 0.422]]
maxi score, test score, baseline:  -0.9814 -0.7392500000000001 -0.7392500000000001
probs:  [0.30854036279818464, 0.3163386666824641, 0.23092619685749471, 0.14341317426975447, 0.00039079969605104243, 0.00039079969605104243]
in main func line 156:  3085
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  75 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  53 total reward:  0.5199999999999997  reward:  1.0 rdn_beta:  0.5
using another actor
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.201]
 [0.267]
 [0.201]
 [0.185]
 [0.241]
 [0.14 ]] [[-0.703]
 [ 0.   ]
 [-3.284]
 [ 0.   ]
 [-1.259]
 [-3.212]
 [-1.731]] [[ 0.338]
 [ 0.659]
 [-0.306]
 [ 0.659]
 [ 0.207]
 [-0.333]
 [-0.042]]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.53 ]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[-2.48 ]
 [-0.645]
 [-2.48 ]
 [-2.48 ]
 [-2.48 ]
 [-2.48 ]
 [-2.48 ]] [[0.501]
 [0.53 ]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]]
maxi score, test score, baseline:  -0.9788000000000001 -0.7392500000000001 -0.7392500000000001
probs:  [0.3047631574464002, 0.3124659931238642, 0.24034133353135412, 0.14165748498341485, 0.00038601545748331487, 0.00038601545748331487]
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.068]
 [ 0.112]
 [-0.073]
 [ 0.112]
 [ 0.112]
 [-0.074]] [[-0.595]
 [-0.628]
 [-1.396]
 [-0.608]
 [-1.396]
 [-1.396]
 [-0.581]] [[1.644]
 [1.634]
 [1.433]
 [1.639]
 [1.433]
 [1.433]
 [1.648]]
actor:  1 policy actor:  1  step number:  76 total reward:  0.5349999999999997  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.272]
 [0.292]
 [0.291]
 [0.275]
 [0.309]
 [0.274]] [[-5.971]
 [-5.274]
 [-6.038]
 [-5.765]
 [-5.952]
 [-5.743]
 [-5.923]] [[0.289]
 [0.272]
 [0.292]
 [0.291]
 [0.275]
 [0.309]
 [0.274]]
maxi score, test score, baseline:  -0.9788000000000001 -0.7392500000000001 -0.7392500000000001
probs:  [0.3009900762357418, 0.3209779197778898, 0.237365818455024, 0.13990371264617277, 0.0003812364425857869, 0.0003812364425857869]
Printing some Q and Qe and total Qs values:  [[-0.078]
 [-0.077]
 [-0.078]
 [-0.079]
 [-0.078]
 [-0.079]
 [-0.079]] [[-0.613]
 [-0.594]
 [-0.565]
 [-0.607]
 [-0.599]
 [-0.578]
 [-0.523]] [[-0.145]
 [-0.106]
 [-0.052]
 [-0.135]
 [-0.117]
 [-0.079]
 [ 0.03 ]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5499999999999997  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.27 ]
 [0.174]
 [0.197]
 [0.234]
 [0.253]
 [0.211]] [[-3.579]
 [-0.795]
 [-3.78 ]
 [-3.622]
 [-3.461]
 [-3.502]
 [-3.537]] [[-0.247]
 [ 0.544]
 [-0.296]
 [-0.238]
 [-0.168]
 [-0.163]
 [-0.205]]
Printing some Q and Qe and total Qs values:  [[ 0.049]
 [ 0.228]
 [ 0.24 ]
 [ 0.243]
 [-0.026]
 [ 0.24 ]
 [ 0.165]] [[0.367]
 [0.593]
 [0.298]
 [0.569]
 [0.125]
 [0.298]
 [0.724]] [[0.468]
 [1.022]
 [0.764]
 [1.027]
 [0.094]
 [0.764]
 [1.028]]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.156]
 [0.257]
 [0.221]
 [0.171]
 [0.241]
 [0.185]] [[-0.037]
 [ 0.065]
 [-4.245]
 [-0.303]
 [-1.089]
 [-4.643]
 [ 0.457]] [[0.073]
 [0.156]
 [0.257]
 [0.221]
 [0.171]
 [0.241]
 [0.185]]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.339]
 [0.478]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[-3.331]
 [-3.908]
 [-1.054]
 [-3.908]
 [-3.908]
 [-3.908]
 [-3.908]] [[0.34 ]
 [0.339]
 [0.478]
 [0.339]
 [0.339]
 [0.339]
 [0.339]]
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]] [[ -4.962]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.154]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]]
3100 4909
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]] [[-7.807]
 [-7.807]
 [-7.807]
 [-7.807]
 [-7.807]
 [-7.807]
 [-7.807]] [[0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]] [[-2.364]
 [-2.364]
 [-2.364]
 [-2.364]
 [-2.364]
 [-2.364]
 [-2.364]] [[0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]]
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.056]
 [0.05 ]
 [0.053]
 [0.056]
 [0.041]
 [0.043]] [[2.246]
 [2.215]
 [2.439]
 [2.242]
 [2.215]
 [2.544]
 [2.359]] [[-0.666]
 [-0.71 ]
 [-0.574]
 [-0.698]
 [-0.71 ]
 [-0.522]
 [-0.64 ]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.5899999999999997  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[-3.557]
 [-6.09 ]
 [-6.09 ]
 [-6.09 ]
 [-6.09 ]
 [-6.09 ]
 [-6.09 ]] [[0.156]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]]
Printing some Q and Qe and total Qs values:  [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]] [[4.687]
 [4.687]
 [4.687]
 [4.687]
 [4.687]
 [4.687]
 [4.687]] [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[2.36]
 [2.36]
 [2.36]
 [2.36]
 [2.36]
 [2.36]
 [2.36]] [[-0.704]
 [-0.704]
 [-0.704]
 [-0.704]
 [-0.704]
 [-0.704]
 [-0.704]]
using explorer policy with actor:  1
from probs:  [0.2933209828039717, 0.3127995449719913, 0.244446753612872, 0.14868967319638954, 0.00037152270738776615, 0.00037152270738776615]
maxi score, test score, baseline:  -0.9788000000000001 -0.7392500000000001 -0.7392500000000001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.579]
 [0.578]] [[-1.251]
 [-1.346]
 [-1.346]
 [-1.346]
 [-1.346]
 [-0.839]
 [-1.346]] [[0.585]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.579]
 [0.578]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9788000000000001 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]] [[-6.193]
 [-9.84 ]
 [-9.84 ]
 [-9.84 ]
 [-9.84 ]
 [-9.84 ]
 [-9.84 ]] [[0.074]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]] [[ -6.133]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.106]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]]
maxi score, test score, baseline:  -0.9788000000000001 -0.7392500000000001 -0.7392500000000001
probs:  [0.2933209828039717, 0.3127995449719913, 0.244446753612872, 0.14868967319638954, 0.00037152270738776615, 0.00037152270738776615]
line 256 mcts: sample exp_bonus -2.9052430082318237
line 256 mcts: sample exp_bonus -0.021677821474229697
Printing some Q and Qe and total Qs values:  [[-0.012]
 [ 0.183]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]] [[-0.125]
 [ 0.957]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]] [[0.236]
 [1.232]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]]
first move QE:  -0.6023201571345546
actor:  1 policy actor:  1  step number:  56 total reward:  0.46499999999999964  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.157]
 [ 0.257]
 [ 0.094]
 [-0.003]
 [ 0.157]
 [ 0.157]
 [ 0.157]] [[1.252]
 [0.936]
 [1.069]
 [0.998]
 [1.252]
 [1.252]
 [1.252]] [[ 0.157]
 [ 0.257]
 [ 0.094]
 [-0.003]
 [ 0.157]
 [ 0.157]
 [ 0.157]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.582]
 [0.6  ]
 [0.599]
 [0.599]
 [0.599]
 [0.599]] [[-3.594]
 [-2.107]
 [-0.739]
 [-3.594]
 [-3.594]
 [-3.594]
 [-3.594]] [[0.599]
 [0.582]
 [0.6  ]
 [0.599]
 [0.599]
 [0.599]
 [0.599]]
siam score:  -0.59908
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.499]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.534]] [[ 0.   ]
 [-2.482]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-2.851]] [[0.739]
 [0.499]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.534]]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.602]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.551]] [[2.028]
 [2.19 ]
 [2.028]
 [2.028]
 [2.028]
 [2.028]
 [0.54 ]] [[0.539]
 [0.602]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.551]]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.387]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[4.131]
 [3.69 ]
 [4.131]
 [4.131]
 [4.131]
 [4.131]
 [4.131]] [[0.516]
 [0.556]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.22 ]
 [0.259]
 [0.233]
 [0.134]
 [0.07 ]
 [0.197]] [[0.499]
 [0.655]
 [0.36 ]
 [0.383]
 [0.109]
 [0.022]
 [0.572]] [[0.571]
 [1.075]
 [0.826]
 [0.808]
 [0.361]
 [0.166]
 [0.951]]
maxi score, test score, baseline:  -0.9788000000000001 -0.7392500000000001 -0.7392500000000001
probs:  [0.29019391733652505, 0.3201257198388191, 0.24184073138239592, 0.14710450755984494, 0.00036756194120752677, 0.00036756194120752677]
line 256 mcts: sample exp_bonus -7.067905229429899
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9788000000000001 -0.7392500000000001 -0.7392500000000001
probs:  [0.29019391733652505, 0.3201257198388191, 0.24184073138239592, 0.14710450755984494, 0.00036756194120752677, 0.00036756194120752677]
actor:  1 policy actor:  1  step number:  74 total reward:  0.5049999999999997  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[ 0.06 ]
 [ 0.079]
 [-0.005]
 [ 0.079]
 [ 0.079]
 [ 0.079]
 [ 0.166]] [[ 0.692]
 [ 0.708]
 [-0.446]
 [ 0.708]
 [ 0.708]
 [ 0.708]
 [ 0.627]] [[1.008]
 [1.03 ]
 [0.417]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.054]]
siam score:  -0.6054011
maxi score, test score, baseline:  -0.9788000000000001 -0.7392500000000001 -0.7392500000000001
probs:  [0.29818123227164206, 0.31652340505787363, 0.23911934291747625, 0.1454491680819881, 0.00036342583551007905, 0.00036342583551007905]
maxi score, test score, baseline:  -0.9788000000000001 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.435]
 [0.43 ]
 [0.448]
 [0.441]
 [0.448]
 [0.455]] [[3.336]
 [4.369]
 [4.101]
 [4.214]
 [3.962]
 [4.21 ]
 [4.092]] [[0.547]
 [1.249]
 [1.04 ]
 [1.152]
 [0.953]
 [1.149]
 [1.071]]
maxi score, test score, baseline:  -0.9788000000000001 -0.7392500000000001 -0.7392500000000001
actor:  0 policy actor:  1  step number:  94 total reward:  0.22499999999999942  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.227]
 [0.22 ]
 [0.227]
 [0.227]
 [0.282]
 [0.227]] [[2.91 ]
 [2.91 ]
 [3.523]
 [2.91 ]
 [2.91 ]
 [4.338]
 [2.91 ]] [[-0.416]
 [-0.416]
 [-0.225]
 [-0.416]
 [-0.416]
 [ 0.172]
 [-0.416]]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.204]
 [0.163]
 [0.135]
 [0.135]
 [0.135]
 [0.165]] [[4.584]
 [4.687]
 [3.478]
 [4.584]
 [4.584]
 [4.584]
 [4.321]] [[1.058]
 [1.176]
 [0.546]
 [1.058]
 [1.058]
 [1.058]
 [0.959]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.6899999999999998  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
probs:  [0.29381867055094774, 0.31189248687402443, 0.250251461472748, 0.14332116368631487, 0.00035810870798253815, 0.00035810870798253815]
Printing some Q and Qe and total Qs values:  [[ 0.188]
 [ 0.188]
 [ 0.188]
 [ 0.188]
 [-0.046]
 [ 0.188]
 [ 0.188]] [[-2.485]
 [-2.485]
 [-2.485]
 [-2.485]
 [-0.809]
 [-2.485]
 [-2.485]] [[1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.548]
 [1.009]
 [1.009]]
using explorer policy with actor:  1
first move QE:  -0.606085485456016
actor:  1 policy actor:  1  step number:  61 total reward:  0.48999999999999966  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.322]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[5.069]
 [5.029]
 [4.683]
 [4.683]
 [4.683]
 [4.683]
 [4.683]] [[0.666]
 [0.713]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]]
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]] [[-9.662]
 [-9.814]
 [-9.814]
 [-9.814]
 [-9.814]
 [-9.814]
 [-9.814]] [[0.063]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]]
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
probs:  [0.3013794516860353, 0.3085531875572718, 0.24757212622268207, 0.14178668535126215, 0.00035427459137436134, 0.00035427459137436134]
line 256 mcts: sample exp_bonus -2.808742665878789
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.038]] [[-4.128]
 [-3.682]
 [-3.682]
 [-3.682]
 [-3.682]
 [-3.682]
 [-3.682]] [[0.07 ]
 [0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.038]]
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.056]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.082]] [[-0.972]
 [-0.076]
 [-0.972]
 [-0.972]
 [-0.972]
 [-0.972]
 [ 0.193]] [[0.118]
 [0.056]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.082]]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.201]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]] [[-2.77 ]
 [-0.692]
 [-2.77 ]
 [-2.77 ]
 [-2.77 ]
 [-2.77 ]
 [-2.77 ]] [[0.21 ]
 [0.201]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.432]
 [0.432]
 [0.5  ]
 [0.432]
 [0.432]
 [0.44 ]] [[-0.895]
 [-1.026]
 [-1.026]
 [-0.93 ]
 [-1.026]
 [-1.026]
 [-0.756]] [[0.543]
 [0.432]
 [0.432]
 [0.5  ]
 [0.432]
 [0.432]
 [0.44 ]]
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
probs:  [0.3013794516860353, 0.3085531875572718, 0.24757212622268207, 0.14178668535126215, 0.00035427459137436134, 0.00035427459137436134]
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]] [[-3.724]
 [-3.938]
 [-3.938]
 [-3.938]
 [-3.938]
 [-3.938]
 [-3.938]] [[0.111]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.455]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[-1.854]
 [-0.634]
 [-1.854]
 [-1.854]
 [-1.854]
 [-1.854]
 [-1.854]] [[0.088]
 [0.662]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
from probs:  [0.3013794516860353, 0.3085531875572718, 0.24757212622268207, 0.14178668535126215, 0.00035427459137436134, 0.00035427459137436134]
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[-0.878]
 [-2.388]
 [-2.388]
 [-2.388]
 [-2.388]
 [-2.388]
 [-2.388]] [[0.389]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]]
using explorer policy with actor:  1
3126 4958
siam score:  -0.6116403
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
line 256 mcts: sample exp_bonus 1.09565498273886
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.469]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[0.992]
 [2.991]
 [0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]] [[0.586]
 [1.243]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]]
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]] [[-1.384]
 [-3.627]
 [-3.627]
 [-3.627]
 [-3.627]
 [-3.627]
 [-3.627]] [[0.323]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]]
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.009]
 [ 0.221]
 [ 0.123]
 [ 0.044]
 [ 0.044]
 [-0.061]] [[0.89 ]
 [0.763]
 [1.301]
 [0.921]
 [1.314]
 [1.314]
 [0.874]] [[0.604]
 [0.617]
 [1.326]
 [0.938]
 [1.026]
 [1.026]
 [0.591]]
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
probs:  [0.3013794516860353, 0.3085531875572718, 0.24757212622268207, 0.14178668535126215, 0.00035427459137436134, 0.00035427459137436134]
actor:  1 policy actor:  1  step number:  107 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.338]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]] [[0.721]
 [1.634]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[-0.642]
 [ 0.065]
 [-0.642]
 [-0.642]
 [-0.642]
 [-0.642]
 [-0.642]]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]] [[ -8.308]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.507]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[-1.828]
 [-2.058]
 [-2.058]
 [-2.058]
 [-2.058]
 [-2.058]
 [-2.058]] [[0.391]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]] [[-3.419]
 [-9.207]
 [-9.207]
 [-9.207]
 [-9.207]
 [-9.207]
 [-9.207]] [[0.314]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]] [[-1.588]
 [-2.248]
 [-2.248]
 [-2.248]
 [-2.248]
 [-2.248]
 [-2.248]] [[0.362]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]] [[-3.532]
 [-9.638]
 [-9.638]
 [-9.638]
 [-9.638]
 [-9.638]
 [-9.638]] [[0.315]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]]
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[5.741]
 [5.741]
 [5.741]
 [5.741]
 [5.741]
 [5.741]
 [5.741]] [[1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.067]
 [0.074]
 [0.258]
 [0.074]
 [0.074]
 [0.258]] [[-8.032]
 [-3.901]
 [-7.816]
 [ 0.   ]
 [-8.043]
 [-8.024]
 [ 0.   ]] [[0.275]
 [0.067]
 [0.074]
 [0.258]
 [0.074]
 [0.074]
 [0.258]]
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]] [[-8.088]
 [-8.088]
 [-8.088]
 [-8.088]
 [-8.088]
 [-8.088]
 [-8.088]] [[0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]]
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
probs:  [0.299737592263077, 0.312320061702581, 0.24622339914116895, 0.14101425775511497, 0.00035234456902907997, 0.00035234456902907997]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[-2.88 ]
 [-5.923]
 [-5.923]
 [-5.923]
 [-5.923]
 [-5.923]
 [-5.923]] [[0.332]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[-1.813]
 [-2.903]
 [-2.903]
 [-2.903]
 [-2.903]
 [-2.903]
 [-2.903]] [[0.364]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]]
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]] [[-5.597]
 [-7.897]
 [-7.897]
 [-7.897]
 [-7.897]
 [-7.897]
 [-7.897]] [[0.08 ]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]]
actor:  1 policy actor:  1  step number:  83 total reward:  0.1899999999999994  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  72 total reward:  0.5449999999999997  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
siam score:  -0.6111748
using another actor
siam score:  -0.6083456
Printing some Q and Qe and total Qs values:  [[1.086]
 [1.086]
 [1.086]
 [1.086]
 [1.086]
 [1.086]
 [1.086]] [[-0.613]
 [-0.595]
 [-0.613]
 [-0.613]
 [-0.613]
 [-0.613]
 [-0.613]] [[2.172]
 [2.196]
 [2.172]
 [2.172]
 [2.172]
 [2.172]
 [2.172]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]] [[-9.374]
 [-9.865]
 [-9.865]
 [-9.865]
 [-9.865]
 [-9.865]
 [-9.865]] [[0.166]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.30050150552066923, 0.31830609539548205, 0.2419392705865979, 0.1385607004963105, 0.0003462140004702402, 0.0003462140004702402]
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
line 256 mcts: sample exp_bonus 1.8406572136642343
siam score:  -0.61934286
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
probs:  [0.3005015055206692, 0.31830609539548205, 0.2419392705865979, 0.1385607004963105, 0.0003462140004702402, 0.0003462140004702402]
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.059]
 [ 0.138]
 [ 0.171]
 [ 0.066]
 [ 0.171]
 [-0.03 ]] [[-0.713]
 [-0.614]
 [-1.075]
 [-0.777]
 [-0.834]
 [-0.777]
 [-0.164]] [[1.443]
 [1.467]
 [1.367]
 [1.493]
 [1.431]
 [1.493]
 [1.649]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.62171185
UNIT TEST: sample policy line 217 mcts : [0.02  0.653 0.163 0.02  0.02  0.102 0.02 ]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.97635 -0.7392500000000001 -0.7392500000000001
actor:  0 policy actor:  1  step number:  61 total reward:  0.3299999999999995  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  60 total reward:  0.6649999999999998  reward:  1.0 rdn_beta:  0.167
3138 4982
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.30992388268117166, 0.31401845202387546, 0.2386803028041145, 0.1366942616261719, 0.0003415504323333207, 0.0003415504323333207]
using explorer policy with actor:  0
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1927729438008707
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.30992388268117166, 0.31401845202387546, 0.2386803028041145, 0.1366942616261719, 0.0003415504323333207, 0.0003415504323333207]
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.30992388268117166, 0.31401845202387546, 0.2386803028041145, 0.1366942616261719, 0.0003415504323333207, 0.0003415504323333207]
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.154]
 [0.266]
 [0.268]
 [0.239]
 [0.245]
 [0.236]] [[0.534]
 [0.915]
 [0.323]
 [0.282]
 [0.411]
 [0.305]
 [0.327]] [[ 0.012]
 [-0.098]
 [-0.071]
 [-0.083]
 [-0.097]
 [-0.119]
 [-0.13 ]]
3140 4991
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.30992388268117166, 0.31401845202387546, 0.2386803028041145, 0.1366942616261719, 0.0003415504323333207, 0.0003415504323333207]
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.266]
 [0.266]
 [0.127]
 [0.239]
 [0.206]
 [0.152]] [[4.567]
 [5.577]
 [5.577]
 [4.113]
 [4.828]
 [4.363]
 [5.395]] [[ 0.004]
 [ 0.889]
 [ 0.889]
 [-0.363]
 [ 0.337]
 [-0.038]
 [ 0.54 ]]
siam score:  -0.63791174
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.399]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[3.985]
 [4.468]
 [3.985]
 [3.985]
 [3.985]
 [3.985]
 [3.985]] [[0.657]
 [0.94 ]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.275]
 [0.511]
 [0.275]
 [0.275]
 [0.275]
 [0.275]] [[1.458]
 [1.458]
 [4.114]
 [1.458]
 [1.458]
 [1.458]
 [1.458]] [[0.221]
 [0.221]
 [1.517]
 [0.221]
 [0.221]
 [0.221]
 [0.221]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.5399999999999997  reward:  1.0 rdn_beta:  0.167
from probs:  [0.30992388268117166, 0.31401845202387535, 0.2386803028041145, 0.1366942616261719, 0.0003415504323333207, 0.0003415504323333207]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[-1.116]
 [-1.917]
 [-1.917]
 [-1.917]
 [-1.917]
 [-1.917]
 [-1.917]] [[0.686]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]]
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
UNIT TEST: sample policy line 217 mcts : [0.02  0.878 0.02  0.02  0.02  0.02  0.02 ]
start point for exploration sampling:  10749
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  127 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.423]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[3.985]
 [4.911]
 [3.985]
 [3.985]
 [3.985]
 [3.985]
 [3.985]] [[-0.353]
 [ 0.111]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.353]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  81 total reward:  0.5499999999999997  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  55 total reward:  0.5699999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.3305035866984806, 0.30836741934685885, 0.22920521394195975, 0.13126779676653255, 0.00032799162308410685, 0.00032799162308410685]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.523]
 [0.41 ]
 [0.383]
 [0.373]
 [0.387]
 [0.402]] [[4.729]
 [4.376]
 [4.759]
 [4.564]
 [4.94 ]
 [4.568]
 [4.721]] [[0.994]
 [0.989]
 [1.018]
 [0.833]
 [1.065]
 [0.844]
 [0.977]]
actor:  1 policy actor:  1  step number:  76 total reward:  0.38499999999999956  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.663]
 [0.653]
 [0.657]
 [0.629]
 [0.636]
 [0.643]] [[2.151]
 [2.268]
 [3.434]
 [2.018]
 [2.03 ]
 [3.088]
 [2.651]] [[0.655]
 [0.663]
 [0.653]
 [0.657]
 [0.629]
 [0.636]
 [0.643]]
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.443]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.221]] [[-0.681]
 [ 0.224]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-1.331]] [[1.403]
 [1.809]
 [1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.277]]
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]] [[1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]] [[0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.33080415614678926, 0.3077384288258306, 0.2294136597424795, 0.13138717546873951, 0.0003282899080806028, 0.0003282899080806028]
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.33080415614678926, 0.3077384288258306, 0.2294136597424795, 0.13138717546873951, 0.0003282899080806028, 0.0003282899080806028]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.62 ]
 [0.438]
 [0.416]
 [0.436]
 [0.419]
 [0.436]] [[3.804]
 [3.34 ]
 [4.277]
 [3.862]
 [4.108]
 [4.187]
 [4.196]] [[0.72 ]
 [0.857]
 [1.118]
 [0.796]
 [1.   ]
 [1.02 ]
 [1.059]]
using explorer policy with actor:  1
from probs:  [0.33080415614678926, 0.3077384288258306, 0.2294136597424795, 0.13138717546873951, 0.0003282899080806028, 0.0003282899080806028]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.33080415614678926, 0.30773842882583063, 0.2294136597424795, 0.13138717546873951, 0.0003282899080806028, 0.0003282899080806028]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.618]
 [0.778]
 [0.618]
 [0.618]
 [0.835]
 [0.818]] [[0.648]
 [0.454]
 [1.223]
 [0.454]
 [0.454]
 [1.283]
 [0.671]] [[0.708]
 [0.618]
 [0.778]
 [0.618]
 [0.618]
 [0.835]
 [0.818]]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.343]] [[2.199]
 [2.199]
 [2.199]
 [2.199]
 [2.199]
 [2.199]
 [3.679]] [[1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.768]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  143 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.839]
 [0.61 ]
 [0.618]
 [0.612]
 [0.645]
 [0.608]] [[1.521]
 [0.603]
 [1.687]
 [1.084]
 [1.943]
 [2.269]
 [1.982]] [[0.696]
 [0.839]
 [0.61 ]
 [0.618]
 [0.612]
 [0.645]
 [0.608]]
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.32942248066448554, 0.3106298097908509, 0.22845546371294984, 0.13083840836390984, 0.00032691873390200074, 0.00032691873390200074]
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.32942248066448554, 0.3106298097908509, 0.22845546371294984, 0.13083840836390984, 0.00032691873390200074, 0.00032691873390200074]
actor:  1 policy actor:  1  step number:  113 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.5
using another actor
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.139]
 [0.172]
 [0.073]
 [0.073]
 [0.073]
 [0.073]] [[0.634]
 [0.1  ]
 [0.283]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[1.861]
 [1.688]
 [1.78 ]
 [1.861]
 [1.861]
 [1.861]
 [1.861]]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.32761240214247694, 0.308922991404128, 0.2326948713911667, 0.13011949023678143, 0.00032512241272354133, 0.00032512241272354133]
using explorer policy with actor:  1
start point for exploration sampling:  10749
line 256 mcts: sample exp_bonus 2.5157292724946725
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.32761240214247694, 0.308922991404128, 0.23269487139116668, 0.13011949023678143, 0.00032512241272354133, 0.00032512241272354133]
3168 5017
Printing some Q and Qe and total Qs values:  [[1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]] [[-0.622]
 [-0.591]
 [-0.622]
 [-0.622]
 [-0.622]
 [-0.622]
 [-0.612]] [[1.79 ]
 [1.831]
 [1.79 ]
 [1.79 ]
 [1.79 ]
 [1.79 ]
 [1.804]]
Printing some Q and Qe and total Qs values:  [[-0.077]
 [ 1.086]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]] [[-0.623]
 [-0.615]
 [-0.623]
 [-0.623]
 [-0.623]
 [-0.623]
 [-0.623]] [[-0.132]
 [ 2.211]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]]
3170 5021
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.139]
 [0.189]
 [0.141]
 [0.141]
 [0.141]
 [0.141]] [[1.907]
 [2.791]
 [3.655]
 [1.885]
 [1.885]
 [1.885]
 [1.885]] [[-0.018]
 [ 0.462]
 [ 0.916]
 [ 0.059]
 [ 0.059]
 [ 0.059]
 [ 0.059]]
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.32761240214247694, 0.308922991404128, 0.23269487139116668, 0.13011949023678143, 0.00032512241272354133, 0.00032512241272354133]
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.32761240214247694, 0.308922991404128, 0.23269487139116668, 0.13011949023678143, 0.00032512241272354133, 0.00032512241272354133]
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[-0.625]
 [-0.625]
 [-0.625]
 [-0.625]
 [-0.625]
 [-0.625]
 [-0.625]] [[1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.268]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[ 0.   ]
 [-3.956]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.324]
 [0.268]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]]
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.531]
 [0.141]
 [0.161]
 [0.174]
 [0.165]
 [0.164]] [[-4.391]
 [-1.716]
 [-5.353]
 [-4.672]
 [-4.501]
 [-4.513]
 [-4.745]] [[ 0.113]
 [ 0.864]
 [-0.119]
 [ 0.041]
 [ 0.085]
 [ 0.079]
 [ 0.027]]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.434]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[-1.314]
 [ 1.892]
 [-1.314]
 [-1.314]
 [-1.314]
 [-1.314]
 [-1.314]] [[0.349]
 [0.434]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [0.725]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[-2.647]
 [-2.647]
 [-1.733]
 [-2.647]
 [-2.647]
 [-2.647]
 [-2.647]] [[0.547]
 [0.547]
 [0.725]
 [0.547]
 [0.547]
 [0.547]
 [0.547]]
from probs:  [0.32761240214247694, 0.308922991404128, 0.23269487139116668, 0.13011949023678143, 0.00032512241272354133, 0.00032512241272354133]
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.548]
 [0.788]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[-4.425]
 [-4.425]
 [-2.43 ]
 [-4.425]
 [-4.425]
 [-4.425]
 [-4.425]] [[0.548]
 [0.548]
 [0.788]
 [0.548]
 [0.548]
 [0.548]
 [0.548]]
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[ -9.098]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.21 ]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
from probs:  [0.32761240214247694, 0.308922991404128, 0.2326948713911667, 0.13011949023678143, 0.00032512241272354133, 0.00032512241272354133]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.464]
 [0.417]
 [0.408]
 [0.443]
 [0.402]
 [0.411]] [[-3.853]
 [-2.975]
 [-4.31 ]
 [-4.314]
 [ 0.   ]
 [-4.244]
 [-4.096]] [[0.411]
 [0.464]
 [0.417]
 [0.408]
 [0.443]
 [0.402]
 [0.411]]
siam score:  -0.62629914
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.616]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]] [[1.155]
 [2.423]
 [1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.155]] [[0.576]
 [0.616]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.6184084
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.1030],
        [-0.0000],
        [-0.0127],
        [-0.2493],
        [ 0.3372],
        [-0.0000],
        [-0.0000],
        [-0.6251],
        [-0.0000],
        [ 0.3369]], dtype=torch.float64)
-0.024259925299500003 -0.12723409866444518
-0.9213435 -0.9213435
-0.024259925299500003 -0.03691737239135527
-0.024259925299500003 -0.2735759588378174
-0.024259925299500003 0.31291150936950207
-0.965595015 -0.965595015
-0.92246522445 -0.92246522445
-0.024259925299500003 -0.649379914273241
-0.96074352 -0.96074352
-0.024259925299500003 0.3126155073686211
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[2.839]
 [2.839]
 [2.839]
 [2.839]
 [2.839]
 [2.839]
 [2.839]] [[1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]]
line 256 mcts: sample exp_bonus 3.660769767407253
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.46 ]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[2.455]
 [2.808]
 [2.455]
 [2.455]
 [2.455]
 [2.455]
 [2.455]] [[0.848]
 [1.149]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.55 ]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[-3.397]
 [-1.88 ]
 [-3.397]
 [-3.397]
 [-3.397]
 [-3.397]
 [-3.397]] [[0.496]
 [0.55 ]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.801]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[-4.226]
 [-4.226]
 [-7.081]
 [-4.226]
 [-4.226]
 [-4.226]
 [-4.226]] [[0.513]
 [0.513]
 [0.801]
 [0.513]
 [0.513]
 [0.513]
 [0.513]]
from probs:  [0.32761240214247694, 0.308922991404128, 0.2326948713911667, 0.13011949023678143, 0.00032512241272354133, 0.00032512241272354133]
siam score:  -0.62563074
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
from probs:  [0.32761240214247694, 0.308922991404128, 0.2326948713911667, 0.13011949023678143, 0.00032512241272354133, 0.00032512241272354133]
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.293]
 [0.26 ]
 [0.279]
 [0.182]
 [0.203]
 [0.323]] [[4.117]
 [4.079]
 [4.142]
 [4.116]
 [4.268]
 [3.818]
 [4.394]] [[0.587]
 [0.524]
 [0.522]
 [0.532]
 [0.494]
 [0.118]
 [0.871]]
using another actor
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.436]] [[4.624]
 [4.624]
 [4.624]
 [4.624]
 [4.624]
 [4.624]
 [5.87 ]] [[0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [1.198]]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.568]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[-3.888]
 [-2.614]
 [-3.888]
 [-3.888]
 [-3.888]
 [-3.888]
 [-3.888]] [[0.375]
 [0.568]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]]
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]] [[-6.739]
 [-9.682]
 [-9.682]
 [-9.682]
 [-9.682]
 [-9.682]
 [-9.682]] [[0.122]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]]
siam score:  -0.6330075
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[-5.669]
 [-9.121]
 [-9.121]
 [-9.121]
 [-9.121]
 [-9.121]
 [-9.121]] [[0.029]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]] [[3.996]
 [3.996]
 [3.996]
 [3.996]
 [3.996]
 [3.996]
 [3.996]] [[0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]]
siam score:  -0.6327777
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[-1.187]
 [-2.221]
 [-2.221]
 [-2.221]
 [-2.221]
 [-2.221]
 [-2.221]] [[0.282]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]]
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.32761240214247694, 0.308922991404128, 0.2326948713911667, 0.13011949023678143, 0.00032512241272354133, 0.00032512241272354133]
using explorer policy with actor:  1
first move QE:  -0.6382394118719665
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.32761240214247694, 0.308922991404128, 0.2326948713911667, 0.13011949023678143, 0.00032512241272354133, 0.00032512241272354133]
start point for exploration sampling:  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.051]
 [ 0.189]
 [ 0.1  ]
 [ 0.1  ]
 [ 0.1  ]
 [ 0.1  ]
 [ 0.181]] [[-0.537]
 [-0.908]
 [-0.796]
 [-0.796]
 [-0.796]
 [-0.796]
 [-0.597]] [[1.46 ]
 [1.424]
 [1.426]
 [1.426]
 [1.426]
 [1.426]
 [1.552]]
first move QE:  -0.6405000751904757
using explorer policy with actor:  1
siam score:  -0.6282643
Printing some Q and Qe and total Qs values:  [[0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]] [[-2.093]
 [-2.093]
 [-2.093]
 [-2.093]
 [-2.093]
 [-2.093]
 [-2.093]] [[-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]]
using another actor
Printing some Q and Qe and total Qs values:  [[ 0.832]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]] [[-0.643]
 [-0.266]
 [-0.266]
 [-0.266]
 [-0.266]
 [-0.266]
 [-0.266]] [[ 1.149]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]]
siam score:  -0.6307747
Printing some Q and Qe and total Qs values:  [[-0.075]
 [ 1.086]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]] [[-0.632]
 [-0.642]
 [-0.632]
 [-0.632]
 [-0.632]
 [-0.632]
 [-0.632]] [[-0.133]
 [ 2.187]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.555]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.493]] [[1.327]
 [1.172]
 [1.327]
 [1.327]
 [1.327]
 [1.327]
 [1.676]] [[0.46 ]
 [0.555]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.493]]
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]] [[-0.134]
 [-0.134]
 [-0.134]
 [-0.134]
 [-0.134]
 [-0.134]
 [-0.134]] [[1.985]
 [1.985]
 [1.985]
 [1.985]
 [1.985]
 [1.985]
 [1.985]]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.449]
 [0.505]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[1.382]
 [1.673]
 [2.803]
 [1.382]
 [1.382]
 [1.382]
 [1.382]] [[0.492]
 [0.449]
 [0.505]
 [0.492]
 [0.492]
 [0.492]
 [0.492]]
Starting evaluation
maxi score, test score, baseline:  -0.9736899999999999 -0.7392500000000001 -0.7392500000000001
probs:  [0.32761240214247694, 0.30892299140412793, 0.23269487139116668, 0.1301194902367814, 0.0003251224127235413, 0.0003251224127235413]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[-2.793]
 [-2.793]
 [-2.793]
 [-2.793]
 [-2.793]
 [-2.793]
 [-2.793]] [[0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.421]
 [0.42 ]
 [0.574]
 [0.574]
 [0.491]
 [0.42 ]] [[ 0.   ]
 [-4.496]
 [-4.835]
 [ 0.   ]
 [ 0.   ]
 [-4.114]
 [-4.588]] [[0.574]
 [0.421]
 [0.42 ]
 [0.574]
 [0.574]
 [0.491]
 [0.42 ]]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.435]
 [0.436]
 [0.437]
 [0.445]
 [0.6  ]
 [0.429]] [[-4.201]
 [-4.492]
 [-4.602]
 [-4.546]
 [-4.396]
 [-3.991]
 [-4.493]] [[0.559]
 [0.435]
 [0.436]
 [0.437]
 [0.445]
 [0.6  ]
 [0.429]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.555]
 [0.542]
 [0.574]
 [0.574]
 [0.529]
 [0.521]] [[5.582]
 [4.731]
 [4.28 ]
 [5.582]
 [5.582]
 [4.981]
 [5.055]] [[1.556]
 [0.923]
 [0.582]
 [1.556]
 [1.556]
 [1.063]
 [1.105]]
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.681]
 [0.641]
 [0.657]
 [0.641]
 [0.641]
 [0.672]] [[3.749]
 [4.182]
 [3.749]
 [2.964]
 [3.749]
 [3.749]
 [4.23 ]] [[0.641]
 [0.681]
 [0.641]
 [0.657]
 [0.641]
 [0.641]
 [0.672]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.635]
 [0.538]] [[-3.232]
 [-3.232]
 [-3.232]
 [-3.232]
 [-3.232]
 [-2.991]
 [-3.139]] [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.635]
 [0.538]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.627]
 [0.543]] [[-3.428]
 [-3.428]
 [-3.428]
 [-3.428]
 [-3.428]
 [-3.676]
 [-3.428]] [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.627]
 [0.543]]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.686]
 [0.546]] [[-3.428]
 [-3.428]
 [-3.428]
 [-3.428]
 [-3.428]
 [-3.594]
 [-3.326]] [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.686]
 [0.546]]
maxi score, test score, baseline:  -0.9766100000000001 -0.7392500000000001 -0.7392500000000001
probs:  [0.32761240214247694, 0.30892299140412793, 0.23269487139116668, 0.1301194902367814, 0.0003251224127235413, 0.0003251224127235413]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.588]
 [0.542]] [[-3.53]
 [-3.53]
 [-3.53]
 [-3.53]
 [-3.53]
 [-3.33]
 [-3.53]] [[0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.588]
 [0.542]]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.339]
 [0.287]
 [0.282]
 [0.365]
 [0.272]
 [0.29 ]] [[-4.157]
 [-3.71 ]
 [-4.521]
 [-4.699]
 [-3.472]
 [-4.441]
 [-3.879]] [[0.291]
 [0.339]
 [0.287]
 [0.282]
 [0.365]
 [0.272]
 [0.29 ]]
maxi score, test score, baseline:  -0.9766100000000001 -0.7392500000000001 -0.7392500000000001
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.536]
 [0.539]] [[-3.464]
 [-3.464]
 [-3.464]
 [-3.464]
 [-3.464]
 [-3.394]
 [-3.464]] [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.536]
 [0.539]]
maxi score, test score, baseline:  -0.9766100000000001 -0.7392500000000001 -0.7392500000000001
probs:  [0.32761240214247694, 0.30892299140412793, 0.23269487139116668, 0.1301194902367814, 0.0003251224127235413, 0.0003251224127235413]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.551]
 [0.704]
 [0.602]] [[-3.519]
 [-3.519]
 [-3.519]
 [-3.519]
 [-3.333]
 [-3.49 ]
 [-3.192]] [[0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.551]
 [0.704]
 [0.602]]
3206 5107
line 256 mcts: sample exp_bonus -3.4971991544512027
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.574]
 [0.549]
 [0.549]
 [0.553]
 [0.582]
 [0.552]] [[-3.426]
 [-3.023]
 [-3.426]
 [-3.426]
 [-3.367]
 [-3.316]
 [-3.278]] [[0.549]
 [0.574]
 [0.549]
 [0.549]
 [0.553]
 [0.582]
 [0.552]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[-3.773]
 [-3.773]
 [-3.773]
 [-3.773]
 [-3.773]
 [-3.773]
 [-3.773]] [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]]
using another actor
from probs:  [0.32761240214247694, 0.30892299140412793, 0.23269487139116668, 0.1301194902367814, 0.0003251224127235413, 0.0003251224127235413]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.538]
 [0.64 ]
 [0.728]
 [0.555]
 [0.645]
 [0.63 ]] [[-3.186]
 [-2.406]
 [-1.958]
 [-2.116]
 [-2.884]
 [-2.092]
 [-2.045]] [[0.519]
 [0.538]
 [0.64 ]
 [0.728]
 [0.555]
 [0.645]
 [0.63 ]]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.526]
 [0.578]
 [0.535]
 [0.551]
 [0.535]
 [0.73 ]] [[-2.855]
 [-1.376]
 [-2.169]
 [-2.855]
 [-2.896]
 [-2.855]
 [-1.517]] [[0.535]
 [0.526]
 [0.578]
 [0.535]
 [0.551]
 [0.535]
 [0.73 ]]
line 256 mcts: sample exp_bonus -3.5448969614563888
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[-2.609]
 [-2.609]
 [-2.609]
 [-2.609]
 [-2.609]
 [-2.609]
 [-2.609]] [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9766100000000001 -0.7392500000000001 -0.7392500000000001
using explorer policy with actor:  0
rdn probs:  [0.327612402142477, 0.308922991404128, 0.2326948713911667, 0.13011949023678143, 0.00032512241272354133, 0.00032512241272354133]
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[-3.952]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.434]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
probs:  [0.3239210361832272, 0.31080548896112303, 0.23342766299098483, 0.13139242470085044, 0.00022669358190730872, 0.00022669358190730872]
actor:  1 policy actor:  1  step number:  40 total reward:  0.7249999999999999  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.625]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.755]] [[0.287]
 [0.837]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [1.229]] [[1.751]
 [1.896]
 [1.751]
 [1.751]
 [1.751]
 [1.751]
 [2.002]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
probs:  [0.3326814072852182, 0.3067781910128717, 0.23040299714170737, 0.1296898922128232, 0.00022375617368987778, 0.00022375617368987778]
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
probs:  [0.33268140728521817, 0.3067781910128717, 0.23040299714170734, 0.12968989221282318, 0.0002237561736898777, 0.0002237561736898777]
line 256 mcts: sample exp_bonus 1.29454814400105
Printing some Q and Qe and total Qs values:  [[1.004]
 [1.004]
 [1.004]
 [1.004]
 [1.004]
 [1.004]
 [1.004]] [[1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]] [[1.004]
 [1.004]
 [1.004]
 [1.004]
 [1.004]
 [1.004]
 [1.004]]
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
probs:  [0.33268140728521817, 0.3067781910128717, 0.23040299714170734, 0.12968989221282315, 0.0002237561736898777, 0.0002237561736898777]
using another actor
from probs:  [0.33268140728521817, 0.3067781910128717, 0.23040299714170734, 0.12968989221282315, 0.0002237561736898777, 0.0002237561736898777]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
probs:  [0.33268140728521817, 0.3067781910128717, 0.23040299714170734, 0.12968989221282318, 0.0002237561736898777, 0.0002237561736898777]
using explorer policy with actor:  1
siam score:  -0.66549104
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.819]
 [0.551]
 [0.539]
 [0.552]
 [0.552]
 [0.552]] [[1.242]
 [2.135]
 [1.087]
 [0.903]
 [1.242]
 [1.242]
 [1.242]] [[1.34 ]
 [1.713]
 [1.299]
 [1.244]
 [1.34 ]
 [1.34 ]
 [1.34 ]]
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
Printing some Q and Qe and total Qs values:  [[1.161]
 [1.045]
 [1.045]
 [1.044]
 [0.995]
 [1.045]
 [1.031]] [[4.166]
 [5.617]
 [5.617]
 [4.935]
 [4.271]
 [5.617]
 [4.665]] [[1.781]
 [2.35 ]
 [2.35 ]
 [1.996]
 [1.579]
 [2.35 ]
 [1.838]]
siam score:  -0.6594208
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.007]
 [0.982]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]] [[4.204]
 [4.66 ]
 [4.204]
 [4.204]
 [4.204]
 [4.204]
 [4.204]] [[1.671]
 [1.947]
 [1.671]
 [1.671]
 [1.671]
 [1.671]
 [1.671]]
Printing some Q and Qe and total Qs values:  [[0.978]
 [0.995]
 [0.975]
 [1.037]
 [1.015]
 [0.946]
 [1.005]] [[3.895]
 [4.219]
 [4.647]
 [5.748]
 [3.8  ]
 [2.31 ]
 [4.389]] [[1.416]
 [1.559]
 [1.714]
 [2.203]
 [1.408]
 [0.758]
 [1.635]]
first move QE:  -0.6456429827118169
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
probs:  [0.33268140728521817, 0.3067781910128717, 0.23040299714170734, 0.12968989221282318, 0.0002237561736898777, 0.0002237561736898777]
from probs:  [0.33268140728521817, 0.3067781910128717, 0.23040299714170734, 0.12968989221282318, 0.0002237561736898777, 0.0002237561736898777]
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]] [[-2.857]
 [-3.667]
 [-3.667]
 [-3.667]
 [-3.667]
 [-3.667]
 [-3.667]] [[0.248]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.708]
 [0.695]] [[2.383]
 [2.383]
 [2.383]
 [2.383]
 [2.383]
 [2.063]
 [1.979]] [[1.444]
 [1.444]
 [1.444]
 [1.444]
 [1.444]
 [1.2  ]
 [1.142]]
using another actor
actor:  1 policy actor:  1  step number:  93 total reward:  0.38999999999999957  reward:  1.0 rdn_beta:  0.333
from probs:  [0.33268140728521817, 0.3067781910128717, 0.23040299714170734, 0.12968989221282318, 0.0002237561736898777, 0.0002237561736898777]
first move QE:  -0.6463997139594588
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.47 ]
 [0.526]
 [0.467]
 [0.467]
 [0.473]
 [0.467]] [[2.303]
 [1.902]
 [2.862]
 [2.303]
 [2.303]
 [2.556]
 [2.303]] [[0.467]
 [0.47 ]
 [0.526]
 [0.467]
 [0.467]
 [0.473]
 [0.467]]
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
probs:  [0.32992395465554075, 0.3125240074555154, 0.22849328611956632, 0.1286149486587392, 0.00022190155531915903, 0.00022190155531915903]
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.679]
 [0.466]
 [0.679]
 [0.679]
 [0.443]
 [0.679]] [[ 0.   ]
 [ 0.   ]
 [-3.135]
 [ 0.   ]
 [ 0.   ]
 [-2.826]
 [ 0.   ]] [[0.679]
 [0.679]
 [0.466]
 [0.679]
 [0.679]
 [0.443]
 [0.679]]
using another actor
3232 5131
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.992]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]] [[-2.874]
 [-1.928]
 [-2.874]
 [-2.874]
 [-2.874]
 [-2.874]
 [-2.874]] [[1.266]
 [2.072]
 [1.266]
 [1.266]
 [1.266]
 [1.266]
 [1.266]]
first move QE:  -0.6480947017812401
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.369]
 [0.387]
 [0.387]
 [0.136]
 [0.387]
 [0.387]] [[-1.435]
 [-0.515]
 [-2.465]
 [-2.465]
 [-0.315]
 [-2.465]
 [-2.465]] [[0.631]
 [1.144]
 [0.528]
 [0.528]
 [0.744]
 [0.528]
 [0.528]]
maxi score, test score, baseline:  -0.9766100000000001 -1.0 -0.9766100000000001
probs:  [0.32992395465554075, 0.3125240074555154, 0.22849328611956632, 0.1286149486587392, 0.00022190155531915903, 0.00022190155531915903]
from probs:  [0.32992395465554075, 0.3125240074555154, 0.22849328611956632, 0.1286149486587392, 0.00022190155531915903, 0.00022190155531915903]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.191]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.001]
 [-0.009]] [[-2.898]
 [-3.374]
 [-2.898]
 [-2.898]
 [-2.898]
 [-3.172]
 [-2.898]] [[-0.639]
 [-0.409]
 [-0.639]
 [-0.639]
 [-0.639]
 [-0.711]
 [-0.639]]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.099]
 [0.118]] [[2.648]
 [2.537]
 [2.537]
 [2.537]
 [2.537]
 [2.973]
 [2.537]] [[-0.162]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [ 0.144]
 [-0.108]]
from probs:  [0.32992395465554075, 0.3125240074555154, 0.22849328611956632, 0.1286149486587392, 0.00022190155531915903, 0.00022190155531915903]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.825]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]] [[-0.837]
 [ 0.628]
 [-0.837]
 [-0.837]
 [-0.837]
 [-0.837]
 [-0.837]] [[1.244]
 [1.772]
 [1.244]
 [1.244]
 [1.244]
 [1.244]
 [1.244]]
from probs:  [0.3259912864851579, 0.3143582070588888, 0.22983431044018157, 0.12936978823006823, 0.00022320389285177968, 0.00022320389285177968]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.564]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[-2.839]
 [-0.237]
 [-2.839]
 [-2.839]
 [-2.839]
 [-2.839]
 [-2.839]] [[0.204]
 [0.894]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]]
actor:  0 policy actor:  1  step number:  47 total reward:  0.6299999999999998  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[1.81]
 [1.81]
 [1.81]
 [1.81]
 [1.81]
 [1.81]
 [1.81]] [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.665]
 [0.733]
 [0.813]
 [0.895]
 [0.629]
 [0.793]] [[0.538]
 [0.394]
 [0.356]
 [0.506]
 [1.513]
 [2.358]
 [0.856]] [[ 0.045]
 [-0.228]
 [-0.173]
 [ 0.063]
 [ 1.051]
 [ 1.44 ]
 [ 0.343]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.423]
 [0.522]] [[0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [2.677]
 [0.49 ]] [[-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [ 1.773]
 [-0.071]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
actor:  1 policy actor:  1  step number:  72 total reward:  0.48499999999999965  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[2.998]
 [2.998]
 [2.998]
 [2.998]
 [2.998]
 [2.998]
 [2.998]] [[0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.896]
 [0.495]] [[-1.592]
 [-1.592]
 [-1.592]
 [-1.592]
 [-1.592]
 [-1.685]
 [-1.592]] [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [1.11 ]
 [0.509]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
siam score:  -0.64026207
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.641056
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.863]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]] [[2.437]
 [4.811]
 [2.437]
 [2.437]
 [2.437]
 [2.437]
 [2.437]] [[1.122]
 [2.115]
 [1.122]
 [1.122]
 [1.122]
 [1.122]
 [1.122]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.3324124573364729, 0.3113606841913984, 0.22765108317101973, 0.1281315764733414, 0.00022209941388380124, 0.00022209941388380124]
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.563]
 [0.158]] [[-2.258]
 [-2.258]
 [-2.258]
 [-2.258]
 [-2.258]
 [-0.739]
 [-2.258]] [[-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [ 1.265]
 [-0.027]]
actor:  1 policy actor:  1  step number:  121 total reward:  0.22999999999999943  reward:  1.0 rdn_beta:  0.5
siam score:  -0.64847684
3254 5164
using another actor
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.011]
 [-0.026]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.192]] [[ 0.303]
 [ 0.747]
 [-0.261]
 [ 0.303]
 [ 0.303]
 [ 0.303]
 [ 0.26 ]] [[ 0.115]
 [ 0.381]
 [ 0.014]
 [ 0.115]
 [ 0.115]
 [ 0.115]
 [-0.143]]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.881]
 [0.462]
 [0.46 ]
 [0.462]
 [0.462]
 [0.716]] [[1.176]
 [2.103]
 [1.176]
 [0.883]
 [1.176]
 [1.176]
 [1.437]] [[0.723]
 [1.469]
 [0.723]
 [0.598]
 [0.723]
 [0.723]
 [1.049]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
actor:  1 policy actor:  1  step number:  69 total reward:  0.3099999999999995  reward:  1.0 rdn_beta:  0.333
using another actor
start point for exploration sampling:  10749
using explorer policy with actor:  1
using explorer policy with actor:  0
from probs:  [0.32790438700260405, 0.3143547932944674, 0.23090874751972776, 0.12639389743640134, 0.00021908737339975358, 0.00021908737339975358]
first move QE:  -0.6570824034496833
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[0.482]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]] [[0.553]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]] [[-0.993]
 [-0.725]
 [-0.725]
 [-0.725]
 [-0.725]
 [-0.725]
 [-0.725]] [[0.811]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
Printing some Q and Qe and total Qs values:  [[0.845]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[-4.535]
 [-1.74 ]
 [-1.74 ]
 [-1.74 ]
 [-1.74 ]
 [-1.74 ]
 [-1.74 ]] [[0.845]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]]
using another actor
3263 5176
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.059]
 [0.071]
 [0.07 ]
 [0.072]
 [0.073]
 [0.077]] [[-9.66 ]
 [-3.041]
 [-6.594]
 [-6.046]
 [-6.69 ]
 [-6.51 ]
 [-5.761]] [[0.175]
 [0.059]
 [0.071]
 [0.07 ]
 [0.072]
 [0.073]
 [0.077]]
3264 5177
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.677]
 [0.649]
 [0.605]
 [0.649]
 [0.649]
 [0.655]] [[1.533]
 [1.079]
 [1.533]
 [1.521]
 [1.533]
 [1.533]
 [2.007]] [[1.764]
 [1.542]
 [1.764]
 [1.678]
 [1.764]
 [1.764]
 [2.06 ]]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.474]
 [0.608]
 [0.592]
 [0.257]
 [0.595]
 [0.52 ]] [[-0.992]
 [-0.843]
 [-4.169]
 [-2.986]
 [-1.034]
 [-4.203]
 [-1.419]] [[1.293]
 [1.449]
 [0.193]
 [0.659]
 [1.241]
 [0.172]
 [1.245]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.32790438700260405, 0.31435479329446747, 0.23090874751972779, 0.12639389743640134, 0.0002190873733997536, 0.0002190873733997536]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[-0.243]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]] [[2.099]
 [1.967]
 [1.967]
 [1.967]
 [1.967]
 [1.967]
 [1.967]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.32790438700260405, 0.31435479329446747, 0.23090874751972779, 0.12639389743640134, 0.0002190873733997536, 0.0002190873733997536]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.702]
 [0.533]
 [0.441]
 [0.533]
 [0.533]
 [0.848]] [[1.418]
 [0.869]
 [1.418]
 [0.424]
 [1.418]
 [1.418]
 [3.699]] [[0.823]
 [0.806]
 [0.823]
 [0.282]
 [0.823]
 [0.823]
 [2.2  ]]
3265 5188
from probs:  [0.327904387002604, 0.3143547932944674, 0.23090874751972773, 0.1263938974364013, 0.00021908737339975355, 0.00021908737339975355]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.223]
 [0.376]
 [0.37 ]
 [0.366]
 [0.395]
 [0.381]] [[-4.481]
 [-0.622]
 [-3.647]
 [-3.297]
 [-3.576]
 [-3.593]
 [-4.011]] [[0.384]
 [0.223]
 [0.376]
 [0.37 ]
 [0.366]
 [0.395]
 [0.381]]
siam score:  -0.6429831
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.392]
 [0.373]
 [0.363]
 [0.355]
 [0.384]
 [0.372]] [[-4.091]
 [ 0.   ]
 [-3.679]
 [-3.008]
 [-3.496]
 [-3.724]
 [-3.958]] [[0.378]
 [0.392]
 [0.373]
 [0.363]
 [0.355]
 [0.384]
 [0.372]]
line 256 mcts: sample exp_bonus 5.142137415766877
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.327904387002604, 0.3143547932944674, 0.23090874751972773, 0.1263938974364013, 0.00021908737339975355, 0.00021908737339975355]
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.181]
 [0.321]] [[-3.589]
 [-3.589]
 [-3.589]
 [-3.589]
 [-3.589]
 [-3.615]
 [-3.222]] [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.034]
 [0.465]]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.081]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.092]] [[0.28 ]
 [1.495]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.444]] [[1.406]
 [1.789]
 [1.406]
 [1.406]
 [1.406]
 [1.406]
 [1.478]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.531078245006906
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.327904387002604, 0.3143547932944674, 0.23090874751972773, 0.1263938974364013, 0.00021908737339975355, 0.00021908737339975355]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]] [[-0.574]
 [-0.574]
 [-0.574]
 [-0.574]
 [-0.574]
 [-0.574]
 [-0.574]] [[1.468]
 [1.468]
 [1.468]
 [1.468]
 [1.468]
 [1.468]
 [1.468]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.32790438700260405, 0.3143547932944674, 0.23090874751972776, 0.12639389743640134, 0.00021908737339975358, 0.00021908737339975358]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.777]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]] [[-0.721]
 [ 5.027]
 [-0.721]
 [-0.721]
 [-0.721]
 [-0.721]
 [-0.721]] [[0.667]
 [1.708]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.261]
 [0.196]
 [0.196]
 [0.196]
 [0.362]
 [0.196]] [[-4.193]
 [-3.613]
 [-4.193]
 [-4.193]
 [-4.193]
 [-2.876]
 [-4.193]] [[-0.513]
 [-0.191]
 [-0.513]
 [-0.513]
 [-0.513]
 [ 0.257]
 [-0.513]]
line 256 mcts: sample exp_bonus -0.591028607026353
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[1.404]
 [1.404]
 [1.404]
 [1.404]
 [1.404]
 [1.404]
 [1.404]] [[2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.244]]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.608]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]] [[-1.527]
 [-0.959]
 [-1.527]
 [-1.527]
 [-1.527]
 [-1.527]
 [-1.527]] [[1.539]
 [1.846]
 [1.539]
 [1.539]
 [1.539]
 [1.539]
 [1.539]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.0624292324303255
siam score:  -0.64069635
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
actor:  1 policy actor:  1  step number:  40 total reward:  0.6849999999999998  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  42 total reward:  0.7249999999999999  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.3440979734835692, 0.30678067521888996, 0.22534519272207643, 0.12334854128584641, 0.00021380864480901934, 0.00021380864480901934]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.3440979734835692, 0.30678067521889, 0.22534519272207645, 0.12334854128584642, 0.00021380864480901937, 0.00021380864480901937]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.3440979734835692, 0.30678067521889, 0.22534519272207645, 0.12334854128584642, 0.00021380864480901937, 0.00021380864480901937]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
actor:  1 policy actor:  1  step number:  54 total reward:  0.5949999999999998  reward:  1.0 rdn_beta:  0.5
from probs:  [0.3440979734835692, 0.30678067521888996, 0.22534519272207643, 0.12334854128584641, 0.00021380864480901934, 0.00021380864480901934]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.873]
 [0.544]] [[-3.627]
 [-3.627]
 [-3.627]
 [-3.627]
 [-3.627]
 [-2.477]
 [-3.627]] [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [1.501]
 [0.509]]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [ 0.51 ]
 [-0.003]] [[-3.511]
 [-3.511]
 [-3.511]
 [-3.511]
 [-3.511]
 [-1.771]
 [-3.511]] [[0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.077]
 [1.012]
 [0.077]]
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[-3.892]
 [-3.892]
 [-3.892]
 [-3.892]
 [-3.892]
 [-3.892]
 [-3.892]] [[-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]]
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.099]
 [0.09 ]
 [0.236]
 [0.236]
 [0.056]
 [0.107]] [[-0.924]
 [-0.303]
 [-0.192]
 [-0.924]
 [-0.924]
 [-0.058]
 [-0.22 ]] [[0.969]
 [1.289]
 [1.364]
 [0.969]
 [0.969]
 [1.428]
 [1.362]]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.505]
 [0.629]
 [0.417]
 [0.417]
 [0.596]
 [0.408]] [[-1.158]
 [-0.725]
 [-3.169]
 [ 0.   ]
 [ 0.   ]
 [-3.327]
 [-0.031]] [[1.149]
 [1.505]
 [0.266]
 [1.831]
 [1.831]
 [0.153]
 [1.806]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.53 ]
 [0.551]
 [0.56 ]
 [0.564]
 [0.555]
 [0.543]] [[-0.766]
 [-0.402]
 [-0.42 ]
 [-0.899]
 [-0.959]
 [-0.965]
 [-0.803]] [[0.549]
 [0.53 ]
 [0.551]
 [0.56 ]
 [0.564]
 [0.555]
 [0.543]]
3284 5219
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.846]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]] [[3.094]
 [3.305]
 [3.094]
 [3.094]
 [3.094]
 [3.094]
 [3.094]] [[1.68 ]
 [1.988]
 [1.68 ]
 [1.68 ]
 [1.68 ]
 [1.68 ]
 [1.68 ]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.340516355927996, 0.3035874827657324, 0.23340835342356847, 0.1220646415393551, 0.000211583171674061, 0.000211583171674061]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.827]
 [0.759]
 [0.726]
 [0.759]
 [0.759]
 [0.747]] [[3.334]
 [3.318]
 [3.334]
 [2.158]
 [3.334]
 [3.334]
 [2.915]] [[1.678]
 [1.747]
 [1.678]
 [0.693]
 [1.678]
 [1.678]
 [1.327]]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.684]] [[-0.991]
 [-0.991]
 [-0.991]
 [-0.991]
 [-0.991]
 [-0.991]
 [ 0.756]] [[0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [1.113]]
from probs:  [0.340516355927996, 0.3035874827657324, 0.23340835342356847, 0.1220646415393551, 0.000211583171674061, 0.000211583171674061]
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.775]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[1.147]
 [1.542]
 [1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]] [[0.65 ]
 [0.775]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.340516355927996, 0.3035874827657324, 0.23340835342356847, 0.1220646415393551, 0.000211583171674061, 0.000211583171674061]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.2209],
        [-0.0000],
        [-0.4385],
        [ 0.2426],
        [-0.3471],
        [-0.5776],
        [ 0.0743],
        [-0.1515],
        [ 0.1824]], dtype=torch.float64)
-0.9652499999999999 -0.9652499999999999
-0.024259925299500003 -0.245167449892812
-0.8230915672154999 -0.8230915672154999
-0.024259925299500003 -0.4627336579240698
-0.024259925299500003 0.21838931723914967
-0.024259925299500003 -0.37131952523954204
-0.024259925299500003 -0.6018343339598179
-0.024259925299500003 0.05002657626593315
-0.024259925299500003 -0.17574937849884134
-0.0628797758985 0.1195583864553054
using explorer policy with actor:  1
siam score:  -0.63438004
first move QE:  -0.668193869751348
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [1.037]
 [0.526]] [[-3.48 ]
 [-3.48 ]
 [-3.48 ]
 [-3.48 ]
 [-3.48 ]
 [-1.405]
 [-3.48 ]] [[0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [1.348]
 [0.312]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [1.115]
 [0.735]] [[-1.471]
 [-1.471]
 [-1.471]
 [-1.471]
 [-1.471]
 [ 0.389]
 [-1.471]] [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [2.112]
 [0.731]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.340516355927996, 0.3035874827657324, 0.23340835342356847, 0.1220646415393551, 0.000211583171674061, 0.000211583171674061]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.718]
 [0.44 ]] [[-3.112]
 [-3.112]
 [-3.112]
 [-3.112]
 [-3.112]
 [-1.463]
 [-3.112]] [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [1.493]
 [0.523]]
start point for exploration sampling:  10749
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.739]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.745]] [[4.731]
 [4.211]
 [4.731]
 [4.731]
 [4.731]
 [4.731]
 [4.771]] [[1.608]
 [1.243]
 [1.608]
 [1.608]
 [1.608]
 [1.608]
 [1.629]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.78 ]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.732]] [[4.587]
 [5.295]
 [4.587]
 [4.587]
 [4.587]
 [4.587]
 [4.467]] [[1.644]
 [2.203]
 [1.644]
 [1.644]
 [1.644]
 [1.644]
 [1.555]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.340516355927996, 0.3035874827657324, 0.23340835342356847, 0.1220646415393551, 0.000211583171674061, 0.000211583171674061]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.14 ]
 [0.249]
 [0.303]
 [0.317]
 [0.401]
 [0.253]] [[-1.669]
 [-1.083]
 [-1.836]
 [-1.711]
 [-1.735]
 [-1.628]
 [-1.916]] [[ 0.688]
 [-0.488]
 [-0.522]
 [-0.372]
 [-0.353]
 [-0.148]
 [-0.542]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.6599999999999998  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.777]
 [0.785]
 [0.663]
 [0.663]
 [0.663]
 [0.75 ]] [[-1.263]
 [-1.129]
 [-0.544]
 [-1.263]
 [-1.263]
 [-1.263]
 [-1.107]] [[1.254]
 [1.37 ]
 [1.646]
 [1.254]
 [1.254]
 [1.254]
 [1.367]]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.518]
 [0.519]] [[-0.812]
 [-0.812]
 [-0.812]
 [-0.812]
 [-0.812]
 [-0.844]
 [-0.812]] [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.243]
 [0.258]]
3296 5235
using explorer policy with actor:  1
using explorer policy with actor:  1
3297 5239
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.3367150557316174, 0.3001984321731098, 0.24196607737997458, 0.12070199232213938, 0.00020922119657937544, 0.00020922119657937544]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.63155615
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.3367150557316174, 0.3001984321731098, 0.24196607737997458, 0.12070199232213938, 0.00020922119657937544, 0.00020922119657937544]
line 256 mcts: sample exp_bonus -0.9567508513626328
siam score:  -0.6295975
using explorer policy with actor:  0
siam score:  -0.6293987
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.723]
 [0.567]
 [0.611]
 [0.611]
 [0.611]
 [0.665]] [[3.638]
 [3.315]
 [3.727]
 [3.638]
 [3.638]
 [3.638]
 [3.602]] [[1.919]
 [1.928]
 [1.891]
 [1.919]
 [1.919]
 [1.919]
 [1.999]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.111]
 [0.109]
 [0.121]
 [0.091]
 [0.059]
 [0.065]] [[0.833]
 [0.532]
 [0.745]
 [0.798]
 [0.815]
 [0.809]
 [0.791]] [[-0.043]
 [-0.252]
 [-0.114]
 [-0.055]
 [-0.103]
 [-0.172]
 [-0.17 ]]
siam score:  -0.62768096
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]] [[-1.928]
 [-6.58 ]
 [-6.58 ]
 [-6.58 ]
 [-6.58 ]
 [-6.58 ]
 [-6.58 ]] [[0.286]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]]
line 256 mcts: sample exp_bonus 0.1365540216938801
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.3367150557316175, 0.3001984321731098, 0.2419660773799746, 0.12070199232213939, 0.00020922119657937547, 0.00020922119657937547]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[0.97 ]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]] [[0.183]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]]
from probs:  [0.3367150557316175, 0.3001984321731098, 0.2419660773799746, 0.12070199232213939, 0.00020922119657937547, 0.00020922119657937547]
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]] [[-3.871]
 [-7.045]
 [-7.045]
 [-7.045]
 [-7.045]
 [-7.045]
 [-7.045]] [[0.27 ]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.3367150557316175, 0.3001984321731098, 0.2419660773799746, 0.12070199232213939, 0.00020922119657937547, 0.00020922119657937547]
using another actor
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[-4.365]
 [-4.365]
 [-4.365]
 [-4.365]
 [-4.365]
 [-4.365]
 [-4.365]] [[0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]]
actor:  1 policy actor:  1  step number:  116 total reward:  0.3049999999999995  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.477]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.648]] [[-0.333]
 [ 0.124]
 [-0.333]
 [-0.333]
 [-0.333]
 [-0.333]
 [-0.019]] [[1.744]
 [1.548]
 [1.744]
 [1.744]
 [1.744]
 [1.744]
 [1.841]]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.543]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[-0.814]
 [ 0.277]
 [-0.814]
 [-0.814]
 [-0.814]
 [-0.814]
 [-0.814]] [[0.618]
 [1.427]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
siam score:  -0.62511486
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.6263443
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.673]
 [0.523]
 [0.453]
 [0.523]
 [0.523]
 [0.655]] [[0.939]
 [0.74 ]
 [0.939]
 [0.319]
 [0.939]
 [0.939]
 [0.777]] [[2.167]
 [2.3  ]
 [2.167]
 [1.727]
 [2.167]
 [2.167]
 [2.292]]
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.799]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.624]] [[-3.051]
 [-2.066]
 [-3.051]
 [-3.051]
 [-3.051]
 [-3.051]
 [-2.806]] [[0.257]
 [0.611]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.33 ]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.33442820338617435, 0.2981595881207649, 0.24032272736590485, 0.12667388065018712, 0.00020780023848448317, 0.00020780023848448317]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[0.916]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[0.644]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [0.907]
 [0.807]
 [0.878]
 [0.85 ]
 [0.807]
 [0.927]] [[2.949]
 [3.963]
 [4.49 ]
 [4.087]
 [2.985]
 [4.49 ]
 [3.691]] [[1.582]
 [1.96 ]
 [2.093]
 [1.986]
 [1.514]
 [2.093]
 [1.865]]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
probs:  [0.33442820338617435, 0.2981595881207649, 0.24032272736590485, 0.12667388065018712, 0.00020780023848448317, 0.00020780023848448317]
maxi score, test score, baseline:  -0.97335 -1.0 -0.97335
actor:  1 policy actor:  1  step number:  51 total reward:  0.5899999999999997  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]] [[-0.102]
 [-0.418]
 [-0.418]
 [-0.418]
 [-0.418]
 [-0.418]
 [-0.418]] [[0.54 ]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]]
actor:  0 policy actor:  1  step number:  78 total reward:  0.17499999999999938  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.683]
 [0.743]
 [0.697]
 [0.697]
 [0.742]
 [0.697]] [[1.273]
 [0.996]
 [2.449]
 [1.273]
 [1.273]
 [1.057]
 [1.273]] [[1.038]
 [0.903]
 [1.592]
 [1.038]
 [1.038]
 [1.004]
 [1.038]]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.706]
 [0.633]
 [0.559]
 [0.559]
 [0.559]
 [0.559]] [[ 0.172]
 [ 0.312]
 [-0.063]
 [ 0.172]
 [ 0.172]
 [ 0.172]
 [ 0.172]] [[1.686]
 [2.025]
 [1.754]
 [1.686]
 [1.686]
 [1.686]
 [1.686]]
siam score:  -0.61314887
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[-2.373]
 [-3.189]
 [-3.189]
 [-3.189]
 [-3.189]
 [-3.189]
 [-3.189]] [[0.453]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]] [[-0.476]
 [-0.476]
 [-0.476]
 [-0.476]
 [-0.476]
 [-0.476]
 [-0.476]] [[0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]]
siam score:  -0.6148756
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]] [[2.736]
 [2.736]
 [2.736]
 [2.736]
 [2.736]
 [2.736]
 [2.736]] [[1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]]
siam score:  -0.617521
maxi score, test score, baseline:  -0.9710000000000001 -1.0 -0.9710000000000001
probs:  [0.3311087748964626, 0.2951367205240844, 0.24795659116229807, 0.12538514816295482, 0.00020638262710010125, 0.00020638262710010125]
from probs:  [0.3311087748964626, 0.2951367205240844, 0.24795659116229807, 0.12538514816295482, 0.00020638262710010125, 0.00020638262710010125]
from probs:  [0.3311087748964626, 0.2951367205240844, 0.24795659116229807, 0.12538514816295482, 0.00020638262710010125, 0.00020638262710010125]
from probs:  [0.3311087748964626, 0.2951367205240844, 0.24795659116229807, 0.12538514816295482, 0.00020638262710010125, 0.00020638262710010125]
maxi score, test score, baseline:  -0.9710000000000001 -1.0 -0.9710000000000001
maxi score, test score, baseline:  -0.9710000000000001 -1.0 -0.9710000000000001
line 256 mcts: sample exp_bonus -4.499221896506732
maxi score, test score, baseline:  -0.9710000000000001 -1.0 -0.9710000000000001
probs:  [0.33110877489646257, 0.29513672052408435, 0.247956591162298, 0.1253851481629548, 0.0002063826271001012, 0.0002063826271001012]
maxi score, test score, baseline:  -0.9710000000000001 -1.0 -0.9710000000000001
probs:  [0.33110877489646257, 0.29513672052408435, 0.247956591162298, 0.1253851481629548, 0.0002063826271001012, 0.0002063826271001012]
3318 5272
siam score:  -0.6295961
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.78 ]
 [0.438]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[-0.223]
 [ 1.42 ]
 [-0.739]
 [-0.223]
 [-0.223]
 [-0.223]
 [-0.223]] [[1.556]
 [2.01 ]
 [1.413]
 [1.556]
 [1.556]
 [1.556]
 [1.556]]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.244]
 [0.268]
 [0.267]
 [0.271]
 [0.275]
 [0.279]] [[-2.146]
 [-0.874]
 [-1.704]
 [-1.647]
 [-1.773]
 [-1.678]
 [-1.801]] [[0.283]
 [0.244]
 [0.268]
 [0.267]
 [0.271]
 [0.275]
 [0.279]]
maxi score, test score, baseline:  -0.9710000000000001 -1.0 -0.9710000000000001
line 256 mcts: sample exp_bonus -4.011462355648708
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[-0.13 ]
 [ 0.233]
 [ 0.233]
 [ 0.233]
 [ 0.233]
 [ 0.233]
 [ 0.233]] [[2.388]
 [2.573]
 [2.573]
 [2.573]
 [2.573]
 [2.573]
 [2.573]]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[-0.894]
 [-1.863]
 [-1.863]
 [-1.863]
 [-1.863]
 [-1.863]
 [-1.863]] [[0.566]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]]
maxi score, test score, baseline:  -0.9710000000000001 -1.0 -0.9710000000000001
probs:  [0.33110877489646257, 0.29513672052408435, 0.247956591162298, 0.1253851481629548, 0.0002063826271001012, 0.0002063826271001012]
from probs:  [0.33110877489646257, 0.29513672052408435, 0.247956591162298, 0.1253851481629548, 0.0002063826271001012, 0.0002063826271001012]
maxi score, test score, baseline:  -0.9710000000000001 -1.0 -0.9710000000000001
probs:  [0.33110877489646257, 0.29513672052408435, 0.24795659116229798, 0.1253851481629548, 0.0002063826271001012, 0.0002063826271001012]
maxi score, test score, baseline:  -0.9710000000000001 -1.0 -0.9710000000000001
actor:  1 policy actor:  1  step number:  72 total reward:  0.5249999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9710000000000002 -1.0 -0.9710000000000002
probs:  [0.3372312683586655, 0.2924352758436208, 0.2456869955220408, 0.12423747314334871, 0.00020449356616207342, 0.00020449356616207342]
actor:  0 policy actor:  1  step number:  64 total reward:  0.4049999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
actor:  1 policy actor:  1  step number:  89 total reward:  0.26999999999999946  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]] [[-0.677]
 [-0.66 ]
 [-0.677]
 [-0.677]
 [-0.677]
 [-0.677]
 [-0.677]] [[2.409]
 [2.414]
 [2.409]
 [2.409]
 [2.409]
 [2.409]
 [2.409]]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.807]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.583]] [[3.86 ]
 [4.043]
 [3.495]
 [3.495]
 [3.495]
 [3.495]
 [4.004]] [[0.433]
 [1.043]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.57 ]]
Printing some Q and Qe and total Qs values:  [[0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]] [[1.656]
 [1.656]
 [1.656]
 [1.656]
 [1.656]
 [1.656]
 [1.656]] [[2.018]
 [2.018]
 [2.018]
 [2.018]
 [2.018]
 [2.018]
 [2.018]]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[ 0.428]
 [-2.882]
 [-2.882]
 [-2.882]
 [-2.882]
 [-2.882]
 [-2.882]] [[0.506]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.445]
 [0.459]
 [0.463]
 [0.495]
 [0.504]
 [0.489]] [[-2.514]
 [ 0.417]
 [-2.027]
 [-1.955]
 [-1.685]
 [-1.517]
 [-2.036]] [[0.457]
 [0.445]
 [0.459]
 [0.463]
 [0.495]
 [0.504]
 [0.489]]
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
actor:  1 policy actor:  1  step number:  74 total reward:  0.34499999999999953  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  94 total reward:  0.36499999999999955  reward:  1.0 rdn_beta:  0.667
from probs:  [0.33445143757285345, 0.2983473589255251, 0.2436176750617899, 0.12317638902083165, 0.00020356970949994636, 0.00020356970949994636]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.4166079511756444
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.42408712396815096
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.33204857535510524, 0.29620388601439146, 0.2418674068876424, 0.12947591741198258, 0.0002021071654391846, 0.0002021071654391846]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.33204857535510524, 0.29620388601439146, 0.2418674068876424, 0.12947591741198258, 0.0002021071654391846, 0.0002021071654391846]
siam score:  -0.6336353
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.3320485753551051, 0.29620388601439146, 0.24186740688764236, 0.12947591741198256, 0.00020210716543918458, 0.00020210716543918458]
using another actor
rdn beta is 0 so we're just using the maxi policy
using another actor
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.3320485753551051, 0.29620388601439146, 0.24186740688764236, 0.12947591741198256, 0.00020210716543918458, 0.00020210716543918458]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.681]] [[-0.627]
 [-0.627]
 [-0.627]
 [-0.627]
 [-0.627]
 [-0.627]
 [ 0.401]] [[1.196]
 [1.196]
 [1.196]
 [1.196]
 [1.196]
 [1.196]
 [1.564]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6349999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.774]
 [0.791]
 [0.774]
 [0.774]
 [0.789]
 [0.774]] [[2.375]
 [2.375]
 [2.619]
 [2.375]
 [2.375]
 [3.04 ]
 [2.375]] [[1.798]
 [1.798]
 [1.913]
 [1.798]
 [1.798]
 [2.049]
 [1.798]]
Printing some Q and Qe and total Qs values:  [[1.023]
 [1.164]
 [1.223]
 [1.023]
 [1.023]
 [1.118]
 [1.023]] [[1.123]
 [1.389]
 [1.373]
 [1.123]
 [1.123]
 [1.654]
 [1.123]] [[2.073]
 [2.403]
 [2.503]
 [2.073]
 [2.073]
 [2.4  ]
 [2.073]]
from probs:  [0.33891669554487713, 0.2931582096152343, 0.23938043798660896, 0.12814459880159226, 0.00020002902584364684, 0.00020002902584364684]
siam score:  -0.638574
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -6.611569877317161
using another actor
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
probs:  [0.3389166955448772, 0.29315820961523437, 0.239380437986609, 0.12814459880159226, 0.00020002902584364684, 0.00020002902584364684]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5249999999999997  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus -0.39540490236266573
siam score:  -0.6424954
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.05 ]
 [0.058]
 [0.057]
 [0.056]
 [0.056]
 [0.057]] [[-8.347]
 [-3.206]
 [-7.697]
 [-7.557]
 [-7.504]
 [-7.404]
 [-7.551]] [[0.056]
 [0.05 ]
 [0.058]
 [0.057]
 [0.056]
 [0.056]
 [0.057]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.7049999999999998  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.235314644115337
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.33223229990127046, 0.2983552916215073, 0.24340301576281181, 0.12561722493880872, 0.00019608388780081732, 0.00019608388780081732]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.553]
 [0.586]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]] [[-1.282]
 [-0.553]
 [-0.433]
 [-1.282]
 [-1.282]
 [-1.282]
 [-1.282]] [[1.074]
 [1.451]
 [1.533]
 [1.074]
 [1.074]
 [1.074]
 [1.074]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.302]
 [0.41 ]
 [0.389]
 [0.409]
 [0.457]
 [0.343]] [[-0.299]
 [-0.372]
 [-3.325]
 [-1.138]
 [-1.508]
 [-2.63 ]
 [-0.365]] [[ 1.639]
 [ 1.571]
 [-0.138]
 [ 1.17 ]
 [ 0.961]
 [ 0.318]
 [ 1.604]]
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
3344 5325
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
probs:  [0.3322322999012705, 0.29835529162150737, 0.24340301576281184, 0.12561722493880875, 0.00019608388780081737, 0.00019608388780081737]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.651]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.656]] [[3.712]
 [4.394]
 [3.712]
 [3.712]
 [3.712]
 [3.712]
 [4.223]] [[1.064]
 [1.505]
 [1.064]
 [1.064]
 [1.064]
 [1.064]
 [1.376]]
from probs:  [0.3322322999012705, 0.29835529162150737, 0.24340301576281184, 0.12561722493880875, 0.00019608388780081737, 0.00019608388780081737]
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
probs:  [0.3322322999012705, 0.29835529162150737, 0.24340301576281184, 0.12561722493880875, 0.00019608388780081737, 0.00019608388780081737]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.39 ]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[-0.798]
 [-0.151]
 [-0.798]
 [-0.798]
 [-0.798]
 [-0.798]
 [-0.798]] [[0.343]
 [0.39 ]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]]
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.362]
 [0.26 ]
 [0.267]
 [0.267]
 [0.267]
 [0.267]] [[-0.649]
 [ 1.007]
 [-0.295]
 [-0.649]
 [-0.649]
 [-0.649]
 [-0.649]] [[0.267]
 [0.362]
 [0.26 ]
 [0.267]
 [0.267]
 [0.267]
 [0.267]]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.321]
 [0.378]
 [0.378]
 [0.392]
 [0.375]
 [0.369]] [[-2.618]
 [-1.775]
 [-2.447]
 [-2.251]
 [-2.075]
 [-2.038]
 [-2.179]] [[0.394]
 [0.321]
 [0.378]
 [0.378]
 [0.392]
 [0.375]
 [0.369]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.48499999999999965  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[1.63]
 [0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]] [[0.736]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[ 0.638]
 [-7.284]
 [-7.284]
 [-7.284]
 [-7.284]
 [-7.284]
 [-7.284]] [[0.782]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]]
siam score:  -0.64340645
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]] [[-1.982]
 [-7.838]
 [-7.838]
 [-7.838]
 [-7.838]
 [-7.838]
 [-7.838]] [[0.194]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]]
Printing some Q and Qe and total Qs values:  [[0.84 ]
 [0.812]
 [0.84 ]
 [0.84 ]
 [0.84 ]
 [0.84 ]
 [0.84 ]] [[ 1.111]
 [-0.261]
 [ 1.111]
 [ 1.111]
 [ 1.111]
 [ 1.111]
 [ 1.111]] [[1.682]
 [1.167]
 [1.682]
 [1.682]
 [1.682]
 [1.682]
 [1.682]]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]] [[-7.794]
 [-8.069]
 [-8.069]
 [-8.069]
 [-8.069]
 [-8.069]
 [-8.069]] [[0.49 ]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]] [[-4.761]
 [-5.223]
 [-5.223]
 [-5.223]
 [-5.223]
 [-5.223]
 [-5.223]] [[0.24 ]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.4249999999999996  reward:  1.0 rdn_beta:  0.5
from probs:  [0.3295034685454086, 0.304118337783686, 0.24140379479085247, 0.12458545222329838, 0.0001944733283772779, 0.0001944733283772779]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]] [[-5.991]
 [-7.486]
 [-7.486]
 [-7.486]
 [-7.486]
 [-7.486]
 [-7.486]] [[0.566]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]]
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
probs:  [0.32703456345884246, 0.30183963852025564, 0.2470878078847315, 0.12365195778069328, 0.00019301617773861642, 0.00019301617773861642]
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
probs:  [0.32703456345884246, 0.30183963852025564, 0.2470878078847315, 0.12365195778069328, 0.00019301617773861642, 0.00019301617773861642]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.12675014736306844
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5306242279203522
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.662]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.514]] [[-0.134]
 [-0.458]
 [-0.134]
 [-0.134]
 [-0.134]
 [-0.134]
 [-0.213]] [[1.203]
 [1.532]
 [1.203]
 [1.203]
 [1.203]
 [1.203]
 [1.319]]
from probs:  [0.32703456345884246, 0.30183963852025564, 0.2470878078847315, 0.12365195778069331, 0.00019301617773861642, 0.00019301617773861642]
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
probs:  [0.32703456345884246, 0.30183963852025564, 0.2470878078847315, 0.12365195778069331, 0.00019301617773861642, 0.00019301617773861642]
actor:  1 policy actor:  1  step number:  45 total reward:  0.7299999999999999  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.377]
 [0.7  ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]] [[ 0.134]
 [ 0.186]
 [-0.034]
 [ 0.134]
 [ 0.134]
 [ 0.134]
 [ 0.134]] [[1.527]
 [1.54 ]
 [1.646]
 [1.527]
 [1.527]
 [1.527]
 [1.527]]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]] [[-1.176]
 [-1.201]
 [-1.201]
 [-1.201]
 [-1.201]
 [-1.201]
 [-1.201]] [[0.373]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]]
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[-1.336]
 [-1.377]
 [-1.377]
 [-1.377]
 [-1.377]
 [-1.377]
 [-1.377]] [[0.397]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]]
Starting evaluation
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
deleting a thread, now have 5 threads
Frames:  240125 train batches done:  28138 episodes:  8696
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[-1.305]
 [-1.511]
 [-1.511]
 [-1.511]
 [-1.511]
 [-1.511]
 [-1.511]] [[0.712]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.437]
 [0.407]
 [0.465]
 [0.409]
 [0.408]
 [0.409]] [[-3.555]
 [-3.513]
 [-3.8  ]
 [-3.947]
 [-3.555]
 [-3.688]
 [-3.644]] [[0.409]
 [0.437]
 [0.407]
 [0.465]
 [0.409]
 [0.408]
 [0.409]]
using explorer policy with actor:  0
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -3.4602231778172228
line 256 mcts: sample exp_bonus -1.7897754331363127
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.834]
 [0.716]
 [0.817]
 [0.717]
 [0.817]
 [0.72 ]] [[4.114]
 [4.064]
 [4.046]
 [3.89 ]
 [4.233]
 [3.89 ]
 [4.123]] [[1.095]
 [1.356]
 [1.108]
 [1.204]
 [1.233]
 [1.204]
 [1.166]]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.381]
 [0.268]
 [0.316]
 [0.396]
 [0.345]
 [0.377]] [[-3.144]
 [-0.277]
 [-2.862]
 [-3.065]
 [-2.38 ]
 [-2.845]
 [-2.827]] [[-0.102]
 [ 0.853]
 [-0.033]
 [-0.075]
 [ 0.184]
 [ 0.009]
 [ 0.031]]
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.453]
 [0.522]
 [0.522]
 [0.522]
 [0.453]
 [0.483]] [[ 0.   ]
 [-3.633]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-4.245]
 [-4.003]] [[0.522]
 [0.453]
 [0.522]
 [0.522]
 [0.522]
 [0.453]
 [0.483]]
deleting a thread, now have 4 threads
Frames:  240262 train batches done:  28149 episodes:  8698
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.599]
 [0.506]
 [0.506]
 [0.506]
 [0.505]
 [0.589]] [[-3.754]
 [-3.358]
 [-3.754]
 [-3.754]
 [-3.754]
 [-3.79 ]
 [-3.317]] [[0.506]
 [0.599]
 [0.506]
 [0.506]
 [0.506]
 [0.505]
 [0.589]]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.527]
 [0.502]
 [0.503]
 [0.506]
 [0.503]
 [0.604]] [[-3.886]
 [-3.491]
 [-4.02 ]
 [-3.795]
 [-3.811]
 [-3.79 ]
 [-3.447]] [[0.509]
 [0.527]
 [0.502]
 [0.503]
 [0.506]
 [0.503]
 [0.604]]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.498]
 [0.502]
 [0.503]
 [0.632]
 [0.503]
 [0.604]] [[-3.873]
 [-3.618]
 [-4.005]
 [-3.784]
 [-3.527]
 [-3.781]
 [-3.431]] [[0.509]
 [0.498]
 [0.502]
 [0.503]
 [0.632]
 [0.503]
 [0.604]]
using explorer policy with actor:  1
siam score:  -0.64136976
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[-3.444]
 [-3.444]
 [-3.444]
 [-3.444]
 [-3.444]
 [-3.444]
 [-3.444]] [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.45 ]
 [0.416]
 [0.416]
 [0.416]
 [0.596]
 [0.419]] [[-3.629]
 [-3.545]
 [-3.629]
 [-3.629]
 [-3.629]
 [-3.233]
 [-3.619]] [[0.416]
 [0.45 ]
 [0.416]
 [0.416]
 [0.416]
 [0.596]
 [0.419]]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.75 ]
 [0.506]
 [0.62 ]
 [0.65 ]
 [0.693]
 [0.588]] [[3.748]
 [3.743]
 [4.226]
 [3.749]
 [4.279]
 [4.193]
 [4.343]] [[0.949]
 [0.945]
 [1.008]
 [0.843]
 [1.154]
 [1.141]
 [1.137]]
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
maxi score, test score, baseline:  -0.96819 -1.0 -0.96819
probs:  [0.33444392719722893, 0.2985163776943292, 0.2443673658760777, 0.12229054710131326, 0.00019089106552541138, 0.00019089106552541138]
UNIT TEST: sample policy line 217 mcts : [0.571 0.102 0.02  0.02  0.02  0.02  0.245]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.444]
 [0.646]
 [0.646]
 [0.646]
 [0.471]
 [0.494]] [[0.685]
 [0.142]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.591]] [[0.485]
 [0.444]
 [0.646]
 [0.646]
 [0.646]
 [0.471]
 [0.494]]
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.624]
 [0.752]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[-4.242]
 [-4.242]
 [-4.707]
 [-4.242]
 [-4.242]
 [-4.242]
 [-4.242]] [[0.624]
 [0.624]
 [0.752]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.5199999999999997  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.567]
 [0.589]
 [0.569]
 [0.569]
 [0.569]
 [0.635]] [[-3.086]
 [-1.318]
 [-2.936]
 [-3.086]
 [-3.086]
 [-3.086]
 [-2.792]] [[0.569]
 [0.567]
 [0.589]
 [0.569]
 [0.569]
 [0.569]
 [0.635]]
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.641]
 [0.498]
 [0.355]
 [0.355]
 [0.355]
 [0.355]] [[0.646]
 [0.67 ]
 [0.491]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[1.422]
 [1.575]
 [1.442]
 [1.422]
 [1.422]
 [1.422]
 [1.422]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.667
from probs:  [0.3295693378068159, 0.302497566107287, 0.24080566099165854, 0.12675121751668733, 0.00018810878877566475, 0.00018810878877566475]
rdn probs:  [0.32956933780681585, 0.30249756610728695, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]] [[-6.227]
 [-9.387]
 [-9.387]
 [-9.387]
 [-9.387]
 [-9.387]
 [-9.387]] [[0.209]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[ 1.662]
 [-3.583]
 [-3.583]
 [-3.583]
 [-3.583]
 [-3.583]
 [-3.583]] [[0.3  ]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]]
deleting a thread, now have 3 threads
Frames:  240501 train batches done:  28181 episodes:  8704
siam score:  -0.6404849
line 256 mcts: sample exp_bonus 0.38243094847355896
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32956933780681585, 0.30249756610728695, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.061]
 [0.082]
 [0.084]
 [0.083]
 [0.082]
 [0.085]] [[-5.675]
 [-2.8  ]
 [-8.303]
 [-8.643]
 [-8.029]
 [-8.297]
 [-7.855]] [[0.238]
 [0.061]
 [0.082]
 [0.084]
 [0.083]
 [0.082]
 [0.085]]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[-8.348]
 [-8.303]
 [-8.303]
 [-8.303]
 [-8.303]
 [-8.303]
 [-8.303]] [[0.418]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
deleting a thread, now have 2 threads
Frames:  240706 train batches done:  28202 episodes:  8706
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32956933780681585, 0.30249756610728695, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32956933780681585, 0.30249756610728695, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32956933780681585, 0.30249756610728695, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]] [[-9.425]
 [-9.265]
 [-9.265]
 [-9.265]
 [-9.265]
 [-9.265]
 [-9.265]] [[0.203]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.19 ]
 [0.195]
 [0.197]
 [0.194]
 [0.201]
 [0.198]] [[-4.351]
 [-2.556]
 [-8.965]
 [-9.011]
 [-9.143]
 [-9.019]
 [-8.927]] [[0.421]
 [0.19 ]
 [0.195]
 [0.197]
 [0.194]
 [0.201]
 [0.198]]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32956933780681585, 0.30249756610728695, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[-4.256]
 [-9.202]
 [-9.202]
 [-9.202]
 [-9.202]
 [-9.202]
 [-9.202]] [[0.317]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[-1.377]
 [-4.837]
 [-4.837]
 [-4.837]
 [-4.837]
 [-4.837]
 [-4.837]] [[0.457]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]]
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[-1.022]
 [-6.968]
 [-6.968]
 [-6.968]
 [-6.968]
 [-6.968]
 [-6.968]] [[0.646]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]]
first move QE:  -0.6885226666234513
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.137]
 [0.163]
 [0.151]
 [0.148]
 [0.162]
 [0.154]] [[-8.291]
 [-1.865]
 [-8.012]
 [-8.307]
 [-7.857]
 [-7.67 ]
 [-7.301]] [[0.333]
 [0.137]
 [0.163]
 [0.151]
 [0.148]
 [0.162]
 [0.154]]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32956933780681585, 0.30249756610728695, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[-2.471]
 [-8.701]
 [-8.701]
 [-8.701]
 [-8.701]
 [-8.701]
 [-8.701]] [[0.481]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]]
line 256 mcts: sample exp_bonus -2.025706307553831
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32956933780681585, 0.30249756610728695, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]] [[-2.894]
 [-8.699]
 [-8.699]
 [-8.699]
 [-8.699]
 [-8.699]
 [-8.699]] [[0.582]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[-0.817]
 [-1.265]
 [-1.265]
 [-1.265]
 [-1.265]
 [-1.265]
 [-1.265]] [[0.619]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]] [[-0.659]
 [-0.74 ]
 [-0.74 ]
 [-0.74 ]
 [-0.74 ]
 [-0.74 ]
 [-0.74 ]] [[0.63 ]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
siam score:  -0.64698577
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.733]] [[-0.499]
 [-0.499]
 [-0.499]
 [-0.499]
 [-0.499]
 [-0.499]
 [-0.676]] [[0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.733]]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32956933780681585, 0.30249756610728695, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
from probs:  [0.32956933780681585, 0.30249756610728695, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
in main func line 156:  3369
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
Printing some Q and Qe and total Qs values:  [[0.943]
 [0.692]
 [0.943]
 [0.943]
 [0.624]
 [0.634]
 [0.943]] [[ 0.165]
 [-2.91 ]
 [ 0.165]
 [ 0.165]
 [-3.202]
 [-3.549]
 [ 0.165]] [[3.71 ]
 [1.161]
 [3.71 ]
 [3.71 ]
 [0.831]
 [0.62 ]
 [3.71 ]]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.3295693378068159, 0.30249756610728695, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.3295693378068159, 0.302497566107287, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
using another actor
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.3295693378068159, 0.302497566107287, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.3295693378068159, 0.302497566107287, 0.24080566099165854, 0.12675121751668733, 0.0001881087887756647, 0.0001881087887756647]
using explorer policy with actor:  1
siam score:  -0.6425985
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.434]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]] [[-2.996]
 [-3.056]
 [-2.996]
 [-2.996]
 [-2.996]
 [-2.996]
 [-2.996]] [[0.422]
 [0.434]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]]
actor:  1 policy actor:  1  step number:  129 total reward:  0.02999999999999925  reward:  1.0 rdn_beta:  0.333
siam score:  -0.6459727
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32833853813322983, 0.30510243758493577, 0.23990635545881211, 0.12627785625624868, 0.00018740628338676998, 0.00018740628338676998]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[-5.298]
 [-7.365]
 [-7.365]
 [-7.365]
 [-7.365]
 [-7.365]
 [-7.365]] [[0.445]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32833853813322983, 0.30510243758493577, 0.23990635545881211, 0.12627785625624868, 0.00018740628338676998, 0.00018740628338676998]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
siam score:  -0.6560324
first move QE:  -0.6903633752724635
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]] [[-2.496]
 [-2.292]
 [-2.292]
 [-2.292]
 [-2.292]
 [-2.292]
 [-2.292]] [[0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]]
line 256 mcts: sample exp_bonus 2.942069671946643
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
siam score:  -0.6604164
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.586]
 [0.663]
 [0.586]
 [0.586]
 [0.586]
 [0.521]] [[0.808]
 [1.408]
 [1.397]
 [1.408]
 [1.408]
 [1.408]
 [1.349]] [[0.261]
 [1.253]
 [1.384]
 [1.253]
 [1.253]
 [1.253]
 [1.102]]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32833853813322983, 0.30510243758493577, 0.23990635545881211, 0.12627785625624868, 0.00018740628338676998, 0.00018740628338676998]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32833853813322983, 0.30510243758493577, 0.23990635545881211, 0.12627785625624868, 0.00018740628338676998, 0.00018740628338676998]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32833853813322983, 0.30510243758493577, 0.23990635545881211, 0.12627785625624868, 0.00018740628338676998, 0.00018740628338676998]
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.945]
 [0.734]
 [0.734]
 [0.753]
 [0.734]
 [0.734]] [[2.822]
 [1.773]
 [2.822]
 [2.822]
 [3.23 ]
 [2.822]
 [2.822]] [[1.221]
 [1.019]
 [1.221]
 [1.221]
 [1.446]
 [1.221]
 [1.221]]
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32833853813322983, 0.30510243758493577, 0.23990635545881211, 0.12627785625624868, 0.00018740628338676998, 0.00018740628338676998]
actor:  1 policy actor:  1  step number:  56 total reward:  0.5749999999999997  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  63 total reward:  0.5499999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.32267083232215765, 0.31709760262097436, 0.23576514604526674, 0.12409807637484205, 0.0001841713183795504, 0.0001841713183795504]
3379 5367
siam score:  -0.6454669
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.3226708323221577, 0.31709760262097436, 0.23576514604526674, 0.12409807637484205, 0.0001841713183795504, 0.0001841713183795504]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.991018419609043
maxi score, test score, baseline:  -0.9681900000000001 -1.0 -0.9681900000000001
probs:  [0.3226708323221577, 0.31709760262097436, 0.23576514604526674, 0.12409807637484205, 0.0001841713183795504, 0.0001841713183795504]
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[-1.664]
 [-2.613]
 [-2.613]
 [-2.613]
 [-2.613]
 [-2.613]
 [-2.613]] [[0.867]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]]
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]] [[ 0.556]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]] [[0.895]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]]
Printing some Q and Qe and total Qs values:  [[1.007]
 [0.931]
 [0.931]
 [1.004]
 [1.   ]
 [0.984]
 [0.998]] [[-0.435]
 [ 1.953]
 [ 1.953]
 [-0.605]
 [-0.362]
 [-0.423]
 [-0.635]] [[1.007]
 [0.931]
 [0.931]
 [1.004]
 [1.   ]
 [0.984]
 [0.998]]
actor:  0 policy actor:  1  step number:  82 total reward:  0.13499999999999934  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.465]
 [0.559]
 [0.509]
 [0.43 ]
 [0.501]
 [0.467]] [[-1.007]
 [-0.307]
 [-3.207]
 [-3.925]
 [-2.013]
 [-3.351]
 [-1.32 ]] [[0.37 ]
 [0.465]
 [0.559]
 [0.509]
 [0.43 ]
 [0.501]
 [0.467]]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.974]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[0.406]
 [1.307]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[0.738]
 [1.785]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]]
line 256 mcts: sample exp_bonus -3.8607393473562066
Printing some Q and Qe and total Qs values:  [[0.741]
 [1.13 ]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.717]] [[0.352]
 [1.28 ]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.423]] [[1.346]
 [2.319]
 [1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.325]]
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.667]
 [0.613]
 [0.611]
 [0.669]
 [0.617]
 [0.62 ]] [[-2.195]
 [-1.755]
 [-3.542]
 [-3.758]
 [ 0.   ]
 [-3.797]
 [-3.233]] [[0.612]
 [0.667]
 [0.613]
 [0.611]
 [0.669]
 [0.617]
 [0.62 ]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
siam score:  -0.6647476
first move QE:  -0.6907199978727336
actor:  1 policy actor:  1  step number:  65 total reward:  0.4499999999999996  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.05 ]
 [1.173]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.14 ]] [[4.041]
 [3.658]
 [4.041]
 [4.041]
 [4.041]
 [4.041]
 [4.522]] [[1.102]
 [1.221]
 [1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.443]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.32769498501311545, 0.3147452482054188, 0.23402527328192613, 0.12316770526191342, 0.00018339411881312804, 0.00018339411881312804]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.32769498501311545, 0.3147452482054188, 0.23402527328192613, 0.12316770526191342, 0.00018339411881312804, 0.00018339411881312804]
using explorer policy with actor:  1
siam score:  -0.6821889
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.578]
 [0.7  ]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[-3.631]
 [-3.631]
 [-2.266]
 [-3.631]
 [-3.631]
 [-3.631]
 [-3.631]] [[0.578]
 [0.578]
 [0.7  ]
 [0.578]
 [0.578]
 [0.578]
 [0.578]]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.3276949850131154, 0.31474524820541877, 0.23402527328192613, 0.1231677052619134, 0.000183394118813128, 0.000183394118813128]
from probs:  [0.3276949850131154, 0.31474524820541877, 0.23402527328192613, 0.1231677052619134, 0.000183394118813128, 0.000183394118813128]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[-0.446]
 [-0.446]
 [-0.446]
 [-0.446]
 [-0.446]
 [-0.446]
 [-0.446]] [[0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.438]
 [0.366]
 [0.438]
 [0.438]
 [0.438]
 [0.427]] [[-0.212]
 [-0.212]
 [-0.76 ]
 [-0.212]
 [-0.212]
 [-0.212]
 [-0.244]] [[1.545]
 [1.545]
 [1.24 ]
 [1.545]
 [1.545]
 [1.545]
 [1.523]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.32769498501311545, 0.3147452482054188, 0.23402527328192613, 0.12316770526191342, 0.00018339411881312804, 0.00018339411881312804]
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]] [[-0.297]
 [-3.741]
 [-3.741]
 [-3.741]
 [-3.741]
 [-3.741]
 [-3.741]] [[0.29 ]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.868571584788226
siam score:  -0.6802368
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.32769498501311545, 0.3147452482054188, 0.23402527328192613, 0.12316770526191342, 0.00018339411881312804, 0.00018339411881312804]
siam score:  -0.68225557
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  87 total reward:  0.5099999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.3250974904170455, 0.3122504005002592, 0.2321702574577771, 0.13011797075697623, 0.00018194043397095982, 0.00018194043397095982]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.2867168621250182
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.32509749041704544, 0.3122504005002592, 0.2321702574577771, 0.13011797075697623, 0.00018194043397095982, 0.00018194043397095982]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.32509749041704544, 0.3122504005002592, 0.23217025745777717, 0.1301179707569762, 0.00018194043397095976, 0.00018194043397095976]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
siam score:  -0.68865234
Printing some Q and Qe and total Qs values:  [[0.868]
 [0.99 ]
 [0.8  ]
 [0.953]
 [0.891]
 [0.953]
 [0.938]] [[2.992]
 [3.666]
 [3.642]
 [3.404]
 [3.689]
 [3.404]
 [3.788]] [[1.074]
 [1.728]
 [1.355]
 [1.493]
 [1.555]
 [1.493]
 [1.706]]
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [1.092]
 [0.82 ]
 [1.013]
 [0.82 ]
 [0.82 ]
 [1.108]] [[4.766]
 [3.922]
 [4.766]
 [4.447]
 [4.766]
 [4.766]
 [4.097]] [[1.748]
 [1.729]
 [1.748]
 [1.919]
 [1.748]
 [1.748]
 [1.876]]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.32509749041704544, 0.3122504005002592, 0.23217025745777717, 0.1301179707569762, 0.00018194043397095976, 0.00018194043397095976]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.32509749041704544, 0.3122504005002592, 0.23217025745777717, 0.1301179707569762, 0.00018194043397095976, 0.00018194043397095976]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
siam score:  -0.6872425
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[-9.985]
 [-9.694]
 [-9.694]
 [-9.694]
 [-9.694]
 [-9.694]
 [-9.694]] [[0.139]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
siam score:  -0.6909616
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[ 1.04 ]
 [-5.305]
 [-5.305]
 [-5.305]
 [-5.305]
 [-5.305]
 [-5.305]] [[0.194]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]]
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.033]
 [0.082]
 [0.083]
 [0.084]
 [0.105]
 [0.082]] [[-6.559]
 [-2.275]
 [-9.204]
 [-8.882]
 [-9.288]
 [-8.622]
 [-9.17 ]] [[0.207]
 [0.033]
 [0.082]
 [0.083]
 [0.084]
 [0.105]
 [0.082]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.32509749041704544, 0.3122504005002592, 0.23217025745777717, 0.1301179707569762, 0.00018194043397095982, 0.00018194043397095982]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.332]
 [0.411]
 [0.332]
 [0.332]
 [0.332]
 [0.332]] [[-4.207]
 [-4.207]
 [-4.18 ]
 [-4.207]
 [-4.207]
 [-4.207]
 [-4.207]] [[0.332]
 [0.332]
 [0.411]
 [0.332]
 [0.332]
 [0.332]
 [0.332]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.32509749041704544, 0.3122504005002592, 0.23217025745777717, 0.1301179707569762, 0.00018194043397095982, 0.00018194043397095982]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.32509749041704544, 0.3122504005002592, 0.23217025745777717, 0.1301179707569762, 0.00018194043397095982, 0.00018194043397095982]
siam score:  -0.70229447
Printing some Q and Qe and total Qs values:  [[1.033]
 [1.17 ]
 [1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.132]] [[3.217]
 [3.714]
 [3.217]
 [3.217]
 [3.217]
 [3.217]
 [3.654]] [[1.56 ]
 [2.001]
 [1.56 ]
 [1.56 ]
 [1.56 ]
 [1.56 ]
 [1.905]]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.43 ]
 [0.402]
 [0.569]
 [0.402]
 [0.402]
 [0.564]] [[2.207]
 [1.667]
 [2.207]
 [2.027]
 [2.207]
 [2.207]
 [1.901]] [[1.314]
 [1.051]
 [1.314]
 [1.352]
 [1.314]
 [1.314]
 [1.281]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5749999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.33087877320793785, 0.3095756321578257, 0.23018146367654327, 0.1290033671298716, 0.00018038191391073094, 0.00018038191391073094]
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]] [[3.603]
 [3.603]
 [3.603]
 [3.603]
 [3.603]
 [3.603]
 [3.603]] [[1.444]
 [1.444]
 [1.444]
 [1.444]
 [1.444]
 [1.444]
 [1.444]]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.532]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[-3.367]
 [-3.35 ]
 [-3.367]
 [-3.367]
 [-3.367]
 [-3.367]
 [-3.367]] [[0.442]
 [0.532]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]]
siam score:  -0.70874083
line 256 mcts: sample exp_bonus -3.530657994730847
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.33087877320793785, 0.3095756321578257, 0.23018146367654327, 0.1290033671298716, 0.00018038191391073094, 0.00018038191391073094]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.33087877320793785, 0.3095756321578257, 0.23018146367654327, 0.1290033671298716, 0.00018038191391073094, 0.00018038191391073094]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.226]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.156]] [[-1.841]
 [-0.433]
 [-1.841]
 [-1.841]
 [-1.841]
 [-1.841]
 [-0.797]] [[0.199]
 [0.226]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.156]]
line 256 mcts: sample exp_bonus 4.321654031955043
first move QE:  -0.6945327701348132
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
using explorer policy with actor:  1
start point for exploration sampling:  10749
actor:  1 policy actor:  1  step number:  61 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
first move QE:  -0.6951046320130718
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.751]
 [0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]] [[2.357]
 [2.992]
 [2.357]
 [2.357]
 [2.357]
 [2.357]
 [2.357]] [[0.784]
 [0.751]
 [0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.5749999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.3416225917611241, 0.3046048969797812, 0.22648552969473865, 0.12693201038060484, 0.00017748559187559756, 0.00017748559187559756]
siam score:  -0.7143283
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.3416225917611241, 0.3046048969797812, 0.22648552969473865, 0.12693201038060484, 0.00017748559187559756, 0.00017748559187559756]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
line 256 mcts: sample exp_bonus -5.339455238965344
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[ 4.372]
 [-6.215]
 [-6.215]
 [-6.215]
 [-6.215]
 [-6.215]
 [-6.215]] [[0.54 ]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]] [[ 5.609]
 [-2.175]
 [-2.175]
 [-2.175]
 [-2.175]
 [-2.175]
 [-2.175]] [[0.547]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]]
Printing some Q and Qe and total Qs values:  [[0.743]
 [1.099]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.984]] [[0.434]
 [0.913]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.04 ]] [[1.143]
 [1.477]
 [1.143]
 [1.143]
 [1.143]
 [1.143]
 [1.168]]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.948]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[-0.378]
 [ 1.007]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.378]] [[0.963]
 [1.794]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.734]
 [0.754]
 [0.738]
 [0.749]
 [0.742]
 [0.747]] [[7.122]
 [4.814]
 [4.645]
 [4.593]
 [4.771]
 [5.282]
 [4.703]] [[1.919]
 [0.944]
 [0.894]
 [0.848]
 [0.944]
 [1.166]
 [0.911]]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.594]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-1.057]] [[1.908]
 [1.908]
 [1.908]
 [1.908]
 [1.908]
 [1.908]
 [1.124]]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
Printing some Q and Qe and total Qs values:  [[0.947]
 [0.947]
 [1.074]
 [0.947]
 [0.947]
 [1.086]
 [0.947]] [[0.883]
 [0.883]
 [2.291]
 [0.883]
 [0.883]
 [2.537]
 [0.883]] [[1.641]
 [1.641]
 [2.286]
 [1.641]
 [1.641]
 [2.381]
 [1.641]]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.795]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[1.079]
 [1.906]
 [1.079]
 [1.079]
 [1.079]
 [1.079]
 [1.079]] [[0.81 ]
 [1.302]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
using another actor
from probs:  [0.3416225917611241, 0.3046048969797812, 0.22648552969473862, 0.12693201038060484, 0.00017748559187559756, 0.00017748559187559756]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.3416225917611241, 0.3046048969797812, 0.22648552969473865, 0.12693201038060484, 0.00017748559187559756, 0.00017748559187559756]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.801]
 [0.795]
 [0.756]
 [0.756]
 [0.756]
 [0.756]] [[1.456]
 [0.925]
 [1.737]
 [1.456]
 [1.456]
 [1.456]
 [1.456]] [[1.582]
 [1.494]
 [1.753]
 [1.582]
 [1.582]
 [1.582]
 [1.582]]
siam score:  -0.71641725
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.3416225917611241, 0.30460489697978116, 0.22648552969473862, 0.1269320103806048, 0.00017748559187559754, 0.00017748559187559754]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.585]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.422]] [[-0.229]
 [ 1.619]
 [-0.229]
 [-0.229]
 [-0.229]
 [-0.229]
 [ 0.188]] [[0.699]
 [1.475]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.869]]
Printing some Q and Qe and total Qs values:  [[0.924]
 [1.139]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [1.105]] [[3.214]
 [3.133]
 [3.214]
 [3.214]
 [3.214]
 [3.214]
 [3.111]] [[0.876]
 [1.278]
 [0.876]
 [0.876]
 [0.876]
 [0.876]
 [1.203]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]] [[-3.108]
 [-9.808]
 [-9.808]
 [-9.808]
 [-9.808]
 [-9.808]
 [-9.808]] [[0.417]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.34162259176112414, 0.3046048969797812, 0.22648552969473862, 0.12693201038060484, 0.00017748559187559756, 0.00017748559187559756]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[-4.352]
 [-9.751]
 [-9.751]
 [-9.751]
 [-9.751]
 [-9.751]
 [-9.751]] [[0.487]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]] [[ 0.421]
 [-7.076]
 [-7.076]
 [-7.076]
 [-7.076]
 [-7.076]
 [-7.076]] [[0.556]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.6099999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.836]
 [1.029]
 [0.836]
 [0.836]
 [0.836]
 [1.022]] [[-0.514]
 [-0.514]
 [ 0.811]
 [-0.514]
 [-0.514]
 [-0.514]
 [ 0.386]] [[1.843]
 [1.843]
 [2.658]
 [1.843]
 [1.843]
 [1.843]
 [2.505]]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
from probs:  [0.34737599058517066, 0.3019430294336724, 0.22450632815485758, 0.12582278264873123, 0.00017593458878405196, 0.00017593458878405196]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.34737599058517066, 0.3019430294336724, 0.22450632815485758, 0.12582278264873123, 0.00017593458878405196, 0.00017593458878405196]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]] [[-0.748]
 [-0.851]
 [-0.851]
 [-0.851]
 [-0.851]
 [-0.851]
 [-0.851]] [[0.626]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[-0.828]
 [-2.025]
 [-2.025]
 [-2.025]
 [-2.025]
 [-2.025]
 [-2.025]] [[0.531]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]]
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [1.119]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]] [[-0.94 ]
 [-0.464]
 [-0.94 ]
 [-0.94 ]
 [-0.94 ]
 [-0.94 ]
 [-0.94 ]] [[1.214]
 [2.049]
 [1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]]
first move QE:  -0.7017074605433941
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.34737599058517066, 0.3019430294336724, 0.22450632815485758, 0.12582278264873123, 0.00017593458878405196, 0.00017593458878405196]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.651]
 [0.642]
 [0.682]
 [0.681]
 [0.642]
 [0.642]] [[-9.481]
 [-2.743]
 [ 0.   ]
 [-8.533]
 [-8.618]
 [ 0.   ]
 [ 0.   ]] [[0.352]
 [1.849]
 [2.462]
 [0.563]
 [0.544]
 [2.462]
 [2.462]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.49499999999999966  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.35224059825616183, 0.29969237000956944, 0.2228328757682262, 0.12488490959345198, 0.00017462318629526066, 0.00017462318629526066]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.974]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[-0.191]
 [ 0.247]
 [-0.191]
 [-0.191]
 [-0.191]
 [-0.191]
 [-0.191]] [[0.943]
 [1.615]
 [0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.35224059825616183, 0.29969237000956944, 0.2228328757682262, 0.12488490959345198, 0.00017462318629526066, 0.00017462318629526066]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.444]
 [0.397]
 [0.435]
 [0.465]
 [0.383]
 [0.437]] [[-3.855]
 [-3.862]
 [-4.01 ]
 [-3.912]
 [-3.698]
 [-2.218]
 [-3.715]] [[ 0.035]
 [ 0.034]
 [-0.069]
 [ 0.003]
 [ 0.133]
 [ 0.877]
 [ 0.109]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]] [[-6.025]
 [-5.189]
 [-5.189]
 [-5.189]
 [-5.189]
 [-5.189]
 [-5.189]] [[0.301]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.35224059825616183, 0.29969237000956944, 0.2228328757682262, 0.12488490959345198, 0.00017462318629526066, 0.00017462318629526066]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.35224059825616183, 0.29969237000956944, 0.2228328757682262, 0.12488490959345198, 0.00017462318629526066, 0.00017462318629526066]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.35224059825616183, 0.29969237000956944, 0.22283287576822625, 0.12488490959345198, 0.00017462318629526066, 0.00017462318629526066]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.35224059825616183, 0.29969237000956944, 0.22283287576822625, 0.12488490959345198, 0.00017462318629526066, 0.00017462318629526066]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.3434],
        [ 0.2447],
        [-0.4162],
        [-0.0000],
        [ 0.5730],
        [-0.2964],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [ 0.2492]], dtype=torch.float64)
-0.0727797758985 -0.41618007098315024
-0.024259925299500003 0.22044354181655104
-0.0727797758985 -0.4889565081686629
-0.6583499999999998 -0.6583499999999998
-0.0439609252995 0.5290163713942466
-0.024259925299500003 -0.3206586629858132
-0.7909411949999999 -0.7909411949999999
-0.95128310475 -0.95128310475
-0.79579269 -0.79579269
-0.024259925299500003 0.22498941094158598
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.35224059825616183, 0.29969237000956944, 0.22283287576822625, 0.12488490959345198, 0.00017462318629526066, 0.00017462318629526066]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.35224059825616183, 0.29969237000956944, 0.22283287576822625, 0.12488490959345198, 0.00017462318629526066, 0.00017462318629526066]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
first move QE:  -0.7051938811463666
line 256 mcts: sample exp_bonus -4.043323404485099
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.35224059825616183, 0.29969237000956944, 0.2228328757682262, 0.12488490959345198, 0.00017462318629526066, 0.00017462318629526066]
siam score:  -0.717083
siam score:  -0.7177476
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
using explorer policy with actor:  1
first move QE:  -0.705381408027711
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.35224059825616183, 0.29969237000956944, 0.2228328757682262, 0.12488490959345198, 0.00017462318629526066, 0.00017462318629526066]
using explorer policy with actor:  1
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  81 total reward:  0.0899999999999993  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]] [[ 0.038]
 [-8.212]
 [-8.212]
 [-8.212]
 [-8.212]
 [-8.212]
 [-8.212]] [[0.28 ]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]]
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.35475317774508097, 0.2985298999630608, 0.22196853429883986, 0.12440049630438928, 0.0001739458443145901, 0.0001739458443145901]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.35475317774508097, 0.2985298999630607, 0.22196853429883978, 0.12440049630438924, 0.00017394584431459, 0.00017394584431459]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.369]
 [0.348]
 [0.349]
 [0.344]
 [0.342]
 [0.34 ]] [[-4.965]
 [-4.245]
 [-5.101]
 [-4.945]
 [-5.057]
 [-4.979]
 [-5.053]] [[0.354]
 [0.369]
 [0.348]
 [0.349]
 [0.344]
 [0.342]
 [0.34 ]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.35475317774508097, 0.2985298999630607, 0.22196853429883978, 0.12440049630438924, 0.00017394584431459, 0.00017394584431459]
line 256 mcts: sample exp_bonus 3.174259775556165
siam score:  -0.71053815
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.35475317774508097, 0.2985298999630607, 0.22196853429883978, 0.12440049630438924, 0.00017394584431459, 0.00017394584431459]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.35475317774508097, 0.2985298999630607, 0.22196853429883978, 0.12440049630438926, 0.00017394584431459, 0.00017394584431459]
actor:  1 policy actor:  1  step number:  53 total reward:  0.6499999999999998  reward:  1.0 rdn_beta:  0.167
from probs:  [0.35475317774508097, 0.2985298999630607, 0.22196853429883978, 0.12440049630438926, 0.00017394584431459, 0.00017394584431459]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.7090001
line 256 mcts: sample exp_bonus -0.11443401502946281
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]] [[-1.976]
 [-7.036]
 [-7.036]
 [-7.036]
 [-7.036]
 [-7.036]
 [-7.036]] [[0.294]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]]
line 256 mcts: sample exp_bonus -6.438458188267184
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.3605619220564973, 0.2958424262733212, 0.21997029360016274, 0.12328059822768085, 0.0001723799211688923, 0.0001723799211688923]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
Printing some Q and Qe and total Qs values:  [[0.806]
 [0.884]
 [0.909]
 [0.89 ]
 [0.787]
 [0.872]
 [0.873]] [[ 0.499]
 [ 2.461]
 [ 1.561]
 [ 2.506]
 [-0.761]
 [ 0.999]
 [ 3.053]] [[0.712]
 [1.643]
 [1.293]
 [1.67 ]
 [0.152]
 [1.008]
 [1.88 ]]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.3605619220564973, 0.2958424262733212, 0.21997029360016274, 0.12328059822768085, 0.0001723799211688923, 0.0001723799211688923]
using another actor
from probs:  [0.3605619220564973, 0.2958424262733212, 0.21997029360016274, 0.12328059822768085, 0.0001723799211688923, 0.0001723799211688923]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.755]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.729]] [[0.66 ]
 [2.066]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [1.926]] [[0.678]
 [0.755]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.729]]
first move QE:  -0.7099064075699042
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.583]
 [0.713]
 [0.713]
 [0.002]
 [0.713]
 [0.713]] [[-0.844]
 [-1.717]
 [-2.135]
 [-2.135]
 [-0.7  ]
 [-2.135]
 [-2.135]] [[1.662]
 [1.404]
 [1.281]
 [1.281]
 [1.591]
 [1.281]
 [1.281]]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]] [[ -9.78]
 [-10.  ]
 [-10.  ]
 [-10.  ]
 [-10.  ]
 [-10.  ]
 [-10.  ]] [[0.084]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]]
Printing some Q and Qe and total Qs values:  [[0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]] [[-0.236]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]] [[1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.192]
 [0.708]
 [0.501]
 [0.238]
 [0.66 ]
 [0.18 ]] [[-1.33 ]
 [-1.649]
 [-3.734]
 [-1.912]
 [-1.604]
 [ 0.   ]
 [-1.708]] [[1.317]
 [1.01 ]
 [0.202]
 [1.133]
 [1.08 ]
 [2.458]
 [0.962]]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
siam score:  -0.7295254
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.3605619220564973, 0.2958424262733212, 0.21997029360016274, 0.12328059822768085, 0.0001723799211688923, 0.0001723799211688923]
line 256 mcts: sample exp_bonus 1.6572960663507081
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.418]
 [0.504]
 [0.466]
 [0.508]
 [0.497]] [[1.374]
 [1.374]
 [1.787]
 [1.374]
 [1.658]
 [1.723]
 [1.729]] [[0.138]
 [0.138]
 [0.239]
 [0.138]
 [0.25 ]
 [0.377]
 [0.359]]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.3605619220564973, 0.2958424262733212, 0.2199702936001628, 0.12328059822768085, 0.0001723799211688923, 0.0001723799211688923]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
first move QE:  -0.711852775991627
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.603]
 [0.64 ]
 [0.547]
 [0.622]
 [0.611]
 [0.609]] [[-6.458]
 [-1.795]
 [-5.078]
 [-2.248]
 [-4.257]
 [-4.364]
 [-3.993]] [[0.412]
 [1.569]
 [0.756]
 [1.409]
 [0.953]
 [0.917]
 [1.011]]
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
actor:  1 policy actor:  1  step number:  43 total reward:  0.7499999999999999  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -1.411799555809527
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.3669876587123903, 0.29286949490053427, 0.21775980406638343, 0.12204174698263189, 0.00017064766903001862, 0.00017064766903001862]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
from probs:  [0.36698765871239036, 0.2928694949005343, 0.2177598040663834, 0.12204174698263191, 0.00017064766903001868, 0.00017064766903001868]
actor:  1 policy actor:  1  step number:  88 total reward:  0.3949999999999996  reward:  1.0 rdn_beta:  0.333
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5410668432226745
Printing some Q and Qe and total Qs values:  [[1.061]
 [1.061]
 [1.009]
 [1.061]
 [1.061]
 [1.021]
 [1.035]] [[2.806]
 [2.806]
 [2.718]
 [2.806]
 [2.806]
 [2.7  ]
 [2.492]] [[2.264]
 [2.264]
 [2.206]
 [2.264]
 [2.264]
 [2.21 ]
 [2.17 ]]
siam score:  -0.716061
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]] [[-7.704]
 [-8.16 ]
 [-8.16 ]
 [-8.16 ]
 [-8.16 ]
 [-8.16 ]
 [-8.16 ]] [[0.093]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]]
line 256 mcts: sample exp_bonus -4.197599442741852
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
siam score:  -0.6977067
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.762]
 [0.629]
 [0.63 ]
 [0.63 ]
 [0.662]
 [0.63 ]] [[2.467]
 [1.428]
 [2.563]
 [2.512]
 [2.57 ]
 [2.384]
 [2.624]] [[0.823]
 [0.416]
 [0.907]
 [0.876]
 [0.913]
 [0.855]
 [0.95 ]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.68 ]
 [0.675]
 [0.655]
 [0.618]
 [0.684]
 [0.615]] [[-2.317]
 [-0.041]
 [-3.944]
 [-2.954]
 [-2.648]
 [-4.041]
 [-2.848]] [[0.9  ]
 [1.866]
 [0.331]
 [0.705]
 [0.795]
 [0.301]
 [0.715]]
actor:  1 policy actor:  1  step number:  115 total reward:  0.16999999999999937  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -5.514505914166872
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.298]
 [0.29 ]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[ 0.   ]
 [-2.967]
 [-3.5  ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.315]
 [0.298]
 [0.29 ]
 [0.315]
 [0.315]
 [0.315]
 [0.315]]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[-0.738]
 [-0.738]
 [-0.738]
 [-0.738]
 [-0.738]
 [-0.738]
 [-0.738]] [[1.933]
 [1.933]
 [1.933]
 [1.933]
 [1.933]
 [1.933]
 [1.933]]
Printing some Q and Qe and total Qs values:  [[1.05 ]
 [1.037]
 [0.993]
 [0.988]
 [1.05 ]
 [1.05 ]
 [1.02 ]] [[2.124]
 [2.174]
 [1.852]
 [1.556]
 [2.124]
 [2.124]
 [1.874]] [[2.403]
 [2.397]
 [2.243]
 [2.158]
 [2.403]
 [2.403]
 [2.292]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.3630935082839859, 0.30037294220727034, 0.2154491284505613, 0.12074674724622905, 0.00016883690597659331, 0.00016883690597659331]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.54 ]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[-4.748]
 [-4.748]
 [-5.161]
 [-4.748]
 [-4.748]
 [-4.748]
 [-4.748]] [[0.517]
 [0.517]
 [0.54 ]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.3630935082839859, 0.30037294220727034, 0.2154491284505613, 0.12074674724622905, 0.00016883690597659331, 0.00016883690597659331]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.519]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[-3.176]
 [-3.176]
 [-2.938]
 [-3.176]
 [-3.176]
 [-3.176]
 [-3.176]] [[0.517]
 [0.517]
 [0.519]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
siam score:  -0.6982351
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.878]
 [1.01 ]
 [0.878]
 [0.878]
 [0.878]
 [0.878]] [[0.674]
 [0.674]
 [1.754]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[1.102]
 [1.102]
 [1.555]
 [1.102]
 [1.102]
 [1.102]
 [1.102]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.3630935082839859, 0.30037294220727034, 0.2154491284505613, 0.12074674724622905, 0.00016883690597659331, 0.00016883690597659331]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.3630935082839859, 0.30037294220727034, 0.2154491284505613, 0.12074674724622905, 0.00016883690597659331, 0.00016883690597659331]
in main func line 156:  3460
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.324]
 [0.432]
 [0.422]
 [0.455]
 [0.427]
 [0.443]] [[-4.024]
 [-1.08 ]
 [-3.431]
 [-3.261]
 [ 0.   ]
 [-3.222]
 [-3.135]] [[0.433]
 [0.324]
 [0.432]
 [0.422]
 [0.455]
 [0.427]
 [0.443]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.3630935082839859, 0.30037294220727034, 0.2154491284505613, 0.12074674724622905, 0.00016883690597659331, 0.00016883690597659331]
siam score:  -0.71032274
Printing some Q and Qe and total Qs values:  [[0.67]
 [0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]] [[-2.61 ]
 [-3.027]
 [-3.027]
 [-3.027]
 [-3.027]
 [-3.027]
 [-3.027]] [[0.67]
 [0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.372]
 [0.435]
 [0.429]
 [0.442]
 [0.46 ]
 [0.45 ]] [[-3.731]
 [-2.144]
 [-3.227]
 [-3.241]
 [-3.27 ]
 [-3.254]
 [-2.953]] [[0.703]
 [0.372]
 [0.435]
 [0.429]
 [0.442]
 [0.46 ]
 [0.45 ]]
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
using explorer policy with actor:  1
siam score:  -0.70475304
actor:  1 policy actor:  1  step number:  52 total reward:  0.6449999999999998  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  57 total reward:  0.5799999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.36570058890688545, 0.30333689065762237, 0.2118825498429994, 0.11874788668569584, 0.00016604195339844335, 0.00016604195339844335]
using another actor
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
probs:  [0.3657005889068855, 0.3033368906576224, 0.21188254984299942, 0.11874788668569584, 0.00016604195339844338, 0.00016604195339844338]
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.914]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]] [[-1.904]
 [-0.337]
 [-1.904]
 [-1.904]
 [-1.904]
 [-1.904]
 [-1.904]] [[0.323]
 [0.863]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]]
siam score:  -0.7128946
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.96592 -1.0 -0.96592
using another actor
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.092]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.092]] [[-1.348]
 [-2.811]
 [-2.758]
 [-2.758]
 [-2.758]
 [-2.758]
 [-1.707]] [[0.057]
 [0.092]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.092]]
using another actor
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
maxi score, test score, baseline:  -0.9659200000000001 -1.0 -0.9659200000000001
probs:  [0.36570058890688545, 0.3033368906576223, 0.2118825498429994, 0.11874788668569583, 0.00016604195339844333, 0.00016604195339844333]
actor:  1 policy actor:  1  step number:  48 total reward:  0.6249999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]] [[3.098]
 [3.098]
 [3.098]
 [3.098]
 [3.098]
 [3.098]
 [3.098]] [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]]
actor:  0 policy actor:  1  step number:  75 total reward:  0.22999999999999943  reward:  1.0 rdn_beta:  0.333
using another actor
from probs:  [0.3710410742609068, 0.3007811457970026, 0.21010761141709133, 0.11773972851491817, 0.00016522000504060044, 0.00016522000504060044]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[-5.309]
 [-5.309]
 [-5.309]
 [-5.309]
 [-5.309]
 [-5.309]
 [-5.309]] [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.696]
 [0.551]
 [0.678]
 [0.665]
 [0.521]
 [0.666]] [[-0.146]
 [ 1.26 ]
 [ 0.042]
 [ 0.638]
 [ 0.008]
 [-0.058]
 [ 0.782]] [[0.558]
 [0.696]
 [0.551]
 [0.678]
 [0.665]
 [0.521]
 [0.666]]
maxi score, test score, baseline:  -0.9634600000000001 -1.0 -0.9634600000000001
probs:  [0.3710410742609068, 0.3007811457970026, 0.2101076114170913, 0.11773972851491817, 0.00016522000504060044, 0.00016522000504060044]
Printing some Q and Qe and total Qs values:  [[1.036]
 [1.038]
 [1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]] [[2.867]
 [3.581]
 [2.867]
 [2.867]
 [2.867]
 [2.867]
 [2.867]] [[1.908]
 [2.133]
 [1.908]
 [1.908]
 [1.908]
 [1.908]
 [1.908]]
maxi score, test score, baseline:  -0.9634600000000001 -1.0 -0.9634600000000001
maxi score, test score, baseline:  -0.9634600000000001 -1.0 -0.9634600000000001
actor:  0 policy actor:  1  step number:  66 total reward:  0.26499999999999946  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.37109632748454935, 0.30075286477869545, 0.21009845145496786, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.885]
 [0.96 ]
 [0.905]
 [0.665]
 [0.945]
 [0.951]] [[ 0.151]
 [ 0.964]
 [ 1.892]
 [-0.069]
 [-0.143]
 [ 2.101]
 [ 3.04 ]] [[0.49 ]
 [1.052]
 [1.59 ]
 [0.598]
 [0.226]
 [1.667]
 [2.113]]
siam score:  -0.72765803
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.3710963274845494, 0.30075286477869545, 0.21009845145496786, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
first move QE:  -0.722235603955917
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.817]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]] [[-4.736]
 [-4.206]
 [-4.096]
 [-4.096]
 [-4.096]
 [-4.096]
 [-4.096]] [[0.46 ]
 [0.901]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]]
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.693]
 [0.763]
 [0.763]
 [0.764]
 [0.763]
 [0.763]] [[-2.843]
 [-2.275]
 [-2.843]
 [-2.843]
 [-3.202]
 [-2.843]
 [-2.843]] [[0.648]
 [0.899]
 [0.648]
 [0.648]
 [0.463]
 [0.648]
 [0.648]]
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.3710963274845494, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.734]
 [0.892]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[-0.504]
 [-0.665]
 [-0.49 ]
 [-0.504]
 [-0.504]
 [-0.504]
 [-0.504]] [[1.047]
 [1.216]
 [1.542]
 [1.047]
 [1.047]
 [1.047]
 [1.047]]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.966]
 [1.006]
 [0.966]
 [0.454]
 [0.966]
 [0.966]] [[-0.294]
 [ 0.11 ]
 [ 0.843]
 [ 0.11 ]
 [-0.817]
 [ 0.11 ]
 [ 0.11 ]] [[0.176]
 [1.541]
 [1.864]
 [1.541]
 [0.207]
 [1.541]
 [1.541]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.3710963274845494, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.3710963274845494, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.3710963274845494, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.3710963274845494, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
Printing some Q and Qe and total Qs values:  [[0.834]
 [1.147]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]] [[-1.398]
 [ 0.272]
 [-1.398]
 [-1.398]
 [-1.398]
 [-1.398]
 [-1.398]] [[0.459]
 [1.642]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]]
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.3710963274845494, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.3710963274845494, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
from probs:  [0.3710963274845494, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.749]
 [0.802]
 [0.753]
 [0.753]
 [0.753]
 [0.753]] [[-0.019]
 [-0.088]
 [-0.009]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[0.753]
 [0.749]
 [0.802]
 [0.753]
 [0.753]
 [0.753]
 [0.753]]
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.701]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.693]] [[-1.735]
 [-2.277]
 [-1.735]
 [-1.735]
 [-1.735]
 [-1.735]
 [-1.953]] [[2.457]
 [2.414]
 [2.457]
 [2.457]
 [2.457]
 [2.457]
 [2.471]]
Printing some Q and Qe and total Qs values:  [[1.091]
 [1.158]
 [1.091]
 [1.091]
 [1.091]
 [1.091]
 [1.091]] [[4.224]
 [4.093]
 [4.224]
 [4.224]
 [4.224]
 [4.224]
 [4.224]] [[1.906]
 [1.932]
 [1.906]
 [1.906]
 [1.906]
 [1.906]
 [1.906]]
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.37109632748454935, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
first move QE:  -0.7272732123941668
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.37109632748454935, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.37109632748454935, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
Printing some Q and Qe and total Qs values:  [[0.691]
 [1.077]
 [0.665]
 [0.661]
 [0.631]
 [0.625]
 [0.643]] [[-3.73 ]
 [-0.477]
 [-3.593]
 [-3.436]
 [-3.412]
 [-3.322]
 [-3.729]] [[0.575]
 [1.916]
 [0.571]
 [0.603]
 [0.566]
 [0.578]
 [0.506]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.14562874603588327
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.7289469585919266
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]] [[-5.117]
 [-8.526]
 [-8.526]
 [-8.526]
 [-8.526]
 [-8.526]
 [-8.526]] [[0.199]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]]
line 256 mcts: sample exp_bonus 5.0495998054239175
siam score:  -0.742502
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.37109632748454935, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
3493 5529
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
Printing some Q and Qe and total Qs values:  [[0.941]
 [0.925]
 [0.974]
 [0.941]
 [0.843]
 [0.941]
 [0.941]] [[3.551]
 [3.373]
 [5.5  ]
 [3.551]
 [1.365]
 [3.551]
 [3.551]] [[2.038]
 [1.952]
 [2.709]
 [2.038]
 [1.17 ]
 [2.038]
 [2.038]]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
probs:  [0.37109632748454935, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
maxi score, test score, baseline:  -0.96093 -1.0 -0.96093
from probs:  [0.37109632748454935, 0.30075286477869545, 0.21009845145496783, 0.11772075426172884, 0.00016580101002923865, 0.00016580101002923865]
actor:  1 policy actor:  1  step number:  47 total reward:  0.7199999999999999  reward:  1.0 rdn_beta:  0.167
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.476]
 [0.342]
 [0.785]
 [0.785]
 [0.785]
 [0.785]] [[-3.445]
 [-1.612]
 [-0.545]
 [-3.445]
 [-3.445]
 [-3.445]
 [-3.445]] [[0.901]
 [1.475]
 [1.827]
 [0.901]
 [0.901]
 [0.901]
 [0.901]]
first move QE:  -0.7347349759954092
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]] [[ -7.628]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]
 [-10.   ]] [[0.084]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[-2.405]
 [-2.405]
 [-2.405]
 [-2.405]
 [-2.405]
 [-2.405]
 [-2.405]] [[0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.658]
 [0.507]] [[-5.017]
 [-5.017]
 [-5.017]
 [-5.017]
 [-5.017]
 [-3.039]
 [-5.017]] [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.658]
 [0.507]]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.646]
 [0.558]] [[-4.523]
 [-4.523]
 [-4.523]
 [-4.523]
 [-4.523]
 [-3.122]
 [-4.523]] [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.646]
 [0.558]]
line 256 mcts: sample exp_bonus -2.1920661450819883
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[-5.372]
 [-5.372]
 [-5.372]
 [-5.372]
 [-5.372]
 [-5.372]
 [-5.372]] [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.699]
 [0.503]] [[-5.293]
 [-5.293]
 [-5.293]
 [-5.293]
 [-5.293]
 [-2.906]
 [-5.293]] [[0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.699]
 [0.503]]
maxi score, test score, baseline:  -0.9609300000000001 -1.0 -0.9609300000000001
probs:  [0.37693414314381063, 0.29796111802906966, 0.20814820679351656, 0.1166280081184277, 0.0001642619575877119, 0.0001642619575877119]
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.82 ]
 [0.604]] [[-3.741]
 [-3.741]
 [-3.741]
 [-3.741]
 [-3.741]
 [-2.502]
 [-3.741]] [[0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.82 ]
 [0.604]]
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.82 ]
 [0.604]] [[-3.741]
 [-3.741]
 [-3.741]
 [-3.741]
 [-3.741]
 [-2.502]
 [-3.741]] [[0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.82 ]
 [0.604]]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.596]
 [0.713]
 [0.593]] [[-3.344]
 [-3.344]
 [-3.344]
 [-3.344]
 [-3.185]
 [-2.405]
 [-3.259]] [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.596]
 [0.713]
 [0.593]]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.734]
 [0.62 ]] [[-3.686]
 [-3.686]
 [-3.686]
 [-3.686]
 [-3.686]
 [-3.106]
 [-3.686]] [[0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.734]
 [0.62 ]]
first move QE:  -0.7348877962250923
line 256 mcts: sample exp_bonus 5.444646106078095
line 256 mcts: sample exp_bonus -3.598807626128158
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.815]
 [0.606]] [[-3.743]
 [-3.743]
 [-3.743]
 [-3.743]
 [-3.743]
 [-2.595]
 [-3.743]] [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.815]
 [0.606]]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.596]
 [0.496]] [[-4.81]
 [-4.81]
 [-4.81]
 [-4.81]
 [-4.81]
 [-4.45]
 [-4.81]] [[0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.596]
 [0.496]]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.832]
 [0.546]] [[-4.857]
 [-4.857]
 [-4.857]
 [-4.857]
 [-4.857]
 [-4.069]
 [-4.857]] [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.832]
 [0.546]]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.603]
 [0.605]] [[-3.669]
 [-3.669]
 [-3.669]
 [-3.669]
 [-3.669]
 [-3.591]
 [-3.669]] [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.603]
 [0.605]]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.657]
 [0.595]
 [0.595]
 [0.595]
 [0.663]
 [0.599]] [[-4.555]
 [-4.19 ]
 [-4.555]
 [-4.555]
 [-4.555]
 [-2.372]
 [-4.335]] [[0.595]
 [0.657]
 [0.595]
 [0.595]
 [0.595]
 [0.663]
 [0.599]]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.806]
 [0.607]] [[-3.754]
 [-3.754]
 [-3.754]
 [-3.754]
 [-3.754]
 [-2.855]
 [-3.754]] [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.806]
 [0.607]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  59 total reward:  0.5199999999999997  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -2.9359206291330224
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.728139849994877
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.488]
 [0.467]
 [0.471]
 [0.479]
 [0.829]
 [0.478]] [[-5.15 ]
 [-4.747]
 [-5.045]
 [-4.925]
 [-4.872]
 [-5.833]
 [-4.852]] [[0.471]
 [0.488]
 [0.467]
 [0.471]
 [0.479]
 [0.829]
 [0.478]]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[-2.561]
 [-2.723]
 [-2.723]
 [-2.723]
 [-2.723]
 [-2.723]
 [-2.723]] [[0.697]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
actor:  0 policy actor:  1  step number:  56 total reward:  0.4049999999999996  reward:  1.0 rdn_beta:  1
siam score:  -0.72486377
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  58 total reward:  0.35499999999999954  reward:  1.0 rdn_beta:  1
maxi score, test score, baseline:  -0.95541 -1.0 -0.95541
probs:  [0.37436273039313556, 0.3029201122622379, 0.20663706151030284, 0.1157514111601168, 0.0001643423371033862, 0.0001643423371033862]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.797]
 [0.536]] [[-4.138]
 [-4.138]
 [-4.138]
 [-4.138]
 [-4.138]
 [-5.371]
 [-4.138]] [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.797]
 [0.536]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.481]
 [0.428]
 [0.428]
 [0.432]
 [0.724]
 [0.442]] [[-4.694]
 [-4.575]
 [-4.805]
 [-4.691]
 [-4.774]
 [-5.201]
 [-4.785]] [[0.444]
 [0.481]
 [0.428]
 [0.428]
 [0.432]
 [0.724]
 [0.442]]
siam score:  -0.7271791
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.741]
 [0.536]] [[-4.101]
 [-4.101]
 [-4.101]
 [-4.101]
 [-4.101]
 [-4.883]
 [-4.101]] [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.741]
 [0.536]]
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.538]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]] [[ 0.   ]
 [-0.507]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[1.457]
 [0.744]
 [1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]]
actor:  0 policy actor:  1  step number:  81 total reward:  0.01999999999999924  reward:  1.0 rdn_beta:  1
rdn probs:  [0.37436273039313567, 0.30292011226223786, 0.20663706151030284, 0.1157514111601168, 0.0001643423371033862, 0.0001643423371033862]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.37798080828078107, 0.3010919958636222, 0.20600357188323115, 0.11451748354334212, 0.0002030702145117997, 0.0002030702145117997]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[1.202]
 [1.202]
 [1.202]
 [1.202]
 [1.202]
 [1.202]
 [1.202]] [[-0.737]
 [-0.737]
 [-0.737]
 [-0.737]
 [-0.737]
 [-0.737]
 [-0.737]] [[2.376]
 [2.376]
 [2.376]
 [2.376]
 [2.376]
 [2.376]
 [2.376]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.37958789778589025, 0.3023721714069936, 0.20262767739661797, 0.11500438616358656, 0.00020393362345583202, 0.00020393362345583202]
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.696]
 [0.671]
 [0.709]
 [0.709]
 [0.709]
 [0.72 ]] [[3.128]
 [2.979]
 [3.385]
 [3.128]
 [3.128]
 [3.128]
 [3.458]] [[1.076]
 [1.001]
 [1.086]
 [1.076]
 [1.076]
 [1.076]
 [1.208]]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.462]
 [0.443]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[1.286]
 [1.286]
 [2.019]
 [1.286]
 [1.286]
 [1.286]
 [1.286]] [[0.37 ]
 [0.37 ]
 [1.094]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.373]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[0.915]
 [2.809]
 [0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]] [[0.258]
 [1.427]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]]
siam score:  -0.73613006
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.37958789778589025, 0.3023721714069936, 0.20262767739661797, 0.11500438616358656, 0.00020393362345583202, 0.00020393362345583202]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5549999999999997  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.777]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[0.223]
 [1.796]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]] [[0.712]
 [1.266]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.38427395679721055, 0.30008831228571353, 0.20109720232977338, 0.11413574201850951, 0.00020239328439652625, 0.00020239328439652625]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
using another actor
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.646]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[3.372]
 [4.096]
 [3.372]
 [3.372]
 [3.372]
 [3.372]
 [3.372]] [[0.857]
 [1.351]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]]
siam score:  -0.72250074
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
using another actor
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.331]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.327]] [[-0.378]
 [-0.765]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.175]] [[0.334]
 [0.331]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.327]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
first move QE:  -0.7389917141431236
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.38427395679721055, 0.30008831228571353, 0.20109720232977338, 0.11413574201850951, 0.00020239328439652625, 0.00020239328439652625]
siam score:  -0.7278551
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.38427395679721055, 0.30008831228571353, 0.20109720232977338, 0.11413574201850951, 0.00020239328439652625, 0.00020239328439652625]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.38427395679721055, 0.30008831228571353, 0.20109720232977338, 0.11413574201850951, 0.00020239328439652625, 0.00020239328439652625]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.624]
 [0.638]
 [0.646]
 [0.629]
 [0.628]
 [0.634]] [[3.62 ]
 [3.604]
 [3.22 ]
 [3.532]
 [3.192]
 [3.029]
 [3.541]] [[0.657]
 [0.684]
 [0.457]
 [0.68 ]
 [0.42 ]
 [0.31 ]
 [0.662]]
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.924]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.914]] [[3.762]
 [3.937]
 [3.762]
 [3.762]
 [3.762]
 [3.762]
 [4.001]] [[1.24 ]
 [1.689]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.708]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
actor:  1 policy actor:  1  step number:  61 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]] [[ 0.28 ]
 [-1.046]
 [-1.046]
 [-1.046]
 [-1.046]
 [-1.046]
 [-1.046]] [[0.575]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
start point for exploration sampling:  10749
siam score:  -0.7059538
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3817774801835111, 0.3046353629747502, 0.19979075297556542, 0.11339424703635853, 0.0002010784149073811, 0.0002010784149073811]
first move QE:  -0.7411424047744252
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.383432502612275, 0.30162092621464354, 0.20065685481340892, 0.11388581616203813, 0.0002019500988172127, 0.0002019500988172127]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  113 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.833
using another actor
Printing some Q and Qe and total Qs values:  [[1.027]
 [1.033]
 [1.027]
 [1.027]
 [1.027]
 [1.027]
 [1.027]] [[-0.475]
 [-0.314]
 [-0.475]
 [-0.475]
 [-0.475]
 [-0.475]
 [-0.475]] [[2.337]
 [2.408]
 [2.337]
 [2.337]
 [2.337]
 [2.337]
 [2.337]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3819175779974507, 0.30042923546762634, 0.19986406858266662, 0.11343585841195743, 0.004152107337261549, 0.00020115220303743375]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3819175779974507, 0.30042923546762634, 0.19986406858266662, 0.11343585841195743, 0.004152107337261549, 0.00020115220303743375]
actor:  1 policy actor:  1  step number:  141 total reward:  0.12999999999999934  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.38056189130608914, 0.299362806636686, 0.20270429824475852, 0.11303319696762536, 0.00413736866854736, 0.0002004381762936903]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.022]
 [0.035]
 [0.035]
 [0.043]
 [0.033]
 [0.035]] [[-8.441]
 [-3.126]
 [-6.755]
 [-6.321]
 [ 0.   ]
 [-6.309]
 [-6.422]] [[0.254]
 [0.022]
 [0.035]
 [0.035]
 [0.043]
 [0.033]
 [0.035]]
using explorer policy with actor:  1
from probs:  [0.38056189130608914, 0.299362806636686, 0.20270429824475852, 0.11303319696762536, 0.00413736866854736, 0.0002004381762936903]
siam score:  -0.69347894
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.327]
 [0.157]
 [0.201]
 [0.327]
 [0.327]
 [0.327]] [[ 0.546]
 [-3.595]
 [-0.288]
 [-0.846]
 [-3.595]
 [-3.595]
 [-3.595]] [[0.638]
 [0.327]
 [0.157]
 [0.201]
 [0.327]
 [0.327]
 [0.327]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.621]
 [0.326]
 [0.532]
 [0.497]
 [0.426]
 [0.554]] [[ 0.587]
 [ 2.854]
 [ 0.836]
 [ 0.047]
 [-1.077]
 [ 0.145]
 [ 1.316]] [[ 0.61 ]
 [ 1.678]
 [ 0.578]
 [ 0.419]
 [-0.084]
 [ 0.372]
 [ 0.973]]
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.191]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[-3.771]
 [-3.604]
 [-3.771]
 [-3.771]
 [-3.771]
 [-3.771]
 [-3.771]] [[0.154]
 [0.191]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.202]
 [0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.189]] [[-2.587]
 [-2.395]
 [-2.587]
 [-2.587]
 [-2.587]
 [-2.587]
 [-2.587]] [[0.189]
 [0.202]
 [0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.189]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]] [[1.807]
 [1.807]
 [1.807]
 [1.807]
 [1.807]
 [1.807]
 [1.807]] [[1.176]
 [1.176]
 [1.176]
 [1.176]
 [1.176]
 [1.176]
 [1.176]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.38056189130608914, 0.299362806636686, 0.20270429824475852, 0.11303319696762536, 0.00413736866854736, 0.0002004381762936903]
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.725]
 [1.055]
 [0.725]
 [0.725]
 [0.725]
 [0.725]] [[-0.524]
 [-0.524]
 [ 0.244]
 [-0.524]
 [-0.524]
 [-0.524]
 [-0.524]] [[1.726]
 [1.726]
 [2.491]
 [1.726]
 [1.726]
 [1.726]
 [1.726]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.201]
 [0.605]
 [0.588]
 [0.594]
 [0.604]
 [0.599]] [[1.522]
 [0.699]
 [1.386]
 [1.517]
 [1.428]
 [1.5  ]
 [1.465]] [[1.485]
 [0.353]
 [1.376]
 [1.442]
 [1.39 ]
 [1.453]
 [1.422]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.38056189130608914, 0.299362806636686, 0.2027042982447585, 0.11303319696762536, 0.00413736866854736, 0.0002004381762936903]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
siam score:  -0.7124835
using explorer policy with actor:  0
start point for exploration sampling:  10749
first move QE:  -0.7461741220648864
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.5061],
        [ 0.4805],
        [ 0.6133],
        [-0.0034],
        [-0.4650],
        [-0.1314],
        [ 0.0018],
        [-0.0018],
        [ 0.2291]], dtype=torch.float64)
-0.96074352 -0.96074352
-0.024259925299500003 -0.5303815478411924
-0.0628797758985 0.41766262198906534
-0.0536639152995 0.5596058491924781
-0.0337698257985 -0.03718478437077308
-0.024259925299500003 -0.48929919817610484
-0.024259925299500003 -0.15563833851902137
-0.024259925299500003 -0.022502916851396604
-0.0337698257985 -0.03558703842439009
-0.024259925299500003 0.20485660112638698
using explorer policy with actor:  1
3526 5587
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.843]
 [0.786]] [[-4.224]
 [-4.224]
 [-4.224]
 [-4.224]
 [-4.224]
 [-4.168]
 [-4.224]] [[0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.843]
 [0.786]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.38056189130608914, 0.299362806636686, 0.20270429824475852, 0.11303319696762536, 0.00413736866854736, 0.0002004381762936903]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.38056189130608914, 0.299362806636686, 0.20270429824475852, 0.11303319696762536, 0.00413736866854736, 0.0002004381762936903]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.37565059168669934
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3816830338246701, 0.2972987162337217, 0.20330146893539536, 0.11336619490048883, 0.004149557434777823, 0.0002010286709462643]
Printing some Q and Qe and total Qs values:  [[0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]] [[-0.685]
 [-0.685]
 [-0.685]
 [-0.685]
 [-0.685]
 [-0.685]
 [-0.685]] [[1.903]
 [1.903]
 [1.903]
 [1.903]
 [1.903]
 [1.903]
 [1.903]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3816830338246701, 0.2972987162337217, 0.20330146893539536, 0.11336619490048881, 0.004149557434777823, 0.0002010286709462643]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3816830338246701, 0.2972987162337217, 0.20330146893539536, 0.11336619490048881, 0.004149557434777823, 0.0002010286709462643]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3816830338246701, 0.2972987162337217, 0.20330146893539536, 0.11336619490048881, 0.004149557434777823, 0.0002010286709462643]
from probs:  [0.3816830338246701, 0.2972987162337217, 0.20330146893539536, 0.11336619490048881, 0.004149557434777823, 0.0002010286709462643]
actor:  1 policy actor:  1  step number:  50 total reward:  0.6249999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[-2.371]
 [-4.518]
 [-4.518]
 [-4.518]
 [-4.518]
 [-4.518]
 [-4.518]] [[0.43 ]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.696]
 [0.683]
 [0.729]
 [0.729]
 [0.69 ]
 [0.729]] [[1.412]
 [1.462]
 [2.618]
 [1.412]
 [1.412]
 [2.429]
 [1.412]] [[1.104]
 [1.098]
 [1.52 ]
 [1.104]
 [1.104]
 [1.454]
 [1.104]]
siam score:  -0.7375779
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3785445931453435, 0.30307677315254233, 0.20162979494485903, 0.11243402593774789, 0.004115437134160817, 0.00019937568534644185]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.603]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]] [[-3.034]
 [ 0.949]
 [-2.669]
 [-2.669]
 [-2.669]
 [-2.669]
 [-2.669]] [[0.609]
 [0.603]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
3531 5609
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.623]
 [0.69 ]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[-4.672]
 [-4.672]
 [-5.165]
 [-4.672]
 [-4.672]
 [-4.672]
 [-4.672]] [[0.623]
 [0.623]
 [0.69 ]
 [0.623]
 [0.623]
 [0.623]
 [0.623]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3785445931453435, 0.30307677315254233, 0.20162979494485903, 0.11243402593774789, 0.004115437134160817, 0.00019937568534644185]
Printing some Q and Qe and total Qs values:  [[0.795]
 [1.003]
 [0.958]
 [0.929]
 [0.621]
 [0.72 ]
 [0.949]] [[3.015]
 [4.164]
 [3.957]
 [4.244]
 [2.108]
 [2.77 ]
 [4.217]] [[ 0.581]
 [ 1.379]
 [ 1.22 ]
 [ 1.258]
 [-0.071]
 [ 0.348]
 [ 1.29 ]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
3533 5612
Printing some Q and Qe and total Qs values:  [[1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.002]] [[3.336]
 [3.336]
 [3.336]
 [3.336]
 [3.336]
 [3.336]
 [3.336]] [[1.143]
 [1.143]
 [1.143]
 [1.143]
 [1.143]
 [1.143]
 [1.143]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7399999999999999  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3844491824701726, 0.3001971717207718, 0.19971406435233824, 0.11136576465628338, 0.004076335428871357, 0.00019748137156247242]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3844491824701726, 0.3001971717207718, 0.19971406435233824, 0.11136576465628338, 0.004076335428871357, 0.00019748137156247242]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.593]
 [0.6  ]
 [0.586]
 [0.586]
 [0.586]
 [0.58 ]] [[1.488]
 [2.678]
 [1.499]
 [1.488]
 [1.488]
 [1.488]
 [2.984]] [[1.063]
 [1.432]
 [1.091]
 [1.063]
 [1.063]
 [1.063]
 [1.5  ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  114 total reward:  0.2849999999999995  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.3826338314843574, 0.2987796547732243, 0.2034929769588821, 0.11083990074533454, 0.004057087164401737, 0.00019654887379971832]
first move QE:  -0.7531589022647817
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3826338314843574, 0.2987796547732243, 0.2034929769588821, 0.11083990074533452, 0.004057087164401737, 0.00019654887379971832]
3543 5620
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.7533676096808021
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.303646899889793
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[-2.817]
 [-2.817]
 [-2.817]
 [-2.817]
 [-2.817]
 [-2.817]
 [-2.817]] [[0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1792923632155086
siam score:  -0.7419194
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.359]
 [0.167]
 [0.342]
 [0.323]
 [0.328]
 [0.342]] [[-0.834]
 [-0.86 ]
 [-0.137]
 [-2.312]
 [-2.084]
 [-1.65 ]
 [-1.673]] [[ 0.931]
 [ 0.938]
 [ 1.429]
 [-0.421]
 [-0.228]
 [ 0.178]
 [ 0.17 ]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3826338314843574, 0.2987796547732244, 0.20349297695888213, 0.11083990074533456, 0.004057087164401738, 0.00019654887379971838]
siam score:  -0.74179476
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.315]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[-2.005]
 [-1.256]
 [-2.005]
 [-2.005]
 [-2.005]
 [-2.005]
 [-2.005]] [[1.653]
 [1.915]
 [1.653]
 [1.653]
 [1.653]
 [1.653]
 [1.653]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.5449999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.38706415007329986, 0.2966355640762524, 0.20203267873634143, 0.11004449584997196, 0.004027972856559725, 0.00019513840757458617]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6949999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
line 256 mcts: sample exp_bonus 1.9473216744692596
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
actor:  1 policy actor:  1  step number:  97 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3924759101401051, 0.2891965201342064, 0.20505276068156708, 0.10908838968960394, 0.0039929763796145955, 0.000193442974902871]
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]] [[-8.637]
 [-6.891]
 [-6.891]
 [-6.891]
 [-6.891]
 [-6.891]
 [-6.891]] [[0.223]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.335]
 [0.301]
 [0.301]
 [0.298]
 [0.3  ]
 [0.297]] [[-5.306]
 [-3.235]
 [-6.051]
 [-5.289]
 [-5.799]
 [-5.456]
 [-4.65 ]] [[0.301]
 [0.335]
 [0.301]
 [0.301]
 [0.298]
 [0.3  ]
 [0.297]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.582]
 [0.603]
 [0.555]
 [0.555]
 [0.555]
 [0.576]] [[2.392]
 [1.703]
 [1.99 ]
 [2.392]
 [2.392]
 [2.392]
 [1.177]] [[0.555]
 [0.582]
 [0.603]
 [0.555]
 [0.555]
 [0.555]
 [0.576]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.782]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]] [[1.798]
 [2.46 ]
 [1.798]
 [1.798]
 [1.798]
 [1.798]
 [1.798]] [[0.749]
 [0.782]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5947467830658966
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
using explorer policy with actor:  1
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3924759101401051, 0.2891965201342064, 0.20505276068156708, 0.10908838968960394, 0.0039929763796145955, 0.000193442974902871]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
actor:  1 policy actor:  1  step number:  44 total reward:  0.7049999999999998  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.39783018109871626, 0.28664775448864477, 0.20324557630842793, 0.108126964773974, 0.003957785219585499, 0.0001917381106514655]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.39783018109871626, 0.28664775448864477, 0.20324557630842793, 0.108126964773974, 0.003957785219585499, 0.0001917381106514655]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[1.36]
 [1.36]
 [1.36]
 [1.36]
 [1.36]
 [1.36]
 [1.36]] [[0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9502846881131061
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
first move QE:  -0.7586583548095437
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.39783018109871626, 0.28664775448864477, 0.20324557630842793, 0.108126964773974, 0.003957785219585499, 0.0001917381106514655]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.807]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]] [[3.594]
 [3.405]
 [3.594]
 [3.594]
 [3.594]
 [3.594]
 [3.594]] [[1.372]
 [1.336]
 [1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3996043427236403, 0.28792608748107856, 0.20415196933503574, 0.10414957193506838, 0.003975435340139168, 0.00019259318503776215]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.3996043427236403, 0.28792608748107856, 0.20415196933503574, 0.10414957193506838, 0.003975435340139168, 0.00019259318503776215]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.74441755
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.621]
 [0.714]
 [0.621]
 [0.621]
 [0.621]
 [0.617]] [[0.297]
 [0.297]
 [0.535]
 [0.297]
 [0.297]
 [0.297]
 [0.134]] [[0.311]
 [0.311]
 [0.736]
 [0.311]
 [0.311]
 [0.311]
 [0.14 ]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.6399999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.40443421311953526, 0.2856098720500089, 0.20250967305756437, 0.10331174286662331, 0.003943455033107575, 0.00019104387316051645]
actor:  1 policy actor:  1  step number:  130 total reward:  0.2849999999999995  reward:  1.0 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.7087054854186492
using explorer policy with actor:  1
using explorer policy with actor:  1
first move QE:  -0.7600745669873897
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.4025864623782358, 0.2843049976460057, 0.20615319107603503, 0.10283973940283545, 0.003925438451610915, 0.0001901710452770141]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4025864623782358, 0.2843049976460057, 0.20615319107603503, 0.10283973940283545, 0.003925438451610915, 0.0001901710452770141]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4025864623782358, 0.2843049976460057, 0.20615319107603503, 0.10283973940283545, 0.003925438451610915, 0.0001901710452770141]
using another actor
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4025864623782358, 0.2843049976460057, 0.20615319107603503, 0.10283973940283545, 0.003925438451610915, 0.0001901710452770141]
Printing some Q and Qe and total Qs values:  [[0.988]
 [0.988]
 [0.963]
 [0.988]
 [0.988]
 [0.942]
 [0.988]] [[1.29 ]
 [1.29 ]
 [2.605]
 [1.29 ]
 [1.29 ]
 [1.818]
 [1.29 ]] [[1.991]
 [1.991]
 [2.311]
 [1.991]
 [1.991]
 [2.06 ]
 [1.991]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.9700118791421857
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4025864623782358, 0.2843049976460057, 0.20615319107603503, 0.10283973940283545, 0.003925438451610915, 0.0001901710452770141]
actor:  1 policy actor:  1  step number:  50 total reward:  0.6349999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.829]
 [0.867]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[1.548]
 [1.548]
 [2.734]
 [1.548]
 [1.548]
 [1.548]
 [1.548]] [[1.325]
 [1.325]
 [1.812]
 [1.325]
 [1.325]
 [1.325]
 [1.325]]
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.871]
 [0.762]
 [0.771]
 [0.719]
 [0.714]
 [0.873]] [[3.15 ]
 [3.782]
 [3.696]
 [3.543]
 [3.062]
 [3.347]
 [3.931]] [[0.917]
 [1.653]
 [1.374]
 [1.249]
 [0.713]
 [0.965]
 [1.793]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.40729998842052956, 0.28206186298974495, 0.20452666543902467, 0.10202834535276478, 0.0038944671809518234, 0.00018867061698412372]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.40729998842052956, 0.28206186298974495, 0.20452666543902467, 0.10202834535276478, 0.0038944671809518234, 0.00018867061698412372]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5649999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4044106810165149, 0.2871547773866551, 0.20307579280074398, 0.10130457598859102, 0.003866840583405746, 0.00018733222408927932]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.842]
 [0.42 ]
 [0.459]
 [0.154]
 [0.299]
 [0.484]] [[-0.908]
 [ 1.036]
 [-0.555]
 [-1.212]
 [-0.917]
 [-0.21 ]
 [ 0.884]] [[ 0.172]
 [ 1.425]
 [ 0.295]
 [ 0.18 ]
 [-0.24 ]
 [ 0.191]
 [ 0.795]]
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.407]
 [0.394]
 [0.407]
 [0.407]
 [0.393]
 [0.338]] [[1.546]
 [1.546]
 [1.411]
 [1.546]
 [1.546]
 [1.51 ]
 [1.745]] [[0.528]
 [0.528]
 [0.367]
 [0.528]
 [0.528]
 [0.464]
 [0.589]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.048]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.054]
 [-0.053]] [[-0.679]
 [-0.768]
 [-0.638]
 [-0.759]
 [-0.75 ]
 [-0.729]
 [-0.537]] [[-0.026]
 [-0.108]
 [ 0.012]
 [-0.11 ]
 [-0.101]
 [-0.081]
 [ 0.112]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.40441068101651484, 0.28715477738665507, 0.20307579280074395, 0.101304575988591, 0.003866840583405745, 0.0001873322240892793]
line 256 mcts: sample exp_bonus 3.3689379338368566
first move QE:  -0.7640551565633581
Printing some Q and Qe and total Qs values:  [[1.084]
 [1.005]
 [1.084]
 [1.084]
 [1.084]
 [1.084]
 [0.993]] [[3.373]
 [3.744]
 [3.373]
 [3.373]
 [3.373]
 [3.373]
 [3.301]] [[1.783]
 [1.75 ]
 [1.783]
 [1.783]
 [1.783]
 [1.783]
 [1.577]]
siam score:  -0.7607185
from probs:  [0.40191185790903733, 0.288359548812679, 0.2039278068772991, 0.10172960412006515, 0.003883064095639373, 0.00018811818528000114]
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.88 ]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]] [[2.915]
 [3.757]
 [2.915]
 [2.915]
 [2.915]
 [2.915]
 [2.915]] [[1.734]
 [2.011]
 [1.734]
 [1.734]
 [1.734]
 [1.734]
 [1.734]]
siam score:  -0.75424594
actor:  1 policy actor:  1  step number:  59 total reward:  0.4299999999999996  reward:  1.0 rdn_beta:  0.167
siam score:  -0.75682837
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[ 0.135]
 [-1.967]
 [-1.967]
 [-1.967]
 [-1.967]
 [-1.967]
 [-1.967]] [[0.75 ]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.406633701898595, 0.28445246845203814, 0.20339135992479215, 0.10146199698523835, 0.0038728494126474243, 0.00018762332668886592]
start point for exploration sampling:  10749
3582 5673
from probs:  [0.406633701898595, 0.2844524684520382, 0.20339135992479215, 0.10146199698523835, 0.0038728494126474243, 0.00018762332668886592]
using another actor
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.406633701898595, 0.2844524684520382, 0.20339135992479215, 0.10146199698523836, 0.0038728494126474243, 0.00018762332668886592]
siam score:  -0.75652206
siam score:  -0.75616777
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[-7.421]
 [-7.829]
 [-7.829]
 [-7.829]
 [-7.829]
 [-7.829]
 [-7.829]] [[0.478]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.406633701898595, 0.2844524684520382, 0.20339135992479215, 0.10146199698523836, 0.0038728494126474243, 0.00018762332668886592]
line 256 mcts: sample exp_bonus 1.7256968332097022
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[-9.444]
 [-8.32 ]
 [-8.32 ]
 [-8.32 ]
 [-8.32 ]
 [-8.32 ]
 [-8.32 ]] [[0.017]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
line 256 mcts: sample exp_bonus -1.9036415489787206
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.406633701898595, 0.2844524684520382, 0.20339135992479215, 0.10146199698523836, 0.0038728494126474243, 0.00018762332668886592]
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.107]
 [0.102]
 [0.103]
 [0.103]
 [0.108]
 [0.105]] [[-4.514]
 [-4.124]
 [-4.423]
 [-4.345]
 [-4.345]
 [-4.488]
 [-4.324]] [[0.102]
 [0.107]
 [0.102]
 [0.103]
 [0.103]
 [0.108]
 [0.105]]
siam score:  -0.7507643
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.406633701898595, 0.2844524684520382, 0.20339135992479215, 0.10146199698523836, 0.0038728494126474243, 0.00018762332668886592]
actor:  1 policy actor:  1  step number:  52 total reward:  0.6449999999999998  reward:  1.0 rdn_beta:  0.333
3588 5676
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.87 ]
 [0.916]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.908]] [[3.843]
 [3.396]
 [3.843]
 [3.843]
 [3.843]
 [3.843]
 [3.645]] [[1.023]
 [0.965]
 [1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.032]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.0406581889418458
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.40342446476194677, 0.2900997173164555, 0.2017861533162489, 0.10066123795527306, 0.0038422840854212665, 0.0001861425646543862]
line 256 mcts: sample exp_bonus 1.154486759110968
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.40342446476194677, 0.2900997173164555, 0.2017861533162489, 0.10066123795527306, 0.0038422840854212665, 0.0001861425646543862]
using explorer policy with actor:  0
siam score:  -0.7461929
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.40342446476194677, 0.2900997173164555, 0.2017861533162489, 0.10066123795527306, 0.0038422840854212665, 0.0001861425646543862]
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.829]
 [0.812]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]] [[ 0.656]
 [ 0.034]
 [-0.057]
 [ 0.656]
 [ 0.656]
 [ 0.656]
 [ 0.656]] [[0.82 ]
 [0.829]
 [0.812]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]]
from probs:  [0.40342446476194677, 0.2900997173164555, 0.2017861533162489, 0.10066123795527306, 0.0038422840854212665, 0.0001861425646543862]
siam score:  -0.74821174
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.40342446476194677, 0.2900997173164555, 0.2017861533162489, 0.10066123795527306, 0.0038422840854212665, 0.0001861425646543862]
siam score:  -0.7491686
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.231]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.357]] [[-1.501]
 [-1.479]
 [-2.274]
 [-2.274]
 [-2.274]
 [-2.274]
 [-1.566]] [[1.668]
 [1.653]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.673]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.001]
 [0.997]
 [0.975]
 [0.998]
 [0.986]
 [0.978]
 [0.972]] [[4.08 ]
 [3.312]
 [4.099]
 [3.741]
 [3.997]
 [4.183]
 [4.159]] [[1.427]
 [0.931]
 [1.389]
 [1.206]
 [1.346]
 [1.449]
 [1.422]]
Printing some Q and Qe and total Qs values:  [[0.957]
 [1.02 ]
 [0.956]
 [0.954]
 [0.951]
 [0.956]
 [0.957]] [[2.858]
 [2.446]
 [2.828]
 [2.887]
 [3.038]
 [2.842]
 [2.993]] [[1.334]
 [1.185]
 [1.311]
 [1.348]
 [1.442]
 [1.322]
 [1.423]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.40342446476194677, 0.29009971731645556, 0.2017861533162489, 0.10066123795527306, 0.0038422840854212665, 0.0001861425646543862]
actor:  1 policy actor:  1  step number:  56 total reward:  0.49499999999999966  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
in main func line 156:  3595
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4008789518923112, 0.29457902004489867, 0.20051293045782886, 0.10002608937339467, 0.0038180401824293967, 0.00018496804913710265]
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.528]
 [0.498]
 [0.528]
 [0.528]
 [0.528]
 [0.528]] [[ 0.   ]
 [ 0.   ]
 [-3.604]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.528]
 [0.528]
 [0.498]
 [0.528]
 [0.528]
 [0.528]
 [0.528]]
line 256 mcts: sample exp_bonus -2.4521169921323303
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
siam score:  -0.7549662
using another actor
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4008789518923112, 0.29457902004489867, 0.20051293045782886, 0.10002608937339467, 0.0038180401824293967, 0.00018496804913710265]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4008789518923112, 0.29457902004489867, 0.20051293045782886, 0.10002608937339467, 0.0038180401824293967, 0.00018496804913710265]
using explorer policy with actor:  0
first move QE:  -0.7702499079403677
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4008789518923112, 0.29457902004489867, 0.20051293045782886, 0.10002608937339467, 0.0038180401824293967, 0.00018496804913710265]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4008789518923112, 0.29457902004489867, 0.20051293045782886, 0.10002608937339467, 0.0038180401824293967, 0.00018496804913710265]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4008789518923112, 0.29457902004489867, 0.20051293045782886, 0.10002608937339467, 0.0038180401824293967, 0.00018496804913710265]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4008789518923112, 0.29457902004489867, 0.20051293045782886, 0.10002608937339467, 0.0038180401824293967, 0.00018496804913710265]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4008789518923112, 0.29457902004489867, 0.20051293045782886, 0.10002608937339466, 0.0038180401824293967, 0.00018496804913710265]
3599 5692
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.6255320055245477
first move QE:  -0.7708608147033053
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
siam score:  -0.7522066
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4008789518923112, 0.29457902004489867, 0.20051293045782886, 0.10002608937339466, 0.0038180401824293967, 0.00018496804913710265]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
line 256 mcts: sample exp_bonus 1.2760854697699466
actor:  1 policy actor:  1  step number:  76 total reward:  0.4449999999999996  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.956]
 [0.947]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.951]] [[1.331]
 [1.478]
 [1.278]
 [1.278]
 [1.278]
 [1.278]
 [1.18 ]] [[0.728]
 [0.758]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.666]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.6149999999999998  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.301]
 [0.066]
 [0.067]
 [0.066]
 [0.064]
 [0.069]] [[-8.565]
 [ 0.   ]
 [-6.236]
 [-6.574]
 [-6.454]
 [-6.18 ]
 [-6.611]] [[0.067]
 [0.301]
 [0.066]
 [0.067]
 [0.066]
 [0.064]
 [0.069]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7499999999999999  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.435]
 [0.447]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[-3.246]
 [-3.246]
 [-2.922]
 [-3.246]
 [-3.246]
 [-3.246]
 [-3.246]] [[0.435]
 [0.435]
 [0.447]
 [0.435]
 [0.435]
 [0.435]
 [0.435]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.408281446549056, 0.28813269990508805, 0.19612507370650376, 0.10354537008163152, 0.003734489394193189, 0.00018092036352733909]
line 256 mcts: sample exp_bonus -2.64362998000317
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.408281446549056, 0.28813269990508805, 0.19612507370650376, 0.10354537008163152, 0.003734489394193189, 0.00018092036352733909]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
in main func line 156:  3606
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  105 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.927]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]] [[-0.613]
 [ 0.445]
 [-0.613]
 [-0.613]
 [-0.613]
 [-0.613]
 [-0.613]] [[1.482]
 [1.762]
 [1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
siam score:  -0.72758573
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
actor:  1 policy actor:  1  step number:  53 total reward:  0.6099999999999998  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7236427
Printing some Q and Qe and total Qs values:  [[-0.028]
 [ 0.123]
 [ 0.567]
 [ 0.305]
 [ 0.305]
 [ 0.374]
 [ 0.043]] [[-0.574]
 [-0.755]
 [ 1.454]
 [-1.476]
 [-1.476]
 [-2.251]
 [-0.776]] [[0.438]
 [0.552]
 [2.017]
 [0.48 ]
 [0.48 ]
 [0.244]
 [0.444]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.41094285904704975, 0.28886446506815694, 0.19393298467764314, 0.10238804396592574, 0.0036927490237515298, 0.00017889821747283082]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.72242635
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
actor:  1 policy actor:  1  step number:  73 total reward:  0.22999999999999943  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.4132180310144185, 0.2877487561027604, 0.19318393868598788, 0.1019925807905738, 0.003678486164552886, 0.00017820724170645956]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[-6.006]
 [-4.419]
 [-4.419]
 [-4.419]
 [-4.419]
 [-4.419]
 [-4.419]] [[0.24 ]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]]
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.41451235954953336, 0.2886500754983669, 0.1906567376144758, 0.10231205355744016, 0.0036900083374772326, 0.00017876544270639788]
using explorer policy with actor:  0
siam score:  -0.728003
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.785]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]] [[-1.562]
 [-0.912]
 [-1.562]
 [-1.562]
 [-1.562]
 [-1.562]
 [-1.562]] [[1.247]
 [1.719]
 [1.247]
 [1.247]
 [1.247]
 [1.247]
 [1.247]]
first move QE:  -0.7744773767074765
line 256 mcts: sample exp_bonus -1.3799390788213295
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
probs:  [0.41451235954953336, 0.2886500754983669, 0.1906567376144758, 0.10231205355744016, 0.0036900083374772326, 0.00017876544270639788]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.95337 -0.8110000000000002 -0.8110000000000002
siam score:  -0.72857714
actor:  0 policy actor:  1  step number:  84 total reward:  0.16499999999999937  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.076]
 [1.076]
 [1.04 ]
 [1.076]
 [1.076]
 [1.076]
 [1.076]] [[4.133]
 [4.133]
 [2.83 ]
 [4.133]
 [4.133]
 [4.133]
 [4.133]] [[2.123]
 [2.123]
 [1.937]
 [2.123]
 [2.123]
 [2.123]
 [2.123]]
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
probs:  [0.41451235954953336, 0.2886500754983669, 0.1906567376144758, 0.10231205355744016, 0.0036900083374772326, 0.00017876544270639788]
from probs:  [0.41451235954953336, 0.2886500754983669, 0.1906567376144758, 0.10231205355744016, 0.0036900083374772326, 0.00017876544270639788]
3617 5713
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.09 ]
 [0.13 ]
 [0.13 ]
 [0.104]
 [0.104]
 [0.105]] [[-10.   ]
 [ -3.114]
 [  0.   ]
 [  0.   ]
 [-10.   ]
 [ -9.788]
 [ -9.685]] [[0.102]
 [0.09 ]
 [0.13 ]
 [0.13 ]
 [0.104]
 [0.104]
 [0.105]]
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.71 ]
 [0.857]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]] [[0.241]
 [0.241]
 [1.484]
 [0.241]
 [0.241]
 [0.241]
 [0.241]] [[1.387]
 [1.387]
 [1.882]
 [1.387]
 [1.387]
 [1.387]
 [1.387]]
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
probs:  [0.41451235954953336, 0.2886500754983669, 0.1906567376144758, 0.10231205355744016, 0.0036900083374772326, 0.00017876544270639788]
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
probs:  [0.41451235954953336, 0.2886500754983669, 0.1906567376144758, 0.10231205355744016, 0.0036900083374772326, 0.00017876544270639788]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.874]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]] [[-0.597]
 [ 0.173]
 [-0.597]
 [-0.597]
 [-0.597]
 [-0.597]
 [-0.597]] [[1.634]
 [2.196]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]]
3619 5721
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.656]
 [0.389]
 [0.408]
 [0.413]
 [0.422]
 [0.402]] [[-2.195]
 [-0.398]
 [-1.655]
 [-1.679]
 [-1.692]
 [-1.708]
 [-1.616]] [[-0.077]
 [ 1.327]
 [ 0.152]
 [ 0.171]
 [ 0.173]
 [ 0.179]
 [ 0.198]]
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
line 256 mcts: sample exp_bonus -3.3562098032720535
3620 5721
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
from probs:  [0.41451235954953336, 0.2886500754983669, 0.1906567376144758, 0.10231205355744016, 0.0036900083374772326, 0.00017876544270639788]
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
probs:  [0.41451235954953336, 0.2886500754983669, 0.1906567376144758, 0.10231205355744016, 0.0036900083374772326, 0.00017876544270639788]
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
probs:  [0.41451235954953336, 0.2886500754983669, 0.1906567376144758, 0.10231205355744016, 0.0036900083374772326, 0.00017876544270639788]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
actor:  1 policy actor:  1  step number:  42 total reward:  0.6849999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
probs:  [0.4191600079127702, 0.2863587477943252, 0.18914328897219151, 0.10149989218048987, 0.0036607167520958375, 0.00017734638812728077]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.266]
 [0.773]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[-0.469]
 [-0.469]
 [ 0.675]
 [-0.469]
 [-0.469]
 [-0.469]
 [-0.469]] [[0.707]
 [0.707]
 [1.679]
 [0.707]
 [0.707]
 [0.707]
 [0.707]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
probs:  [0.41916000791277025, 0.2863587477943252, 0.18914328897219151, 0.10149989218048987, 0.0036607167520958375, 0.00017734638812728077]
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.976]
 [0.848]
 [0.847]
 [0.859]
 [0.866]
 [0.78 ]] [[-0.485]
 [ 1.987]
 [ 0.069]
 [-0.107]
 [-0.006]
 [ 0.303]
 [-0.258]] [[1.125]
 [2.152]
 [1.34 ]
 [1.284]
 [1.337]
 [1.444]
 [1.118]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9510400000000001 -0.8110000000000002 -0.8110000000000002
probs:  [0.41916000791277025, 0.2863587477943252, 0.18914328897219151, 0.10149989218048987, 0.0036607167520958375, 0.00017734638812728077]
Printing some Q and Qe and total Qs values:  [[0.896]
 [0.902]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.954]] [[3.685]
 [1.881]
 [3.685]
 [3.685]
 [3.685]
 [3.685]
 [1.029]] [[0.896]
 [0.902]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.954]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.36999999999999955  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.612]
 [0.588]
 [0.611]
 [0.67 ]
 [0.554]
 [0.55 ]] [[2.708]
 [1.852]
 [1.663]
 [1.304]
 [2.273]
 [1.195]
 [0.91 ]] [[0.685]
 [0.612]
 [0.588]
 [0.611]
 [0.67 ]
 [0.554]
 [0.55 ]]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.522]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.83 ]] [[ 0.317]
 [-0.246]
 [ 0.317]
 [ 0.317]
 [ 0.317]
 [ 0.317]
 [ 0.837]] [[1.209]
 [1.075]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.68 ]]
start point for exploration sampling:  10749
actor:  0 policy actor:  1  step number:  73 total reward:  0.1899999999999994  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[-8.602]
 [-8.602]
 [-8.602]
 [-8.602]
 [-8.602]
 [-8.602]
 [-8.602]] [[0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]]
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.125]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]] [[ 0.   ]
 [-0.677]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[ 1.008]
 [-0.246]
 [ 1.008]
 [ 1.008]
 [ 1.008]
 [ 1.008]
 [ 1.008]]
using another actor
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[-2.258]
 [-2.258]
 [-2.258]
 [-2.258]
 [-2.258]
 [-2.258]
 [-2.258]] [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.3556137261099557
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.819]
 [0.904]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.926]] [[2.432]
 [2.336]
 [2.432]
 [2.432]
 [2.432]
 [2.432]
 [2.38 ]] [[0.819]
 [0.904]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.926]]
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
probs:  [0.42201934555259074, 0.2849490715371552, 0.18821218138172724, 0.10100023226363998, 0.0036426959110026215, 0.00017647335388417312]
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.285]
 [0.372]
 [0.285]
 [0.285]
 [0.285]
 [0.724]] [[-1.042]
 [-1.042]
 [-1.269]
 [-1.042]
 [-1.042]
 [-1.042]
 [ 0.612]] [[0.593]
 [0.593]
 [0.68 ]
 [0.593]
 [0.593]
 [0.593]
 [1.849]]
from probs:  [0.42201934555259074, 0.2849490715371552, 0.18821218138172724, 0.10100023226363998, 0.0036426959110026215, 0.00017647335388417312]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
probs:  [0.42201934555259074, 0.2849490715371552, 0.18821218138172724, 0.10100023226363998, 0.0036426959110026215, 0.00017647335388417312]
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
probs:  [0.42201934555259074, 0.28494907153715526, 0.18821218138172724, 0.10100023226363998, 0.0036426959110026215, 0.00017647335388417312]
siam score:  -0.7285031
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[ 0.27 ]
 [-0.022]
 [ 0.29 ]
 [ 0.268]
 [ 0.272]
 [ 0.272]
 [ 0.275]] [[-2.467]
 [-0.619]
 [-2.237]
 [-2.302]
 [-2.491]
 [-2.491]
 [-2.418]] [[-0.076]
 [ 1.143]
 [ 0.131]
 [ 0.056]
 [-0.094]
 [-0.093]
 [-0.032]]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.601]
 [0.419]] [[-4.178]
 [-4.178]
 [-4.178]
 [-4.178]
 [-4.178]
 [-0.703]
 [-4.178]] [[0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.601]
 [0.419]]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.383]
 [0.394]
 [0.393]
 [0.39 ]
 [0.522]
 [0.399]] [[-4.647]
 [-3.929]
 [-4.434]
 [-4.365]
 [-4.474]
 [-5.334]
 [-4.368]] [[0.386]
 [0.383]
 [0.394]
 [0.393]
 [0.39 ]
 [0.522]
 [0.399]]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.663]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[-1.901]
 [ 0.006]
 [-1.901]
 [-1.901]
 [-1.901]
 [-1.901]
 [-1.901]] [[0.422]
 [1.277]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]]
using explorer policy with actor:  0
siam score:  -0.72927505
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
probs:  [0.4247352693509, 0.2803473302803118, 0.18942342903644557, 0.10165022363804109, 0.0036661386384959347, 0.00017760905580550067]
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
from probs:  [0.4247352693509, 0.2803473302803118, 0.18942342903644557, 0.10165022363804109, 0.0036661386384959347, 0.00017760905580550067]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.364]
 [0.316]
 [0.316]
 [0.316]
 [0.314]
 [0.316]] [[-3.164]
 [-2.799]
 [-3.164]
 [-3.164]
 [-3.164]
 [-3.374]
 [-3.164]] [[0.316]
 [0.364]
 [0.316]
 [0.316]
 [0.316]
 [0.314]
 [0.316]]
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
siam score:  -0.725531
Printing some Q and Qe and total Qs values:  [[0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [1.014]] [[0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [1.322]] [[1.655]
 [1.655]
 [1.655]
 [1.655]
 [1.655]
 [1.655]
 [2.161]]
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
siam score:  -0.7211078
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
probs:  [0.4247352693509, 0.2803473302803118, 0.1894234290364456, 0.10165022363804109, 0.0036661386384959347, 0.00017760905580550067]
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.964]
 [0.964]
 [0.964]
 [1.032]
 [0.964]
 [0.964]
 [0.883]] [[3.918]
 [3.918]
 [3.918]
 [2.986]
 [3.918]
 [3.918]
 [3.71 ]] [[1.166]
 [1.166]
 [1.166]
 [0.992]
 [1.166]
 [1.166]
 [0.935]]
Printing some Q and Qe and total Qs values:  [[1.035]
 [1.128]
 [0.829]
 [1.012]
 [1.001]
 [0.878]
 [1.057]] [[2.892]
 [1.516]
 [2.321]
 [1.68 ]
 [2.326]
 [1.702]
 [1.903]] [[1.4  ]
 [1.128]
 [0.797]
 [0.949]
 [1.142]
 [0.69 ]
 [1.115]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.7049999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
first move QE:  -0.785166753638138
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.586]
 [0.547]
 [0.55 ]
 [0.557]
 [0.55 ]
 [0.559]] [[-3.479]
 [-2.372]
 [-3.8  ]
 [-4.043]
 [-3.65 ]
 [-3.817]
 [-3.689]] [[0.559]
 [0.586]
 [0.547]
 [0.55 ]
 [0.557]
 [0.55 ]
 [0.559]]
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
probs:  [0.42939359819802325, 0.2780771579817507, 0.18788953242017672, 0.10082708927248504, 0.0036364512989679426, 0.00017617082859620254]
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
probs:  [0.42939359819802325, 0.2780771579817507, 0.18788953242017672, 0.10082708927248504, 0.0036364512989679426, 0.00017617082859620254]
siam score:  -0.72611475
siam score:  -0.72707045
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]] [[-3.421]
 [-7.671]
 [-7.671]
 [-7.671]
 [-7.671]
 [-7.671]
 [-7.671]] [[0.383]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]]
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
probs:  [0.42939359819802325, 0.2780771579817507, 0.18788953242017672, 0.10082708927248504, 0.0036364512989679426, 0.00017617082859620254]
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
probs:  [0.42939359819802325, 0.2780771579817507, 0.18788953242017672, 0.10082708927248504, 0.0036364512989679426, 0.00017617082859620254]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.225]
 [0.369]
 [0.36 ]
 [0.354]
 [0.356]
 [0.36 ]] [[-2.356]
 [-0.911]
 [-2.377]
 [-2.354]
 [-2.444]
 [-2.501]
 [-2.536]] [[0.368]
 [0.225]
 [0.369]
 [0.36 ]
 [0.354]
 [0.356]
 [0.36 ]]
maxi score, test score, baseline:  -0.94866 -0.8110000000000002 -0.8110000000000002
actor:  0 policy actor:  0  step number:  94 total reward:  0.034999999999999254  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.7864091692392127
maxi score, test score, baseline:  -0.94659 -0.8110000000000002 -0.8110000000000002
probs:  [0.42939359819802325, 0.2780771579817507, 0.18788953242017672, 0.10082708927248504, 0.0036364512989679426, 0.00017617082859620254]
using explorer policy with actor:  1
siam score:  -0.7260201
maxi score, test score, baseline:  -0.94659 -0.8110000000000002 -0.8110000000000002
3640 5782
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.616]
 [0.79 ]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[0.053]
 [0.053]
 [0.027]
 [0.053]
 [0.053]
 [0.053]
 [0.053]] [[1.259]
 [1.259]
 [1.59 ]
 [1.259]
 [1.259]
 [1.259]
 [1.259]]
maxi score, test score, baseline:  -0.94659 -0.8110000000000002 -0.8110000000000002
probs:  [0.42939359819802325, 0.2780771579817507, 0.18788953242017672, 0.10082708927248504, 0.0036364512989679426, 0.00017617082859620254]
first move QE:  -0.7879663166189406
maxi score, test score, baseline:  -0.94659 -0.8110000000000002 -0.8110000000000002
siam score:  -0.7193293
maxi score, test score, baseline:  -0.94659 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.22 ]
 [0.054]] [[-4.258]
 [-4.258]
 [-4.258]
 [-4.258]
 [-4.258]
 [-3.976]
 [-4.258]] [[-0.24 ]
 [-0.24 ]
 [-0.24 ]
 [-0.24 ]
 [-0.24 ]
 [ 0.188]
 [-0.24 ]]
actor:  0 policy actor:  1  step number:  57 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9437900000000001 -0.8110000000000002 -0.8110000000000002
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.475]
 [0.5  ]
 [0.475]
 [0.831]
 [0.475]] [[-5.044]
 [-5.044]
 [-5.044]
 [-4.587]
 [-5.044]
 [-0.818]
 [-5.044]] [[0.475]
 [0.475]
 [0.475]
 [0.5  ]
 [0.475]
 [0.831]
 [0.475]]
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.522]
 [0.503]
 [0.504]
 [0.511]
 [0.547]
 [0.508]] [[-5.362]
 [-4.591]
 [-5.497]
 [-5.298]
 [-5.496]
 [-5.458]
 [-5.422]] [[0.505]
 [0.522]
 [0.503]
 [0.504]
 [0.511]
 [0.547]
 [0.508]]
siam score:  -0.72369057
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[-3.779]
 [-3.779]
 [-3.779]
 [-3.779]
 [-3.779]
 [-3.779]
 [-3.779]] [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
Printing some Q and Qe and total Qs values:  [[0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.6 ]
 [0.52]] [[-3.884]
 [-3.884]
 [-3.884]
 [-3.884]
 [-3.884]
 [-3.691]
 [-3.884]] [[0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.6 ]
 [0.52]]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.76 ]
 [0.507]] [[-3.425]
 [-3.425]
 [-3.425]
 [-3.425]
 [-3.425]
 [ 0.937]
 [-3.425]] [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.76 ]
 [0.507]]
siam score:  -0.7218359
maxi score, test score, baseline:  -0.9437900000000001 -0.8110000000000002 -0.8110000000000002
probs:  [0.42939359819802325, 0.2780771579817507, 0.18788953242017672, 0.10082708927248504, 0.0036364512989679426, 0.00017617082859620254]
using explorer policy with actor:  0
from probs:  [0.42939359819802325, 0.2780771579817507, 0.18788953242017672, 0.10082708927248504, 0.0036364512989679426, 0.00017617082859620254]
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.845]
 [0.79 ]
 [0.763]
 [0.788]
 [0.712]
 [0.763]] [[-1.821]
 [ 0.068]
 [-1.809]
 [-1.682]
 [ 0.098]
 [-2.595]
 [-1.386]] [[0.799]
 [0.845]
 [0.79 ]
 [0.763]
 [0.788]
 [0.712]
 [0.763]]
line 256 mcts: sample exp_bonus -1.5781611222312772
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.52 ]
 [0.515]] [[-3.96 ]
 [-3.96 ]
 [-3.96 ]
 [-3.96 ]
 [-3.96 ]
 [-3.994]
 [-3.96 ]] [[0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.52 ]
 [0.515]]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.53 ]
 [0.516]
 [0.516]
 [0.508]
 [0.516]
 [0.516]] [[-4.948]
 [-4.572]
 [-4.948]
 [-4.948]
 [-5.07 ]
 [-4.948]
 [-4.948]] [[0.516]
 [0.53 ]
 [0.516]
 [0.516]
 [0.508]
 [0.516]
 [0.516]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.589]
 [0.508]] [[-4.927]
 [-4.927]
 [-4.927]
 [-4.927]
 [-4.927]
 [-5.655]
 [-4.927]] [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.589]
 [0.508]]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.573]
 [0.761]
 [0.713]
 [0.694]
 [0.749]
 [0.72 ]] [[-1.891]
 [-0.598]
 [-1.843]
 [-1.724]
 [-1.455]
 [-1.273]
 [-1.874]] [[0.749]
 [0.573]
 [0.761]
 [0.713]
 [0.694]
 [0.749]
 [0.72 ]]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.292]
 [0.555]
 [0.221]
 [0.283]
 [0.348]
 [0.283]] [[-1.013]
 [-0.616]
 [-3.183]
 [-0.164]
 [-1.271]
 [ 0.   ]
 [-0.386]] [[1.301]
 [1.364]
 [0.019]
 [1.569]
 [0.92 ]
 [1.843]
 [1.504]]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.669]
 [0.629]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[-1.676]
 [-2.583]
 [-1.217]
 [-1.676]
 [-1.676]
 [-1.676]
 [-1.676]] [[0.676]
 [0.669]
 [0.629]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.708]
 [0.499]] [[-4.454]
 [-4.454]
 [-4.454]
 [-4.454]
 [-4.454]
 [-3.184]
 [-4.454]] [[0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.708]
 [0.499]]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.59 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.414]] [[-0.441]
 [-0.418]
 [-0.441]
 [-0.441]
 [-0.441]
 [-0.441]
 [-0.302]] [[0.861]
 [1.259]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [1.126]]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.514]
 [0.82 ]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[-0.081]
 [-0.081]
 [ 0.409]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]] [[0.661]
 [0.661]
 [1.182]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  49 total reward:  0.6099999999999998  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[-4.137]
 [-4.137]
 [-4.137]
 [-4.137]
 [-4.137]
 [-4.137]
 [-4.137]] [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.508]
 [0.53 ]
 [0.508]
 [0.741]
 [0.508]] [[-4.852]
 [-4.852]
 [-4.852]
 [-4.708]
 [-4.852]
 [-5.061]
 [-4.852]] [[0.508]
 [0.508]
 [0.508]
 [0.53 ]
 [0.508]
 [0.741]
 [0.508]]
actor:  0 policy actor:  1  step number:  56 total reward:  0.4249999999999996  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  58 total reward:  0.4349999999999996  reward:  1.0 rdn_beta:  1
siam score:  -0.7266799
actor:  0 policy actor:  1  step number:  59 total reward:  0.47999999999999965  reward:  1.0 rdn_beta:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
actor:  0 policy actor:  1  step number:  61 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  61 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  65 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  66 total reward:  0.49499999999999966  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]] [[2.478]
 [2.478]
 [2.478]
 [2.478]
 [2.478]
 [2.478]
 [2.478]] [[1.809]
 [1.809]
 [1.809]
 [1.809]
 [1.809]
 [1.809]
 [1.809]]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.717]
 [0.555]] [[-4.75]
 [-4.75]
 [-4.75]
 [-4.75]
 [-4.75]
 [-3.97]
 [-4.75]] [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.717]
 [0.555]]
maxi score, test score, baseline:  -0.9234 -0.8110000000000002 -0.8110000000000002
probs:  [0.42939359819802325, 0.2780771579817507, 0.18788953242017672, 0.10082708927248504, 0.0036364512989679426, 0.00017617082859620254]
actor:  0 policy actor:  1  step number:  78 total reward:  0.15499999999999936  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  84 total reward:  0.05499999999999927  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.88 ]
 [0.763]
 [0.788]
 [0.781]
 [0.784]
 [0.787]] [[-0.55 ]
 [ 0.553]
 [-0.435]
 [-0.715]
 [-0.542]
 [-0.556]
 [-0.7  ]] [[0.814]
 [0.88 ]
 [0.763]
 [0.788]
 [0.781]
 [0.784]
 [0.787]]
actor:  0 policy actor:  1  step number:  85 total reward:  0.009999999999999232  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  87 total reward:  0.06999999999999929  reward:  1.0 rdn_beta:  1
maxi score, test score, baseline:  -0.9148200000000001 -0.8110000000000002 -0.8110000000000002
probs:  [0.42939359819802325, 0.2780771579817507, 0.18788953242017672, 0.10082708927248504, 0.0036364512989679426, 0.00017617082859620254]
actor:  1 policy actor:  1  step number:  59 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.773]
 [0.828]
 [0.829]
 [0.773]
 [0.773]
 [0.751]
 [0.786]] [[-0.534]
 [ 1.084]
 [ 1.629]
 [-0.534]
 [-0.534]
 [ 0.252]
 [ 0.972]] [[0.773]
 [0.828]
 [0.829]
 [0.773]
 [0.773]
 [0.751]
 [0.786]]
maxi score, test score, baseline:  -0.9148200000000001 -0.8110000000000002 -0.8110000000000002
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.582]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[-1.647]
 [ 0.251]
 [-1.647]
 [-1.647]
 [-1.647]
 [-1.647]
 [-1.647]] [[0.538]
 [0.582]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]]
rdn probs:  [0.4269633774796205, 0.2821629940424734, 0.18682614201013165, 0.10025644247580588, 0.0036158702299317457, 0.00017517376203668848]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4582851432876913, 0.2710100412202918, 0.17728957028648387, 0.09045210051703047, 0.002377720131753452, 0.0005854245567489984]
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
using another actor
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4582851432876913, 0.2710100412202918, 0.17728957028648387, 0.09045210051703047, 0.002377720131753452, 0.0005854245567489984]
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4582851432876913, 0.2710100412202918, 0.17728957028648387, 0.09045210051703047, 0.002377720131753452, 0.0005854245567489984]
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.083]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[-0.238]
 [ 3.086]
 [-0.238]
 [-0.238]
 [-0.238]
 [-0.238]
 [-0.238]] [[-0.262]
 [ 1.09 ]
 [-0.262]
 [-0.262]
 [-0.262]
 [-0.262]
 [-0.262]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
3655 5806
using another actor
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
actor:  1 policy actor:  1  step number:  150 total reward:  0.014999999999999236  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4580106361314652, 0.27144669722678266, 0.17718337601752118, 0.09039792082290056, 0.0023762959066804183, 0.0005850738946500588]
siam score:  -0.74346685
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4580106361314652, 0.27144669722678266, 0.17718337601752118, 0.09039792082290056, 0.0023762959066804183, 0.0005850738946500588]
3657 5810
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]] [[-0.795]
 [-0.781]
 [-0.795]
 [-0.795]
 [-0.795]
 [-0.795]
 [-0.795]] [[0.514]
 [0.541]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]]
maxi score, test score, baseline:  -0.91482 -0.2112500000000003 -0.2112500000000003
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.91482 -0.2112500000000003 -0.2112500000000003
probs:  [0.4580106361314652, 0.27144669722678266, 0.17718337601752118, 0.09039792082290056, 0.0023762959066804183, 0.0005850738946500588]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.47610370060414886
maxi score, test score, baseline:  -0.91482 -0.2112500000000003 -0.2112500000000003
probs:  [0.4580106361314652, 0.27144669722678266, 0.17718337601752118, 0.09039792082290056, 0.0023762959066804183, 0.0005850738946500588]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.723]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[-3.202]
 [-2.791]
 [-3.202]
 [-3.202]
 [-3.202]
 [-3.202]
 [-3.202]] [[0.653]
 [0.723]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
siam score:  -0.7423143
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.91482 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9148200000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4580106361314652, 0.27144669722678266, 0.17718337601752118, 0.09039792082290056, 0.0023762959066804183, 0.0005850738946500588]
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.094]
 [-0.082]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.061]] [[-1.221]
 [-1.221]
 [-2.523]
 [-1.221]
 [-1.221]
 [-1.221]
 [-0.821]] [[1.216]
 [1.216]
 [0.608]
 [1.216]
 [1.216]
 [1.216]
 [1.438]]
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.018]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]] [[-2.467]
 [ 1.575]
 [-2.467]
 [-2.467]
 [-2.467]
 [-2.467]
 [-2.467]] [[0.418]
 [1.35 ]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
siam score:  -0.74517626
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0833333320419714
actor:  1 policy actor:  1  step number:  57 total reward:  0.5099999999999997  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
actor:  0 policy actor:  0  step number:  67 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.455239535227105, 0.26980436377679085, 0.1821616619152005, 0.0898509864498029, 0.002361918607952929, 0.00058153402314778]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.455239535227105, 0.26980436377679085, 0.1821616619152005, 0.0898509864498029, 0.002361918607952929, 0.00058153402314778]
from probs:  [0.455239535227105, 0.26980436377679085, 0.1821616619152005, 0.0898509864498029, 0.002361918607952929, 0.00058153402314778]
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.875]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]] [[-1.912]
 [-0.517]
 [-1.912]
 [-1.912]
 [-1.912]
 [-1.912]
 [-1.912]] [[0.772]
 [0.875]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.527]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[1.195]
 [1.336]
 [1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.195]] [[0.54 ]
 [0.527]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]]
using explorer policy with actor:  0
using another actor
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.455239535227105, 0.26980436377679085, 0.1821616619152005, 0.0898509864498029, 0.002361918607952929, 0.00058153402314778]
actor:  1 policy actor:  1  step number:  69 total reward:  0.26999999999999946  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.178]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.332]] [[-2.384]
 [-1.759]
 [-2.384]
 [-2.384]
 [-2.384]
 [-2.384]
 [-1.161]] [[0.146]
 [0.178]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.332]]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]] [[3.729]
 [3.729]
 [3.729]
 [3.729]
 [3.729]
 [3.729]
 [3.729]] [[-0.412]
 [-0.412]
 [-0.412]
 [-0.412]
 [-0.412]
 [-0.412]
 [-0.412]]
line 256 mcts: sample exp_bonus 4.28967494677639
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.7351381
actor:  1 policy actor:  1  step number:  42 total reward:  0.6449999999999998  reward:  1.0 rdn_beta:  0.167
siam score:  -0.73309904
first move QE:  -0.8000649994881619
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.45862060506619506, 0.2694743084762013, 0.18014032877566583, 0.08885396668932545, 0.0023357098859587183, 0.0005750811066537334]
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[-0.788]
 [-0.801]
 [-0.788]
 [-0.801]
 [-0.801]
 [-0.781]
 [-0.773]] [[0.188]
 [0.168]
 [0.188]
 [0.168]
 [0.168]
 [0.199]
 [0.211]]
line 256 mcts: sample exp_bonus -0.8012297763713317
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.45862060506619506, 0.2694743084762013, 0.18014032877566583, 0.08885396668932545, 0.0023357098859587183, 0.0005750811066537334]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.45862060506619506, 0.2694743084762013, 0.1801403287756658, 0.08885396668932545, 0.0023357098859587183, 0.0005750811066537334]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
first move QE:  -0.8023741734941402
line 256 mcts: sample exp_bonus 4.561224389637473
siam score:  -0.7416621
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.036]
 [-0.035]
 [-0.036]
 [-0.036]
 [-0.037]
 [-0.035]] [[ 1.077]
 [ 0.493]
 [ 0.782]
 [ 1.077]
 [ 1.077]
 [-0.439]
 [ 0.296]] [[1.072]
 [0.807]
 [0.939]
 [1.072]
 [1.072]
 [0.383]
 [0.718]]
Printing some Q and Qe and total Qs values:  [[-0.069]
 [-0.016]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]] [[-0.309]
 [-0.805]
 [-0.309]
 [-0.309]
 [-0.309]
 [-0.309]
 [-0.309]] [[ 0.131]
 [-0.093]
 [ 0.131]
 [ 0.131]
 [ 0.131]
 [ 0.131]
 [ 0.131]]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.45862060506619506, 0.2694743084762013, 0.1801403287756658, 0.08885396668932545, 0.0023357098859587183, 0.0005750811066537334]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.45862060506619506, 0.2694743084762013, 0.1801403287756658, 0.08885396668932545, 0.0023357098859587183, 0.0005750811066537334]
Printing some Q and Qe and total Qs values:  [[-0.116]
 [-0.116]
 [-0.229]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]] [[-0.728]
 [-0.728]
 [ 0.2  ]
 [-0.728]
 [-0.728]
 [-0.728]
 [-0.728]] [[-0.003]
 [-0.003]
 [ 0.975]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]]
using another actor
3674 5878
maxi score, test score, baseline:  -0.9119800000000001 -0.2112500000000003 -0.2112500000000003
3674 5882
maxi score, test score, baseline:  -0.9119800000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
line 256 mcts: sample exp_bonus -0.5302713618031678
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
3677 5898
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.342]
 [0.55 ]
 [0.406]
 [0.406]
 [0.516]
 [0.535]] [[-1.62 ]
 [-1.214]
 [-3.433]
 [ 0.   ]
 [ 0.   ]
 [-4.091]
 [-3.045]] [[0.341]
 [0.342]
 [0.55 ]
 [0.406]
 [0.406]
 [0.516]
 [0.535]]
start point for exploration sampling:  10749
line 256 mcts: sample exp_bonus 0.22044124232278628
maxi score, test score, baseline:  -0.9119800000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9119800000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9119800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4615046303805416, 0.2740391383549208, 0.18319186255566672, 0.07830426961795077, 0.002375276248836634, 0.0005848228420836691]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9119800000000001 -0.2112500000000003 -0.2112500000000003
3679 5907
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]] [[-7.451]
 [-7.934]
 [-7.934]
 [-7.934]
 [-7.934]
 [-7.934]
 [-7.934]] [[0.264]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.36 ]
 [0.5  ]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[-4.432]
 [-3.233]
 [-2.524]
 [-4.432]
 [-4.432]
 [-4.432]
 [-4.432]] [[0.313]
 [0.36 ]
 [0.5  ]
 [0.313]
 [0.313]
 [0.313]
 [0.313]]
Printing some Q and Qe and total Qs values:  [[-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]] [[0.072]
 [1.711]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]] [[0.414]
 [0.875]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]]
siam score:  -0.71026903
using explorer policy with actor:  1
from probs:  [0.46150463038054157, 0.2740391383549208, 0.1831918625556667, 0.07830426961795077, 0.002375276248836634, 0.0005848228420836691]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.4635637313220441, 0.27526182218223116, 0.18400921196417191, 0.07419192832975269, 0.0023858740483782663, 0.0005874321534220054]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.4635637313220441, 0.27526182218223116, 0.18400921196417191, 0.07419192832975269, 0.0023858740483782663, 0.0005874321534220054]
using explorer policy with actor:  1
3684 5912
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.4635637313220441, 0.27526182218223116, 0.18400921196417191, 0.07419192832975269, 0.0023858740483782663, 0.0005874321534220054]
siam score:  -0.71196765
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.722]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[-0.988]
 [ 0.264]
 [-0.988]
 [-0.988]
 [-0.988]
 [-0.988]
 [-0.988]] [[0.642]
 [0.722]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]]
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.055]
 [-0.036]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]] [[0.822]
 [0.822]
 [1.38 ]
 [0.822]
 [0.822]
 [0.822]
 [0.822]] [[1.05 ]
 [1.05 ]
 [1.261]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.05 ]]
from probs:  [0.4635637313220441, 0.27526182218223116, 0.18400921196417191, 0.07419192832975269, 0.0023858740483782663, 0.0005874321534220054]
maxi score, test score, baseline:  -0.9119800000000001 -0.2112500000000003 -0.2112500000000003
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9119800000000001 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
using explorer policy with actor:  1
siam score:  -0.72106576
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.4635637313220441, 0.27526182218223116, 0.18400921196417191, 0.07419192832975269, 0.0023858740483782663, 0.0005874321534220054]
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.647]
 [0.682]
 [0.647]
 [0.647]
 [0.676]
 [0.683]] [[-2.171]
 [-1.929]
 [-2.062]
 [-1.929]
 [-1.929]
 [-2.03 ]
 [-2.042]] [[0.69 ]
 [0.647]
 [0.682]
 [0.647]
 [0.647]
 [0.676]
 [0.683]]
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.809]
 [0.77 ]
 [0.762]
 [0.779]
 [0.763]
 [0.757]] [[-1.298]
 [ 0.969]
 [-2.516]
 [-2.406]
 [-0.409]
 [-2.331]
 [-2.289]] [[0.768]
 [0.809]
 [0.77 ]
 [0.762]
 [0.779]
 [0.763]
 [0.757]]
maxi score, test score, baseline:  -0.9119800000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9119800000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9119800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4635637313220441, 0.27526182218223116, 0.18400921196417191, 0.07419192832975269, 0.0023858740483782663, 0.0005874321534220054]
using another actor
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
siam score:  -0.7206056
from probs:  [0.46686884061903106, 0.2700945954827747, 0.18532115790838885, 0.07472090075687662, 0.0024028848151098348, 0.0005916204178189785]
start point for exploration sampling:  10749
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]] [[-9.254]
 [-9.254]
 [-9.254]
 [-9.254]
 [-9.254]
 [-9.254]
 [-9.254]] [[0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]]
rdn beta is 0 so we're just using the maxi policy
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -7.038004421559503
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[-7.951]
 [-8.846]
 [-8.846]
 [-8.846]
 [-8.846]
 [-8.846]
 [-8.846]] [[0.384]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]] [[-8.463]
 [-9.467]
 [-9.467]
 [-9.467]
 [-9.467]
 [-9.467]
 [-9.467]] [[0.319]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.46577360645328464, 0.2706494623738783, 0.1857018710972646, 0.07487440310233943, 0.002407821164260609, 0.0005928358089724442]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.494]
 [0.493]] [[-4.113]
 [-4.113]
 [-4.113]
 [-4.113]
 [-4.113]
 [-3.965]
 [-4.113]] [[0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.494]
 [0.493]]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.391]
 [0.275]] [[-3.983]
 [-3.983]
 [-3.983]
 [-3.983]
 [-3.983]
 [-3.761]
 [-3.983]] [[0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.391]
 [0.275]]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.46577360645328464, 0.2706494623738783, 0.1857018710972646, 0.07487440310233943, 0.002407821164260609, 0.0005928358089724442]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.46577360645328464, 0.2706494623738783, 0.1857018710972646, 0.07487440310233943, 0.002407821164260609, 0.0005928358089724442]
first move QE:  -0.8153223588443601
using explorer policy with actor:  1
first move QE:  -0.8159124270770338
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]] [[-3.943]
 [-3.943]
 [-3.943]
 [-3.943]
 [-3.943]
 [-3.96 ]
 [-3.943]] [[0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.46577360645328464, 0.2706494623738783, 0.1857018710972646, 0.07487440310233942, 0.002407821164260609, 0.0005928358089724442]
first move QE:  -0.815953123762158
actor:  1 policy actor:  1  step number:  54 total reward:  0.5249999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[-0.153]
 [-0.122]
 [-0.072]
 [-0.09 ]
 [-0.118]
 [-0.085]
 [-0.118]] [[-0.66 ]
 [-1.093]
 [-3.738]
 [-2.544]
 [-1.265]
 [-3.632]
 [-1.06 ]] [[ 1.165]
 [ 0.95 ]
 [-0.482]
 [ 0.168]
 [ 0.857]
 [-0.434]
 [ 0.971]]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.4691852060773681, 0.26892107977195734, 0.18451596856365712, 0.07439625097701752, 0.0023924446836559804, 0.0005890499263439087]
first move QE:  -0.815820336466385
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.4691852060773681, 0.26892107977195734, 0.18451596856365712, 0.07439625097701752, 0.0023924446836559804, 0.0005890499263439087]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.4691852060773681, 0.26892107977195734, 0.18451596856365712, 0.07439625097701752, 0.0023924446836559804, 0.0005890499263439087]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.235]
 [0.402]
 [0.388]
 [0.383]
 [0.634]
 [0.453]] [[-2.302]
 [-0.763]
 [-2.281]
 [-2.187]
 [-2.271]
 [-1.35 ]
 [-2.106]] [[0.386]
 [0.235]
 [0.402]
 [0.388]
 [0.383]
 [0.634]
 [0.453]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.536]
 [0.559]] [[-1.942]
 [-1.942]
 [-1.942]
 [-1.942]
 [-1.942]
 [-2.259]
 [-1.942]] [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.536]
 [0.559]]
line 256 mcts: sample exp_bonus -1.2425940365006234
siam score:  -0.7295725
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.554]
 [0.605]
 [0.605]
 [0.605]
 [0.783]
 [0.605]] [[-1.   ]
 [-0.714]
 [-1.   ]
 [-1.   ]
 [-1.   ]
 [-1.227]
 [-1.   ]] [[0.605]
 [0.554]
 [0.605]
 [0.605]
 [0.605]
 [0.783]
 [0.605]]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
from probs:  [0.4691852060773681, 0.26892107977195734, 0.18451596856365712, 0.07439625097701752, 0.0023924446836559804, 0.0005890499263439087]
from probs:  [0.4691852060773681, 0.2689210797719574, 0.18451596856365712, 0.07439625097701752, 0.0023924446836559804, 0.0005890499263439087]
siam score:  -0.72682905
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
using another actor
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.695]
 [0.645]
 [0.648]
 [0.646]
 [0.636]
 [0.641]] [[-4.516]
 [-2.889]
 [-5.728]
 [-4.638]
 [-4.743]
 [-4.596]
 [-4.391]] [[0.635]
 [0.695]
 [0.645]
 [0.648]
 [0.646]
 [0.636]
 [0.641]]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.469185206077368, 0.26892107977195734, 0.18451596856365715, 0.0743962509770175, 0.00239244468365598, 0.0005890499263439086]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.96 ]] [[ 0.022]
 [ 0.022]
 [ 0.022]
 [ 0.022]
 [ 0.022]
 [ 0.022]
 [-0.793]] [[0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.96 ]]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.124]
 [-0.214]
 [-0.099]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.118]] [[-1.054]
 [-0.334]
 [-1.394]
 [-1.054]
 [-1.054]
 [-1.054]
 [-1.021]] [[0.603]
 [0.784]
 [0.498]
 [0.603]
 [0.603]
 [0.603]
 [0.624]]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
from probs:  [0.469185206077368, 0.26892107977195734, 0.18451596856365715, 0.0743962509770175, 0.00239244468365598, 0.0005890499263439086]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.469185206077368, 0.26892107977195734, 0.18451596856365715, 0.0743962509770175, 0.00239244468365598, 0.0005890499263439086]
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.469185206077368, 0.26892107977195734, 0.18451596856365715, 0.0743962509770175, 0.00239244468365598, 0.0005890499263439086]
line 256 mcts: sample exp_bonus -2.6560872474790496
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.388]
 [0.341]
 [0.409]
 [0.393]
 [0.306]
 [0.092]] [[4.323]
 [3.417]
 [3.788]
 [3.557]
 [3.929]
 [4.793]
 [3.948]] [[0.944]
 [0.641]
 [0.721]
 [0.712]
 [0.825]
 [1.036]
 [0.517]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  85 total reward:  0.2899999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
first move QE:  -0.8198637762145926
siam score:  -0.74513507
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.6149999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.91198 -0.2112500000000003 -0.2112500000000003
probs:  [0.4753721373849843, 0.2637525916422599, 0.18379842927482717, 0.07410694142059693, 0.0023831410278789826, 0.0005867592494527737]
from probs:  [0.4753721373849843, 0.2637525916422599, 0.18379842927482717, 0.07410694142059693, 0.0023831410278789826, 0.0005867592494527737]
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[-7.046]
 [-7.061]
 [-7.061]
 [-7.061]
 [-7.061]
 [-7.061]
 [-7.061]] [[0.26 ]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[-4.158]
 [-4.158]
 [-4.89 ]
 [-4.158]
 [-4.158]
 [-4.158]
 [-4.158]] [[0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]]
actor:  0 policy actor:  1  step number:  61 total reward:  0.36999999999999955  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.90924 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.982]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]] [[4.713]
 [4.465]
 [4.713]
 [4.713]
 [4.713]
 [4.713]
 [4.713]] [[0.895]
 [0.982]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]]
siam score:  -0.7469052
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.90924 -0.2112500000000003 -0.2112500000000003
probs:  [0.4753721373849843, 0.2637525916422599, 0.18379842927482717, 0.07410694142059694, 0.0023831410278789826, 0.0005867592494527737]
actor:  0 policy actor:  1  step number:  56 total reward:  0.5449999999999997  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  116 total reward:  0.05499999999999927  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9040400000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4753721373849843, 0.2637525916422599, 0.18379842927482717, 0.07410694142059694, 0.0023831410278789826, 0.0005867592494527737]
maxi score, test score, baseline:  -0.9040400000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9040400000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4753721373849843, 0.2637525916422599, 0.18379842927482717, 0.07410694142059694, 0.0023831410278789826, 0.0005867592494527737]
maxi score, test score, baseline:  -0.9040400000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9040400000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9040400000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4753721373849843, 0.2637525916422599, 0.18379842927482717, 0.07410694142059694, 0.0023831410278789826, 0.0005867592494527737]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.569753288436924
maxi score, test score, baseline:  -0.9040400000000001 -0.2112500000000003 -0.2112500000000003
siam score:  -0.73857576
actor:  0 policy actor:  1  step number:  85 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.098]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[2.91 ]
 [4.194]
 [2.91 ]
 [2.91 ]
 [2.91 ]
 [2.91 ]
 [2.91 ]] [[-0.74 ]
 [-0.567]
 [-0.74 ]
 [-0.74 ]
 [-0.74 ]
 [-0.74 ]
 [-0.74 ]]
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[-0.122]
 [-0.1  ]
 [-0.074]
 [-0.075]
 [-0.103]
 [-0.081]
 [-0.086]] [[-0.794]
 [-1.261]
 [-4.071]
 [-3.369]
 [-1.235]
 [-3.727]
 [-1.966]] [[ 1.159]
 [ 0.945]
 [-0.422]
 [-0.077]
 [ 0.955]
 [-0.258]
 [ 0.607]]
3709 6001
actor:  1 policy actor:  1  step number:  63 total reward:  0.5599999999999997  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7348411
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]] [[ 0.4  ]
 [-2.054]
 [-2.054]
 [-2.054]
 [-2.054]
 [-2.054]
 [-2.054]] [[0.254]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]]
siam score:  -0.73408216
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
using explorer policy with actor:  0
from probs:  [0.4790088637806172, 0.26192425563440686, 0.18252433644283791, 0.07359323124778881, 0.002366621066782362, 0.000582691827566848]
3717 6014
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
probs:  [0.47900886378061713, 0.26192425563440686, 0.18252433644283791, 0.07359323124778881, 0.002366621066782362, 0.000582691827566848]
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
probs:  [0.47900886378061713, 0.26192425563440686, 0.18252433644283791, 0.07359323124778883, 0.002366621066782362, 0.000582691827566848]
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
probs:  [0.47900886378061713, 0.26192425563440686, 0.18252433644283791, 0.07359323124778883, 0.002366621066782362, 0.000582691827566848]
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
3720 6023
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
probs:  [0.4790088637806172, 0.26192425563440686, 0.18252433644283791, 0.07359323124778881, 0.002366621066782362, 0.000582691827566848]
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]] [[-9.034]
 [-8.586]
 [-8.586]
 [-8.586]
 [-8.586]
 [-8.586]
 [-8.586]] [[0.059]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]]
from probs:  [0.4790088637806172, 0.26192425563440686, 0.18252433644283791, 0.07359323124778881, 0.002366621066782362, 0.000582691827566848]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.063]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]] [[-3.58 ]
 [-1.931]
 [-3.58 ]
 [-3.58 ]
 [-3.58 ]
 [-3.58 ]
 [-3.58 ]] [[-0.035]
 [ 0.747]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]]
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.11 ]
 [-0.078]
 [-0.077]
 [-0.078]
 [-0.08 ]
 [-0.078]] [[-5.131]
 [-1.593]
 [-4.913]
 [-3.991]
 [-5.097]
 [-4.409]
 [-4.398]] [[-0.067]
 [ 1.377]
 [ 0.02 ]
 [ 0.403]
 [-0.056]
 [ 0.228]
 [ 0.234]]
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
probs:  [0.4790088637806172, 0.26192425563440686, 0.18252433644283791, 0.07359323124778881, 0.002366621066782362, 0.000582691827566848]
from probs:  [0.4790088637806172, 0.26192425563440686, 0.18252433644283791, 0.07359323124778881, 0.002366621066782362, 0.000582691827566848]
maxi score, test score, baseline:  -0.90152 -0.2112500000000003 -0.2112500000000003
probs:  [0.4790088637806172, 0.26192425563440686, 0.18252433644283791, 0.07359323124778881, 0.002366621066782362, 0.000582691827566848]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9040900000000001 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.214]
 [-0.197]] [[-1.018]
 [-1.018]
 [-1.018]
 [-1.018]
 [-1.018]
 [ 0.417]
 [-0.655]] [[0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]
 [1.491]
 [0.999]]
from probs:  [0.4790088637806172, 0.2619242556344069, 0.18252433644283797, 0.07359323124778884, 0.0023666210667823624, 0.0005826918275668483]
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.059]
 [0.079]
 [0.083]
 [0.082]
 [0.086]
 [0.083]] [[-7.567]
 [-3.044]
 [-7.869]
 [-8.015]
 [-8.017]
 [ 0.   ]
 [-7.908]] [[0.098]
 [0.059]
 [0.079]
 [0.083]
 [0.082]
 [0.086]
 [0.083]]
maxi score, test score, baseline:  -0.9040900000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9040900000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4824404323501761, 0.26380065315270373, 0.1838319213822342, 0.06695655165654713, 0.0023835752884740634, 0.0005868661698648918]
start point for exploration sampling:  10749
in main func line 156:  3726
actor:  1 policy actor:  1  step number:  48 total reward:  0.6249999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9040900000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9040900000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9040900000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.48665401485522214, 0.2616529857412184, 0.18233529951255742, 0.06641144154305943, 0.002364170003048503, 0.000582088344894184]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.585]
 [0.527]
 [0.52 ]
 [0.52 ]
 [0.527]
 [0.519]] [[-3.505]
 [-2.657]
 [-3.978]
 [-4.126]
 [-4.126]
 [-3.844]
 [-3.882]] [[0.536]
 [0.585]
 [0.527]
 [0.52 ]
 [0.52 ]
 [0.527]
 [0.519]]
maxi score, test score, baseline:  -0.9040900000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9040900000000001 -0.2112500000000003 -0.2112500000000003
using another actor
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.591]
 [0.324]
 [0.534]
 [0.534]
 [0.534]
 [0.54 ]] [[2.237]
 [3.097]
 [0.848]
 [2.237]
 [2.237]
 [2.237]
 [2.717]] [[0.534]
 [0.591]
 [0.324]
 [0.534]
 [0.534]
 [0.534]
 [0.54 ]]
maxi score, test score, baseline:  -0.9040900000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9040900000000001 -0.2112500000000003 -0.2112500000000003
actor:  0 policy actor:  1  step number:  77 total reward:  0.3099999999999995  reward:  1.0 rdn_beta:  0.167
siam score:  -0.6891882
actor:  1 policy actor:  1  step number:  55 total reward:  0.5899999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[-3.495]
 [-3.495]
 [-3.495]
 [-3.495]
 [-3.495]
 [-3.495]
 [-3.495]] [[0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.433]
 [0.549]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[-3.013]
 [-3.013]
 [-1.787]
 [-3.013]
 [-3.013]
 [-3.013]
 [-3.013]] [[0.433]
 [0.433]
 [0.549]
 [0.433]
 [0.433]
 [0.433]
 [0.433]]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.43 ]
 [0.435]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]] [[-3.058]
 [-3.058]
 [-2.524]
 [-3.058]
 [-3.058]
 [-3.058]
 [-3.058]] [[0.43 ]
 [0.43 ]
 [0.435]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49046231608450147, 0.25971189062003647, 0.18098262945110005, 0.06591876256672476, 0.0023466312050655098, 0.0005777700725718934]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49046231608450147, 0.25971189062003647, 0.18098262945110005, 0.06591876256672476, 0.0023466312050655098, 0.0005777700725718934]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.438]
 [0.437]
 [0.46 ]
 [0.451]
 [0.475]
 [0.44 ]] [[-1.488]
 [-0.361]
 [-1.159]
 [-1.052]
 [-0.869]
 [-1.488]
 [-1.118]] [[0.42 ]
 [0.438]
 [0.437]
 [0.46 ]
 [0.451]
 [0.475]
 [0.44 ]]
siam score:  -0.6897923
using another actor
from probs:  [0.49046231608450147, 0.25971189062003647, 0.1809826294511, 0.06591876256672476, 0.0023466312050655098, 0.0005777700725718934]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49046231608450147, 0.25971189062003647, 0.1809826294511, 0.06591876256672476, 0.0023466312050655098, 0.0005777700725718934]
line 256 mcts: sample exp_bonus 0.3593668589000587
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]] [[-0.835]
 [-0.791]
 [-0.835]
 [-0.835]
 [-0.835]
 [-0.835]
 [-0.835]] [[0.28 ]
 [0.295]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[ 0.137]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]] [[-0.523]
 [-0.849]
 [-0.849]
 [-0.849]
 [-0.849]
 [-0.849]
 [-0.849]] [[ 0.025]
 [-0.591]
 [-0.591]
 [-0.591]
 [-0.591]
 [-0.591]
 [-0.591]]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.124]
 [-0.126]
 [-0.122]
 [-0.119]
 [-0.113]
 [-0.097]
 [-0.115]] [[-4.286]
 [-4.484]
 [-4.291]
 [-4.121]
 [-4.323]
 [-4.691]
 [-4.416]] [[-0.231]
 [-0.369]
 [-0.233]
 [-0.114]
 [-0.249]
 [-0.491]
 [-0.314]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49046231608450147, 0.25971189062003647, 0.18098262945110005, 0.06591876256672476, 0.0023466312050655098, 0.0005777700725718934]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49046231608450147, 0.25971189062003647, 0.18098262945110005, 0.06591876256672476, 0.0023466312050655098, 0.0005777700725718934]
in main func line 156:  3737
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]] [[-8.643]
 [-8.812]
 [-8.812]
 [-8.812]
 [-8.812]
 [-8.812]
 [-8.812]] [[0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49046231608450147, 0.25971189062003647, 0.1809826294511, 0.06591876256672476, 0.0023466312050655098, 0.0005777700725718934]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
line 256 mcts: sample exp_bonus -0.32192157510417907
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.49046231608450147, 0.25971189062003647, 0.1809826294511, 0.06591876256672476, 0.0023466312050655098, 0.0005777700725718934]
using explorer policy with actor:  1
siam score:  -0.6977637
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
siam score:  -0.69428664
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.691]
 [0.696]
 [0.691]
 [0.691]
 [0.691]
 [0.691]] [[-3.848]
 [-3.848]
 [-4.14 ]
 [-3.848]
 [-3.848]
 [-3.848]
 [-3.848]] [[0.691]
 [0.691]
 [0.696]
 [0.691]
 [0.691]
 [0.691]
 [0.691]]
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]] [[-1.377]
 [-1.377]
 [-1.377]
 [-1.377]
 [-1.377]
 [-1.377]
 [-1.377]] [[0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.004]
 [-0.005]
 [-0.002]
 [-0.002]
 [-0.007]
 [-0.   ]] [[4.639]
 [4.565]
 [4.358]
 [4.639]
 [4.639]
 [4.1  ]
 [4.435]] [[0.576]
 [0.525]
 [0.393]
 [0.576]
 [0.576]
 [0.224]
 [0.451]]
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.49046231608450147, 0.25971189062003647, 0.18098262945110005, 0.06591876256672476, 0.0023466312050655098, 0.0005777700725718934]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49046231608450147, 0.25971189062003647, 0.18098262945110005, 0.06591876256672476, 0.0023466312050655098, 0.0005777700725718934]
line 256 mcts: sample exp_bonus 2.6536421561822108
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49436221411445896, 0.25382550945538335, 0.1824217080039389, 0.06644291384979074, 0.002365290380528228, 0.0005823641958998177]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49436221411445896, 0.25382550945538335, 0.18242170800393886, 0.06644291384979074, 0.002365290380528228, 0.0005823641958998177]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49436221411445896, 0.25382550945538335, 0.18242170800393886, 0.06644291384979074, 0.002365290380528228, 0.0005823641958998177]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49436221411445896, 0.25382550945538335, 0.18242170800393886, 0.06644291384979074, 0.002365290380528228, 0.0005823641958998177]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
using another actor
from probs:  [0.49436221411445896, 0.2538255094553834, 0.18242170800393886, 0.06644291384979074, 0.002365290380528228, 0.0005823641958998177]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[-0.131]
 [-0.054]
 [-0.069]
 [-0.067]
 [-0.176]
 [-0.132]
 [-0.158]] [[ 0.53 ]
 [ 0.369]
 [-0.497]
 [-1.285]
 [ 1.603]
 [ 0.045]
 [ 0.321]] [[-0.356]
 [-0.309]
 [-0.914]
 [-1.436]
 [ 0.27 ]
 [-0.679]
 [-0.548]]
actor:  1 policy actor:  1  step number:  122 total reward:  0.11499999999999932  reward:  1.0 rdn_beta:  0.333
from probs:  [0.49436221411445896, 0.2538255094553834, 0.18242170800393886, 0.06644291384979074, 0.002365290380528228, 0.0005823641958998177]
using explorer policy with actor:  0
siam score:  -0.6952106
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.753]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.661]] [[-1.278]
 [ 0.735]
 [-1.278]
 [-1.278]
 [-1.278]
 [-1.278]
 [-1.445]] [[0.668]
 [0.753]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.661]]
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.198]] [[-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.609]] [[1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [0.008]]
3744 6096
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.653]
 [0.659]
 [0.659]
 [0.655]
 [0.659]
 [0.659]] [[-0.729]
 [-0.356]
 [ 0.   ]
 [ 0.   ]
 [-0.599]
 [ 0.   ]
 [ 0.   ]] [[0.645]
 [0.653]
 [0.659]
 [0.659]
 [0.655]
 [0.659]
 [0.659]]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4964565979070639, 0.25320703265610833, 0.18065182530267215, 0.06672440171016397, 0.002375311020650686, 0.0005848314033409664]
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]] [[-0.403]
 [-1.605]
 [-1.605]
 [-1.605]
 [-1.605]
 [-1.605]
 [-1.605]] [[0.992]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]]
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.4964565979070639, 0.25320703265610833, 0.18065182530267215, 0.06672440171016397, 0.002375311020650686, 0.0005848314033409664]
Printing some Q and Qe and total Qs values:  [[ 0.027]
 [ 0.135]
 [-0.047]
 [ 0.039]
 [-0.017]
 [-0.057]
 [ 0.237]] [[4.056]
 [3.777]
 [2.845]
 [4.481]
 [2.342]
 [1.466]
 [4.396]] [[ 0.447]
 [ 0.468]
 [-0.232]
 [ 0.666]
 [-0.428]
 [-0.899]
 [ 0.907]]
first move QE:  -0.8393829953851704
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.4964565979070639, 0.25320703265610833, 0.18065182530267215, 0.06672440171016397, 0.002375311020650686, 0.0005848314033409664]
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
actor:  1 policy actor:  1  step number:  79 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.49443491926599015, 0.25624813513666356, 0.17991617199838517, 0.06645268551514401, 0.002365638240438678, 0.000582449843378327]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.8357586476548096
siam score:  -0.6864917
line 256 mcts: sample exp_bonus -0.269881926508411
actor:  1 policy actor:  1  step number:  46 total reward:  0.6649999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [ 0.044]
 [-0.02 ]
 [-0.008]
 [-0.02 ]
 [-0.02 ]
 [-0.003]] [[3.844]
 [4.048]
 [3.844]
 [3.863]
 [3.844]
 [3.844]
 [4.043]] [[-0.737]
 [-0.474]
 [-0.737]
 [-0.701]
 [-0.737]
 [-0.737]
 [-0.572]]
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.4989167400331588, 0.25397650234917696, 0.17832121999964134, 0.06586358425533494, 0.0023446669214183596, 0.000577286441269746]
actor:  1 policy actor:  1  step number:  60 total reward:  0.46499999999999964  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.4962962836680175, 0.25789483554207404, 0.17738462489570322, 0.06551764948357032, 0.002332352046279156, 0.0005742543643556009]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.483]
 [0.457]
 [0.464]
 [0.464]
 [0.461]
 [0.464]] [[-5.077]
 [-4.362]
 [-4.912]
 [-4.814]
 [-4.814]
 [-4.784]
 [-4.814]] [[0.464]
 [0.483]
 [0.457]
 [0.464]
 [0.464]
 [0.461]
 [0.464]]
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.4962962836680175, 0.25789483554207404, 0.17738462489570322, 0.06551764948357032, 0.002332352046279156, 0.0005742543643556009]
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
line 256 mcts: sample exp_bonus 3.2367095039551637
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]] [[-7.486]
 [-7.84 ]
 [-7.84 ]
 [-7.84 ]
 [-7.84 ]
 [-7.84 ]
 [-7.84 ]] [[0.025]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]]
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.4962962836680175, 0.25789483554207404, 0.17738462489570322, 0.06551764948357032, 0.002332352046279156, 0.0005742543643556009]
first move QE:  -0.8435662294540041
actor:  1 policy actor:  1  step number:  88 total reward:  0.09499999999999931  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.5650502165192445
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.495762260672596, 0.25761733644100876, 0.17826977262126403, 0.06544715141098432, 0.0023298423969686155, 0.0005736364571782178]
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.495762260672596, 0.25761733644100876, 0.17826977262126403, 0.06544715141098432, 0.0023298423969686155, 0.0005736364571782178]
siam score:  -0.7151099
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.495762260672596, 0.25761733644100876, 0.17826977262126403, 0.06544715141098432, 0.0023298423969686155, 0.0005736364571782178]
3753 6123
using another actor
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.4997719514423821, 0.2597009275854073, 0.17162367555146155, 0.06597648343106466, 0.002348686000637655, 0.0005782759890466577]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.113]] [[4.021]
 [4.021]
 [4.021]
 [4.021]
 [4.021]
 [4.021]
 [3.94 ]] [[0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.817]]
Printing some Q and Qe and total Qs values:  [[-0.084]
 [ 0.068]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]] [[-0.497]
 [ 1.3  ]
 [-0.497]
 [-0.497]
 [-0.497]
 [-0.497]
 [-0.497]] [[-0.155]
 [ 0.428]
 [-0.155]
 [-0.155]
 [-0.155]
 [-0.155]
 [-0.155]]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4997719514423821, 0.25970092758540725, 0.17162367555146155, 0.06597648343106466, 0.002348686000637655, 0.0005782759890466577]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.269]
 [0.245]
 [0.235]
 [0.239]
 [0.431]
 [0.265]] [[-4.456]
 [-4.179]
 [-4.149]
 [-4.172]
 [-4.457]
 [-3.066]
 [-4.858]] [[0.239]
 [0.269]
 [0.245]
 [0.235]
 [0.239]
 [0.431]
 [0.265]]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4997719514423821, 0.25970092758540725, 0.17162367555146155, 0.06597648343106466, 0.002348686000637655, 0.0005782759890466577]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4997719514423821, 0.25970092758540725, 0.17162367555146155, 0.06597648343106466, 0.002348686000637655, 0.0005782759890466577]
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
actor:  1 policy actor:  1  step number:  124 total reward:  0.25499999999999945  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.236]
 [-0.237]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]] [[-0.868]
 [-0.843]
 [-0.843]
 [-0.805]
 [-0.864]
 [-0.881]
 [-0.84 ]] [[-0.399]
 [-0.353]
 [-0.351]
 [-0.28 ]
 [-0.392]
 [-0.424]
 [-0.346]]
from probs:  [0.49851856733506594, 0.2590496204955408, 0.1737011705308101, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
3756 6141
rdn beta is 0 so we're just using the maxi policy
3756 6144
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.9014700000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49851856733506594, 0.2590496204955408, 0.17370117053081013, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
using explorer policy with actor:  1
3758 6153
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.49851856733506594, 0.2590496204955408, 0.17370117053081013, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
siam score:  -0.7156789
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.49851856733506594, 0.2590496204955408, 0.17370117053081013, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
maxi score, test score, baseline:  -0.9014700000000002 -0.2112500000000003 -0.2112500000000003
probs:  [0.49851856733506594, 0.2590496204955408, 0.17370117053081013, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
using another actor
using another actor
actor:  0 policy actor:  1  step number:  52 total reward:  0.49499999999999966  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49851856733506594, 0.2590496204955408, 0.17370117053081013, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49851856733506594, 0.2590496204955408, 0.17370117053081013, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
using another actor
using explorer policy with actor:  1
using explorer policy with actor:  0
using explorer policy with actor:  1
siam score:  -0.738508
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49851856733506594, 0.2590496204955408, 0.17370117053081013, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49851856733506594, 0.2590496204955408, 0.17370117053081013, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]] [[-7.837]
 [-7.535]
 [-7.535]
 [-7.535]
 [-7.535]
 [-7.535]
 [-7.535]] [[0.073]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]]
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49851856733506594, 0.2590496204955408, 0.17370117053081013, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49851856733506594, 0.2590496204955408, 0.17370117053081013, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -8.145955460631935
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.751]
 [0.803]
 [0.82 ]
 [0.798]
 [0.798]
 [0.817]] [[-1.103]
 [-0.267]
 [-0.525]
 [-0.765]
 [-0.829]
 [-0.75 ]
 [-0.77 ]] [[0.815]
 [0.751]
 [0.803]
 [0.82 ]
 [0.798]
 [0.798]
 [0.817]]
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49851856733506594, 0.2590496204955408, 0.17370117053081013, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.8839783804326695
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.49851856733506594, 0.2590496204955408, 0.17370117053081013, 0.06581102021218964, 0.0023427957026771968, 0.0005768257237161969]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.5025244212179922, 0.26113121784442184, 0.16706143313410482, 0.06633984571264316, 0.0023616212748372094, 0.0005814608160006679]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.007]
 [-0.012]
 [-0.006]
 [-0.006]
 [-0.016]
 [-0.005]] [[4.023]
 [3.979]
 [3.842]
 [4.201]
 [4.023]
 [3.481]
 [3.618]] [[ 0.162]
 [ 0.146]
 [-0.008]
 [ 0.316]
 [ 0.162]
 [-0.33 ]
 [-0.191]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  36 total reward:  0.7749999999999999  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.09 ]
 [-0.113]
 [-0.189]
 [-0.066]
 [-0.079]
 [-0.078]
 [-0.021]] [[-0.916]
 [-0.436]
 [-0.129]
 [-1.115]
 [-1.681]
 [-1.063]
 [ 1.889]] [[-0.345]
 [-0.267]
 [-0.301]
 [-0.358]
 [-0.503]
 [-0.362]
 [ 0.388]]
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
3768 6196
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.08 ]
 [-0.017]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.026]] [[0.618]
 [0.618]
 [0.312]
 [0.618]
 [0.618]
 [0.618]
 [2.78 ]] [[0.386]
 [0.386]
 [0.293]
 [0.386]
 [0.386]
 [0.386]
 [1.381]]
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4988752599083746, 0.2552671610148739, 0.17707776031411002, 0.06585810833225113, 0.00234447198493162, 0.0005772384454586243]
siam score:  -0.72547394
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
first move QE:  -0.8555910143441557
Printing some Q and Qe and total Qs values:  [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]] [[5.481]
 [5.481]
 [5.481]
 [5.481]
 [5.481]
 [5.481]
 [5.481]] [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.001]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[2.612]
 [3.103]
 [2.612]
 [2.612]
 [2.612]
 [2.612]
 [2.612]] [[0.527]
 [0.674]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
start point for exploration sampling:  10749
siam score:  -0.7127996
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]] [[3.281]
 [3.781]
 [3.281]
 [3.281]
 [3.281]
 [3.281]
 [3.281]] [[-0.874]
 [-0.541]
 [-0.874]
 [-0.874]
 [-0.874]
 [-0.874]
 [-0.874]]
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.4988752599083746, 0.2552671610148739, 0.17707776031411002, 0.06585810833225113, 0.00234447198493162, 0.0005772384454586243]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  76 total reward:  0.4249999999999996  reward:  1.0 rdn_beta:  0.167
3773 6202
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.5012063643841318, 0.25407972328939504, 0.17625403973797207, 0.06555175320983164, 0.002333566099230663, 0.0005745532794387994]
start point for exploration sampling:  10749
from probs:  [0.5012063643841318, 0.25407972328939504, 0.17625403973797207, 0.06555175320983164, 0.002333566099230663, 0.0005745532794387994]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]] [[-6.845]
 [-5.371]
 [-5.371]
 [-5.371]
 [-5.371]
 [-5.371]
 [-5.371]] [[0.421]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.659]
 [0.546]] [[-1.598]
 [-1.598]
 [-1.598]
 [-1.598]
 [-1.598]
 [-1.233]
 [-1.598]] [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.659]
 [0.546]]
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.5012063643841318, 0.25407972328939504, 0.17625403973797207, 0.06555175320983164, 0.002333566099230663, 0.0005745532794387994]
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
from probs:  [0.5012063643841318, 0.25407972328939504, 0.17625403973797207, 0.06555175320983164, 0.002333566099230663, 0.0005745532794387994]
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
probs:  [0.5012063643841318, 0.25407972328939504, 0.17625403973797207, 0.06555175320983164, 0.002333566099230663, 0.0005745532794387994]
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
Starting evaluation
line 256 mcts: sample exp_bonus -0.40103230229962106
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.526]
 [0.517]
 [0.475]
 [0.509]
 [0.514]
 [0.475]] [[ 0.   ]
 [-4.228]
 [-4.201]
 [ 0.   ]
 [-4.703]
 [-4.496]
 [ 0.   ]] [[0.475]
 [0.526]
 [0.517]
 [0.475]
 [0.509]
 [0.514]
 [0.475]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.562]
 [0.581]
 [0.623]
 [0.624]
 [0.622]
 [0.629]] [[-2.81 ]
 [-2.765]
 [-2.655]
 [-3.079]
 [-2.735]
 [-2.689]
 [-3.23 ]] [[0.611]
 [0.562]
 [0.581]
 [0.623]
 [0.624]
 [0.622]
 [0.629]]
line 256 mcts: sample exp_bonus -3.0538744992497815
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.558]
 [0.54 ]
 [0.544]
 [0.534]
 [0.535]
 [0.545]] [[-4.755]
 [-3.888]
 [-4.97 ]
 [-4.956]
 [-4.917]
 [-4.99 ]
 [-4.381]] [[0.555]
 [0.558]
 [0.54 ]
 [0.544]
 [0.534]
 [0.535]
 [0.545]]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[-3.729]
 [-3.729]
 [-3.729]
 [-3.729]
 [-3.729]
 [-3.729]
 [-3.729]] [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.8984800000000001 -0.2112500000000003 -0.2112500000000003
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.589]
 [0.68 ]
 [0.678]
 [0.648]
 [0.666]
 [0.67 ]] [[-3.048]
 [-1.24 ]
 [-2.556]
 [-2.681]
 [ 0.   ]
 [-3.228]
 [-3.326]] [[0.673]
 [0.589]
 [0.68 ]
 [0.678]
 [0.648]
 [0.666]
 [0.67 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]] [[0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]] [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -2.0813884572306365
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.771]
 [0.771]
 [0.821]
 [0.804]
 [0.765]
 [0.8  ]] [[-0.07 ]
 [ 0.127]
 [ 0.127]
 [-0.039]
 [-0.168]
 [-0.197]
 [-0.401]] [[0.826]
 [0.771]
 [0.771]
 [0.821]
 [0.804]
 [0.765]
 [0.8  ]]
actor:  0 policy actor:  1  step number:  63 total reward:  0.4099999999999996  reward:  1.0 rdn_beta:  1
main train batch thing paused
add a thread
Adding thread: now have 4 threads
actor:  0 policy actor:  1  step number:  68 total reward:  0.25499999999999945  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  71 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1
rdn probs:  [0.5065287619343339, 0.2567778400590279, 0.16750653812498909, 0.0662478586775349, 0.002358346643478224, 0.000580654560636023]
Printing some Q and Qe and total Qs values:  [[-0.17 ]
 [-0.109]
 [-0.163]
 [-0.109]
 [-0.109]
 [-0.165]
 [-0.161]] [[-0.603]
 [-0.301]
 [-0.329]
 [-0.301]
 [-0.301]
 [-0.19 ]
 [-0.707]] [[1.074]
 [1.34 ]
 [1.28 ]
 [1.34 ]
 [1.34 ]
 [1.38 ]
 [1.005]]
Printing some Q and Qe and total Qs values:  [[-0.015]
 [ 0.097]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.017]] [[3.385]
 [4.128]
 [3.385]
 [3.385]
 [3.385]
 [3.385]
 [3.321]] [[0.492]
 [0.917]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.462]]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]] [[4.691]
 [4.691]
 [4.691]
 [4.691]
 [4.691]
 [4.691]
 [4.691]] [[0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.671]
 [0.616]
 [0.624]
 [0.627]
 [0.63 ]
 [0.601]] [[-5.129]
 [-4.79 ]
 [-4.866]
 [-4.835]
 [-4.965]
 [-5.19 ]
 [-4.996]] [[0.591]
 [0.671]
 [0.616]
 [0.624]
 [0.627]
 [0.63 ]
 [0.601]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  60 total reward:  0.4049999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8935500000000001 -0.8047500000000001 -0.8047500000000001
maxi score, test score, baseline:  -0.8935500000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.466209653823413, 0.2782449965874132, 0.17919946106898668, 0.07261348641894164, 0.0035579471845642546, 0.00017445491668136557]
maxi score, test score, baseline:  -0.8935500000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.466209653823413, 0.2782449965874132, 0.17919946106898668, 0.07261348641894164, 0.0035579471845642546, 0.00017445491668136557]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.504]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.439]] [[-0.789]
 [-0.941]
 [-2.551]
 [-2.551]
 [-2.551]
 [-2.551]
 [-2.838]] [[1.428]
 [1.664]
 [1.268]
 [1.268]
 [1.268]
 [1.268]
 [1.207]]
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.713]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[-0.15 ]
 [ 0.475]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]] [[1.069]
 [1.864]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]]
Printing some Q and Qe and total Qs values:  [[0.887]
 [0.932]
 [0.893]
 [0.905]
 [0.853]
 [0.818]
 [0.912]] [[4.535]
 [4.592]
 [4.181]
 [4.708]
 [4.433]
 [2.867]
 [4.922]] [[1.453]
 [1.533]
 [1.267]
 [1.566]
 [1.361]
 [0.474]
 [1.69 ]]
actor:  0 policy actor:  1  step number:  56 total reward:  0.46499999999999964  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8934599999999999 -0.8047500000000001 -0.8047500000000001
probs:  [0.466209653823413, 0.27824499658741325, 0.17919946106898668, 0.07261348641894164, 0.0035579471845642546, 0.00017445491668136557]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.796]
 [0.866]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[2.773]
 [2.773]
 [3.939]
 [2.773]
 [2.773]
 [2.773]
 [2.773]] [[1.602]
 [1.602]
 [1.987]
 [1.602]
 [1.602]
 [1.602]
 [1.602]]
Printing some Q and Qe and total Qs values:  [[1.061]
 [0.963]
 [0.996]
 [1.013]
 [0.998]
 [0.825]
 [1.026]] [[2.939]
 [2.802]
 [3.472]
 [3.869]
 [4.02 ]
 [4.667]
 [3.452]] [[1.227]
 [1.105]
 [1.376]
 [1.535]
 [1.579]
 [1.69 ]
 [1.391]]
maxi score, test score, baseline:  -0.8934599999999999 -0.8047500000000001 -0.8047500000000001
3787 6225
line 256 mcts: sample exp_bonus -0.1800992037205537
maxi score, test score, baseline:  -0.89346 -0.8047500000000001 -0.8047500000000001
probs:  [0.466209653823413, 0.27824499658741325, 0.17919946106898668, 0.07261348641894164, 0.0035579471845642546, 0.00017445491668136557]
siam score:  -0.7004838
maxi score, test score, baseline:  -0.89346 -0.8047500000000001 -0.8047500000000001
probs:  [0.466209653823413, 0.27824499658741325, 0.17919946106898668, 0.07261348641894164, 0.0035579471845642546, 0.00017445491668136557]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.808]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[-1.389]
 [ 0.265]
 [-1.389]
 [-1.389]
 [-1.389]
 [-1.389]
 [-1.389]] [[0.481]
 [1.159]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.5699999999999997  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.8624525564612262
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.645]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[ 0.904]
 [-0.047]
 [ 0.904]
 [ 0.904]
 [ 0.904]
 [ 0.904]
 [ 0.904]] [[0.448]
 [0.123]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
first move QE:  -0.862783979832308
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]] [[-0.863]
 [-0.842]
 [-0.863]
 [-0.863]
 [-0.863]
 [-0.863]
 [-0.852]] [[0.931]
 [0.939]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.935]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.4299999999999996  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -1.087412412226773
maxi score, test score, baseline:  -0.89346 -0.8047500000000001 -0.8047500000000001
probs:  [0.4750348925059574, 0.2736447288517818, 0.17623672855228817, 0.07141295637225012, 0.003499123091268871, 0.00017157062645378036]
maxi score, test score, baseline:  -0.89346 -0.8047500000000001 -0.8047500000000001
siam score:  -0.69611466
maxi score, test score, baseline:  -0.89346 -0.8047500000000001 -0.8047500000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89346 -0.8047500000000001 -0.8047500000000001
probs:  [0.4750348925059574, 0.2736447288517818, 0.17623672855228817, 0.07141295637225012, 0.003499123091268871, 0.00017157062645378036]
from probs:  [0.4750348925059574, 0.2736447288517818, 0.17623672855228817, 0.07141295637225012, 0.003499123091268871, 0.00017157062645378036]
siam score:  -0.7004091
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]] [[0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]] [[0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]]
maxi score, test score, baseline:  -0.8934599999999999 -0.8047500000000001 -0.8047500000000001
probs:  [0.4750348925059574, 0.2736447288517818, 0.17623672855228817, 0.07141295637225012, 0.003499123091268871, 0.00017157062645378036]
maxi score, test score, baseline:  -0.8934599999999999 -0.8047500000000001 -0.8047500000000001
probs:  [0.4750348925059574, 0.2736447288517818, 0.17623672855228817, 0.07141295637225012, 0.003499123091268871, 0.00017157062645378036]
using explorer policy with actor:  1
start point for exploration sampling:  10749
from probs:  [0.4750348925059574, 0.2736447288517818, 0.17623672855228817, 0.07141295637225012, 0.003499123091268871, 0.00017157062645378036]
using explorer policy with actor:  1
siam score:  -0.702524
maxi score, test score, baseline:  -0.8961600000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.4750348925059574, 0.27364472885178176, 0.17623672855228817, 0.07141295637225012, 0.003499123091268871, 0.00017157062645378036]
Printing some Q and Qe and total Qs values:  [[1.066]
 [1.12 ]
 [1.066]
 [1.066]
 [1.066]
 [1.066]
 [1.066]] [[3.363]
 [3.522]
 [3.363]
 [3.363]
 [3.363]
 [3.363]
 [3.363]] [[1.093]
 [1.252]
 [1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.093]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.637]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]] [[2.292]
 [3.97 ]
 [2.292]
 [2.292]
 [2.292]
 [2.292]
 [2.292]] [[1.141]
 [1.719]
 [1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]]
siam score:  -0.7041116
maxi score, test score, baseline:  -0.8961600000000001 -0.8047500000000001 -0.8047500000000001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
siam score:  -0.70526093
maxi score, test score, baseline:  -0.8961600000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.4750348925059575, 0.27364472885178176, 0.17623672855228817, 0.07141295637225012, 0.003499123091268871, 0.00017157062645378036]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7699999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8961600000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.4710267629544878, 0.27977338607903673, 0.17474972275353853, 0.07081040614844768, 0.0034695990733754514, 0.00017012299111393017]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.824]
 [0.932]
 [0.799]
 [0.799]
 [0.799]
 [0.859]] [[3.943]
 [4.229]
 [4.614]
 [3.943]
 [3.943]
 [3.943]
 [4.258]] [[1.765]
 [1.829]
 [1.938]
 [1.765]
 [1.765]
 [1.765]
 [1.845]]
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.228]] [[-9.621]
 [-6.988]
 [-6.988]
 [-6.988]
 [-6.988]
 [-6.988]
 [-6.876]] [[0.227]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.228]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.8981700000000001 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.887]
 [0.729]
 [0.707]
 [0.676]
 [0.667]
 [0.682]] [[ 1.413]
 [-0.018]
 [ 1.253]
 [ 1.061]
 [ 1.312]
 [ 1.322]
 [ 1.351]] [[ 0.493]
 [-0.074]
 [ 0.456]
 [ 0.284]
 [ 0.389]
 [ 0.378]
 [ 0.426]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  73 total reward:  0.38999999999999957  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.819]
 [0.788]
 [0.795]
 [0.801]
 [0.799]
 [0.78 ]] [[3.089]
 [3.076]
 [2.971]
 [2.981]
 [3.153]
 [3.118]
 [3.079]] [[1.322]
 [1.415]
 [1.253]
 [1.276]
 [1.454]
 [1.416]
 [1.341]]
line 256 mcts: sample exp_bonus 3.49533970972236
maxi score, test score, baseline:  -0.8981700000000001 -0.8047500000000001 -0.8047500000000001
using explorer policy with actor:  1
using explorer policy with actor:  1
start point for exploration sampling:  10749
siam score:  -0.7012747
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.044]
 [0.032]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.044]] [[-9.013]
 [ 0.   ]
 [-7.33 ]
 [-6.334]
 [-6.313]
 [-6.066]
 [ 0.   ]] [[0.038]
 [0.044]
 [0.032]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.044]]
line 256 mcts: sample exp_bonus 3.908842424976642
using explorer policy with actor:  1
3812 6242
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[-0.889]
 [-1.931]
 [-1.931]
 [-1.931]
 [-1.931]
 [-1.931]
 [-1.931]] [[1.922]
 [1.498]
 [1.498]
 [1.498]
 [1.498]
 [1.498]
 [1.498]]
maxi score, test score, baseline:  -0.8981700000000001 -0.8047500000000001 -0.8047500000000001
from probs:  [0.46875015364703243, 0.2832544507269237, 0.17390510653088373, 0.07046815886574323, 0.0034528294921863925, 0.0001693007372306205]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]] [[-6.794]
 [-7.004]
 [-7.004]
 [-7.004]
 [-7.004]
 [-7.004]
 [-7.004]] [[0.119]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]]
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.996]
 [0.917]
 [0.903]
 [0.917]
 [0.917]
 [0.971]] [[4.648]
 [4.841]
 [4.648]
 [4.49 ]
 [4.648]
 [4.648]
 [4.701]] [[1.294]
 [1.531]
 [1.294]
 [1.183]
 [1.294]
 [1.294]
 [1.413]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6549999999999998  reward:  1.0 rdn_beta:  0.333
siam score:  -0.70268613
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.6995833522292862
3816 6245
maxi score, test score, baseline:  -0.8981700000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.465392275105907, 0.28838883730925524, 0.1726593421916748, 0.06996336219176481, 0.0034280952452367735, 0.00016808795616138702]
using another actor
from probs:  [0.46539227510590714, 0.28838883730925524, 0.1726593421916748, 0.06996336219176481, 0.0034280952452367735, 0.00016808795616138702]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  126 total reward:  0.3349999999999995  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8981700000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.463368731730879, 0.28713491163754873, 0.17625665094936405, 0.06965915882262722, 0.0034131897562686003, 0.0001673571033124974]
line 256 mcts: sample exp_bonus 3.8512563450039914
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.806]
 [0.913]
 [0.859]
 [0.861]
 [0.882]
 [0.864]
 [0.873]] [[4.725]
 [4.669]
 [4.446]
 [4.353]
 [4.276]
 [4.274]
 [4.203]] [[1.537]
 [1.639]
 [1.425]
 [1.368]
 [1.345]
 [1.321]
 [1.286]]
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.914]
 [0.846]
 [0.874]
 [0.876]
 [0.879]
 [0.875]] [[4.581]
 [4.54 ]
 [4.605]
 [4.524]
 [4.463]
 [4.408]
 [4.416]] [[1.298]
 [1.382]
 [1.334]
 [1.319]
 [1.28 ]
 [1.248]
 [1.247]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.095]] [[4.551]
 [4.551]
 [4.551]
 [4.551]
 [4.551]
 [4.551]
 [4.551]] [[1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]]
from probs:  [0.463368731730879, 0.28713491163754873, 0.17625665094936405, 0.06965915882262722, 0.0034131897562686003, 0.0001673571033124974]
Printing some Q and Qe and total Qs values:  [[0.941]
 [1.007]
 [1.016]
 [0.964]
 [0.939]
 [0.949]
 [0.961]] [[4.626]
 [4.486]
 [5.057]
 [4.944]
 [4.191]
 [4.396]
 [5.261]] [[1.552]
 [1.547]
 [1.926]
 [1.786]
 [1.269]
 [1.414]
 [1.987]]
Printing some Q and Qe and total Qs values:  [[0.958]
 [0.842]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]] [[3.798]
 [3.651]
 [3.798]
 [3.798]
 [3.798]
 [3.798]
 [3.798]] [[1.078]
 [0.813]
 [1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]]
siam score:  -0.7031588
maxi score, test score, baseline:  -0.89817 -0.8047500000000001 -0.8047500000000001
probs:  [0.463368731730879, 0.28713491163754873, 0.17625665094936405, 0.06965915882262722, 0.0034131897562686003, 0.0001673571033124974]
line 256 mcts: sample exp_bonus 2.8409625626309216
actor:  1 policy actor:  1  step number:  71 total reward:  0.5899999999999997  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  42 total reward:  0.6949999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]] [[3.449]
 [3.449]
 [3.449]
 [3.449]
 [3.449]
 [3.449]
 [3.449]] [[4.966]
 [4.966]
 [4.966]
 [4.966]
 [4.966]
 [4.966]
 [4.966]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.599]
 [1.062]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.884]] [[2.86 ]
 [4.103]
 [3.514]
 [3.514]
 [3.514]
 [3.514]
 [3.689]] [[0.295]
 [1.635]
 [1.277]
 [1.277]
 [1.277]
 [1.277]
 [1.14 ]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.7099999999999999  reward:  1.0 rdn_beta:  0.167
3826 6253
maxi score, test score, baseline:  -0.8947800000000001 -0.8047500000000001 -0.8047500000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8947800000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.4644574338656051, 0.2831171406297476, 0.18021053552468758, 0.06868444436819453, 0.0033654302735618424, 0.00016501533820353245]
3827 6254
maxi score, test score, baseline:  -0.8947800000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.4644574338656051, 0.2831171406297476, 0.18021053552468758, 0.06868444436819453, 0.0033654302735618424, 0.00016501533820353245]
Printing some Q and Qe and total Qs values:  [[ 0.541]
 [-0.022]
 [ 0.517]
 [ 0.516]
 [ 0.511]
 [ 0.511]
 [ 0.512]] [[-2.643]
 [-0.727]
 [-1.983]
 [-2.213]
 [-2.211]
 [-2.144]
 [-1.872]] [[-0.009]
 [ 0.854]
 [ 0.494]
 [ 0.307]
 [ 0.303]
 [ 0.357]
 [ 0.577]]
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
probs:  [0.46674924904174453, 0.27957975938267643, 0.18109976499998798, 0.0690233603602535, 0.0033820366267231956, 0.0001658295886144755]
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.917]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]] [[0.93 ]
 [1.776]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]] [[0.951]
 [1.3  ]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.76 ]
 [0.882]
 [0.897]
 [0.832]
 [0.777]
 [0.897]
 [0.83 ]] [[-0.338]
 [ 2.763]
 [ 1.999]
 [ 0.101]
 [-0.05 ]
 [ 1.999]
 [ 0.926]] [[0.08 ]
 [1.36 ]
 [1.135]
 [0.37 ]
 [0.211]
 [1.135]
 [0.643]]
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
probs:  [0.46674924904174453, 0.27957975938267643, 0.18109976499998798, 0.0690233603602535, 0.0033820366267231956, 0.0001658295886144755]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.907]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]] [[-0.305]
 [ 1.087]
 [-0.305]
 [-0.305]
 [-0.305]
 [-0.305]
 [-0.305]] [[0.944]
 [1.46 ]
 [0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]]
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
probs:  [0.46898815271157207, 0.2761240462265275, 0.18196846469108727, 0.0693544517441854, 0.0033982595863328953, 0.00016662504029495961]
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
actor:  1 policy actor:  1  step number:  52 total reward:  0.5649999999999997  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
probs:  [0.4660740303329436, 0.2744083114947782, 0.1870514161253288, 0.06892350832116563, 0.0033771440330898866, 0.0001655896926940533]
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
actor:  1 policy actor:  1  step number:  112 total reward:  0.37499999999999956  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.336]] [[-0.333]
 [-0.333]
 [-0.333]
 [-0.333]
 [-0.333]
 [-0.333]
 [ 0.129]] [[0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.336]]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.634]
 [0.63 ]
 [0.636]
 [0.636]
 [0.636]
 [0.645]] [[1.169]
 [2.892]
 [3.576]
 [1.169]
 [1.169]
 [1.169]
 [4.43 ]] [[0.636]
 [0.634]
 [0.63 ]
 [0.636]
 [0.636]
 [0.636]
 [0.645]]
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
probs:  [0.468523810309705, 0.27314926056796895, 0.18619317951598305, 0.06860727078973665, 0.0033616488890045332, 0.00016482992760195086]
Printing some Q and Qe and total Qs values:  [[0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]] [[2.777]
 [2.777]
 [2.777]
 [2.777]
 [2.777]
 [2.777]
 [2.777]] [[1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]]
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [0.04 ]
 [0.448]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.108]] [[-1.101]
 [-0.949]
 [-1.347]
 [-1.101]
 [-1.101]
 [-1.101]
 [-0.965]] [[1.655]
 [1.652]
 [1.656]
 [1.655]
 [1.655]
 [1.655]
 [1.67 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
probs:  [0.47168020310702863, 0.2682525479822072, 0.18744754225658006, 0.06906947034203177, 0.0033842959436619496, 0.00016594036849058416]
3841 6273
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.80618737070297
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.744]
 [0.555]
 [0.713]
 [0.713]
 [0.713]
 [0.768]] [[1.916]
 [2.845]
 [2.821]
 [1.916]
 [1.916]
 [1.916]
 [3.389]] [[1.288]
 [1.622]
 [1.517]
 [1.288]
 [1.288]
 [1.288]
 [1.821]]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.598]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.683]] [[0.287]
 [0.769]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [1.309]] [[1.227]
 [1.546]
 [1.227]
 [1.227]
 [1.227]
 [1.227]
 [1.789]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.723]
 [0.626]
 [0.488]
 [0.342]
 [0.626]
 [0.63 ]] [[3.939]
 [5.222]
 [5.017]
 [4.473]
 [2.561]
 [5.017]
 [5.061]] [[0.926]
 [1.513]
 [1.397]
 [1.16 ]
 [0.519]
 [1.397]
 [1.412]]
using explorer policy with actor:  1
siam score:  -0.7166985
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.866]
 [0.633]
 [0.637]
 [0.6  ]
 [0.623]
 [0.608]] [[ 2.504]
 [-0.466]
 [ 2.484]
 [ 2.429]
 [ 2.556]
 [ 2.702]
 [ 2.568]] [[ 1.375]
 [-0.116]
 [ 1.342]
 [ 1.314]
 [ 1.345]
 [ 1.458]
 [ 1.362]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.5915829608042362
UNIT TEST: sample policy line 217 mcts : [0.224 0.02  0.143 0.082 0.061 0.204 0.265]
using explorer policy with actor:  1
first move QE:  -0.8831524892474514
actor:  1 policy actor:  1  step number:  86 total reward:  0.26499999999999946  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
probs:  [0.46905585024587776, 0.2708771557077097, 0.18928154633681604, 0.0672004755571983, 0.003417408208003651, 0.0001675639443946684]
Printing some Q and Qe and total Qs values:  [[0.933]
 [0.978]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]] [[-0.286]
 [ 0.144]
 [-0.286]
 [-0.286]
 [-0.286]
 [-0.286]
 [-0.286]] [[1.281]
 [1.447]
 [1.281]
 [1.281]
 [1.281]
 [1.281]
 [1.281]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5549999999999997  reward:  1.0 rdn_beta:  0.167
from probs:  [0.4723254519602335, 0.2692090698777935, 0.18811593359075718, 0.06678664899900175, 0.00339636350162405, 0.00016653207059003048]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.537]
 [0.551]
 [0.553]
 [0.547]
 [0.546]
 [0.545]] [[-10.   ]
 [ -2.611]
 [ -9.82 ]
 [ -9.819]
 [ -9.793]
 [ -9.681]
 [ -9.393]] [[0.222]
 [1.521]
 [0.239]
 [0.241]
 [0.24 ]
 [0.259]
 [0.309]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
probs:  [0.46997147034709386, 0.2704100245248139, 0.18895512784497367, 0.06708458746188742, 0.003411514843040955, 0.0001672749781902232]
actor:  1 policy actor:  1  step number:  58 total reward:  0.4549999999999996  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
probs:  [0.46748947166695354, 0.2689819435320004, 0.18795722391354863, 0.06673030242961685, 0.008674666887091124, 0.00016639157078938245]
maxi score, test score, baseline:  -0.89478 -0.8047500000000001 -0.8047500000000001
probs:  [0.46748947166695354, 0.2689819435320004, 0.1879572239135486, 0.06673030242961685, 0.008674666887091124, 0.00016639157078938245]
actor:  1 policy actor:  1  step number:  54 total reward:  0.6549999999999998  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  50 total reward:  0.5849999999999997  reward:  1.0 rdn_beta:  0.167
from probs:  [0.4675956392352577, 0.26539419439194323, 0.19244679539524448, 0.06584023679170116, 0.008558961987885721, 0.0001641721979676475]
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.799]
 [0.794]
 [0.82 ]
 [0.82 ]
 [0.794]
 [0.82 ]] [[2.853]
 [2.607]
 [3.623]
 [2.853]
 [2.853]
 [3.295]
 [2.853]] [[0.82 ]
 [0.799]
 [0.794]
 [0.82 ]
 [0.82 ]
 [0.794]
 [0.82 ]]
start point for exploration sampling:  10749
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  69 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [1.048]
 [1.018]] [[-1.68 ]
 [-1.68 ]
 [-1.68 ]
 [-1.68 ]
 [-1.68 ]
 [-0.412]
 [-1.412]] [[1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.977]
 [1.582]]
UNIT TEST: sample policy line 217 mcts : [0.122 0.122 0.02  0.082 0.449 0.02  0.184]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89468 -0.8047500000000001 -0.8047500000000001
probs:  [0.4675956392352577, 0.26539419439194323, 0.19244679539524448, 0.06584023679170116, 0.008558961987885721, 0.0001641721979676475]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.668]
 [0.698]
 [0.706]
 [0.668]
 [0.646]
 [0.668]] [[0.253]
 [0.229]
 [0.346]
 [0.086]
 [0.229]
 [0.361]
 [0.229]] [[ 0.02 ]
 [ 0.007]
 [ 0.146]
 [-0.013]
 [ 0.007]
 [ 0.05 ]
 [ 0.007]]
first move QE:  -0.890824214696861
actor:  1 policy actor:  1  step number:  87 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.48 ]
 [0.542]
 [0.542]
 [0.542]
 [0.385]
 [0.418]] [[1.771]
 [3.743]
 [3.192]
 [3.192]
 [3.192]
 [2.582]
 [4.244]] [[-0.406]
 [ 1.267]
 [ 0.921]
 [ 0.921]
 [ 0.921]
 [ 0.283]
 [ 1.574]]
in main func line 156:  3863
Printing some Q and Qe and total Qs values:  [[0.823]
 [1.117]
 [0.813]
 [0.806]
 [0.809]
 [0.821]
 [0.805]] [[2.008]
 [0.222]
 [2.046]
 [2.076]
 [1.904]
 [2.045]
 [2.014]] [[1.44 ]
 [0.242]
 [1.458]
 [1.474]
 [1.308]
 [1.472]
 [1.41 ]]
start point for exploration sampling:  10749
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89468 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.443]
 [0.665]
 [0.443]
 [0.443]
 [0.513]
 [0.443]] [[1.44 ]
 [1.719]
 [1.601]
 [1.719]
 [1.719]
 [1.673]
 [1.719]] [[0.948]
 [0.997]
 [1.159]
 [0.997]
 [0.997]
 [1.042]
 [0.997]]
maxi score, test score, baseline:  -0.89468 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]] [[-2.132]
 [-2.132]
 [-2.132]
 [-2.132]
 [-2.132]
 [-2.132]
 [-2.132]] [[0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.2403491745338229
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.89468 -0.8047500000000001 -0.8047500000000001
probs:  [0.46572929741563257, 0.26832626950322525, 0.19167867124659674, 0.06557744480429226, 0.008524800102379907, 0.000163516927873196]
actor:  1 policy actor:  1  step number:  125 total reward:  0.26999999999999946  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  102 total reward:  0.35499999999999954  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.89468 -0.8047500000000001 -0.8047500000000001
3867 6309
from probs:  [0.46195228390777043, 0.2705131711270138, 0.19387107031160844, 0.06504561892129256, 0.00845566490878744, 0.00016219082352724156]
siam score:  -0.69870573
maxi score, test score, baseline:  -0.89468 -0.8047500000000001 -0.8047500000000001
probs:  [0.46195228390777043, 0.2705131711270138, 0.19387107031160844, 0.06504561892129256, 0.00845566490878744, 0.00016219082352724156]
start point for exploration sampling:  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]] [[4.495]
 [4.495]
 [4.495]
 [4.495]
 [4.495]
 [4.495]
 [4.495]] [[1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]]
using another actor
maxi score, test score, baseline:  -0.89468 -0.8047500000000001 -0.8047500000000001
probs:  [0.46195228390777043, 0.2705131711270138, 0.19387107031160844, 0.06504561892129256, 0.00845566490878744, 0.00016219082352724156]
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]] [[4.444]
 [4.444]
 [4.444]
 [4.444]
 [4.444]
 [4.444]
 [4.444]] [[2.005]
 [2.005]
 [2.005]
 [2.005]
 [2.005]
 [2.005]
 [2.005]]
line 256 mcts: sample exp_bonus -10.0
maxi score, test score, baseline:  -0.89468 -0.8047500000000001 -0.8047500000000001
maxi score, test score, baseline:  -0.89468 -0.8047500000000001 -0.8047500000000001
probs:  [0.46195228390777043, 0.27051317112701384, 0.19387107031160847, 0.06504561892129257, 0.008455664908787441, 0.0001621908235272416]
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.791]] [[0.998]
 [0.998]
 [0.998]
 [0.998]
 [0.998]
 [0.998]
 [1.949]] [[1.983]
 [1.983]
 [1.983]
 [1.983]
 [1.983]
 [1.983]
 [2.163]]
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.852]
 [0.939]
 [0.852]
 [0.852]
 [0.852]
 [0.903]] [[2.921]
 [2.921]
 [2.833]
 [2.921]
 [2.921]
 [2.921]
 [2.863]] [[1.76 ]
 [1.76 ]
 [1.778]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.769]]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[-0.898]
 [-0.898]
 [-0.898]
 [-0.898]
 [-0.898]
 [-0.898]
 [-0.898]] [[0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]]
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.46195228390777043, 0.27051317112701384, 0.19387107031160847, 0.06504561892129257, 0.008455664908787441, 0.0001621908235272416]
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.46195228390777043, 0.27051317112701384, 0.19387107031160847, 0.06504561892129257, 0.008455664908787441, 0.0001621908235272416]
Printing some Q and Qe and total Qs values:  [[1.177]
 [1.177]
 [1.177]
 [1.177]
 [1.177]
 [1.177]
 [1.177]] [[-0.9]
 [-0.9]
 [-0.9]
 [-0.9]
 [-0.9]
 [-0.9]
 [-0.9]] [[1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]]
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
actor:  1 policy actor:  1  step number:  52 total reward:  0.6249999999999998  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.695]
 [0.715]
 [0.653]
 [0.655]
 [0.744]
 [0.649]] [[-1.856]
 [-1.741]
 [-2.087]
 [-1.917]
 [-2.254]
 [-0.139]
 [-1.743]] [[0.197]
 [0.251]
 [0.112]
 [0.153]
 [0.009]
 [0.967]
 [0.226]]
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.738]
 [0.796]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[3.292]
 [3.179]
 [7.594]
 [3.292]
 [3.292]
 [3.292]
 [3.292]] [[0.636]
 [0.617]
 [1.817]
 [0.636]
 [0.636]
 [0.636]
 [0.636]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89468 -0.8047500000000001 -0.8047500000000001
line 256 mcts: sample exp_bonus 3.602536887575117
maxi score, test score, baseline:  -0.89468 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[3.663]
 [3.663]
 [3.663]
 [3.663]
 [3.663]
 [3.663]
 [3.663]] [[1.964]
 [1.964]
 [1.964]
 [1.964]
 [1.964]
 [1.964]
 [1.964]]
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.838]
 [0.816]
 [0.828]
 [0.828]
 [0.79 ]
 [0.822]] [[-0.768]
 [ 0.86 ]
 [ 1.631]
 [ 2.9  ]
 [ 2.9  ]
 [ 2.929]
 [ 3.725]] [[0.027]
 [0.759]
 [1.071]
 [1.616]
 [1.616]
 [1.604]
 [1.961]]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.806]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]] [[4.516]
 [4.935]
 [4.516]
 [4.516]
 [4.516]
 [4.516]
 [4.516]] [[1.583]
 [1.858]
 [1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.583]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.858]
 [0.762]
 [0.77 ]
 [0.789]
 [0.77 ]
 [0.77 ]
 [0.77 ]] [[3.516]
 [4.569]
 [3.64 ]
 [3.406]
 [3.64 ]
 [3.64 ]
 [3.64 ]] [[1.019]
 [1.518]
 [1.015]
 [0.902]
 [1.015]
 [1.015]
 [1.015]]
maxi score, test score, baseline:  -0.89468 -0.8047500000000001 -0.8047500000000001
actor:  1 policy actor:  1  step number:  56 total reward:  0.5249999999999997  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 3.3392524124603686
Printing some Q and Qe and total Qs values:  [[0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]] [[3.706]
 [3.706]
 [3.706]
 [3.706]
 [3.706]
 [3.706]
 [3.706]] [[1.738]
 [1.738]
 [1.738]
 [1.738]
 [1.738]
 [1.738]
 [1.738]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.806]
 [0.917]
 [0.806]
 [0.69 ]
 [0.806]
 [0.806]
 [0.842]] [[4.892]
 [4.57 ]
 [4.892]
 [4.397]
 [4.892]
 [4.892]
 [4.638]] [[1.826]
 [1.78 ]
 [1.826]
 [1.488]
 [1.826]
 [1.826]
 [1.741]]
actor:  1 policy actor:  1  step number:  75 total reward:  0.5399999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.4536804395948626, 0.27130241075467626, 0.1969117871110638, 0.06964182049577301, 0.00830425545347328, 0.00015928659015090543]
actor:  1 policy actor:  1  step number:  47 total reward:  0.6499999999999998  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.674]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.683]] [[4.03 ]
 [4.033]
 [4.03 ]
 [4.03 ]
 [4.03 ]
 [4.03 ]
 [3.94 ]] [[0.324]
 [0.401]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.388]]
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.4506422293105427, 0.26948555090669224, 0.20228991377174632, 0.06917544267389035, 0.008248643458508514, 0.0001582198786198046]
actor:  1 policy actor:  1  step number:  74 total reward:  0.35499999999999954  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  135 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.45190566623775763, 0.2677126263969189, 0.20095906413992304, 0.07107108798924658, 0.008194376273827303, 0.00015717896232653904]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5749999999999997  reward:  1.0 rdn_beta:  0.167
3889 6344
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.306]
 [0.475]
 [0.625]
 [0.601]
 [0.656]
 [0.557]] [[-0.203]
 [-0.527]
 [-0.196]
 [-1.057]
 [-1.134]
 [-1.378]
 [-0.296]] [[1.209]
 [0.802]
 [1.161]
 [0.77 ]
 [0.7  ]
 [0.6  ]
 [1.175]]
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.4551662041718851, 0.26612004074141626, 0.1997635862605272, 0.07064829584538697, 0.008145629054523049, 0.0001562439262614978]
siam score:  -0.6968784
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.621]
 [0.667]
 [0.588]
 [0.588]
 [0.588]
 [0.632]] [[1.96 ]
 [2.216]
 [2.117]
 [1.96 ]
 [1.96 ]
 [1.96 ]
 [2.648]] [[0.953]
 [1.103]
 [1.162]
 [0.953]
 [0.953]
 [0.953]
 [1.27 ]]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.622]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[-0.361]
 [-0.132]
 [-0.361]
 [-0.361]
 [-0.361]
 [-0.361]
 [-0.361]] [[0.158]
 [0.322]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]]
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.4551662041718851, 0.26612004074141626, 0.1997635862605272, 0.07064829584538697, 0.008145629054523049, 0.0001562439262614978]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.642]
 [0.632]
 [0.634]
 [0.628]
 [0.628]
 [0.623]] [[3.873]
 [3.56 ]
 [3.74 ]
 [3.816]
 [3.59 ]
 [3.755]
 [3.593]] [[1.5  ]
 [1.255]
 [1.398]
 [1.466]
 [1.263]
 [1.406]
 [1.259]]
3892 6347
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.876]
 [0.855]
 [0.918]
 [0.938]
 [0.855]
 [0.906]] [[3.484]
 [3.85 ]
 [4.01 ]
 [4.597]
 [3.612]
 [4.01 ]
 [4.258]] [[0.731]
 [1.024]
 [1.036]
 [1.357]
 [1.069]
 [1.036]
 [1.22 ]]
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.803]
 [0.638]
 [0.618]
 [0.614]
 [0.752]
 [0.629]] [[1.327]
 [1.558]
 [1.538]
 [1.499]
 [1.497]
 [1.556]
 [1.494]] [[0.538]
 [0.598]
 [0.259]
 [0.207]
 [0.198]
 [0.494]
 [0.228]]
Printing some Q and Qe and total Qs values:  [[1.109]
 [1.15 ]
 [1.109]
 [1.109]
 [1.109]
 [1.109]
 [1.109]] [[-0.824]
 [-0.655]
 [-0.824]
 [-0.824]
 [-0.824]
 [-0.824]
 [-0.824]] [[1.996]
 [2.19 ]
 [1.996]
 [1.996]
 [1.996]
 [1.996]
 [1.996]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  61 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.4  ]
 [0.733]
 [0.685]
 [0.687]
 [0.624]
 [0.628]] [[-0.108]
 [-0.055]
 [-0.362]
 [-0.435]
 [-0.06 ]
 [ 0.161]
 [-0.108]] [[0.995]
 [0.759]
 [0.915]
 [0.797]
 [1.105]
 [1.209]
 [0.995]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  72 total reward:  0.46499999999999964  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
probs:  [0.45534243352057463, 0.2685868165877408, 0.1978692549380671, 0.06997834752195325, 0.008068385145620652, 0.00015476228604366902]
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.936]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.885]] [[3.256]
 [3.522]
 [3.256]
 [3.256]
 [3.256]
 [3.256]
 [3.531]] [[1.597]
 [1.752]
 [1.597]
 [1.597]
 [1.597]
 [1.597]
 [1.731]]
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.356]
 [0.362]] [[-2.609]
 [-2.609]
 [-2.609]
 [-2.609]
 [-2.609]
 [-2.715]
 [-2.609]] [[0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.211]
 [0.26 ]]
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.221]
 [0.513]
 [0.506]
 [0.082]
 [0.489]
 [0.389]] [[-1.712]
 [-1.992]
 [-4.212]
 [-4.084]
 [-1.689]
 [-4.059]
 [-1.572]] [[1.124]
 [0.948]
 [0.012]
 [0.072]
 [1.016]
 [0.075]
 [1.259]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.7149999999999999  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  56 total reward:  0.5549999999999997  reward:  1.0 rdn_beta:  0.333
from probs:  [0.4566139098996145, 0.2708236993677693, 0.19535469512637982, 0.06908904948309388, 0.007965850585419562, 0.00015279553772305668]
maxi score, test score, baseline:  -0.8946800000000001 -0.8047500000000001 -0.8047500000000001
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.563]
 [0.546]
 [0.561]
 [0.555]
 [0.546]
 [0.545]] [[2.516]
 [2.582]
 [2.487]
 [2.497]
 [2.437]
 [2.647]
 [2.233]] [[0.983]
 [1.06 ]
 [0.963]
 [1.   ]
 [0.947]
 [1.07 ]
 [0.791]]
line 256 mcts: sample exp_bonus 2.4914452715951874
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  81 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  56 total reward:  0.6849999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.89713 -0.8047500000000001 -0.8047500000000001
3905 6368
actor:  1 policy actor:  1  step number:  74 total reward:  0.5449999999999997  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.467]
 [0.459]
 [0.466]
 [0.466]
 [0.43 ]
 [0.469]] [[0.62 ]
 [0.37 ]
 [0.423]
 [0.839]
 [0.903]
 [1.105]
 [0.947]] [[0.459]
 [0.175]
 [0.22 ]
 [0.696]
 [0.768]
 [0.933]
 [0.821]]
maxi score, test score, baseline:  -0.89713 -0.8047500000000001 -0.8047500000000001
probs:  [0.45540292690285916, 0.27673612006837445, 0.1919850155208312, 0.06789733018062556, 0.007828447364876687, 0.00015015996243293562]
actor:  1 policy actor:  1  step number:  49 total reward:  0.7099999999999999  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.1475836620092754
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.25544711029736206
Printing some Q and Qe and total Qs values:  [[0.879]
 [0.879]
 [0.893]
 [0.879]
 [0.879]
 [0.879]
 [0.879]] [[2.463]
 [2.463]
 [3.031]
 [2.463]
 [2.463]
 [2.463]
 [2.463]] [[1.517]
 [1.517]
 [1.725]
 [1.517]
 [1.517]
 [1.517]
 [1.517]]
from probs:  [0.45912992373529243, 0.27484225266829987, 0.190671149582016, 0.06743266897142232, 0.007774872713133083, 0.00014913232983630208]
Printing some Q and Qe and total Qs values:  [[1.132]
 [1.132]
 [1.132]
 [1.132]
 [1.132]
 [1.132]
 [1.132]] [[-0.92 ]
 [-0.908]
 [-0.92 ]
 [-0.92 ]
 [-0.92 ]
 [-0.92 ]
 [-0.92 ]] [[1.907]
 [1.922]
 [1.907]
 [1.907]
 [1.907]
 [1.907]
 [1.907]]
first move QE:  -0.9198648394735562
maxi score, test score, baseline:  -0.89713 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.562]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[1.744]
 [2.584]
 [1.744]
 [1.744]
 [1.744]
 [1.744]
 [1.744]] [[0.543]
 [0.562]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]]
line 256 mcts: sample exp_bonus 3.93403417748294
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.229]
 [0.572]
 [0.522]
 [0.529]
 [0.499]
 [0.523]] [[-2.529]
 [-1.039]
 [-2.894]
 [-2.652]
 [-2.806]
 [-2.722]
 [-2.79 ]] [[0.058]
 [0.059]
 [0.127]
 [0.107]
 [0.069]
 [0.039]
 [0.063]]
maxi score, test score, baseline:  -0.89713 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.877]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]] [[0.215]
 [0.971]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]] [[1.601]
 [1.906]
 [1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.601]]
Printing some Q and Qe and total Qs values:  [[0.887]
 [0.626]
 [0.759]
 [0.595]
 [0.473]
 [0.674]
 [0.595]] [[ 1.285]
 [-1.639]
 [-3.605]
 [-2.8  ]
 [-2.191]
 [-3.812]
 [-1.62 ]] [[1.446]
 [0.626]
 [0.28 ]
 [0.349]
 [0.403]
 [0.178]
 [0.61 ]]
start point for exploration sampling:  10749
3913 6381
actor:  1 policy actor:  1  step number:  97 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.186]
 [0.159]
 [0.182]
 [0.182]
 [0.143]
 [0.158]] [[ 0.   ]
 [-4.221]
 [-4.575]
 [ 0.   ]
 [ 0.   ]
 [-4.566]
 [-4.167]] [[ 3.02 ]
 [-0.2  ]
 [-0.501]
 [ 3.02 ]
 [ 3.02 ]
 [-0.513]
 [-0.191]]
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.718]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.739]] [[-0.621]
 [ 1.755]
 [-0.621]
 [-0.621]
 [-0.621]
 [-0.621]
 [ 0.189]] [[0.971]
 [1.536]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [1.169]]
siam score:  -0.68608797
actor:  1 policy actor:  1  step number:  124 total reward:  0.2849999999999995  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus -0.25226763714074946
Printing some Q and Qe and total Qs values:  [[ 0.075]
 [-0.017]
 [ 0.075]
 [ 0.075]
 [ 0.075]
 [-0.08 ]
 [ 0.075]] [[4.492]
 [4.368]
 [4.492]
 [4.492]
 [4.492]
 [0.284]
 [4.492]] [[ 1.61 ]
 [ 1.491]
 [ 1.61 ]
 [ 1.61 ]
 [ 1.61 ]
 [-0.41 ]
 [ 1.61 ]]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.285]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]] [[2.445]
 [3.09 ]
 [2.445]
 [2.445]
 [2.445]
 [2.445]
 [2.445]] [[0.223]
 [0.554]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]]
actor:  1 policy actor:  1  step number:  86 total reward:  0.05499999999999927  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.89713 -0.8047500000000001 -0.8047500000000001
using another actor
from probs:  [0.45454995983923285, 0.2721006156517193, 0.19227586946133138, 0.07322859442144221, 0.007697315937846229, 0.0001476446884278991]
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[-2.565]
 [-2.644]
 [-2.644]
 [-2.644]
 [-2.644]
 [-2.644]
 [-2.644]] [[0.669]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]]
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[-2.017]
 [-2.343]
 [-2.343]
 [-2.343]
 [-2.343]
 [-2.343]
 [-2.343]] [[0.752]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
actor:  1 policy actor:  1  step number:  100 total reward:  0.26499999999999946  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  52 total reward:  0.49499999999999966  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.89713 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.375]
 [0.388]
 [0.368]
 [0.368]
 [0.37 ]
 [0.37 ]] [[-3.623]
 [-1.445]
 [-3.258]
 [-3.202]
 [-3.218]
 [-3.073]
 [-3.086]] [[0.373]
 [0.375]
 [0.388]
 [0.368]
 [0.368]
 [0.37 ]
 [0.37 ]]
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.686]
 [0.695]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[2.391]
 [2.391]
 [3.444]
 [2.391]
 [2.391]
 [2.391]
 [2.391]] [[0.686]
 [0.686]
 [0.695]
 [0.686]
 [0.686]
 [0.686]
 [0.686]]
3924 6400
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.598]
 [0.538]
 [0.538]
 [0.538]
 [0.636]
 [0.547]] [[-3.748]
 [-3.563]
 [-3.748]
 [-3.748]
 [-3.748]
 [-1.084]
 [-3.653]] [[0.538]
 [0.598]
 [0.538]
 [0.538]
 [0.538]
 [0.636]
 [0.547]]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]] [[-5.707]
 [-5.707]
 [-5.707]
 [-5.707]
 [-5.707]
 [-5.707]
 [-5.707]] [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]]
maxi score, test score, baseline:  -0.89713 -0.8047500000000001 -0.8047500000000001
Printing some Q and Qe and total Qs values:  [[0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]] [[-5.751]
 [-5.751]
 [-5.751]
 [-5.751]
 [-5.751]
 [-5.751]
 [-5.751]] [[0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.681]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]] [[-2.007]
 [-1.233]
 [-2.007]
 [-2.007]
 [-2.007]
 [-2.007]
 [-2.007]] [[0.666]
 [0.681]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.662]
 [0.49 ]] [[-5.518]
 [-5.518]
 [-5.518]
 [-5.518]
 [-5.518]
 [-5.343]
 [-5.518]] [[0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.662]
 [0.49 ]]
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.602]
 [0.485]] [[-5.198]
 [-5.198]
 [-5.198]
 [-5.198]
 [-5.198]
 [-4.162]
 [-5.198]] [[0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.602]
 [0.485]]
from probs:  [0.45572507872049944, 0.26984266405704405, 0.19403146861618126, 0.07262092721292196, 0.007633441893478666, 0.00014641949987445678]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
3925 6404
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.883]
 [0.797]
 [0.741]
 [0.797]
 [0.797]
 [0.883]] [[1.77 ]
 [2.048]
 [1.77 ]
 [2.367]
 [1.77 ]
 [1.77 ]
 [1.992]] [[0.797]
 [0.883]
 [0.797]
 [0.741]
 [0.797]
 [0.797]
 [0.883]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.648]
 [0.49 ]] [[-5.524]
 [-5.524]
 [-5.524]
 [-5.524]
 [-5.524]
 [-5.363]
 [-5.524]] [[0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.648]
 [0.49 ]]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.559]
 [0.63 ]
 [0.666]
 [0.627]
 [0.624]
 [0.63 ]] [[-1.091]
 [-0.773]
 [-0.903]
 [-1.158]
 [-1.093]
 [-1.103]
 [-1.028]] [[0.672]
 [0.559]
 [0.63 ]
 [0.666]
 [0.627]
 [0.624]
 [0.63 ]]
Printing some Q and Qe and total Qs values:  [[0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.479]
 [0.954]] [[-0.355]
 [-0.355]
 [-0.355]
 [-0.355]
 [-0.355]
 [-0.932]
 [-0.355]] [[0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.479]
 [0.954]]
actor:  0 policy actor:  1  step number:  38 total reward:  0.7349999999999999  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.53 ]
 [0.529]
 [0.505]
 [0.49 ]
 [0.536]
 [0.505]] [[-5.142]
 [-4.303]
 [-4.99 ]
 [-5.025]
 [-5.619]
 [-6.193]
 [-5.458]] [[0.501]
 [0.53 ]
 [0.529]
 [0.505]
 [0.49 ]
 [0.536]
 [0.505]]
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]] [[3.183]
 [3.183]
 [3.183]
 [3.183]
 [3.183]
 [3.183]
 [3.183]] [[1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.699]]
actor:  0 policy actor:  1  step number:  39 total reward:  0.7199999999999999  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  40 total reward:  0.6949999999999998  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  40 total reward:  0.6549999999999998  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  42 total reward:  0.7149999999999999  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.495]
 [0.484]] [[-2.904]
 [-2.904]
 [-2.904]
 [-2.904]
 [-2.904]
 [-2.282]
 [-2.904]] [[0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.495]
 [0.484]]
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.575]
 [0.524]] [[-2.114]
 [-2.114]
 [-2.114]
 [-2.114]
 [-2.114]
 [-1.87 ]
 [-1.966]] [[0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.575]
 [0.524]]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.88275 -0.8047500000000001 -0.8047500000000001
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  61 total reward:  0.5299999999999997  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.613]
 [0.513]] [[-2.444]
 [-2.444]
 [-2.444]
 [-2.444]
 [-2.444]
 [-1.933]
 [-2.444]] [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.613]
 [0.513]]
rdn probs:  [0.45572507872049944, 0.26984266405704405, 0.19403146861618126, 0.07262092721292196, 0.007633441893478666, 0.00014641949987445678]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.531]
 [0.542]
 [0.527]
 [0.542]
 [0.541]
 [0.547]] [[-1.672]
 [-0.685]
 [-1.462]
 [-2.119]
 [-1.634]
 [-1.602]
 [-1.084]] [[0.519]
 [0.531]
 [0.542]
 [0.527]
 [0.542]
 [0.541]
 [0.547]]
from probs:  [0.4699400291110989, 0.2645444775002823, 0.18973012240494772, 0.06845299460168548, 0.007082340103284358, 0.000250036278701313]
maxi score, test score, baseline:  -0.87969 -0.49750000000000005 -0.49750000000000005
from probs:  [0.4699400291110989, 0.2645444775002823, 0.18973012240494772, 0.06845299460168548, 0.007082340103284358, 0.000250036278701313]
maxi score, test score, baseline:  -0.87969 -0.49750000000000005 -0.49750000000000005
maxi score, test score, baseline:  -0.87969 -0.49750000000000005 -0.49750000000000005
probs:  [0.4699400291110989, 0.2645444775002823, 0.18973012240494772, 0.06845299460168548, 0.007082340103284358, 0.000250036278701313]
line 256 mcts: sample exp_bonus 3.4147302277969533
maxi score, test score, baseline:  -0.87969 -0.49750000000000005 -0.49750000000000005
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.268]
 [0.341]
 [0.336]
 [0.316]
 [0.31 ]
 [0.335]] [[0.961]
 [1.483]
 [3.379]
 [0.964]
 [3.223]
 [1.528]
 [1.806]] [[-1.024]
 [-0.71 ]
 [ 0.07 ]
 [-0.746]
 [-0.032]
 [-0.609]
 [-0.468]]
maxi score, test score, baseline:  -0.87969 -0.49750000000000005 -0.49750000000000005
maxi score, test score, baseline:  -0.87969 -0.49750000000000005 -0.49750000000000005
probs:  [0.4699400291110989, 0.2645444775002823, 0.18973012240494766, 0.06845299460168548, 0.007082340103284358, 0.000250036278701313]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.627312830105
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.282]
 [0.315]
 [0.315]
 [0.315]
 [0.507]] [[0.72 ]
 [0.72 ]
 [3.832]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [3.547]] [[0.891]
 [0.891]
 [1.805]
 [0.891]
 [0.891]
 [0.891]
 [1.855]]
maxi score, test score, baseline:  -0.87969 -0.49750000000000005 -0.49750000000000005
maxi score, test score, baseline:  -0.87969 -0.49750000000000005 -0.49750000000000005
probs:  [0.4735078513520511, 0.2589608404235071, 0.1911705686503449, 0.06897269520489147, 0.007136109795102869, 0.00025193457410271317]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.546]
 [0.508]] [[-4.836]
 [-4.836]
 [-4.836]
 [-4.836]
 [-4.836]
 [-4.479]
 [-4.836]] [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.546]
 [0.508]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.87969 -0.49750000000000005 -0.49750000000000005
actor:  1 policy actor:  1  step number:  82 total reward:  0.3349999999999995  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.9433316654401064
maxi score, test score, baseline:  -0.87969 -0.49750000000000005 -0.49750000000000005
probs:  [0.47186007039348454, 0.26153961543899107, 0.19050530571550467, 0.0687326740658647, 0.00711127653034277, 0.00025105785581213856]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.751]
 [0.705]
 [0.751]
 [0.751]
 [0.594]
 [0.751]] [[2.275]
 [2.275]
 [1.346]
 [2.275]
 [2.275]
 [2.456]
 [2.275]] [[1.231]
 [1.231]
 [0.211]
 [1.231]
 [1.231]
 [1.099]
 [1.231]]
actor:  0 policy actor:  1  step number:  82 total reward:  0.034999999999999254  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.373]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[4.186]
 [3.908]
 [4.186]
 [4.186]
 [4.186]
 [4.186]
 [4.186]] [[1.19 ]
 [0.965]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]]
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
probs:  [0.47186007039348454, 0.26153961543899107, 0.19050530571550467, 0.0687326740658647, 0.00711127653034277, 0.00025105785581213856]
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.153]
 [0.083]
 [0.089]
 [0.089]
 [0.089]
 [0.077]] [[3.136]
 [3.496]
 [2.992]
 [3.994]
 [3.994]
 [3.994]
 [3.033]] [[1.179]
 [1.349]
 [1.127]
 [1.507]
 [1.507]
 [1.507]
 [1.14 ]]
siam score:  -0.6877948
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.76 ]
 [0.484]
 [0.463]
 [0.47 ]
 [0.463]
 [0.463]] [[1.772]
 [0.735]
 [2.021]
 [1.772]
 [1.899]
 [1.772]
 [1.772]] [[0.896]
 [0.452]
 [1.187]
 [0.896]
 [1.038]
 [0.896]
 [0.896]]
actor:  1 policy actor:  1  step number:  159 total reward:  0.009999999999999232  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.04 ]
 [0.039]
 [0.039]
 [0.036]
 [0.034]
 [0.035]] [[-8.685]
 [-3.272]
 [-6.643]
 [-6.743]
 [-8.32 ]
 [-6.49 ]
 [-6.497]] [[0.214]
 [0.04 ]
 [0.039]
 [0.039]
 [0.036]
 [0.034]
 [0.035]]
from probs:  [0.471250634747778, 0.2612018213037934, 0.19155081677410285, 0.06864390168560049, 0.007102091889225642, 0.00025073359949958604]
start point for exploration sampling:  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]] [[-6.76]
 [-6.76]
 [-6.76]
 [-6.76]
 [-6.76]
 [-6.76]
 [-6.76]] [[0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]]
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
probs:  [0.471250634747778, 0.2612018213037934, 0.19155081677410288, 0.06864390168560049, 0.007102091889225642, 0.00025073359949958604]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  134 total reward:  0.2049999999999994  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
probs:  [0.47008727593442434, 0.2605570021386363, 0.19354660503008644, 0.06847444305335554, 0.0070845592206553384, 0.00025011462284199945]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.453]
 [0.43 ]
 [0.443]
 [0.449]
 [0.439]
 [0.445]] [[2.223]
 [2.415]
 [2.506]
 [2.329]
 [2.296]
 [2.323]
 [2.11 ]] [[0.891]
 [1.094]
 [1.139]
 [0.989]
 [0.968]
 [0.975]
 [0.775]]
siam score:  -0.6954157
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.324]
 [0.325]
 [0.324]
 [0.321]
 [0.32 ]
 [0.322]] [[-6.805]
 [-5.059]
 [-5.166]
 [-4.782]
 [-5.047]
 [-5.072]
 [-4.787]] [[0.33 ]
 [0.324]
 [0.325]
 [0.324]
 [0.321]
 [0.32 ]
 [0.322]]
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.452]
 [0.449]
 [0.452]
 [0.452]
 [0.45 ]
 [0.452]] [[2.681]
 [2.681]
 [2.508]
 [2.681]
 [2.681]
 [2.544]
 [2.681]] [[1.192]
 [1.192]
 [1.015]
 [1.192]
 [1.192]
 [1.053]
 [1.192]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]] [[-0.95]
 [-0.95]
 [-0.95]
 [-0.95]
 [-0.95]
 [-0.95]
 [-0.95]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.6930171
line 256 mcts: sample exp_bonus -1.2827109113418338
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[-1.248]
 [-1.981]
 [-1.981]
 [-1.981]
 [-1.981]
 [-1.981]
 [-1.981]] [[0.752]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]]
line 256 mcts: sample exp_bonus -2.8368177931812593
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
probs:  [0.47008727593442434, 0.2605570021386363, 0.19354660503008644, 0.06847444305335554, 0.0070845592206553384, 0.00025011462284199945]
actor:  1 policy actor:  1  step number:  122 total reward:  0.22499999999999942  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  42 total reward:  0.7049999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.579]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[-0.443]
 [-0.443]
 [ 1.003]
 [-0.443]
 [-0.443]
 [-0.443]
 [-0.443]] [[1.468]
 [1.468]
 [1.991]
 [1.468]
 [1.468]
 [1.468]
 [1.468]]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.487]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]] [[2.145]
 [2.418]
 [2.145]
 [2.145]
 [2.145]
 [2.145]
 [2.145]] [[1.097]
 [1.269]
 [1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.097]]
siam score:  -0.68875414
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
probs:  [0.4655045815946067, 0.26517976643934, 0.19424556807157056, 0.06780691287615807, 0.007015494663704579, 0.00024767635462015643]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]] [[-6.673]
 [-6.673]
 [-6.673]
 [-6.673]
 [-6.673]
 [-6.673]
 [-6.673]] [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[-5.087]
 [-5.62 ]
 [-5.62 ]
 [-5.62 ]
 [-5.62 ]
 [-5.62 ]
 [-5.62 ]] [[0.013]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.167
siam score:  -0.6816463
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.244]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]] [[-4.571]
 [-2.264]
 [-4.571]
 [-4.571]
 [-4.571]
 [-4.571]
 [-4.571]] [[-0.281]
 [ 0.664]
 [-0.281]
 [-0.281]
 [-0.281]
 [-0.281]
 [-0.281]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.2949999999999995  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  70 total reward:  0.46499999999999964  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]] [[3.707]
 [3.707]
 [3.707]
 [3.707]
 [3.707]
 [3.707]
 [3.707]] [[1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]]
from probs:  [0.4717746075863078, 0.26206901193931387, 0.1919669241044903, 0.06701148771158151, 0.006933197730239181, 0.00024477092806733175]
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.303]
 [0.107]
 [0.102]
 [0.067]
 [0.055]
 [0.268]] [[3.668]
 [3.84 ]
 [3.79 ]
 [3.67 ]
 [3.678]
 [3.709]
 [3.902]] [[0.185]
 [0.653]
 [0.304]
 [0.2  ]
 [0.152]
 [0.156]
 [0.646]]
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[-9.748]
 [-7.704]
 [-7.704]
 [-7.704]
 [-7.704]
 [-7.704]
 [-7.704]] [[0.169]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]]
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
actor:  1 policy actor:  1  step number:  54 total reward:  0.4349999999999996  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.713]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[3.3  ]
 [3.056]
 [3.3  ]
 [3.3  ]
 [3.3  ]
 [3.3  ]
 [3.3  ]] [[1.489]
 [1.401]
 [1.489]
 [1.489]
 [1.489]
 [1.489]
 [1.489]]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.284]
 [0.319]
 [0.325]
 [0.321]
 [0.321]
 [0.313]] [[-2.599]
 [-0.453]
 [-2.041]
 [-2.083]
 [-2.074]
 [-1.993]
 [-1.911]] [[0.314]
 [0.284]
 [0.319]
 [0.325]
 [0.321]
 [0.321]
 [0.313]]
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.298]
 [0.127]
 [0.285]
 [0.203]
 [0.32 ]
 [0.343]] [[0.303]
 [0.74 ]
 [0.594]
 [1.195]
 [1.187]
 [0.964]
 [0.671]] [[0.511]
 [0.94 ]
 [0.673]
 [1.274]
 [1.193]
 [1.131]
 [0.929]]
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
probs:  [0.4676846740801671, 0.2619868655492216, 0.19616225892879974, 0.06699048274128959, 0.006931024496703783, 0.00024469420381828197]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.655]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.663]] [[4.589]
 [3.99 ]
 [4.589]
 [4.589]
 [4.589]
 [4.589]
 [4.565]] [[1.19 ]
 [1.026]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.371]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.524]
 [0.55 ]
 [0.534]
 [0.534]
 [0.585]
 [0.557]] [[3.31 ]
 [3.124]
 [3.13 ]
 [3.017]
 [3.017]
 [3.384]
 [3.26 ]] [[0.142]
 [0.037]
 [0.092]
 [0.022]
 [0.022]
 [0.245]
 [0.148]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.311]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[3.442]
 [3.952]
 [3.442]
 [3.442]
 [3.442]
 [3.442]
 [3.442]] [[-0.076]
 [ 0.385]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.7499999999999999  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.535]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.213]] [[-0.918]
 [-0.318]
 [-3.418]
 [-3.418]
 [-3.418]
 [-3.418]
 [-1.223]] [[1.101]
 [1.561]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [1.191]]
siam score:  -0.706665
actor:  1 policy actor:  1  step number:  46 total reward:  0.6849999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.415]
 [0.312]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[1.408]
 [1.408]
 [4.136]
 [1.408]
 [1.408]
 [1.408]
 [1.408]] [[0.679]
 [0.679]
 [1.435]
 [0.679]
 [0.679]
 [0.679]
 [0.679]]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.756]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[0.083]
 [0.083]
 [2.574]
 [0.083]
 [0.083]
 [0.083]
 [0.083]] [[-0.023]
 [-0.023]
 [ 1.223]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.504]
 [0.527]
 [0.531]
 [0.522]
 [0.511]
 [0.508]] [[-4.068]
 [-1.645]
 [-2.789]
 [-3.169]
 [-2.727]
 [-2.893]
 [-2.596]] [[0.523]
 [0.504]
 [0.527]
 [0.531]
 [0.522]
 [0.511]
 [0.508]]
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
probs:  [0.4677642850195249, 0.26576029896102094, 0.1933663199342905, 0.06603565430497273, 0.006832235250657904, 0.0002412065295331166]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.354]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]] [[-4.527]
 [-4.14 ]
 [-4.527]
 [-4.527]
 [-4.527]
 [-4.527]
 [-4.527]] [[0.124]
 [0.48 ]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]]
using another actor
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
probs:  [0.4677642850195249, 0.26576029896102094, 0.1933663199342905, 0.06603565430497273, 0.006832235250657904, 0.0002412065295331166]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.5087583168609013
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
actor:  1 policy actor:  1  step number:  146 total reward:  0.06499999999999928  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.431]
 [0.334]
 [0.333]
 [0.37 ]
 [0.37 ]
 [0.345]] [[-1.854]
 [ 2.052]
 [-1.657]
 [-2.186]
 [ 1.067]
 [ 1.067]
 [-2.191]] [[-0.156]
 [ 1.093]
 [-0.104]
 [-0.266]
 [ 0.754]
 [ 0.754]
 [-0.261]]
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.603]
 [0.605]
 [0.582]
 [0.576]
 [0.587]
 [0.58 ]] [[3.763]
 [3.732]
 [3.815]
 [3.763]
 [3.675]
 [3.732]
 [3.676]] [[0.772]
 [0.794]
 [0.854]
 [0.772]
 [0.703]
 [0.763]
 [0.712]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.34499999999999953  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus -2.3834609412592416
UNIT TEST: sample policy line 217 mcts : [0.02  0.388 0.061 0.02  0.102 0.122 0.286]
line 256 mcts: sample exp_bonus -2.400592768721966
maxi score, test score, baseline:  -0.8776200000000001 -0.49750000000000005 -0.49750000000000005
siam score:  -0.7210126
using another actor
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.395]
 [0.442]
 [0.446]
 [0.458]
 [0.455]
 [0.453]] [[-1.283]
 [ 0.492]
 [-1.057]
 [-1.256]
 [-1.197]
 [-1.15 ]
 [-0.725]] [[0.443]
 [0.395]
 [0.442]
 [0.446]
 [0.458]
 [0.455]
 [0.453]]
siam score:  -0.7265623
actor:  0 policy actor:  1  step number:  62 total reward:  0.35499999999999954  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  97 total reward:  0.22999999999999943  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7853684189117305
3983 6470
3984 6470
Printing some Q and Qe and total Qs values:  [[-0.067]
 [ 0.006]
 [ 0.346]
 [ 0.153]
 [-0.07 ]
 [ 0.291]
 [-0.   ]] [[-1.342]
 [-1.392]
 [-4.245]
 [-4.338]
 [-1.455]
 [-4.003]
 [-1.54 ]] [[ 0.598]
 [ 0.632]
 [-0.129]
 [-0.297]
 [ 0.557]
 [-0.083]
 [ 0.575]]
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.318]
 [0.12 ]] [[0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [1.935]
 [0.684]] [[-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [ 0.824]
 [-0.05 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8935879790440868
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
using another actor
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.29 ]
 [0.276]
 [0.288]
 [0.277]
 [0.291]
 [0.288]] [[3.619]
 [3.551]
 [3.69 ]
 [3.57 ]
 [3.629]
 [3.627]
 [3.681]] [[0.414]
 [0.396]
 [0.464]
 [0.406]
 [0.428]
 [0.444]
 [0.473]]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.374]
 [0.354]
 [0.444]
 [0.444]
 [0.546]
 [0.444]] [[1.881]
 [1.94 ]
 [1.51 ]
 [1.881]
 [1.881]
 [3.72 ]
 [1.881]] [[ 0.164]
 [ 0.081]
 [-0.183]
 [ 0.164]
 [ 0.164]
 [ 1.326]
 [ 0.164]]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.46547629820246406, 0.26597929817630106, 0.1957929078347839, 0.06571265250397063, 0.0067988165723081645, 0.00024002671017230815]
actor:  1 policy actor:  1  step number:  47 total reward:  0.6399999999999998  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.207]
 [ 0.331]
 [-0.005]
 [ 0.165]
 [-0.019]
 [ 0.201]
 [ 0.244]] [[-0.09 ]
 [ 1.318]
 [ 0.479]
 [-0.318]
 [-0.604]
 [-0.214]
 [ 0.801]] [[-0.044]
 [ 1.142]
 [-0.089]
 [-0.279]
 [-0.839]
 [-0.137]
 [ 0.624]]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.46260286768663156, 0.270510479445934, 0.19458425914884936, 0.0653070019011142, 0.006756846815553932, 0.00023854500191715657]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.46260286768663156, 0.270510479445934, 0.19458425914884936, 0.0653070019011142, 0.006756846815553932, 0.00023854500191715657]
line 256 mcts: sample exp_bonus 1.3753436702585349
start point for exploration sampling:  10749
actor:  1 policy actor:  1  step number:  68 total reward:  0.3349999999999995  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  81 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.5
siam score:  -0.72845596
using another actor
siam score:  -0.7265122
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]] [[6.375]
 [4.044]
 [4.044]
 [4.044]
 [4.044]
 [4.044]
 [4.044]] [[1.515]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]]
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]] [[3.939]
 [4.1  ]
 [4.1  ]
 [4.1  ]
 [4.1  ]
 [4.1  ]
 [4.1  ]] [[0.441]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.633 0.163 0.02  0.041 0.102 0.02 ]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.729]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]] [[-0.009]
 [ 2.185]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[0.702]
 [0.729]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]]
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.74 ]
 [0.683]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]] [[2.871]
 [2.871]
 [3.989]
 [2.871]
 [2.871]
 [2.871]
 [2.871]] [[0.74 ]
 [0.74 ]
 [0.683]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.4626066058773551, 0.26400960771613435, 0.19777427961958202, 0.06861405844125443, 0.0067569014161278805, 0.00023854692954617833]
line 256 mcts: sample exp_bonus 3.9359423665678825
actor:  1 policy actor:  1  step number:  47 total reward:  0.5499999999999997  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5072215920843716
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.23209308355983047
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]] [[-8.09 ]
 [-8.824]
 [-8.824]
 [-8.824]
 [-8.824]
 [-8.824]
 [-8.824]] [[0.597]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
line 256 mcts: sample exp_bonus -8.221007837918702
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]] [[-9.016]
 [-8.808]
 [-8.808]
 [-8.808]
 [-8.808]
 [-8.808]
 [-8.808]] [[0.268]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.532]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[2.533]
 [2.533]
 [3.268]
 [2.533]
 [2.533]
 [2.533]
 [2.533]] [[1.356]
 [1.356]
 [1.661]
 [1.356]
 [1.356]
 [1.356]
 [1.356]]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.46540517885963195, 0.2626347300877591, 0.19674433440331035, 0.06825673836203842, 0.006721713633855351, 0.00023730465340483694]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.46540517885963195, 0.2626347300877591, 0.19674433440331035, 0.06825673836203842, 0.006721713633855351, 0.00023730465340483694]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.46540517885963195, 0.2626347300877591, 0.19674433440331035, 0.06825673836203842, 0.006721713633855351, 0.00023730465340483694]
from probs:  [0.46540517885963195, 0.2626347300877591, 0.19674433440331035, 0.06825673836203842, 0.006721713633855351, 0.00023730465340483694]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.46540517885963195, 0.2626347300877591, 0.19674433440331035, 0.06825673836203842, 0.006721713633855351, 0.00023730465340483694]
from probs:  [0.46540517885963195, 0.2626347300877591, 0.19674433440331035, 0.06825673836203842, 0.006721713633855351, 0.00023730465340483694]
from probs:  [0.46540517885963195, 0.2626347300877591, 0.19674433440331035, 0.06825673836203842, 0.006721713633855351, 0.00023730465340483694]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.021]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]] [[-0.964]
 [-0.793]
 [-0.964]
 [-0.964]
 [-0.964]
 [-0.964]
 [-0.964]] [[1.578]
 [1.589]
 [1.578]
 [1.578]
 [1.578]
 [1.578]
 [1.578]]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.103]
 [0.394]
 [0.103]
 [0.103]
 [0.103]
 [0.245]] [[-0.641]
 [-0.641]
 [-0.521]
 [-0.641]
 [-0.641]
 [-0.641]
 [-0.072]] [[1.26 ]
 [1.26 ]
 [1.404]
 [1.26 ]
 [1.26 ]
 [1.26 ]
 [1.462]]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.46540517885963195, 0.2626347300877591, 0.19674433440331035, 0.06825673836203842, 0.006721713633855351, 0.00023730465340483694]
Printing some Q and Qe and total Qs values:  [[ 0.119]
 [-0.016]
 [ 0.111]
 [ 0.103]
 [ 0.101]
 [ 0.104]
 [ 0.11 ]] [[-0.91 ]
 [-0.617]
 [-0.548]
 [-0.913]
 [-0.828]
 [-0.796]
 [-0.82 ]] [[-0.542]
 [-0.518]
 [-0.195]
 [-0.576]
 [-0.495]
 [-0.457]
 [-0.47 ]]
Printing some Q and Qe and total Qs values:  [[ 0.345]
 [ 0.099]
 [-0.007]
 [ 0.345]
 [ 0.345]
 [ 0.345]
 [ 0.13 ]] [[0.589]
 [2.105]
 [0.382]
 [0.589]
 [0.589]
 [0.589]
 [1.1  ]] [[0.566]
 [0.976]
 [0.2  ]
 [0.566]
 [0.566]
 [0.566]
 [0.598]]
from probs:  [0.46540517885963195, 0.2626347300877591, 0.19674433440331035, 0.06825673836203842, 0.006721713633855351, 0.00023730465340483694]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[2.103]
 [2.103]
 [2.103]
 [2.103]
 [2.103]
 [2.103]
 [2.103]] [[0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]]
using another actor
using explorer policy with actor:  0
siam score:  -0.7360823
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.3  ]
 [0.318]
 [0.314]
 [0.32 ]
 [0.327]
 [0.323]] [[-3.967]
 [-1.292]
 [-4.416]
 [-4.285]
 [-4.127]
 [-4.111]
 [-4.125]] [[-0.023]
 [ 0.578]
 [-0.127]
 [-0.098]
 [-0.06 ]
 [-0.054]
 [-0.059]]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.465405178859632, 0.2626347300877591, 0.19674433440331032, 0.06825673836203844, 0.006721713633855352, 0.00023730465340483697]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.275]
 [0.393]] [[0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [4.875]
 [0.669]] [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [1.654]
 [0.025]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6349999999999998  reward:  1.0 rdn_beta:  0.333
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.587]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[4.059]
 [3.265]
 [4.059]
 [4.059]
 [4.059]
 [4.059]
 [4.059]] [[0.867]
 [0.456]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]]
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]] [[-0.996]
 [-0.996]
 [-0.996]
 [-0.996]
 [-0.996]
 [-0.996]
 [-0.996]] [[0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.793]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.813]] [[3.584]
 [3.419]
 [3.421]
 [3.421]
 [3.421]
 [3.421]
 [3.935]] [[0.801]
 [0.793]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.813]]
line 256 mcts: sample exp_bonus 0.23609957017581854
siam score:  -0.72706014
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.358]
 [0.317]
 [0.307]
 [0.306]
 [0.306]
 [0.306]] [[-4.166]
 [-0.813]
 [-4.783]
 [-4.974]
 [-4.99 ]
 [-4.943]
 [-4.791]] [[0.328]
 [0.358]
 [0.317]
 [0.307]
 [0.306]
 [0.306]
 [0.306]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.509]
 [0.409]
 [0.419]
 [0.419]
 [0.42 ]
 [0.427]] [[2.503]
 [2.185]
 [2.423]
 [2.412]
 [2.412]
 [2.531]
 [2.454]] [[0.409]
 [0.429]
 [0.387]
 [0.399]
 [0.399]
 [0.481]
 [0.445]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]] [[2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]] [[1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.46182269025826406, 0.2698007844478094, 0.19522988126140378, 0.06624119327055725, 0.006669972777567611, 0.00023547798439793534]
actor:  1 policy actor:  1  step number:  56 total reward:  0.6049999999999998  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  82 total reward:  0.4349999999999996  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.575]] [[1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]
 [0.95 ]] [[ 0.553]
 [ 0.553]
 [ 0.553]
 [ 0.553]
 [ 0.553]
 [ 0.553]
 [-0.198]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.452]
 [0.496]
 [0.461]
 [0.461]
 [0.454]
 [0.421]] [[0.746]
 [0.936]
 [1.027]
 [1.055]
 [1.067]
 [1.082]
 [1.051]] [[-0.068]
 [ 0.047]
 [ 0.256]
 [ 0.224]
 [ 0.239]
 [ 0.244]
 [ 0.139]]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.355]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[-3.249]
 [-2.155]
 [-3.249]
 [-3.249]
 [-3.249]
 [-3.249]
 [-3.249]] [[-0.05 ]
 [ 0.265]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.46300064983705114, 0.26717717769012106, 0.19738682373908903, 0.06559704821124, 0.006605112381818062, 0.00023318814068077313]
actor:  1 policy actor:  1  step number:  70 total reward:  0.47499999999999964  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  50 total reward:  0.6849999999999998  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.148]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.121]] [[ 0.478]
 [-0.423]
 [ 0.478]
 [ 0.478]
 [ 0.478]
 [ 0.478]
 [-0.39 ]] [[1.272]
 [0.723]
 [1.272]
 [1.272]
 [1.272]
 [1.272]
 [0.72 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.567223788363345
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.4638399110150082, 0.2651356592157587, 0.19914256432293415, 0.06509581683008436, 0.006554642281529928, 0.00023140633468477542]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
Printing some Q and Qe and total Qs values:  [[0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]] [[3.168]
 [3.168]
 [3.168]
 [3.168]
 [3.168]
 [3.168]
 [3.168]] [[1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.195]]
siam score:  -0.7240309
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.034]
 [ 0.213]
 [ 0.333]
 [ 0.314]
 [-0.134]
 [ 0.327]
 [ 0.164]] [[-1.517]
 [-1.934]
 [-3.767]
 [-3.974]
 [-1.344]
 [-3.818]
 [-1.85 ]] [[ 0.952]
 [ 0.88 ]
 [-0.133]
 [-0.271]
 [ 0.984]
 [-0.168]
 [ 0.895]]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.325]
 [0.262]
 [0.295]
 [0.262]
 [0.262]
 [0.315]] [[2.99 ]
 [3.362]
 [2.99 ]
 [2.984]
 [2.99 ]
 [2.99 ]
 [3.473]] [[1.369]
 [1.558]
 [1.369]
 [1.383]
 [1.369]
 [1.369]
 [1.599]]
maxi score, test score, baseline:  -0.87491 -0.49750000000000005 -0.49750000000000005
probs:  [0.4638399110150082, 0.2651356592157587, 0.19914256432293415, 0.06509581683008436, 0.006554642281529928, 0.00023140633468477542]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5799999999999997  reward:  1.0 rdn_beta:  0.167
from probs:  [0.46670545407507175, 0.2637186241475958, 0.19807823371556063, 0.06474790793125203, 0.006519610562297348, 0.00023016956822260545]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.017]
 [0.016]] [[0.128]
 [0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.137]
 [0.398]] [[0.686]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.687]
 [0.96 ]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.7181176487287484
Printing some Q and Qe and total Qs values:  [[ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [-0.08]
 [-0.08]] [[-1.008]
 [-1.008]
 [-1.008]
 [-1.008]
 [-1.008]
 [ 0.358]
 [ 0.358]] [[-1.061]
 [-1.061]
 [-1.061]
 [-1.061]
 [-1.061]
 [ 1.053]
 [ 1.053]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.436]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[-1.866]
 [ 0.414]
 [-1.866]
 [-1.866]
 [-1.866]
 [-1.866]
 [-1.866]] [[0.243]
 [0.853]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]]
siam score:  -0.7239696
maxi score, test score, baseline:  -0.87817 -0.49750000000000005 -0.49750000000000005
probs:  [0.46670545407507175, 0.2637186241475957, 0.19807823371556063, 0.06474790793125203, 0.006519610562297348, 0.00023016956822260545]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.661]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[-2.85 ]
 [-1.608]
 [-2.85 ]
 [-2.85 ]
 [-2.85 ]
 [-2.85 ]
 [-2.85 ]] [[0.646]
 [0.661]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.0122274488019056
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.0096148459139123
first move QE:  -1.0098244748068703
using another actor
from probs:  [0.46670545407507175, 0.2637186241475958, 0.19807823371556063, 0.06474790793125203, 0.006519610562297348, 0.00023016956822260545]
siam score:  -0.7178859
actor:  1 policy actor:  1  step number:  64 total reward:  0.36499999999999955  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.323]
 [ 0.455]
 [ 0.414]
 [-0.008]
 [ 0.16 ]
 [-0.002]] [[-0.163]
 [-0.246]
 [ 0.33 ]
 [-0.913]
 [-1.111]
 [-1.101]
 [ 0.047]] [[ 0.199]
 [ 0.538]
 [ 1.034]
 [ 0.252]
 [-0.363]
 [-0.159]
 [ 0.328]]
using explorer policy with actor:  0
4042 6541
maxi score, test score, baseline:  -0.87817 -0.49750000000000005 -0.49750000000000005
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.87817 -0.49750000000000005 -0.49750000000000005
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]] [[-1.013]
 [-1.013]
 [-1.013]
 [-1.013]
 [-1.013]
 [-1.013]
 [-1.013]] [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]]
actor:  0 policy actor:  1  step number:  81 total reward:  0.10999999999999932  reward:  1.0 rdn_beta:  0.167
4044 6546
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8759500000000001 -0.49750000000000005 -0.49750000000000005
probs:  [0.46511267960734853, 0.2662314085307181, 0.197402231430781, 0.06452693598054753, 0.006497360406117567, 0.0002293840444874299]
using explorer policy with actor:  0
4045 6546
actor:  0 policy actor:  1  step number:  67 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.23]
 [0.29]
 [0.23]
 [0.23]
 [0.23]
 [0.23]
 [0.23]] [[1.078]
 [2.248]
 [1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]] [[0.84 ]
 [1.299]
 [0.84 ]
 [0.84 ]
 [0.84 ]
 [0.84 ]
 [0.84 ]]
maxi score, test score, baseline:  -0.8731900000000001 -0.49750000000000005 -0.49750000000000005
probs:  [0.46511267960734853, 0.2662314085307181, 0.197402231430781, 0.06452693598054753, 0.006497360406117567, 0.0002293840444874299]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
from probs:  [0.46511267960734853, 0.2662314085307181, 0.197402231430781, 0.06452693598054753, 0.006497360406117567, 0.0002293840444874299]
using another actor
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.454]
 [0.41 ]
 [0.426]
 [0.41 ]
 [0.41 ]
 [0.452]] [[3.137]
 [3.019]
 [3.137]
 [2.79 ]
 [3.137]
 [3.137]
 [3.149]] [[-0.567]
 [-0.519]
 [-0.567]
 [-0.651]
 [-0.567]
 [-0.567]
 [-0.479]]
maxi score, test score, baseline:  -0.8731900000000001 -0.49750000000000005 -0.49750000000000005
probs:  [0.46511267960734853, 0.2662314085307181, 0.197402231430781, 0.06452693598054753, 0.006497360406117567, 0.0002293840444874299]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.532]
 [0.521]
 [0.524]
 [0.523]
 [0.527]
 [0.53 ]] [[-4.445]
 [-3.944]
 [-4.827]
 [-4.686]
 [-4.892]
 [-4.875]
 [ 0.   ]] [[0.529]
 [0.532]
 [0.521]
 [0.524]
 [0.523]
 [0.527]
 [0.53 ]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.6349999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8731900000000001 -0.49750000000000005 -0.49750000000000005
siam score:  -0.6996472
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[-2.908]
 [-2.908]
 [-2.908]
 [-2.908]
 [-2.908]
 [-2.908]
 [-2.908]] [[0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[2.283]
 [2.283]
 [2.283]
 [2.283]
 [2.283]
 [2.283]
 [2.283]] [[1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.677]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.48999999999999966  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.87319 -0.49750000000000005 -0.49750000000000005
using explorer policy with actor:  1
4059 6566
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.255]
 [0.197]
 [0.255]
 [0.255]
 [0.255]
 [0.255]] [[-0.951]
 [-0.951]
 [-0.731]
 [-0.951]
 [-0.951]
 [-0.951]
 [-0.951]] [[0.255]
 [0.255]
 [0.197]
 [0.255]
 [0.255]
 [0.255]
 [0.255]]
Printing some Q and Qe and total Qs values:  [[ 0.077]
 [ 0.087]
 [ 0.138]
 [ 0.072]
 [-0.036]
 [ 0.199]
 [-0.12 ]] [[-0.629]
 [-0.948]
 [-0.87 ]
 [-2.108]
 [-0.683]
 [-0.84 ]
 [ 1.287]] [[ 0.239]
 [-0.011]
 [ 0.139]
 [-1.009]
 [ 0.005]
 [ 0.266]
 [ 1.513]]
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]] [[-1.177]
 [-1.177]
 [-1.177]
 [-1.177]
 [-1.177]
 [-1.177]
 [-1.177]] [[1.438]
 [1.438]
 [1.438]
 [1.438]
 [1.438]
 [1.438]
 [1.438]]
maxi score, test score, baseline:  -0.87319 -0.49750000000000005 -0.49750000000000005
probs:  [0.47190267113312295, 0.2663502412146212, 0.19046160306196858, 0.06455573764985069, 0.0065002605101271045, 0.00022948643030960402]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.504]
 [0.518]
 [0.486]
 [0.535]
 [0.56 ]
 [0.507]] [[ 0.642]
 [ 0.503]
 [ 0.355]
 [ 0.418]
 [-0.119]
 [ 0.411]
 [ 0.567]] [[ 0.176]
 [ 0.145]
 [ 0.123]
 [ 0.081]
 [-0.   ]
 [ 0.225]
 [ 0.172]]
siam score:  -0.69845605
from probs:  [0.47190267113312295, 0.2663502412146212, 0.19046160306196858, 0.06455573764985069, 0.0065002605101271045, 0.00022948643030960402]
actor:  1 policy actor:  1  step number:  72 total reward:  0.3149999999999995  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.47349793974582344, 0.2655456543391174, 0.18988625946395926, 0.06436072863084372, 0.006480624619166565, 0.00022879320108971931]
line 256 mcts: sample exp_bonus -7.554060473635239
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[-0.33]
 [-0.33]
 [-0.33]
 [-0.33]
 [-0.33]
 [-0.33]
 [-0.33]] [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]]
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[-6.548]
 [-6.733]
 [-6.733]
 [-6.733]
 [-6.733]
 [-6.733]
 [-6.733]] [[0.746]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.4  ]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[0.704]
 [3.42 ]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[0.66 ]
 [1.736]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
line 256 mcts: sample exp_bonus 2.2023270529822465
maxi score, test score, baseline:  -0.87319 -0.49750000000000005 -0.49750000000000005
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.259]
 [0.332]
 [0.259]
 [0.259]
 [0.259]
 [0.259]] [[-0.05 ]
 [-0.05 ]
 [ 1.761]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]] [[0.496]
 [0.496]
 [1.13 ]
 [0.496]
 [0.496]
 [0.496]
 [0.496]]
start point for exploration sampling:  10749
actor:  0 policy actor:  1  step number:  67 total reward:  0.3299999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.87053 -0.49750000000000005 -0.49750000000000005
actor:  1 policy actor:  1  step number:  67 total reward:  0.4299999999999996  reward:  1.0 rdn_beta:  0.667
4069 6576
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.416]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[1.375]
 [2.006]
 [1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.375]] [[1.01 ]
 [1.261]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.755]
 [0.781]
 [0.726]
 [0.726]
 [0.758]
 [0.762]] [[0.895]
 [1.005]
 [2.288]
 [0.895]
 [0.895]
 [1.241]
 [1.307]] [[0.726]
 [0.755]
 [0.781]
 [0.726]
 [0.726]
 [0.758]
 [0.762]]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.515]
 [0.503]
 [0.46 ]
 [0.555]
 [0.503]
 [0.539]] [[2.489]
 [2.83 ]
 [3.878]
 [2.831]
 [2.705]
 [3.878]
 [3.03 ]] [[-0.075]
 [-0.145]
 [ 0.182]
 [-0.253]
 [-0.106]
 [ 0.182]
 [-0.029]]
maxi score, test score, baseline:  -0.87053 -0.49750000000000005 -0.49750000000000005
probs:  [0.47164136331439577, 0.2645044548703748, 0.18914171904597563, 0.06802935244887898, 0.006455214213082573, 0.000227896107292348]
maxi score, test score, baseline:  -0.87053 -0.49750000000000005 -0.49750000000000005
probs:  [0.47164136331439577, 0.2645044548703748, 0.18914171904597563, 0.06802935244887898, 0.006455214213082573, 0.000227896107292348]
4070 6577
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.87053 -0.49750000000000005 -0.49750000000000005
actor:  1 policy actor:  1  step number:  59 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.167
Starting evaluation
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.565]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]] [[-3.995]
 [-4.303]
 [-3.995]
 [-3.995]
 [-3.995]
 [-3.995]
 [-3.995]] [[0.57 ]
 [0.565]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.87053 -0.49750000000000005 -0.49750000000000005
maxi score, test score, baseline:  -0.87053 -0.49750000000000005 -0.49750000000000005
probs:  [0.47206691356210734, 0.2642914178752819, 0.1889893806534445, 0.06797456029488319, 0.006450015058915925, 0.0002277125553672406]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.659]
 [0.713]
 [0.659]
 [0.659]
 [0.659]
 [0.659]] [[-0.404]
 [-0.404]
 [ 0.487]
 [-0.404]
 [-0.404]
 [-0.404]
 [-0.404]] [[0.659]
 [0.659]
 [0.713]
 [0.659]
 [0.659]
 [0.659]
 [0.659]]
maxi score, test score, baseline:  -0.87053 -0.49750000000000005 -0.49750000000000005
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.703]] [[-0.348]
 [-0.348]
 [-0.348]
 [-0.348]
 [-0.348]
 [-0.348]
 [-0.676]] [[0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.703]]
from probs:  [0.47206691356210734, 0.2642914178752819, 0.1889893806534445, 0.06797456029488319, 0.006450015058915925, 0.0002277125553672406]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.612]
 [0.615]
 [0.587]
 [0.63 ]
 [0.63 ]
 [0.595]] [[-2.523]
 [-1.035]
 [-1.454]
 [-1.38 ]
 [ 0.101]
 [ 0.101]
 [-1.671]] [[0.625]
 [0.612]
 [0.615]
 [0.587]
 [0.63 ]
 [0.63 ]
 [0.595]]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9123520796759111
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.593]
 [0.606]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[0.671]
 [1.937]
 [1.159]
 [1.215]
 [1.215]
 [1.215]
 [1.215]] [[0.578]
 [0.593]
 [0.606]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
actor:  0 policy actor:  1  step number:  34 total reward:  0.7149999999999999  reward:  1.0 rdn_beta:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.583]
 [0.579]
 [0.582]
 [0.586]
 [0.596]
 [0.592]] [[0.455]
 [1.817]
 [0.818]
 [1.232]
 [1.028]
 [1.163]
 [1.168]] [[0.555]
 [0.583]
 [0.579]
 [0.582]
 [0.586]
 [0.596]
 [0.592]]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.6  ]
 [0.599]
 [0.579]
 [0.577]
 [0.594]
 [0.704]] [[1.5  ]
 [2.6  ]
 [1.48 ]
 [1.742]
 [1.745]
 [1.388]
 [1.052]] [[0.605]
 [0.6  ]
 [0.599]
 [0.579]
 [0.577]
 [0.594]
 [0.704]]
actor:  0 policy actor:  1  step number:  38 total reward:  0.6649999999999998  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.59 ]
 [0.58 ]
 [0.582]
 [0.587]
 [0.575]
 [0.593]] [[0.41 ]
 [2.138]
 [0.786]
 [1.214]
 [1.011]
 [0.897]
 [1.139]] [[0.556]
 [0.59 ]
 [0.58 ]
 [0.582]
 [0.587]
 [0.575]
 [0.593]]
actor:  0 policy actor:  1  step number:  39 total reward:  0.6399999999999998  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  40 total reward:  0.6249999999999998  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.552]
 [0.552]
 [0.548]
 [0.547]
 [0.573]
 [0.555]] [[-0.647]
 [-0.053]
 [-0.581]
 [-0.798]
 [-0.663]
 [-0.357]
 [-0.758]] [[0.548]
 [0.552]
 [0.552]
 [0.548]
 [0.547]
 [0.573]
 [0.555]]
actor:  0 policy actor:  1  step number:  42 total reward:  0.5749999999999997  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[1.152]
 [1.152]
 [1.152]
 [1.152]
 [1.152]
 [1.152]
 [1.152]] [[0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
actor:  0 policy actor:  1  step number:  43 total reward:  0.6499999999999998  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  44 total reward:  0.5749999999999997  reward:  1.0 rdn_beta:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.558]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.582]] [[0.671]
 [1.66 ]
 [1.241]
 [1.241]
 [1.241]
 [1.241]
 [0.987]] [[0.563]
 [0.558]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.582]]
first move QE:  -1.0404421208134762
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]] [[2.199]
 [2.199]
 [2.199]
 [2.199]
 [2.199]
 [2.199]
 [2.199]] [[1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]]
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.612]
 [0.594]
 [0.594]
 [0.6  ]
 [0.594]
 [0.609]] [[2.109]
 [2.642]
 [2.244]
 [2.244]
 [1.753]
 [2.244]
 [2.911]] [[0.563]
 [0.612]
 [0.594]
 [0.594]
 [0.6  ]
 [0.594]
 [0.609]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  58 total reward:  0.4149999999999996  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[-2.946]
 [-4.714]
 [-4.714]
 [-4.714]
 [-4.714]
 [-4.714]
 [-4.714]] [[0.662]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]]
using explorer policy with actor:  1
using explorer policy with actor:  0
rdn probs:  [0.4740613390478942, 0.26329297782099065, 0.18827541661708996, 0.06771776601750232, 0.006425648193592123, 0.0002268523029308525]
actor:  0 policy actor:  1  step number:  66 total reward:  0.38499999999999956  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.077]
 [0.145]
 [0.077]
 [0.077]
 [0.077]
 [0.077]] [[3.437]
 [3.437]
 [3.865]
 [3.437]
 [3.437]
 [3.437]
 [3.437]] [[1.084]
 [1.084]
 [1.35 ]
 [1.084]
 [1.084]
 [1.084]
 [1.084]]
using another actor
maxi score, test score, baseline:  -0.8500300000000002 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.8500300000000002 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.8500300000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.47850544881260465, 0.26178209806531105, 0.18653333263489533, 0.06663528624429675, 0.00628178898095926, 0.00026204526193295604]
line 256 mcts: sample exp_bonus 2.116169128559778
from probs:  [0.47850544881260465, 0.26178209806531105, 0.18653333263489533, 0.06663528624429675, 0.00628178898095926, 0.00026204526193295604]
maxi score, test score, baseline:  -0.8500300000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.48183201804115944, 0.2566500082163071, 0.18783011210936698, 0.06709853466352851, 0.0063254599694008515, 0.0002638670002370817]
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.161]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[1.484]
 [1.739]
 [1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]] [[-0.209]
 [ 0.006]
 [-0.209]
 [-0.209]
 [-0.209]
 [-0.209]
 [-0.209]]
actor:  0 policy actor:  1  step number:  63 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8470300000000002 -0.4277500000000001 -0.4277500000000001
actor:  1 policy actor:  1  step number:  98 total reward:  0.1949999999999994  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8470300000000002 -0.4277500000000001 -0.4277500000000001
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]] [[1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]] [[0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8470300000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.48084789303917524, 0.25612580957365166, 0.1894889406276156, 0.06696148825920832, 0.006312540439208057, 0.000263328061141279]
4090 6592
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]] [[1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]] [[0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]]
maxi score, test score, baseline:  -0.8470300000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.48084789303917513, 0.2561258095736516, 0.1894889406276156, 0.06696148825920831, 0.006312540439208056, 0.00026332806114127895]
actor:  1 policy actor:  1  step number:  58 total reward:  0.6849999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8470300000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.48421141347596486, 0.2544664030464711, 0.18826126589452485, 0.06652765329789763, 0.00627164229299878, 0.00026162199214270753]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.531]
 [0.541]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[1.485]
 [1.485]
 [1.48 ]
 [1.485]
 [1.485]
 [1.485]
 [1.485]] [[1.545]
 [1.545]
 [1.552]
 [1.545]
 [1.545]
 [1.545]
 [1.545]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
line 256 mcts: sample exp_bonus -1.608043617941901
maxi score, test score, baseline:  -0.8470300000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.48567055665599607, 0.25523322288222616, 0.18882858036615632, 0.06371468819519518, 0.006290541525492561, 0.0002624103749336889]
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[-1.941]
 [-1.941]
 [-1.941]
 [-1.941]
 [-1.941]
 [-1.941]
 [-1.941]] [[0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.071]
 [-0.075]
 [-0.075]
 [-0.074]
 [-0.082]
 [-0.091]] [[0.337]
 [0.442]
 [0.448]
 [0.463]
 [0.434]
 [0.456]
 [0.489]] [[0.938]
 [1.108]
 [1.11 ]
 [1.136]
 [1.088]
 [1.109]
 [1.147]]
maxi score, test score, baseline:  -0.8470300000000002 -0.4277500000000001 -0.4277500000000001
using explorer policy with actor:  1
siam score:  -0.6787704
actor:  0 policy actor:  1  step number:  65 total reward:  0.48999999999999966  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  64 total reward:  0.6149999999999998  reward:  1.0 rdn_beta:  0.333
4099 6598
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.368]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]] [[-9.346]
 [-2.764]
 [-9.346]
 [-9.346]
 [-9.346]
 [-9.346]
 [-9.346]] [[0.396]
 [0.368]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using another actor
start point for exploration sampling:  10749
using explorer policy with actor:  1
siam score:  -0.68046904
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.165]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[0.344]
 [0.828]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]] [[0.837]
 [1.129]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.837]]
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.207]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]] [[-0.906]
 [ 0.572]
 [-0.906]
 [-0.906]
 [-0.906]
 [-0.906]
 [-0.906]] [[-0.285]
 [ 0.215]
 [-0.285]
 [-0.285]
 [-0.285]
 [-0.285]
 [-0.285]]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.591]
 [0.529]
 [0.507]
 [0.507]
 [0.5  ]
 [0.507]] [[0.879]
 [0.904]
 [0.804]
 [0.879]
 [0.879]
 [0.89 ]
 [0.879]] [[-0.025]
 [ 0.16 ]
 [-0.03 ]
 [-0.025]
 [-0.025]
 [-0.031]
 [-0.025]]
siam score:  -0.6791002
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[-7.266]
 [-8.872]
 [-8.872]
 [-8.872]
 [-8.872]
 [-8.872]
 [-8.872]] [[0.525]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.4299999999999996  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -1.031788114299471
actor:  1 policy actor:  1  step number:  80 total reward:  0.2949999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.122]
 [0.112]
 [0.109]
 [0.106]
 [0.107]
 [0.107]] [[-8.487]
 [-3.158]
 [-8.928]
 [-9.206]
 [-9.261]
 [-8.896]
 [-8.292]] [[0.505]
 [0.122]
 [0.112]
 [0.109]
 [0.106]
 [0.107]
 [0.107]]
from probs:  [0.48131345276000803, 0.261914760771872, 0.1871345395561633, 0.06314308361187765, 0.006234107091465275, 0.0002600562086139441]
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.21 ]
 [0.277]
 [0.277]
 [0.197]
 [0.277]
 [0.277]] [[ 1.028]
 [ 0.279]
 [ 1.028]
 [ 1.028]
 [-0.885]
 [ 1.028]
 [ 1.028]] [[ 0.376]
 [-0.257]
 [ 0.376]
 [ 0.376]
 [-1.059]
 [ 0.376]
 [ 0.376]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.46999999999999964  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.385]
 [0.669]
 [0.602]
 [0.538]
 [0.452]
 [0.331]] [[-1.29 ]
 [-1.097]
 [-1.443]
 [-3.126]
 [-3.121]
 [-3.438]
 [-3.783]] [[0.608]
 [0.385]
 [0.669]
 [0.602]
 [0.538]
 [0.452]
 [0.331]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
line 256 mcts: sample exp_bonus 1.8290468420687576
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.31 ]
 [0.502]
 [0.385]
 [0.442]
 [0.441]
 [0.489]] [[1.793]
 [1.715]
 [1.855]
 [1.842]
 [1.654]
 [1.779]
 [1.851]] [[-0.176]
 [-0.36 ]
 [ 0.118]
 [-0.126]
 [-0.136]
 [-0.054]
 [ 0.088]]
maxi score, test score, baseline:  -0.8440500000000001 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.8440500000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.48347864435272125, 0.2608214306266265, 0.18635337001569752, 0.06287950077075342, 0.006208083597441109, 0.0002589706367603458]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  75 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.565]
 [0.498]
 [0.491]
 [0.442]
 [0.502]
 [0.494]] [[ 1.083]
 [-0.638]
 [ 0.888]
 [ 0.733]
 [ 0.847]
 [ 0.924]
 [ 1.063]] [[ 0.691]
 [-0.79 ]
 [ 0.601]
 [ 0.434]
 [ 0.449]
 [ 0.645]
 [ 0.768]]
maxi score, test score, baseline:  -0.8440500000000001 -0.4277500000000001 -0.4277500000000001
actor:  1 policy actor:  1  step number:  59 total reward:  0.46999999999999964  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.442]] [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.073]] [[-0.547]
 [-0.547]
 [-0.547]
 [-0.547]
 [-0.547]
 [-0.547]
 [-0.336]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[1.514]
 [1.514]
 [1.514]
 [1.514]
 [1.514]
 [1.514]
 [1.514]] [[-0.417]
 [-0.417]
 [-0.417]
 [-0.417]
 [-0.417]
 [-0.417]
 [-0.417]]
actor:  1 policy actor:  1  step number:  112 total reward:  0.0849999999999993  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  57 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.68 ]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]] [[-3.955]
 [-2.053]
 [-3.955]
 [-3.955]
 [-3.955]
 [-3.955]
 [-3.955]] [[0.647]
 [0.68 ]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]]
maxi score, test score, baseline:  -0.8440500000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.4833148183948625, 0.25792363887318775, 0.18872302051466786, 0.062283891411872405, 0.007498113199176461, 0.00025651760623308093]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.303]
 [0.245]
 [0.244]
 [0.236]
 [0.235]
 [0.232]] [[1.28 ]
 [0.745]
 [1.457]
 [1.233]
 [1.478]
 [1.544]
 [1.595]] [[-0.753]
 [-1.012]
 [-0.654]
 [-0.804]
 [-0.659]
 [-0.617]
 [-0.588]]
maxi score, test score, baseline:  -0.8440500000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.4833148183948625, 0.25792363887318775, 0.18872302051466786, 0.062283891411872405, 0.007498113199176461, 0.00025651760623308093]
using explorer policy with actor:  0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.7449999999999999  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.918 0.02  0.02  0.02  0.02 ]
maxi score, test score, baseline:  -0.8440500000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.48693500073922225, 0.25611648311009616, 0.18740072258322926, 0.06184749599730525, 0.0074455772682342535, 0.00025472030191284184]
siam score:  -0.6835329
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.388]
 [0.38 ]
 [0.38 ]] [[2.29 ]
 [2.29 ]
 [2.29 ]
 [2.29 ]
 [2.562]
 [2.29 ]
 [2.29 ]] [[0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.374]
 [0.087]
 [0.087]]
maxi score, test score, baseline:  -0.8440500000000001 -0.4277500000000001 -0.4277500000000001
siam score:  -0.6856021
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.592]
 [0.541]
 [0.536]
 [0.536]
 [0.573]
 [0.632]] [[2.397]
 [2.869]
 [2.779]
 [2.397]
 [2.397]
 [2.348]
 [2.704]] [[1.365]
 [1.56 ]
 [1.502]
 [1.365]
 [1.365]
 [1.368]
 [1.524]]
actor:  1 policy actor:  1  step number:  75 total reward:  0.3299999999999995  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]] [[-8.583]
 [-9.933]
 [-9.933]
 [-9.933]
 [-9.933]
 [-9.933]
 [-9.933]] [[0.664]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.722]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[-1.126]
 [ 1.464]
 [-1.126]
 [-1.126]
 [-1.126]
 [-1.126]
 [-1.126]] [[0.686]
 [0.722]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]]
start point for exploration sampling:  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.143]
 [0.143]
 [0.141]
 [0.141]
 [0.141]
 [0.145]] [[-4.491]
 [-4.048]
 [-4.739]
 [-4.371]
 [-4.658]
 [-4.642]
 [-4.445]] [[0.423]
 [0.697]
 [0.275]
 [0.499]
 [0.323]
 [0.333]
 [0.456]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.8440500000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.48344257990779665, 0.25706534058904057, 0.18968662604881345, 0.0620766279079828, 0.007473161559531061, 0.00025566398683531946]
first move QE:  -1.0794792281068604
Printing some Q and Qe and total Qs values:  [[-0.156]
 [ 0.625]
 [-0.156]
 [-0.156]
 [-0.156]
 [-0.156]
 [-0.156]] [[-0.506]
 [-1.054]
 [-0.506]
 [-0.506]
 [-0.506]
 [-0.506]
 [-0.506]] [[0.626]
 [1.106]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]] [[ 0.367]
 [-9.38 ]
 [-9.38 ]
 [-9.38 ]
 [-9.38 ]
 [-9.38 ]
 [-9.38 ]] [[0.337]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.7643179589874491
UNIT TEST: sample policy line 217 mcts : [0.02  0.878 0.02  0.02  0.02  0.02  0.02 ]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.475]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.329]] [[-0.675]
 [ 1.128]
 [-0.675]
 [-0.675]
 [-0.675]
 [-0.675]
 [-0.739]] [[0.881]
 [1.282]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.6  ]]
from probs:  [0.48535463503920157, 0.25808205513593335, 0.18648177006569996, 0.06232214607263103, 0.007502718527621137, 0.00025667515891295717]
maxi score, test score, baseline:  -0.84405 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.84405 -0.4277500000000001 -0.4277500000000001
probs:  [0.48535463503920157, 0.25808205513593335, 0.18648177006569996, 0.06232214607263103, 0.007502718527621137, 0.00025667515891295717]
UNIT TEST: sample policy line 217 mcts : [0.041 0.857 0.02  0.02  0.02  0.02  0.02 ]
actor:  1 policy actor:  1  step number:  59 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.338]
 [0.337]
 [0.311]
 [0.311]
 [0.311]
 [0.322]] [[1.749]
 [2.863]
 [2.924]
 [1.749]
 [1.749]
 [1.749]
 [2.546]] [[0.535]
 [0.939]
 [0.96 ]
 [0.535]
 [0.535]
 [0.535]
 [0.817]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.867]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]] [[-2.541]
 [-1.791]
 [-2.763]
 [-2.763]
 [-2.763]
 [-2.763]
 [-2.763]] [[0.867]
 [0.867]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6899999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.84405 -0.4277500000000001 -0.4277500000000001
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.529]
 [0.476]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.499]] [[3.034]
 [3.789]
 [2.745]
 [3.768]
 [3.768]
 [3.768]
 [3.391]] [[0.685]
 [1.24 ]
 [0.546]
 [1.095]
 [1.095]
 [1.095]
 [0.963]]
from probs:  [0.4845247101985748, 0.26166132550258175, 0.18448342189750433, 0.06165429877367851, 0.007422319012854214, 0.00025392461480639746]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.84405 -0.4277500000000001 -0.4277500000000001
probs:  [0.4845247101985748, 0.26166132550258175, 0.18448342189750433, 0.06165429877367851, 0.007422319012854214, 0.00025392461480639746]
from probs:  [0.4845247101985748, 0.26166132550258175, 0.18448342189750433, 0.06165429877367851, 0.007422319012854214, 0.00025392461480639746]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.721]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.647]
 [0.67 ]] [[-4.44 ]
 [-3.613]
 [-4.44 ]
 [-4.44 ]
 [-4.44 ]
 [-4.453]
 [-4.492]] [[0.67 ]
 [0.721]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.647]
 [0.67 ]]
maxi score, test score, baseline:  -0.84405 -0.4277500000000001 -0.4277500000000001
Printing some Q and Qe and total Qs values:  [[0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]] [[-0.63]
 [-0.63]
 [-0.63]
 [-0.63]
 [-0.63]
 [-0.63]
 [-0.63]] [[0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]]
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.872]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]] [[-1.551]
 [ 2.408]
 [-2.44 ]
 [-2.44 ]
 [-2.44 ]
 [-2.44 ]
 [-2.44 ]] [[0.863]
 [0.872]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]]
using another actor
maxi score, test score, baseline:  -0.84405 -0.4277500000000001 -0.4277500000000001
probs:  [0.4845247101985748, 0.26166132550258175, 0.18448342189750427, 0.06165429877367851, 0.007422319012854214, 0.00025392461480639746]
4144 6647
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.559]
 [0.56 ]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[ 0.292]
 [-1.16 ]
 [ 1.165]
 [ 0.292]
 [ 0.292]
 [ 0.292]
 [ 0.292]] [[0.572]
 [0.559]
 [0.56 ]
 [0.572]
 [0.572]
 [0.572]
 [0.572]]
Printing some Q and Qe and total Qs values:  [[0.931]
 [0.947]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]] [[2.093]
 [3.609]
 [2.523]
 [2.523]
 [2.523]
 [2.523]
 [2.523]] [[0.931]
 [0.947]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]]
maxi score, test score, baseline:  -0.84405 -0.4277500000000001 -0.4277500000000001
probs:  [0.4845247101985748, 0.26166132550258175, 0.18448342189750427, 0.06165429877367851, 0.007422319012854214, 0.00025392461480639746]
maxi score, test score, baseline:  -0.84405 -0.4277500000000001 -0.4277500000000001
probs:  [0.4845247101985748, 0.26166132550258175, 0.18448342189750427, 0.06165429877367851, 0.007422319012854214, 0.00025392461480639746]
Printing some Q and Qe and total Qs values:  [[0.858]
 [0.801]
 [0.864]
 [0.858]
 [0.858]
 [0.858]
 [0.819]] [[3.883]
 [3.765]
 [4.24 ]
 [3.883]
 [3.883]
 [3.883]
 [4.544]] [[0.858]
 [0.801]
 [0.864]
 [0.858]
 [0.858]
 [0.858]
 [0.819]]
actor:  0 policy actor:  1  step number:  53 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.679]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[1.343]
 [0.817]
 [1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.343]] [[0.558]
 [0.679]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.089018937772785
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.334]
 [0.426]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[0.8  ]
 [0.8  ]
 [2.399]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]] [[0.431]
 [0.431]
 [1.217]
 [0.431]
 [0.431]
 [0.431]
 [0.431]]
using explorer policy with actor:  0
from probs:  [0.4845247101985748, 0.26166132550258175, 0.18448342189750427, 0.06165429877367851, 0.007422319012854214, 0.00025392461480639746]
actor:  1 policy actor:  1  step number:  65 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8410500000000001 -0.4277500000000001 -0.4277500000000001
actor:  0 policy actor:  1  step number:  74 total reward:  0.27499999999999947  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]] [[-5.276]
 [-8.755]
 [-8.755]
 [-8.755]
 [-8.755]
 [-8.755]
 [-8.755]] [[0.426]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]]
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
actor:  1 policy actor:  1  step number:  56 total reward:  0.5149999999999997  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]] [[-1.076]
 [-1.031]
 [-1.062]
 [-1.09 ]
 [-1.103]
 [-1.063]
 [-1.063]] [[1.276]
 [1.317]
 [1.288]
 [1.264]
 [1.251]
 [1.288]
 [1.288]]
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
using explorer policy with actor:  1
4153 6670
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.45 ]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]] [[-1.266]
 [ 1.331]
 [-1.266]
 [-1.266]
 [-1.266]
 [-1.266]
 [-1.266]] [[-0.268]
 [ 0.726]
 [-0.268]
 [-0.268]
 [-0.268]
 [-0.268]
 [-0.268]]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.373]
 [0.389]
 [0.386]
 [0.386]
 [0.392]
 [0.386]] [[ 0.404]
 [ 0.792]
 [ 2.115]
 [ 0.404]
 [ 0.404]
 [-0.05 ]
 [ 0.404]] [[0.436]
 [0.518]
 [0.899]
 [0.436]
 [0.436]
 [0.323]
 [0.436]]
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.48247497559902564
siam score:  -0.6942834
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.48025771157899116, 0.26383088813789524, 0.18719142184785223, 0.06111133615026378, 0.0073569538739555644, 0.00025168841104206694]
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]] [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]]
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
Printing some Q and Qe and total Qs values:  [[ 0.124]
 [-0.076]
 [-0.155]
 [ 0.134]
 [ 0.096]
 [ 0.163]
 [ 0.053]] [[-1.068]
 [-0.452]
 [ 0.15 ]
 [-1.817]
 [-1.697]
 [-0.57 ]
 [-0.269]] [[ 0.108]
 [ 0.435]
 [ 0.878]
 [-0.535]
 [-0.469]
 [ 0.583]
 [ 0.73 ]]
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.4768789029747947, 0.2655460344608956, 0.18840834031083273, 0.061508616712185625, 0.007404780921328437, 0.0002533246199628169]
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.33 ]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[3.131]
 [3.569]
 [3.131]
 [3.131]
 [3.131]
 [3.131]
 [3.131]] [[0.394]
 [0.637]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]] [[-9.638]
 [-9.669]
 [-9.669]
 [-9.669]
 [-9.669]
 [-9.669]
 [-9.669]] [[0.265]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  0.167
from probs:  [0.4748083437196606, 0.2643930608319778, 0.18759029063335378, 0.061241552610986245, 0.0073726301225129414, 0.0045941220815086245]
from probs:  [0.4748083437196606, 0.2643930608319778, 0.18759029063335378, 0.061241552610986245, 0.0073726301225129414, 0.0045941220815086245]
maxi score, test score, baseline:  -0.8385 -0.4277500000000001 -0.4277500000000001
probs:  [0.4754852340801434, 0.26405229929832497, 0.1873485159251468, 0.06116262177473309, 0.007363127948969895, 0.004588200972681649]
start point for exploration sampling:  10749
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]] [[-2.079]
 [-6.873]
 [-6.873]
 [-6.873]
 [-6.873]
 [-6.873]
 [-6.873]] [[0.666]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]]
maxi score, test score, baseline:  -0.8407700000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.4754852340801434, 0.26405229929832497, 0.1873485159251468, 0.06116262177473309, 0.007363127948969895, 0.004588200972681649]
maxi score, test score, baseline:  -0.8407700000000001 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.8407700000000001 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.8407700000000001 -0.4277500000000001 -0.4277500000000001
first move QE:  -1.1177091265256411
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.009]
 [-0.028]
 [-0.027]
 [-0.026]
 [-0.023]
 [-0.026]] [[3.35 ]
 [2.89 ]
 [3.344]
 [3.576]
 [3.518]
 [3.646]
 [3.525]] [[-0.199]
 [-0.621]
 [-0.203]
 [ 0.03 ]
 [-0.026]
 [ 0.108]
 [-0.02 ]]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.025]
 [-0.026]
 [-0.025]
 [-0.026]
 [-0.026]
 [-0.025]] [[3.332]
 [3.294]
 [3.413]
 [3.294]
 [3.594]
 [3.496]
 [3.294]] [[-0.194]
 [-0.225]
 [-0.108]
 [-0.225]
 [ 0.072]
 [-0.024]
 [-0.225]]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.523]] [[-1.053]
 [-3.301]
 [-3.301]
 [-3.301]
 [-3.301]
 [-3.301]
 [-2.74 ]] [[0.494]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.523]]
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.017]
 [-0.017]
 [-0.018]
 [-0.018]
 [-0.017]
 [-0.017]] [[2.743]
 [1.183]
 [2.676]
 [2.743]
 [2.743]
 [2.679]
 [2.696]] [[ 0.114]
 [-1.445]
 [ 0.047]
 [ 0.114]
 [ 0.114]
 [ 0.052]
 [ 0.069]]
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.604]
 [0.673]
 [0.604]
 [0.604]
 [0.604]
 [0.604]] [[-1.06 ]
 [-1.06 ]
 [-1.047]
 [-1.06 ]
 [-1.06 ]
 [-1.06 ]
 [-1.06 ]] [[0.604]
 [0.604]
 [0.673]
 [0.604]
 [0.604]
 [0.604]
 [0.604]]
siam score:  -0.69081
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[-1.12]
 [-1.12]
 [-1.12]
 [-1.12]
 [-1.12]
 [-1.12]
 [-1.12]] [[1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]]
Printing some Q and Qe and total Qs values:  [[-0.109]
 [-0.071]
 [-0.011]
 [-0.032]
 [-0.094]
 [-0.138]
 [-0.163]] [[-1.008]
 [-1.343]
 [-3.748]
 [-3.59 ]
 [-2.334]
 [-0.096]
 [ 0.5  ]] [[-0.259]
 [-0.294]
 [-0.977]
 [-0.967]
 [-0.67 ]
 [-0.012]
 [ 0.137]]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.346]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[-1.906]
 [ 1.404]
 [-1.906]
 [-1.906]
 [-1.906]
 [-1.906]
 [-1.906]] [[-0.657]
 [ 0.317]
 [-0.657]
 [-0.657]
 [-0.657]
 [-0.657]
 [-0.657]]
actor:  1 policy actor:  1  step number:  94 total reward:  0.12499999999999933  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[-1.087]
 [-1.123]
 [-1.123]
 [-1.123]
 [-1.109]
 [-1.123]
 [-1.109]] [[1.233]
 [1.165]
 [1.165]
 [1.165]
 [1.192]
 [1.165]
 [1.192]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8407700000000001 -0.4277500000000001 -0.4277500000000001
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.383]
 [0.398]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[0.037]
 [2.773]
 [2.339]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[0.473]
 [1.347]
 [1.228]
 [0.473]
 [0.473]
 [0.473]
 [0.473]]
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.852]
 [0.778]
 [0.803]
 [0.782]
 [0.786]
 [0.848]] [[-1.704]
 [ 2.095]
 [ 0.891]
 [ 1.486]
 [ 0.353]
 [ 1.308]
 [ 1.788]] [[0.781]
 [0.852]
 [0.778]
 [0.803]
 [0.782]
 [0.786]
 [0.848]]
maxi score, test score, baseline:  -0.8407700000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.4747544584044906, 0.26364647597596524, 0.1885974841260079, 0.061068620630115866, 0.007351811520159575, 0.004581149343260751]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6249999999999998  reward:  1.0 rdn_beta:  0.167
first move QE:  -1.1270578417640877
maxi score, test score, baseline:  -0.8407700000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.477666435669372, 0.262184811890867, 0.1875518939353315, 0.060730054339134344, 0.007311052853389424, 0.004555751311905588]
actor:  1 policy actor:  1  step number:  42 total reward:  0.7349999999999999  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8407700000000001 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.8407700000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.47445761664130304, 0.26714123176467547, 0.18629197688638346, 0.06032208815313679, 0.007261939405679206, 0.004525147148821977]
actor:  1 policy actor:  1  step number:  47 total reward:  0.6099999999999998  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  90 total reward:  0.13499999999999934  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  68 total reward:  0.2849999999999995  reward:  1.0 rdn_beta:  0.167
siam score:  -0.67716265
using explorer policy with actor:  1
siam score:  -0.6736097
using another actor
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.4792610044216392, 0.2700781750843561, 0.17820708132036467, 0.060610329534027334, 0.007296639654067865, 0.004546769985544706]
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]] [[-7.561]
 [-7.226]
 [-7.226]
 [-7.226]
 [-7.226]
 [-7.226]
 [-7.226]] [[0.052]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]]
maxi score, test score, baseline:  -0.8385000000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.4792610044216392, 0.2700781750843561, 0.17820708132036467, 0.060610329534027334, 0.007296639654067865, 0.004546769985544706]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.009]
 [ 0.167]
 [ 0.033]
 [-0.005]
 [ 0.167]
 [ 0.167]] [[-1.492]
 [-1.47 ]
 [ 0.   ]
 [-3.685]
 [-3.193]
 [ 0.   ]
 [ 0.   ]] [[ 0.648]
 [ 0.654]
 [ 1.804]
 [-0.547]
 [-0.33 ]
 [ 1.804]
 [ 1.804]]
line 256 mcts: sample exp_bonus 1.6269868812402697
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.486]
 [0.466]
 [0.462]
 [0.463]
 [0.462]
 [0.466]] [[-2.494]
 [-0.771]
 [-2.607]
 [-2.568]
 [-2.538]
 [-2.682]
 [-2.629]] [[0.467]
 [0.486]
 [0.466]
 [0.462]
 [0.463]
 [0.462]
 [0.466]]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.463]
 [0.358]
 [0.436]
 [0.358]
 [0.358]
 [0.482]] [[4.296]
 [3.763]
 [4.296]
 [3.803]
 [4.296]
 [4.296]
 [3.623]] [[1.267]
 [1.182]
 [1.267]
 [1.165]
 [1.267]
 [1.267]
 [1.149]]
actor:  0 policy actor:  1  step number:  73 total reward:  0.24999999999999944  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.456]
 [0.438]] [[-4.813]
 [-4.813]
 [-4.813]
 [-4.813]
 [-4.813]
 [-5.006]
 [-4.813]] [[0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.456]
 [0.438]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.5149999999999997  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.511]
 [0.488]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.49 ]] [[-2.992]
 [ 0.147]
 [-2.434]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-2.29 ]] [[0.5  ]
 [0.511]
 [0.488]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.49 ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.7399999999999999  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]] [[-2.376]
 [-2.325]
 [-2.325]
 [-2.325]
 [-2.325]
 [-2.325]
 [-2.325]] [[0.707]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]]
line 256 mcts: sample exp_bonus -5.542747208098404
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
4191 6784
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.48221712922022125, 0.2723400028593305, 0.1767948812435517, 0.05689843000810224, 0.0072388175125255375, 0.004510739156268656]
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.34 ]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[-2.198]
 [ 0.553]
 [-2.198]
 [-2.198]
 [-2.198]
 [-2.198]
 [-2.198]] [[-0.113]
 [ 0.667]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.473]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[-3.532]
 [-2.858]
 [-3.532]
 [-3.532]
 [-3.532]
 [-3.532]
 [-3.532]] [[0.433]
 [0.473]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]] [[3.888]
 [3.888]
 [3.888]
 [3.888]
 [3.888]
 [3.888]
 [3.888]] [[1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]]
siam score:  -0.6730927
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[-1.169]
 [-1.169]
 [-1.169]
 [-1.169]
 [-1.169]
 [-1.169]
 [-1.169]] [[1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.4870715880333167, 0.26692382449528124, 0.17666552450598005, 0.05747122402197474, 0.007311690372780198, 0.004556148570666963]
4198 6808
line 256 mcts: sample exp_bonus 0.7274234983893724
4200 6815
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.014]
 [-0.023]
 [-0.022]
 [-0.022]
 [-0.023]
 [-0.024]] [[-3.724]
 [-0.444]
 [-3.853]
 [-3.681]
 [-3.929]
 [-3.966]
 [-3.799]] [[-0.17 ]
 [ 0.704]
 [-0.204]
 [-0.158]
 [-0.224]
 [-0.234]
 [-0.19 ]]
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.4870715880333167, 0.26692382449528124, 0.17666552450598005, 0.05747122402197474, 0.007311690372780198, 0.004556148570666963]
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.76 ]
 [0.526]
 [0.608]
 [0.608]
 [0.608]
 [0.593]] [[2.578]
 [2.955]
 [2.796]
 [3.263]
 [3.263]
 [3.263]
 [3.38 ]] [[0.418]
 [0.65 ]
 [0.129]
 [0.448]
 [0.448]
 [0.448]
 [0.457]]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.58 ]
 [0.538]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.512]] [[2.909]
 [2.662]
 [2.854]
 [3.064]
 [3.064]
 [3.064]
 [2.894]] [[0.63 ]
 [0.679]
 [0.659]
 [0.734]
 [0.734]
 [0.734]
 [0.62 ]]
line 256 mcts: sample exp_bonus -0.0810215961036108
first move QE:  -1.1799933838656642
actor:  1 policy actor:  1  step number:  53 total reward:  0.5599999999999997  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.169]
 [0.378]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[2.4  ]
 [2.4  ]
 [2.437]
 [2.4  ]
 [2.4  ]
 [2.4  ]
 [2.4  ]] [[0.914]
 [0.914]
 [1.238]
 [0.914]
 [0.914]
 [0.914]
 [0.914]]
in main func line 156:  4205
Printing some Q and Qe and total Qs values:  [[-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]] [[-0.925]
 [-0.925]
 [-0.925]
 [-0.925]
 [-0.925]
 [-0.925]
 [-0.925]] [[1.627]
 [1.627]
 [1.627]
 [1.627]
 [1.627]
 [1.627]
 [1.627]]
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.195]
 [0.376]
 [0.195]
 [0.195]
 [0.195]
 [0.195]] [[-0.16]
 [-0.16]
 [-0.2 ]
 [-0.16]
 [-0.16]
 [-0.16]
 [-0.16]] [[1.077]
 [1.077]
 [1.232]
 [1.077]
 [1.077]
 [1.077]
 [1.077]]
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.48956302084708936, 0.26562730287625963, 0.17580741200032082, 0.057192070654690945, 0.007276175503855227, 0.004534018117783958]
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.48956302084708936, 0.26562730287625963, 0.17580741200032082, 0.057192070654690945, 0.007276175503855227, 0.004534018117783958]
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.368]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[-0.746]
 [-0.39 ]
 [-0.746]
 [-0.746]
 [-0.746]
 [-0.746]
 [-0.746]] [[0.939]
 [1.29 ]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]]
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.48956302084708936, 0.26562730287625963, 0.1758074120003208, 0.057192070654690945, 0.007276175503855227, 0.004534018117783958]
from probs:  [0.48956302084708936, 0.26562730287625963, 0.1758074120003208, 0.057192070654690945, 0.007276175503855227, 0.004534018117783958]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
Printing some Q and Qe and total Qs values:  [[-0.013]
 [ 0.284]
 [-0.032]
 [-0.004]
 [-0.03 ]
 [-0.047]
 [ 0.098]] [[-0.142]
 [ 2.242]
 [-0.024]
 [-0.72 ]
 [-1.018]
 [-0.682]
 [ 0.069]] [[-0.173]
 [ 0.96 ]
 [-0.171]
 [-0.314]
 [-0.438]
 [-0.374]
 [ 0.066]]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.321]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]] [[3.443]
 [3.49 ]
 [3.443]
 [3.443]
 [3.443]
 [3.443]
 [3.443]] [[1.451]
 [1.476]
 [1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]]
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.48956302084708936, 0.26562730287625963, 0.1758074120003208, 0.057192070654690945, 0.007276175503855227, 0.004534018117783958]
actor:  1 policy actor:  1  step number:  74 total reward:  0.4449999999999996  reward:  1.0 rdn_beta:  0.333
siam score:  -0.67578554
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.281]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[2.899]
 [3.705]
 [2.899]
 [2.899]
 [2.899]
 [2.899]
 [2.899]] [[-0.405]
 [-0.09 ]
 [-0.405]
 [-0.405]
 [-0.405]
 [-0.405]
 [-0.405]]
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.48771356954090417, 0.2684015862393539, 0.17514325389622534, 0.056976012771902465, 0.007248687863415486, 0.004516889688198542]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.8360000000000001 -0.4277500000000001 -0.4277500000000001
maxi score, test score, baseline:  -0.8360000000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.484962644261953, 0.26984287506645543, 0.17608375510011065, 0.05728196808226976, 0.007287612569394573, 0.004541144919816454]
Printing some Q and Qe and total Qs values:  [[-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]] [[-1.065]
 [-1.065]
 [-1.065]
 [-1.065]
 [-1.065]
 [-1.065]
 [-1.065]] [[1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.742]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.729]] [[-0.321]
 [ 2.05 ]
 [-0.321]
 [-0.321]
 [-0.321]
 [-0.321]
 [ 0.169]] [[0.692]
 [0.742]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.729]]
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.785]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[-0.714]
 [ 1.038]
 [-0.714]
 [-0.714]
 [-0.714]
 [-0.714]
 [-0.714]] [[0.796]
 [0.785]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]]
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.484962644261953, 0.26984287506645543, 0.17608375510011065, 0.05728196808226976, 0.007287612569394573, 0.004541144919816454]
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
using explorer policy with actor:  0
using another actor
from probs:  [0.484962644261953, 0.26984287506645543, 0.17608375510011065, 0.05728196808226976, 0.007287612569394573, 0.004541144919816454]
from probs:  [0.4872379929673178, 0.26641712240573495, 0.17690990521485309, 0.05755072373481099, 0.007321804604639282, 0.004562451072643758]
maxi score, test score, baseline:  -0.8360000000000002 -0.4277500000000001 -0.4277500000000001
actor:  0 policy actor:  1  step number:  71 total reward:  0.16999999999999937  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.264]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.182]] [[ 0.025]
 [ 0.883]
 [ 0.025]
 [ 0.025]
 [ 0.025]
 [ 0.025]
 [-0.133]] [[0.363]
 [0.764]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.421]]
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.006]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[-5.087]
 [-4.343]
 [-5.087]
 [-5.087]
 [-5.087]
 [-5.087]
 [-5.087]] [[0.06 ]
 [0.532]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]]
line 256 mcts: sample exp_bonus 4.577313930398917
actor:  0 policy actor:  1  step number:  49 total reward:  0.7299999999999999  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8302 -0.4277500000000001 -0.4277500000000001
probs:  [0.4872379929673178, 0.26641712240573495, 0.17690990521485309, 0.05755072373481099, 0.007321804604639282, 0.004562451072643758]
actor:  0 policy actor:  1  step number:  71 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.82764 -0.4277500000000001 -0.4277500000000001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.124]
 [0.129]
 [0.171]
 [0.176]
 [0.145]
 [0.182]] [[3.252]
 [2.045]
 [2.798]
 [2.84 ]
 [3.068]
 [2.756]
 [2.775]] [[-0.073]
 [-0.912]
 [-0.401]
 [-0.288]
 [-0.128]
 [-0.398]
 [-0.312]]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.102]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]] [[3.293]
 [3.29 ]
 [3.077]
 [3.077]
 [3.077]
 [3.077]
 [3.077]] [[-0.401]
 [-0.255]
 [-0.144]
 [-0.144]
 [-0.144]
 [-0.144]
 [-0.144]]
actor:  0 policy actor:  1  step number:  52 total reward:  0.4549999999999996  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
line 256 mcts: sample exp_bonus -0.1987539607152883
4221 6842
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.357]
 [0.37 ]
 [0.357]
 [0.357]
 [0.347]
 [0.356]] [[0.878]
 [0.886]
 [0.904]
 [0.886]
 [0.886]
 [1.068]
 [1.212]] [[-0.108]
 [-0.117]
 [-0.078]
 [-0.117]
 [-0.117]
 [-0.016]
 [ 0.098]]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.631]
 [0.101]
 [0.322]
 [0.132]
 [0.573]
 [0.42 ]] [[3.029]
 [4.102]
 [1.656]
 [4.065]
 [1.38 ]
 [3.262]
 [3.932]] [[ 0.187]
 [ 1.028]
 [-0.849]
 [ 0.399]
 [-0.878]
 [ 0.633]
 [ 0.549]]
maxi score, test score, baseline:  -0.8247300000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.4872379929673178, 0.26641712240573495, 0.1769099052148531, 0.05755072373481099, 0.007321804604639282, 0.004562451072643758]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[0.178]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[-0.467]
 [-0.34 ]
 [-0.34 ]
 [-0.34 ]
 [-0.34 ]
 [-0.34 ]
 [-0.34 ]]
siam score:  -0.6575156
line 256 mcts: sample exp_bonus -0.856622892416918
actor:  1 policy actor:  1  step number:  49 total reward:  0.7399999999999999  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  109 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  0.333
Starting evaluation
maxi score, test score, baseline:  -0.8247300000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.4917009493910064, 0.26770841764688696, 0.17606182512739363, 0.052701523532672837, 0.007286704949354732, 0.004540579352685409]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.368]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[-1.504]
 [-1.248]
 [-1.732]
 [-1.732]
 [-1.732]
 [-1.732]
 [-1.732]] [[0.576]
 [0.368]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]]
maxi score, test score, baseline:  -0.8247300000000001 -0.4277500000000001 -0.4277500000000001
probs:  [0.4917009493910064, 0.26770841764688696, 0.17606182512739363, 0.052701523532672837, 0.007286704949354732, 0.004540579352685409]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.587]
 [0.557]
 [0.564]
 [0.561]
 [0.562]
 [0.56 ]] [[-7.358]
 [-5.434]
 [-7.199]
 [-7.31 ]
 [-7.582]
 [-7.561]
 [-7.309]] [[0.548]
 [0.587]
 [0.557]
 [0.564]
 [0.561]
 [0.562]
 [0.56 ]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[-6.37]
 [-6.37]
 [-6.37]
 [-6.37]
 [-6.37]
 [-6.37]
 [-6.37]] [[0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[-7.216]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.553]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]] [[-6.405]
 [-7.252]
 [-7.252]
 [-7.252]
 [-7.252]
 [-7.252]
 [-7.252]] [[0.068]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]]
maxi score, test score, baseline:  -0.8247300000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.4917009493910064, 0.26770841764688696, 0.1760618251273936, 0.052701523532672837, 0.007286704949354732, 0.004540579352685409]
maxi score, test score, baseline:  -0.8247300000000002 -0.4277500000000001 -0.4277500000000001
probs:  [0.4917009493910064, 0.26770841764688696, 0.1760618251273936, 0.052701523532672837, 0.007286704949354732, 0.004540579352685409]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[-1.21]
 [-1.21]
 [-1.21]
 [-1.21]
 [-1.21]
 [-1.21]
 [-1.21]] [[0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  60 total reward:  0.4549999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.82428 -0.4277500000000001 -0.4277500000000001
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.798]
 [0.624]] [[-6.522]
 [-6.522]
 [-6.522]
 [-6.522]
 [-6.522]
 [-4.116]
 [-6.036]] [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.798]
 [0.624]]
rdn probs:  [0.4917009493910064, 0.26770841764688696, 0.1760618251273936, 0.052701523532672837, 0.007286704949354732, 0.004540579352685409]
maxi score, test score, baseline:  -0.82681 -1.0 -0.82681
probs:  [0.4717559513972744, 0.27410291058489755, 0.18431391362044056, 0.056757053395470915, 0.008646043008085833, 0.004424127993830687]
maxi score, test score, baseline:  -0.82962 -1.0 -0.82962
using another actor
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.409]
 [0.792]
 [0.409]
 [0.409]
 [0.409]
 [0.336]] [[-1.004]
 [-1.004]
 [ 3.365]
 [-1.004]
 [-1.004]
 [-1.004]
 [ 0.088]] [[0.738]
 [0.738]
 [2.031]
 [0.738]
 [0.738]
 [0.738]
 [0.942]]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.403]
 [0.487]
 [0.487]
 [0.489]
 [0.488]
 [0.506]] [[-2.217]
 [-1.137]
 [-2.496]
 [-2.611]
 [-2.613]
 [-2.381]
 [-2.422]] [[0.455]
 [0.403]
 [0.487]
 [0.487]
 [0.489]
 [0.488]
 [0.506]]
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.62 ]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[-2.702]
 [-1.384]
 [-2.702]
 [-2.702]
 [-2.702]
 [-2.702]
 [-2.702]] [[1.725]
 [2.456]
 [1.725]
 [1.725]
 [1.725]
 [1.725]
 [1.725]]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.781]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[-3.318]
 [-0.86 ]
 [-3.318]
 [-3.318]
 [-3.318]
 [-3.318]
 [-3.318]] [[0.689]
 [1.53 ]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]] [[-1.985]
 [-1.985]
 [-1.985]
 [-1.985]
 [-1.985]
 [-1.985]
 [-1.985]] [[0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]]
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.933]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]] [[3.201]
 [3.292]
 [3.201]
 [3.201]
 [3.201]
 [3.201]
 [3.201]] [[0.614]
 [0.849]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.3073369847048326
line 256 mcts: sample exp_bonus 2.0089413616582483
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
4237 6877
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.416]
 [0.404]
 [0.393]
 [0.403]
 [0.393]
 [0.418]] [[ 0.308]
 [-0.165]
 [ 0.309]
 [ 0.298]
 [ 0.262]
 [ 0.327]
 [ 0.335]] [[0.408]
 [0.416]
 [0.404]
 [0.393]
 [0.403]
 [0.393]
 [0.418]]
4238 6884
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.023]
 [0.091]
 [0.099]
 [0.096]
 [0.099]
 [0.057]] [[-5.627]
 [-3.044]
 [-4.453]
 [-4.918]
 [-4.919]
 [-4.893]
 [-3.688]] [[0.107]
 [0.023]
 [0.091]
 [0.099]
 [0.096]
 [0.099]
 [0.057]]
maxi score, test score, baseline:  -0.8323300000000001 -1.0 -0.8323300000000001
probs:  [0.4695874178860785, 0.2751872955525412, 0.1850838935795587, 0.05700932843820465, 0.008692534187484174, 0.004439530356132894]
maxi score, test score, baseline:  -0.8323300000000001 -1.0 -0.8323300000000001
probs:  [0.4695874178860785, 0.2751872955525411, 0.1850838935795587, 0.05700932843820465, 0.008692534187484174, 0.004439530356132894]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]] [[-8.99 ]
 [-6.835]
 [-6.835]
 [-6.835]
 [-6.835]
 [-6.835]
 [-6.835]] [[0.239]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.7170695153592708
maxi score, test score, baseline:  -0.8323300000000001 -1.0 -0.8323300000000001
probs:  [0.4695874178860785, 0.2751872955525412, 0.1850838935795587, 0.05700932843820465, 0.008692534187484174, 0.004439530356132894]
maxi score, test score, baseline:  -0.8323300000000001 -1.0 -0.8323300000000001
probs:  [0.4695874178860785, 0.2751872955525412, 0.1850838935795587, 0.05700932843820465, 0.008692534187484174, 0.004439530356132894]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]] [[2.679]
 [2.679]
 [2.679]
 [2.679]
 [2.679]
 [2.679]
 [2.679]] [[0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]]
UNIT TEST: sample policy line 217 mcts : [0.061 0.122 0.408 0.061 0.122 0.163 0.061]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.795]
 [0.671]
 [0.548]
 [0.293]
 [0.643]
 [0.513]] [[-1.48 ]
 [-1.315]
 [-4.34 ]
 [-2.95 ]
 [-2.57 ]
 [-4.368]
 [-2.013]] [[0.664]
 [1.161]
 [0.076]
 [0.455]
 [0.412]
 [0.049]
 [0.743]]
maxi score, test score, baseline:  -0.8323300000000001 -1.0 -0.8323300000000001
probs:  [0.46719968928251343, 0.2764260907830574, 0.18591707537362143, 0.0572659641379828, 0.008731664881620811, 0.004459515541204272]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.2305579472631478
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.614]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.417]] [[-3.09 ]
 [-1.86 ]
 [-3.09 ]
 [-3.09 ]
 [-3.09 ]
 [-3.09 ]
 [-2.076]] [[1.753]
 [2.124]
 [1.753]
 [1.753]
 [1.753]
 [1.753]
 [1.827]]
actor:  0 policy actor:  1  step number:  73 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.834]
 [0.248]
 [0.722]
 [0.722]
 [0.722]
 [0.807]] [[-0.313]
 [-0.739]
 [-0.263]
 [-0.313]
 [-0.313]
 [-0.313]
 [-0.649]] [[1.531]
 [1.187]
 [0.65 ]
 [1.531]
 [1.531]
 [1.531]
 [1.252]]
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
start point for exploration sampling:  10749
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.659]
 [0.644]
 [0.668]
 [0.579]
 [0.54 ]
 [0.651]] [[ 1.559]
 [ 2.166]
 [ 0.837]
 [ 0.552]
 [-1.305]
 [-1.176]
 [ 2.277]] [[0.592]
 [0.659]
 [0.644]
 [0.668]
 [0.579]
 [0.54 ]
 [0.651]]
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.652]
 [0.61 ]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[3.057]
 [3.057]
 [2.04 ]
 [3.057]
 [3.057]
 [3.057]
 [3.057]] [[0.652]
 [0.652]
 [0.61 ]
 [0.652]
 [0.652]
 [0.652]
 [0.652]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.5399999999999997  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.935]
 [0.995]
 [0.935]
 [0.935]
 [0.935]
 [0.935]
 [0.935]] [[1.025]
 [1.244]
 [1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]] [[1.292]
 [1.486]
 [1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]]
siam score:  -0.6628585
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.845]
 [0.617]] [[-5.452]
 [-5.452]
 [-5.452]
 [-5.452]
 [-5.452]
 [-4.628]
 [-5.452]] [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.845]
 [0.617]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10749
maxi score, test score, baseline:  -0.82969 -1.0 -0.82969
probs:  [0.46778775816064544, 0.2761405556139086, 0.18570548461523054, 0.05719352706128416, 0.008716760014925547, 0.004455914534005658]
maxi score, test score, baseline:  -0.82969 -1.0 -0.82969
maxi score, test score, baseline:  -0.82969 -1.0 -0.82969
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.761]
 [0.604]
 [0.761]
 [0.761]
 [0.761]
 [0.903]] [[-0.905]
 [-0.905]
 [-1.474]
 [-0.905]
 [-0.905]
 [-0.905]
 [-1.282]] [[2.031]
 [2.031]
 [1.527]
 [2.031]
 [2.031]
 [2.031]
 [2.19 ]]
