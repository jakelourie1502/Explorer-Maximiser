dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
res_block_channels:[32, 64, 64]
res_block_ds:[False, False, False]
reward_support:[-1, 1, 41]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 41]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
channels:3
env_size:[8, 60]
observable_size:[8, 10]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.
  0. 0. 0. 0. 0. 0. 2. 2. 2. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.
  1. 1. 1. 1. 2. 1. 1. 1. 0. 0. 2. 2. 2. 1. 1. 0. 0. 0. 2. 1. 1. 1. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 2. 1. 1. 1. 2. 2. 2. 2. 2.
  1. 1. 1. 1. 2. 2. 2. 1. 1. 1. 2. 2. 2. 0. 1. 1. 1. 1. 1. 1. 1. 2. 0. 0.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.
  1. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 0. 2. 2. 1. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.
  1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 2. 2. 1. 2. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2.
  2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:200
actions_size:7
optimal_score:0.86
total_frames:305000
exp_gamma:0.975
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
timesteps_in_obs:2
store_prev_actions:True
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 1, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
state_size:[6, 6]
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 480)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.
  0. 0. 0. 0. 0. 0. 2. 2. 2. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.
  1. 1. 1. 1. 2. 1. 1. 1. 0. 0. 2. 2. 2. 1. 1. 0. 0. 0. 2. 1. 1. 1. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 2. 1. 1. 1. 2. 2. 2. 2. 2.
  1. 1. 1. 1. 2. 2. 2. 1. 1. 1. 2. 2. 2. 0. 1. 1. 1. 1. 1. 1. 1. 2. 0. 0.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.
  1. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 0. 2. 2. 1. 1. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.
  1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 2. 2. 1. 2. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2.
  2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.
  3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[ 0.]
 [-0.]
 [ 0.]
 [ 0.]
 [ 0.]
 [ 0.]
 [ 0.]] [[-0.001]
 [-0.002]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]]
using explorer policy with actor:  1
Starting evaluation
siam score:  -0.007752330571582372
11 9
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.14706630735019027, 0.1960672056413813, 0.29406900222376336, 0.11766576837547565, 0.14706630735019027, 0.09806540905899926]
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
deleting a thread, now have 2 threads
Frames:  972 train batches done:  24 episodes:  23
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.14706630735019027, 0.1960672056413813, 0.29406900222376336, 0.11766576837547565, 0.14706630735019027, 0.09806540905899926]
siam score:  -0.14552478
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.14706630735019027, 0.1960672056413813, 0.29406900222376336, 0.11766576837547565, 0.14706630735019027, 0.09806540905899926]
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.14706630735019027, 0.1960672056413813, 0.29406900222376336, 0.11766576837547565, 0.14706630735019027, 0.09806540905899926]
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
15 11
siam score:  -0.41948846
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
siam score:  -0.4211656
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.12660202968789597, 0.15190601093764589, 0.3796418421853953, 0.09497205312570854, 0.15190601093764589, 0.09497205312570854]
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.12660202968789597, 0.15190601093764589, 0.3796418421853953, 0.09497205312570854, 0.15190601093764589, 0.09497205312570854]
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
siam score:  -0.4264693
siam score:  -0.4288983
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.16739689611260042, 0.20085468163538386, 0.251041359919559, 0.11163392024129473, 0.14349847788204084, 0.12557466420912117]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.1762418912067902, 0.2114675347704564, 0.2114675347704564, 0.11753248526734662, 0.15108071723274297, 0.13220983675220752]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
siam score:  -0.4702117
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.17585988546504555, 0.20514801615014647, 0.20514801615014647, 0.12314125023186386, 0.13680904455157764, 0.15389378745121984]
siam score:  -0.49786025
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.2 0.  0.2 0.  0.2]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.20781398523539907, 0.18185656464297442, 0.18185656464297442, 0.12128924992731688, 0.1455161758135799, 0.16166745973775526]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.20781398523539907, 0.18185656464297442, 0.18185656464297442, 0.12128924992731688, 0.1455161758135799, 0.16166745973775526]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.19393869031892083, 0.19393869031892083, 0.17240814533029913, 0.12934705535305574, 0.1551837093394018, 0.1551837093394018]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
first move QE:  -0.27235179589423375
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
siam score:  -0.5345713
siam score:  -0.5334832
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.17877679227551801, 0.2011027221689311, 0.17877679227551801, 0.13412493248869187, 0.16091604836078754, 0.14630271243055354]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
first move QE:  -0.23123598730332973
siam score:  -0.47892684
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.18075506404050928, 0.2008178772301474, 0.164340035067169, 0.1390861443389532, 0.164340035067169, 0.1506608442560521]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.18075506404050928, 0.2008178772301474, 0.164340035067169, 0.1390861443389532, 0.164340035067169, 0.1506608442560521]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
siam score:  -0.4780421
siam score:  -0.4667694
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
probs:  [0.1885928451715203, 0.1885928451715203, 0.17146595895742864, 0.13476548849866077, 0.17146595895742864, 0.14511690324344145]
siam score:  -0.46998665
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.19056549556146302, 0.19056549556146302, 0.1732594470514381, 0.13617505738709892, 0.1732594470514381, 0.13617505738709892]
siam score:  -0.4896994
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
from probs:  [0.19056549556146302, 0.19056549556146302, 0.1732594470514381, 0.13617505738709892, 0.1732594470514381, 0.13617505738709892]
siam score:  -0.5272847
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
siam score:  -0.52278423
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.1765588873433364, 0.1912522183809957, 0.1912522183809957, 0.12087889604273276, 0.1765588873433364, 0.14349889250860298]
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.1765588873433364, 0.1912522183809957, 0.1912522183809957, 0.12087889604273276, 0.1765588873433364, 0.14349889250860298]
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.2 0.  0.2 0.2 0. ]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.375]
 [0.374]
 [0.447]
 [0.46 ]
 [0.374]
 [0.374]] [[ 0.   ]
 [-0.166]
 [ 0.   ]
 [-0.213]
 [-0.158]
 [ 0.   ]
 [ 0.   ]] [[0.617]
 [0.508]
 [0.617]
 [0.62 ]
 [0.683]
 [0.617]
 [0.617]]
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.18843043575020238, 0.1749893317081717, 0.18843043575020238, 0.1290066073538563, 0.1749893317081717, 0.14415385772939548]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.18965136111459413, 0.17612315087980704, 0.18965136111459413, 0.12336313096413748, 0.17612315087980704, 0.14508784504706024]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.18965136111459413, 0.17612315087980704, 0.18965136111459413, 0.12336313096413748, 0.17612315087980704, 0.14508784504706024]
line 256 mcts: sample exp_bonus -0.1321830328755431
siam score:  -0.529936
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
siam score:  -0.54001814
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.15543128129864167, 0.1651285946260866, 0.20317190075683192, 0.13215772931277392, 0.18867921270702417, 0.15543128129864167]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.15543128129864167, 0.1651285946260866, 0.20317190075683192, 0.13215772931277392, 0.18867921270702417, 0.15543128129864167]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
using another actor
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
80 51
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.094]
 [0.032]
 [0.094]
 [0.094]
 [0.094]
 [0.072]] [[-0.404]
 [ 0.   ]
 [ 0.025]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.043]] [[0.048]
 [0.094]
 [0.032]
 [0.094]
 [0.094]
 [0.094]
 [0.072]]
siam score:  -0.5277912
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
Printing some Q and Qe and total Qs values:  [[1.171]
 [1.035]
 [1.079]
 [1.2  ]
 [1.229]
 [1.228]
 [1.171]] [[ 0.   ]
 [ 0.035]
 [ 0.064]
 [-0.024]
 [ 0.01 ]
 [ 0.009]
 [ 0.   ]] [[1.87 ]
 [1.654]
 [1.792]
 [1.886]
 [2.001]
 [1.998]
 [1.87 ]]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.17466326646746277, 0.1849184478913845, 0.1849184478913845, 0.13107874541579537, 0.14975782586650999, 0.17466326646746277]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
line 256 mcts: sample exp_bonus -0.1216474270842677
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.032]
 [0.041]
 [0.047]
 [0.068]
 [0.11 ]
 [0.156]] [[0.566]
 [0.588]
 [0.407]
 [0.644]
 [0.686]
 [0.48 ]
 [0.491]] [[-0.023]
 [ 0.054]
 [-0.109]
 [ 0.141]
 [ 0.224]
 [ 0.104]
 [ 0.205]]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.17647302940039292, 0.18683446926999042, 0.17647302940039292, 0.13243690995460344, 0.15130953257422752, 0.17647302940039292]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
siam score:  -0.5703682
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.17647302940039292, 0.18683446926999042, 0.17647302940039292, 0.13243690995460344, 0.15130953257422752, 0.17647302940039292]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.17647302940039292, 0.18683446926999042, 0.17647302940039292, 0.13243690995460344, 0.15130953257422752, 0.17647302940039292]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.159]
 [0.339]
 [0.351]
 [0.345]
 [0.344]
 [0.34 ]] [[-0.082]
 [-0.092]
 [-0.075]
 [-0.061]
 [-0.074]
 [-0.061]
 [-0.032]] [[0.318]
 [0.159]
 [0.339]
 [0.351]
 [0.345]
 [0.344]
 [0.34 ]]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.16876686484736567, 0.18858277529489664, 0.1781243781142553, 0.1336761900965295, 0.15272541353269772, 0.1781243781142553]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 5 threads
using explorer policy with actor:  1
using another actor
line 256 mcts: sample exp_bonus -0.07661350961115232
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.1712832541882893, 0.1913946991943207, 0.18078032544113745, 0.13025590637598522, 0.15500256061197815, 0.1712832541882893]
siam score:  -0.60453415
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.173120823311375, 0.18271978149401488, 0.18271978149401488, 0.13165332396237073, 0.15666546642684948, 0.173120823311375]
siam score:  -0.61000943
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.173120823311375, 0.18271978149401488, 0.18271978149401488, 0.13165332396237073, 0.15666546642684948, 0.173120823311375]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
siam score:  -0.616349
using explorer policy with actor:  1
siam score:  -0.61707294
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.16737366635321374, 0.18593240812507394, 0.18593240812507394, 0.13396793116386538, 0.15941991987955936, 0.16737366635321374]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
using another actor
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.1694700200683326, 0.18826124833887437, 0.18826124833887437, 0.13044208442951502, 0.15409537875607113, 0.1694700200683326]
Printing some Q and Qe and total Qs values:  [[1.259]
 [0.93 ]
 [1.319]
 [1.336]
 [1.316]
 [1.313]
 [1.295]] [[ 0.462]
 [-0.009]
 [ 0.28 ]
 [ 0.214]
 [ 0.042]
 [ 0.097]
 [ 0.103]] [[2.675]
 [1.861]
 [2.735]
 [2.747]
 [2.648]
 [2.661]
 [2.628]]
first move QE:  -0.10887187223592316
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.1694700200683326, 0.18826124833887437, 0.18826124833887437, 0.13044208442951502, 0.15409537875607113, 0.1694700200683326]
main train batch thing paused
add a thread
from probs:  [0.1694700200683326, 0.18826124833887437, 0.18826124833887437, 0.13044208442951502, 0.15409537875607113, 0.1694700200683326]
Adding thread: now have 6 threads
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]] [[0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]] [[0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.017]
 [-0.016]
 [-0.016]
 [-0.018]
 [-0.019]
 [-0.016]] [[ 0.447]
 [-0.354]
 [-0.02 ]
 [-0.048]
 [-0.017]
 [-0.022]
 [-0.069]] [[-0.505]
 [-1.299]
 [-0.963]
 [-0.992]
 [-0.965]
 [-0.971]
 [-1.014]]
from probs:  [0.1694700200683326, 0.18826124833887437, 0.18826124833887437, 0.13044208442951502, 0.15409537875607113, 0.1694700200683326]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.17200503198451975, 0.19107734824670988, 0.19107734824670988, 0.1323932982092017, 0.14961579115499216, 0.1638311821578668]
siam score:  -0.63566893
siam score:  -0.6418986
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
siam score:  -0.6413635
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.17374913644800327, 0.19301484305196465, 0.18287499747093233, 0.13373574580900657, 0.15113287217378774, 0.16549240504630552]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.1668702075288165, 0.1946217828377757, 0.1843975182502644, 0.1348491590954021, 0.15239112475892477, 0.1668702075288165]
first move QE:  -0.10240134317162233
main train batch thing paused
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.16841998450195592, 0.19642935401672773, 0.17682279535638745, 0.13610148121568075, 0.15380640040729235, 0.16841998450195592]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.16984717821465253, 0.19809389958636872, 0.16984717821465253, 0.13725480740113388, 0.15510975836853977, 0.16984717821465253]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
siam score:  -0.64647025
siam score:  -0.6470576
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.232]
 [0.248]
 [0.264]
 [0.267]
 [0.265]
 [0.261]] [[0.157]
 [0.003]
 [0.15 ]
 [0.181]
 [0.175]
 [0.157]
 [0.135]] [[-0.313]
 [-0.612]
 [-0.336]
 [-0.251]
 [-0.256]
 [-0.289]
 [-0.334]]
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.205]
 [0.21 ]
 [0.232]
 [0.234]
 [0.238]
 [0.234]] [[ 0.02 ]
 [-0.017]
 [ 0.013]
 [ 0.042]
 [ 0.008]
 [ 0.031]
 [ 0.063]] [[-0.532]
 [-0.639]
 [-0.581]
 [-0.488]
 [-0.54 ]
 [-0.496]
 [-0.448]]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.16819002391142834, 0.17618096805500286, 0.17618096805500286, 0.1423731274475723, 0.16089394447599076, 0.17618096805500286]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.16954484731616268, 0.1776001610216164, 0.16954484731616268, 0.14351998765238905, 0.16218999567205272, 0.1776001610216164]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.16954484731616268, 0.1776001610216164, 0.16954484731616268, 0.14351998765238905, 0.16218999567205272, 0.1776001610216164]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
using explorer policy with actor:  1
siam score:  -0.6333829
main train batch thing paused
from probs:  [0.167121236022851, 0.167121236022851, 0.17469970528931358, 0.14788358326952292, 0.160174305861927, 0.18299993353353453]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.17048477823080183, 0.15687822479674582, 0.1782157745001518, 0.15085994154706722, 0.15687822479674582, 0.1866830561284875]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.17048477823080183, 0.15687822479674582, 0.1782157745001518, 0.15085994154706722, 0.15687822479674582, 0.1866830561284875]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.745]
 [0.75 ]
 [0.756]
 [0.764]
 [0.752]
 [0.747]] [[-0.003]
 [-0.083]
 [-0.012]
 [-0.039]
 [-0.11 ]
 [-0.15 ]
 [-0.084]] [[0.643]
 [0.548]
 [0.652]
 [0.629]
 [0.551]
 [0.472]
 [0.551]]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.17048477823080183, 0.15687822479674582, 0.1782157745001518, 0.15085994154706722, 0.15687822479674582, 0.1866830561284875]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[-0.269]
 [-0.269]
 [-0.269]
 [-0.269]
 [-0.269]
 [-0.269]
 [-0.269]] [[-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.17048477823080183, 0.15687822479674582, 0.1782157745001518, 0.15085994154706722, 0.15687822479674582, 0.1866830561284875]
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.507]
 [0.464]] [[ 0.006]
 [-0.32 ]
 [-0.32 ]
 [-0.32 ]
 [-0.32 ]
 [-0.003]
 [-0.32 ]] [[-0.037]
 [-0.137]
 [-0.137]
 [-0.137]
 [-0.137]
 [ 0.162]
 [-0.137]]
siam score:  -0.654589
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
main train batch thing paused
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.17048477823080183, 0.15687822479674582, 0.1782157745001518, 0.15085994154706722, 0.15687822479674582, 0.1866830561284875]
using explorer policy with actor:  1
main train batch thing paused
using another actor
Printing some Q and Qe and total Qs values:  [[0.882]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.877]
 [0.878]] [[ 0.005]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.11 ]
 [-0.095]] [[1.262]
 [1.222]
 [1.222]
 [1.222]
 [1.222]
 [1.023]
 [1.056]]
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.021]
 [0.08 ]
 [0.08 ]
 [0.123]
 [0.08 ]
 [0.08 ]] [[0.685]
 [0.258]
 [0.81 ]
 [0.81 ]
 [0.996]
 [0.81 ]
 [0.81 ]] [[-0.558]
 [-0.983]
 [-0.496]
 [-0.496]
 [-0.286]
 [-0.496]
 [-0.496]]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.1599458182586861, 0.1599458182586861, 0.18170064821487056, 0.14812837976396864, 0.1599458182586861, 0.19033351724510247]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.1599458182586861, 0.1599458182586861, 0.18170064821487056, 0.14812837976396864, 0.1599458182586861, 0.19033351724510247]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.1599458182586861, 0.1599458182586861, 0.18170064821487056, 0.14812837976396864, 0.1599458182586861, 0.19033351724510247]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.16093330140406198, 0.1547594410548104, 0.18282244264231765, 0.14904290369439227, 0.16093330140406198, 0.19150860980035558]
Printing some Q and Qe and total Qs values:  [[1.175]
 [1.175]
 [1.175]
 [1.175]
 [1.175]
 [1.153]
 [1.148]] [[-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.149]
 [-0.064]] [[1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.481]
 [1.555]]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.608]
 [0.617]
 [0.619]
 [0.617]
 [0.618]
 [0.611]] [[0.139]
 [0.003]
 [0.028]
 [0.036]
 [0.019]
 [0.105]
 [0.279]] [[0.618]
 [0.608]
 [0.617]
 [0.619]
 [0.617]
 [0.618]
 [0.611]]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.1609332957682769, 0.15475942935024217, 0.18282245852312726, 0.14904288637058039, 0.1609332957682769, 0.19150863421949643]
using explorer policy with actor:  0
siam score:  -0.66056734
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.16365271525665187, 0.1573745178338593, 0.1778469007342698, 0.15156137207201434, 0.16365271525665187, 0.18591177884655277]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.16365271240896276, 0.15737450905430464, 0.17784691129775498, 0.15156135779999155, 0.16365271240896276, 0.18591179703002333]
line 256 mcts: sample exp_bonus -0.07322701287661529
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
siam score:  -0.6590105
Printing some Q and Qe and total Qs values:  [[1.075]
 [1.043]
 [1.043]
 [1.043]
 [1.043]
 [1.043]
 [1.071]] [[-0.046]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [ 0.024]] [[1.806]
 [1.792]
 [1.792]
 [1.792]
 [1.792]
 [1.792]
 [1.893]]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.16365270963874615, 0.15737450051360127, 0.17784692157385634, 0.15156134391624487, 0.16365270963874615, 0.18591181471880533]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.16468664684777537, 0.15836877278805725, 0.17897053602626853, 0.1525188893994293, 0.15836877278805725, 0.1870863821504124]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.1672888876833162, 0.16087118385484217, 0.17424140016416312, 0.15492886549514395, 0.16087118385484217, 0.18179847894769235]
using explorer policy with actor:  1
main train batch thing paused
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.204]
 [0.183]] [[ 0.024]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [ 0.008]
 [ 0.004]] [[-0.535]
 [-0.645]
 [-0.645]
 [-0.645]
 [-0.645]
 [-0.431]
 [-0.474]]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.16975192150159635, 0.163239722371191, 0.16975192150159635, 0.1572099083615564, 0.163239722371191, 0.17680680389286885]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.16975192150159635, 0.163239722371191, 0.16975192150159635, 0.1572099083615564, 0.163239722371191, 0.17680680389286885]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
Starting evaluation
Printing some Q and Qe and total Qs values:  [[1.169]
 [0.95 ]
 [1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.134]] [[ 0.065]
 [-0.   ]
 [ 0.065]
 [ 0.065]
 [ 0.065]
 [ 0.065]
 [ 0.047]] [[1.848]
 [1.303]
 [1.848]
 [1.848]
 [1.848]
 [1.848]
 [1.75 ]]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]]
Printing some Q and Qe and total Qs values:  [[1.285]
 [1.056]
 [1.315]
 [1.312]
 [1.302]
 [1.29 ]
 [1.256]] [[-0.004]
 [-0.003]
 [-0.007]
 [-0.006]
 [-0.006]
 [-0.005]
 [-0.004]] [[2.468]
 [2.011]
 [2.523]
 [2.519]
 [2.499]
 [2.476]
 [2.409]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[-0.008]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[0.546]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]]
line 256 mcts: sample exp_bonus -0.006774064681539649
Printing some Q and Qe and total Qs values:  [[1.254]
 [1.254]
 [1.254]
 [1.254]
 [1.254]
 [1.254]
 [1.254]] [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.52 ]
 [0.497]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.52 ]
 [0.497]]
Printing some Q and Qe and total Qs values:  [[0.742]
 [0.724]
 [0.732]
 [0.735]
 [0.743]
 [0.741]
 [0.737]] [[-0.093]
 [-0.219]
 [-0.083]
 [-0.246]
 [-0.437]
 [-0.311]
 [-0.232]] [[1.204]
 [0.96 ]
 [1.202]
 [0.936]
 [0.634]
 [0.838]
 [0.963]]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.361]
 [0.451]
 [0.455]
 [0.455]
 [0.454]
 [0.449]] [[ 0.001]
 [-0.004]
 [ 0.002]
 [ 0.003]
 [ 0.004]
 [ 0.003]
 [ 0.004]] [[0.435]
 [0.361]
 [0.451]
 [0.455]
 [0.455]
 [0.454]
 [0.449]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.448]
 [0.448]
 [0.448]
 [0.454]
 [0.448]
 [0.448]] [[-0.001]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]] [[0.427]
 [0.448]
 [0.448]
 [0.448]
 [0.454]
 [0.448]
 [0.448]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.302]
 [0.391]
 [0.395]
 [0.395]
 [0.393]
 [0.391]] [[ 0.001]
 [-0.004]
 [ 0.002]
 [ 0.003]
 [ 0.004]
 [ 0.004]
 [ 0.003]] [[0.376]
 [0.302]
 [0.391]
 [0.395]
 [0.395]
 [0.393]
 [0.391]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.64 ]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.003]] [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.64 ]]
main train batch thing paused
main train batch thing paused
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]] [[0.78 ]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[1.1  ]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]]
main train batch thing paused
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.003307067414377404
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.643]] [[0.001]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.643]]
main train batch thing paused
using explorer policy with actor:  0
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.562]
 [0.637]
 [0.639]
 [0.645]
 [0.644]
 [0.634]] [[-0.001]
 [-0.006]
 [ 0.001]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]] [[0.614]
 [0.562]
 [0.637]
 [0.639]
 [0.645]
 [0.644]
 [0.634]]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.338]
 [0.441]
 [0.453]
 [0.455]
 [0.452]
 [0.446]] [[-0.001]
 [-0.006]
 [ 0.   ]
 [ 0.002]
 [ 0.003]
 [ 0.003]
 [ 0.003]] [[0.396]
 [0.338]
 [0.441]
 [0.453]
 [0.455]
 [0.452]
 [0.446]]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.628]
 [0.623]
 [0.625]
 [0.623]
 [0.619]
 [0.622]] [[-0.001]
 [-0.004]
 [ 0.   ]
 [ 0.001]
 [-0.   ]
 [ 0.001]
 [ 0.   ]] [[0.626]
 [0.628]
 [0.623]
 [0.625]
 [0.623]
 [0.619]
 [0.622]]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]]
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.16850131622825246, 0.16227714162513424, 0.17522342479962014, 0.15649755092223874, 0.16227714162513424, 0.17522342479962014]
line 256 mcts: sample exp_bonus -0.0015866269621835794
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.373]
 [0.374]
 [0.372]
 [0.378]
 [0.373]
 [0.376]] [[-0.007]
 [-0.008]
 [-0.007]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.007]] [[0.373]
 [0.373]
 [0.374]
 [0.372]
 [0.378]
 [0.373]
 [0.376]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.1694808487210134, 0.16322048610011494, 0.17624204035158375, 0.1574072922378521, 0.1574072922378521, 0.17624204035158375]
line 256 mcts: sample exp_bonus -0.01956652406571217
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.1706345519495629, 0.164331557173404, 0.17744178630781451, 0.15847877630982787, 0.15847877630982787, 0.1706345519495629]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.373]
 [0.373]
 [0.377]
 [0.374]
 [0.375]
 [0.376]] [[-0.007]
 [-0.008]
 [-0.006]
 [-0.005]
 [-0.005]
 [-0.006]
 [-0.01 ]] [[0.373]
 [0.373]
 [0.373]
 [0.377]
 [0.374]
 [0.375]
 [0.376]]
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.376]
 [0.376]
 [0.374]
 [0.377]
 [0.419]
 [0.443]] [[-0.006]
 [-0.008]
 [-0.007]
 [-0.006]
 [-0.006]
 [-0.022]
 [-0.026]] [[0.369]
 [0.376]
 [0.376]
 [0.374]
 [0.377]
 [0.419]
 [0.443]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.17266376061440025, 0.16628576747018886, 0.1795519932101485, 0.16036334526484977, 0.15484936597022367, 0.16628576747018886]
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.668]
 [0.666]
 [0.655]
 [0.648]
 [0.635]
 [0.638]] [[ 0.002]
 [-0.001]
 [-0.   ]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [ 0.001]] [[0.652]
 [0.668]
 [0.666]
 [0.655]
 [0.648]
 [0.635]
 [0.638]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  0
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.613]
 [0.611]
 [0.605]
 [0.605]
 [0.603]
 [0.602]] [[0.007]
 [0.007]
 [0.008]
 [0.009]
 [0.008]
 [0.009]
 [0.009]] [[0.61 ]
 [0.613]
 [0.611]
 [0.605]
 [0.605]
 [0.603]
 [0.602]]
STARTED EXPV TRAINING ON FRAME NO.  10935
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.17711126663374036, 0.1644939535338915, 0.1644939535338915, 0.1644939535338915, 0.15883791662706273, 0.1705689561375224]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.772]
 [0.81 ]
 [0.811]
 [0.808]
 [0.807]
 [0.806]] [[-0.009]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.609]
 [0.597]
 [0.673]
 [0.675]
 [0.671]
 [0.67 ]
 [0.666]]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
siam score:  -0.8356201
from probs:  [0.17267531311522943, 0.1665252867502299, 0.16079940013454064, 0.1665252867502299, 0.16079940013454064, 0.17267531311522943]
siam score:  -0.83586276
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.498]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.007]] [[0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.498]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.01 ]
 [0.017]
 [0.015]
 [0.013]
 [0.017]] [[-0.002]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[-1.301]
 [-1.3  ]
 [-1.31 ]
 [-1.295]
 [-1.3  ]
 [-1.304]
 [-1.297]]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
rdn probs:  [0.17549542343674202, 0.16924495565648393, 0.16342555461969188, 0.15291308823064823, 0.16342555461969188, 0.17549542343674202]
first move QE:  -0.04836972617416052
siam score:  -0.8624978
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.17756978388758254, 0.17124543186917018, 0.1598615982360279, 0.15472051207912493, 0.16535724205892416, 0.17124543186917018]
using another actor
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.17756979027182707, 0.17124543455023433, 0.15986159425136737, 0.15472050508413715, 0.1653572412921997, 0.17124543455023433]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.543]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.551]
 [0.552]
 [0.566]
 [0.566]
 [0.566]
 [0.543]] [[0.002]
 [0.004]
 [0.003]
 [0.002]
 [0.002]
 [0.002]
 [0.005]] [[0.546]
 [0.551]
 [0.552]
 [0.566]
 [0.566]
 [0.566]
 [0.543]]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.17336263810205277, 0.16740164247195474, 0.1618380465505299, 0.15663339230145507, 0.16740164247195474, 0.17336263810205277]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.17336263810205277, 0.16740164247195474, 0.1618380465505299, 0.15663339230145507, 0.16740164247195474, 0.17336263810205277]
siam score:  -0.87080854
deleting a thread, now have 5 threads
Frames:  12024 train batches done:  1411 episodes:  234
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.16935337396529504, 0.16935337396529504, 0.16372490589962468, 0.15845956480593307, 0.16372490589962468, 0.17538387546422757]
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[1.059]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.17019349283250432, 0.17019349283250432, 0.16453710337321342, 0.1542848974782486, 0.16453710337321342, 0.17625391011031602]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.16641977333921734, 0.16641977333921734, 0.16641977333921734, 0.15605025358634558, 0.16641977333921734, 0.17827065305678505]
deleting a thread, now have 4 threads
Frames:  12240 train batches done:  1433 episodes:  239
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.127]
 [0.111]
 [0.105]
 [0.107]
 [0.112]
 [0.11 ]] [[-0.005]
 [-0.004]
 [-0.006]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[-0.405]
 [-0.355]
 [-0.389]
 [-0.402]
 [-0.399]
 [-0.387]
 [-0.392]]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.1646147507317442, 0.17008462134377988, 0.1646147507317442, 0.15466953143713383, 0.17008462134377988, 0.17593172441181806]
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.649]
 [0.702]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[-0.004]
 [-0.003]
 [-0.005]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.353]
 [0.355]
 [0.459]
 [0.395]
 [0.395]
 [0.395]
 [0.395]]
first move QE:  -0.045014036272848335
deleting a thread, now have 3 threads
Frames:  12466 train batches done:  1459 episodes:  242
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.323]
 [0.309]
 [0.309]
 [0.309]
 [0.318]
 [0.309]] [[-0.011]
 [-0.009]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.011]
 [-0.011]] [[-0.281]
 [-0.295]
 [-0.323]
 [-0.323]
 [-0.323]
 [-0.305]
 [-0.322]]
using explorer policy with actor:  1
siam score:  -0.87090945
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.1672448227382495, 0.16203488525703696, 0.16203488525703696, 0.15714070156256457, 0.1728020893848762, 0.1787426158002358]
Printing some Q and Qe and total Qs values:  [[0.846]
 [0.846]
 [0.846]
 [0.846]
 [1.225]
 [0.846]
 [1.137]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.017]
 [ 0.   ]
 [-0.016]] [[1.442]
 [1.442]
 [1.442]
 [1.442]
 [2.166]
 [1.442]
 [1.992]]
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.86 ]
 [0.87 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.872]] [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[0.949]
 [0.983]
 [1.004]
 [0.983]
 [0.983]
 [0.983]
 [1.007]]
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[1.276]
 [1.259]
 [1.259]
 [1.259]
 [1.259]
 [1.259]
 [1.259]]
line 256 mcts: sample exp_bonus -0.0065747455308304585
siam score:  -0.8755705
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.019]
 [-0.023]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.021]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.   ]
 [-0.001]
 [-0.001]] [[-0.549]
 [-0.536]
 [-0.545]
 [-0.539]
 [-0.539]
 [-0.54 ]
 [-0.541]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.6  ]
 [0.705]
 [0.703]
 [0.7  ]
 [0.704]
 [0.692]] [[0.006]
 [0.009]
 [0.005]
 [0.005]
 [0.006]
 [0.006]
 [0.006]] [[1.38 ]
 [1.192]
 [1.395]
 [1.392]
 [1.386]
 [1.394]
 [1.371]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.1735574115986581, 0.1582917612877848, 0.16815082711355714, 0.1582917612877848, 0.1735574115986581, 0.16815082711355714]
Printing some Q and Qe and total Qs values:  [[1.014]
 [0.593]
 [0.599]
 [0.593]
 [0.585]
 [0.595]
 [0.612]] [[-0.01 ]
 [ 0.006]
 [ 0.008]
 [ 0.008]
 [ 0.008]
 [ 0.008]
 [ 0.008]] [[1.498]
 [0.677]
 [0.691]
 [0.679]
 [0.665]
 [0.684]
 [0.718]]
Printing some Q and Qe and total Qs values:  [[1.266]
 [0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]] [[-0.013]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[1.997]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
siam score:  -0.90477484
using explorer policy with actor:  1
siam score:  -0.90270126
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.16983449969775882, 0.15987670489698144, 0.16983449969775882, 0.15532457013091178, 0.1752952258788303, 0.16983449969775882]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.17071020430816158, 0.1607010648934689, 0.17071020430816158, 0.15612545830389507, 0.1761990872129931, 0.16555398097331986]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.16303748095502865, 0.16303748095502865, 0.1731921422062218, 0.15401111539841247, 0.17876082740848906, 0.16796095307681927]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.15992513040119963, 0.16461209490046685, 0.17486482974261386, 0.15549855281855837, 0.18048729723669452, 0.16461209490046685]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.512]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.503]] [[0.2  ]
 [0.615]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.468]] [[0.485]
 [0.512]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.503]]
siam score:  -0.9058385
from probs:  [0.15992513040119963, 0.16461209490046685, 0.17486482974261386, 0.15549855281855837, 0.18048729723669452, 0.16461209490046685]
line 256 mcts: sample exp_bonus 0.8879255087297969
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.15618993355084995, 0.16534400218052422, 0.17564232938890775, 0.15618993355084995, 0.1812897991483439, 0.16534400218052422]
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
siam score:  -0.90190315
from probs:  [0.1578242421159095, 0.1670741009236754, 0.17748019208241203, 0.1578242421159095, 0.17748019208241203, 0.1623170306796815]
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.119]
 [0.14 ]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[1.575]
 [1.652]
 [1.322]
 [1.652]
 [1.652]
 [1.652]
 [1.3  ]] [[0.114]
 [0.119]
 [0.14 ]
 [0.119]
 [0.119]
 [0.119]
 [0.119]]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.15422977352026776, 0.16778718557767217, 0.17823769070525472, 0.15849784768648767, 0.17823769070525472, 0.163009811805063]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.15422977352026776, 0.16778718557767217, 0.17823769070525472, 0.15849784768648767, 0.17823769070525472, 0.163009811805063]
start point for exploration sampling:  10935
siam score:  -0.89942
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
using explorer policy with actor:  0
first move QE:  -0.007860999014245126
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.15079606949724872, 0.16846837713075752, 0.17896130978815336, 0.159141325879739, 0.17896130978815336, 0.16367160791594798]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
Printing some Q and Qe and total Qs values:  [[1.102]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.886]] [[1.766]
 [2.375]
 [2.375]
 [2.375]
 [2.375]
 [2.375]
 [1.941]] [[2.219]
 [2.08 ]
 [2.08 ]
 [2.08 ]
 [2.08 ]
 [2.08 ]
 [2.031]]
Printing some Q and Qe and total Qs values:  [[1.021]
 [0.545]
 [0.581]
 [0.585]
 [0.586]
 [0.594]
 [0.595]] [[1.204]
 [0.632]
 [0.183]
 [0.156]
 [0.203]
 [0.324]
 [0.378]] [[2.682]
 [1.349]
 [1.12 ]
 [1.111]
 [1.144]
 [1.241]
 [1.28 ]]
UNIT TEST: sample policy line 217 mcts : [0.837 0.02  0.02  0.02  0.02  0.061 0.02 ]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.1489068017291033, 0.16126660831365444, 0.18135129401355005, 0.16126660831365444, 0.18135129401355005, 0.16585739361648774]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
Printing some Q and Qe and total Qs values:  [[1.059]
 [1.05 ]
 [1.005]
 [0.933]
 [0.931]
 [0.98 ]
 [1.002]] [[1.269]
 [1.368]
 [1.558]
 [1.675]
 [1.625]
 [1.549]
 [1.456]] [[2.753]
 [2.801]
 [2.838]
 [2.771]
 [2.735]
 [2.781]
 [2.764]]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.020405622387360194
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.1503836103336374, 0.16286599740814928, 0.17761790940529967, 0.15848029384142887, 0.18314987640423108, 0.1675023126072537]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.1503836103336374, 0.16286599740814928, 0.17761790940529967, 0.15848029384142887, 0.18314987640423108, 0.1675023126072537]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.1503836103336374, 0.16286599740814928, 0.17761790940529967, 0.15848029384142887, 0.18314987640423108, 0.1675023126072537]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.15175272627788153, 0.15992312326057512, 0.17923497067421454, 0.15992312326057512, 0.18481730156721968, 0.16434875495953416]
first move QE:  0.015327017672305755
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.658]
 [0.61 ]
 [0.563]
 [0.557]
 [0.557]
 [0.561]] [[ 0.63 ]
 [ 0.68 ]
 [ 0.63 ]
 [-0.321]
 [-0.354]
 [-0.374]
 [-0.31 ]] [[0.727]
 [0.839]
 [0.727]
 [0.315]
 [0.292]
 [0.285]
 [0.316]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.15000708037946547, 0.15787016182304128, 0.18169768134902856, 0.16212047611686603, 0.18169768134902856, 0.16660691898256996]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[9.583]
 [9.583]
 [9.583]
 [9.583]
 [9.583]
 [9.583]
 [9.583]] [[2.117]
 [2.117]
 [2.117]
 [2.117]
 [2.117]
 [2.117]
 [2.117]]
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.295]
 [0.293]] [[1.058]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.579]
 [0.533]] [[1.248]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.513]
 [0.435]]
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[1.017]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[0.984]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.003]] [[2.881]
 [2.878]
 [2.878]
 [2.878]
 [2.878]
 [2.878]
 [2.939]] [[1.177]
 [1.175]
 [1.175]
 [1.175]
 [1.175]
 [1.175]
 [1.252]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.014]] [[9.777]
 [9.777]
 [9.777]
 [9.777]
 [9.777]
 [9.777]
 [9.764]] [[1.788]
 [1.788]
 [1.788]
 [1.788]
 [1.788]
 [1.788]
 [1.785]]
line 256 mcts: sample exp_bonus 6.20184903661177
siam score:  -0.9015187
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.15145750801257263, 0.15939661807145303, 0.18345452734078754, 0.15939661807145303, 0.17807687703352457, 0.16821785147020904]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.15227639722690883, 0.16025843187687722, 0.17903968987680283, 0.16025843187687722, 0.17903968987680283, 0.16912735926573097]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.632]] [[4.295]
 [4.013]
 [4.013]
 [4.013]
 [4.013]
 [4.013]
 [4.052]] [[1.865]
 [1.702]
 [1.702]
 [1.702]
 [1.702]
 [1.702]
 [1.724]]
UNIT TEST: sample policy line 217 mcts : [0.184 0.02  0.367 0.265 0.02  0.02  0.122]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.1530566448034585, 0.16107957852449103, 0.1748331791891182, 0.16107957852449103, 0.17995706963280284, 0.16999394932563827]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[2.043]
 [1.801]
 [1.801]
 [1.801]
 [1.801]
 [1.801]
 [1.801]] [[1.76 ]
 [1.436]
 [1.436]
 [1.436]
 [1.436]
 [1.436]
 [1.436]]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.1536889711775154, 0.15761372763696474, 0.17555547145159037, 0.1617450502258588, 0.18070053033960803, 0.1706962491684626]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.088]
 [0.087]] [[4.032]
 [3.791]
 [3.791]
 [3.791]
 [3.791]
 [4.329]
 [4.426]] [[0.936]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [1.102]
 [1.159]]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]] [[1.956]
 [1.956]
 [1.956]
 [1.956]
 [1.956]
 [1.956]
 [1.956]] [[3.094]
 [3.094]
 [3.094]
 [3.094]
 [3.094]
 [3.094]
 [3.094]]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]] [[2.324]
 [0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.965]] [[ 1.107]
 [-0.265]
 [-0.265]
 [-0.265]
 [-0.265]
 [-0.265]
 [-0.265]]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.15520087924550777, 0.15916424531311887, 0.1772824901936268, 0.16333620959481476, 0.1772824901936268, 0.16773368545930503]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.15520087924550777, 0.15916424531311887, 0.1772824901936268, 0.16333620959481476, 0.1772824901936268, 0.16773368545930503]
line 256 mcts: sample exp_bonus 2.5648132532449797
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]] [[2.989]
 [2.736]
 [2.736]
 [2.736]
 [2.736]
 [2.736]
 [2.736]] [[2.391]
 [2.064]
 [2.064]
 [2.064]
 [2.064]
 [2.064]
 [2.064]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]] [[0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.129]
 [0.388]
 [0.399]
 [0.377]
 [0.384]
 [0.376]] [[0.865]
 [0.49 ]
 [0.762]
 [0.835]
 [0.846]
 [0.893]
 [0.922]] [[0.95 ]
 [0.206]
 [0.814]
 [0.86 ]
 [0.82 ]
 [0.85 ]
 [0.844]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using another actor
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
siam score:  -0.9033348
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.15357039679817858, 0.1653331961675826, 0.17905646209855397, 0.15357039679817858, 0.1742347740687532, 0.1742347740687532]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.1541184565443517, 0.16592325070636266, 0.17969551056204208, 0.1505495652860693, 0.17485660845058715, 0.17485660845058715]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.1554735486036014, 0.1632141309584707, 0.18127548978649904, 0.15187327774087148, 0.1717695114559578, 0.17639404145459947]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.1554735486036014, 0.1632141309584707, 0.18127548978649904, 0.15187327774087148, 0.1717695114559578, 0.17639404145459947]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[3.108]
 [2.951]
 [2.951]
 [2.951]
 [2.951]
 [2.951]
 [2.951]] [[1.436]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.041 0.816 0.041 0.02  0.02  0.02  0.041]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.15623620757846765, 0.16401476053770442, 0.17725932368451291, 0.15261827596952032, 0.17261210854528186, 0.17725932368451291]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.15623620757846765, 0.16401476053770442, 0.17725932368451291, 0.15261827596952032, 0.17261210854528186, 0.17725932368451291]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]] [[2.674]
 [2.599]
 [2.599]
 [2.599]
 [2.599]
 [2.599]
 [2.599]] [[1.455]
 [1.366]
 [1.366]
 [1.366]
 [1.366]
 [1.366]
 [1.366]]
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.064]
 [-0.064]
 [-0.073]
 [-0.073]
 [-0.064]
 [-0.073]] [[1.916]
 [1.916]
 [1.916]
 [1.814]
 [1.841]
 [1.916]
 [1.854]] [[1.351]
 [1.351]
 [1.351]
 [1.176]
 [1.217]
 [1.351]
 [1.238]]
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.035]] [[1.643]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.591]] [[ 0.371]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [ 0.289]]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.15686116392695348, 0.16067075794867566, 0.1779683740473061, 0.15322876032484628, 0.17330256970491237, 0.1779683740473061]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[4.073]
 [1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]] [[1.415]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.1575964782595629, 0.16142393042421524, 0.17880263214479872, 0.15394704712582463, 0.17411495602279922, 0.17411495602279922]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.789]
 [0.675]
 [0.663]
 [0.653]
 [0.643]
 [0.667]] [[0.007]
 [0.679]
 [0.039]
 [0.028]
 [0.05 ]
 [0.032]
 [0.037]] [[0.727]
 [1.46 ]
 [0.805]
 [0.773]
 [0.768]
 [0.736]
 [0.788]]
first move QE:  0.10516190807331718
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.15814739227587715, 0.16198822416238345, 0.1794276770524661, 0.15098947830556997, 0.1747236141018517, 0.1747236141018517]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.1588563409536762, 0.16271439065609716, 0.1802320217373598, 0.1516663392355281, 0.175506871248335, 0.1710240361690038]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
using explorer policy with actor:  1
siam score:  -0.8985759
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.404]
 [0.371]
 [0.4  ]
 [0.371]
 [0.4  ]
 [0.362]] [[-0.242]
 [ 0.093]
 [-0.349]
 [ 0.   ]
 [-0.344]
 [ 0.   ]
 [-0.314]] [[0.39 ]
 [0.404]
 [0.371]
 [0.4  ]
 [0.371]
 [0.4  ]
 [0.362]]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
siam score:  -0.90389997
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.557]
 [0.572]
 [0.569]
 [0.572]
 [0.568]
 [0.564]] [[2.587]
 [0.952]
 [1.036]
 [1.01 ]
 [1.05 ]
 [1.025]
 [1.133]] [[1.309]
 [0.688]
 [0.738]
 [0.725]
 [0.743]
 [0.729]
 [0.766]]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[7.493]
 [7.493]
 [7.493]
 [7.493]
 [7.493]
 [7.493]
 [7.493]] [[1.906]
 [1.906]
 [1.906]
 [1.906]
 [1.906]
 [1.906]
 [1.906]]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[3.674]
 [3.01 ]
 [3.01 ]
 [3.01 ]
 [3.01 ]
 [3.01 ]
 [3.01 ]] [[1.642]
 [1.295]
 [1.295]
 [1.295]
 [1.295]
 [1.295]
 [1.295]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.16156161982054318, 0.1578203591185492, 0.17849574720851596, 0.1542491557211913, 0.1739365590656002, 0.1739365590656002]
line 256 mcts: sample exp_bonus 2.702717394396211
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.16156161982054318, 0.1578203591185492, 0.17849574720851596, 0.1542491557211913, 0.1739365590656002, 0.1739365590656002]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 5 threads
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.439]
 [0.577]
 [0.576]
 [0.576]
 [0.575]
 [0.574]] [[1.062]
 [1.16 ]
 [0.381]
 [0.262]
 [0.262]
 [0.268]
 [0.697]] [[1.484]
 [1.311]
 [1.069]
 [0.986]
 [0.987]
 [0.988]
 [1.273]]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.379]] [[2.352]
 [2.352]
 [2.352]
 [2.352]
 [2.352]
 [2.573]
 [2.597]] [[1.715]
 [1.715]
 [1.715]
 [1.715]
 [1.715]
 [2.038]
 [2.073]]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.436]
 [0.444]
 [0.446]
 [0.445]
 [0.444]
 [0.44 ]] [[2.081]
 [2.247]
 [1.734]
 [1.749]
 [1.88 ]
 [1.94 ]
 [2.465]] [[1.01 ]
 [1.178]
 [0.681]
 [0.698]
 [0.828]
 [0.886]
 [1.403]]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.638]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.612]] [[0.773]
 [0.689]
 [1.917]
 [1.917]
 [1.917]
 [1.917]
 [1.034]] [[0.58 ]
 [0.638]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.612]]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[3.473]
 [3.013]
 [3.013]
 [3.013]
 [3.013]
 [3.013]
 [3.013]] [[1.377]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]]
Printing some Q and Qe and total Qs values:  [[1.085]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]] [[0.676]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]] [[2.506]
 [2.008]
 [2.008]
 [2.008]
 [2.008]
 [2.008]
 [2.008]]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.15917238912471934, 0.15565098160500918, 0.17501872296341522, 0.15565098160500918, 0.17501872296341522, 0.179488201738432]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[3.63]
 [3.63]
 [3.63]
 [3.63]
 [3.63]
 [3.63]
 [3.63]] [[1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.48542051747439025
using another actor
siam score:  -0.9074409
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.1523769954856868, 0.15567378361224163, 0.17043083522634406, 0.1591170956555322, 0.17891613990445296, 0.1834851501157424]
line 256 mcts: sample exp_bonus 2.7795423296816346
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.082]
 [0.079]
 [0.084]
 [0.085]
 [0.084]
 [0.08 ]] [[1.69 ]
 [1.184]
 [1.48 ]
 [1.236]
 [1.25 ]
 [1.24 ]
 [1.557]] [[0.074]
 [0.082]
 [0.079]
 [0.084]
 [0.085]
 [0.084]
 [0.08 ]]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.534]
 [0.532]] [[2.028]
 [1.972]
 [1.972]
 [1.972]
 [1.972]
 [1.914]
 [1.996]] [[1.851]
 [1.796]
 [1.796]
 [1.796]
 [1.796]
 [1.733]
 [1.835]]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.1528810117901322, 0.1528810117901322, 0.17099456808812236, 0.15964340614138187, 0.17950793954817773, 0.18409206264205372]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.319]
 [0.319]
 [0.319]
 [0.3  ]
 [0.319]
 [0.298]] [[2.987]
 [2.649]
 [2.649]
 [2.649]
 [2.819]
 [2.649]
 [3.057]] [[0.065]
 [0.012]
 [0.012]
 [0.012]
 [0.032]
 [0.012]
 [0.106]]
Printing some Q and Qe and total Qs values:  [[1.272]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]] [[0.246]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]] [[ 2.08 ]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
first move QE:  0.25582648098024996
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.462]
 [0.294]
 [0.258]
 [0.282]
 [0.351]
 [0.413]] [[1.473]
 [2.021]
 [0.478]
 [0.479]
 [0.677]
 [0.804]
 [1.112]] [[0.424]
 [0.462]
 [0.294]
 [0.258]
 [0.282]
 [0.351]
 [0.413]]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.545]
 [0.526]
 [0.51 ]
 [0.512]
 [0.506]
 [0.517]] [[1.751]
 [2.656]
 [1.762]
 [1.403]
 [1.647]
 [1.638]
 [1.671]] [[0.53 ]
 [0.545]
 [0.526]
 [0.51 ]
 [0.512]
 [0.506]
 [0.517]]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.15210739289305342, 0.14605264043257607, 0.17373150882332974, 0.15868864556748535, 0.18238115519544024, 0.18703865708811512]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.15210739289305342, 0.14605264043257607, 0.17373150882332974, 0.15868864556748535, 0.18238115519544024, 0.18703865708811512]
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.264]
 [0.264]
 [0.237]
 [0.264]
 [0.241]
 [0.264]] [[2.552]
 [2.4  ]
 [2.4  ]
 [2.58 ]
 [2.4  ]
 [2.54 ]
 [2.4  ]] [[1.579]
 [1.36 ]
 [1.36 ]
 [1.61 ]
 [1.36 ]
 [1.552]
 [1.36 ]]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.15210739289305342, 0.14605264043257607, 0.17373150882332974, 0.15868864556748535, 0.18238115519544024, 0.18703865708811512]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.14998561052755902, 0.14700076583714797, 0.17485931628098433, 0.15633634391141227, 0.18356511329468317, 0.18825285014821333]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.587]
 [0.59 ]] [[2.802]
 [2.802]
 [2.802]
 [2.802]
 [2.802]
 [2.907]
 [2.928]] [[0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.587]
 [0.59 ]]
from probs:  [0.14998561052755902, 0.14700076583714797, 0.17485931628098433, 0.15633634391141227, 0.18356511329468317, 0.18825285014821333]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8240740787377924
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.052]
 [-0.039]] [[3.493]
 [3.493]
 [3.493]
 [3.493]
 [3.493]
 [3.501]
 [3.493]] [[1.409]
 [1.409]
 [1.409]
 [1.409]
 [1.409]
 [1.404]
 [1.409]]
siam score:  -0.9051898
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.604]
 [0.571]
 [0.571]
 [0.571]
 [0.57 ]
 [0.575]] [[2.354]
 [2.983]
 [2.397]
 [2.397]
 [2.397]
 [2.428]
 [2.418]] [[0.59 ]
 [0.604]
 [0.571]
 [0.571]
 [0.571]
 [0.57 ]
 [0.575]]
line 256 mcts: sample exp_bonus 1.4388133386565154
using another actor
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.147]
 [0.13 ]
 [0.116]
 [0.116]
 [0.116]
 [0.125]] [[1.77 ]
 [1.832]
 [1.774]
 [1.872]
 [1.872]
 [1.872]
 [1.575]] [[0.134]
 [0.147]
 [0.13 ]
 [0.116]
 [0.116]
 [0.116]
 [0.125]]
first move QE:  0.32683240860469326
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.15354321566347237, 0.14755172391284996, 0.1709049247135714, 0.1567261956559905, 0.18791939958266843, 0.18335454047144728]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.15397787547874023, 0.14513851202063727, 0.17138874289621572, 0.15716986783861073, 0.18845139296534175, 0.18387360880045428]
from probs:  [0.15439846308094762, 0.14280342734178605, 0.1718568975745716, 0.15759917607144536, 0.1889661633783231, 0.18437587255292634]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.156094216673781, 0.14437182664418785, 0.17374440620697526, 0.15298778331593882, 0.18640088357955847, 0.18640088357955847]
siam score:  -0.8998498
UNIT TEST: sample policy line 217 mcts : [0.102 0.041 0.265 0.143 0.122 0.184 0.143]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.156094216673781, 0.14437182664418785, 0.17374440620697526, 0.15298778331593882, 0.18640088357955847, 0.18640088357955847]
siam score:  -0.90224516
Printing some Q and Qe and total Qs values:  [[0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]] [[3.85]
 [3.85]
 [3.85]
 [3.85]
 [3.85]
 [3.85]
 [3.85]] [[1.961]
 [1.961]
 [1.961]
 [1.961]
 [1.961]
 [1.961]
 [1.961]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.572]
 [0.415]
 [0.417]
 [0.51 ]
 [0.423]
 [0.446]] [[0.318]
 [0.762]
 [0.208]
 [0.247]
 [0.297]
 [0.28 ]
 [0.337]] [[0.464]
 [0.572]
 [0.415]
 [0.417]
 [0.51 ]
 [0.423]
 [0.446]]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.15609421137425283, 0.14437181546871447, 0.17374440975475094, 0.15298777645928519, 0.18640089347149832, 0.18640089347149832]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
siam score:  -0.89891887
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.508]
 [0.483]
 [0.478]
 [0.477]
 [0.487]
 [0.488]] [[0.195]
 [0.976]
 [0.357]
 [0.246]
 [0.228]
 [0.499]
 [0.672]] [[0.476]
 [0.508]
 [0.483]
 [0.478]
 [0.477]
 [0.487]
 [0.488]]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[5.113]
 [2.485]
 [2.485]
 [2.485]
 [2.485]
 [2.485]
 [2.485]] [[1.074]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.15947458292979702, 0.14478148362451074, 0.17357995826287187, 0.15031965182419557, 0.1859221616793124, 0.1859221616793124]
line 256 mcts: sample exp_bonus 2.5436161126712813
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.15947458292979702, 0.14478148362451074, 0.17357995826287187, 0.15031965182419557, 0.1859221616793124, 0.1859221616793124]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.15947458292979702, 0.14478148362451074, 0.17357995826287187, 0.15031965182419557, 0.1859221616793124, 0.1859221616793124]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
UNIT TEST: sample policy line 217 mcts : [0.163 0.143 0.184 0.143 0.082 0.082 0.204]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.15924893192413472, 0.14484483150997782, 0.17685394354143757, 0.15315488944122221, 0.18085508254537008, 0.18504232103785753]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.15924893192413472, 0.14484483150997782, 0.17685394354143757, 0.15315488944122221, 0.18085508254537008, 0.18504232103785753]
from probs:  [0.15924893192413472, 0.14484483150997782, 0.17685394354143757, 0.15315488944122221, 0.18085508254537008, 0.18504232103785753]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.15991854902828873, 0.14545388160190303, 0.1775975869938712, 0.15379888204020248, 0.18161555016786726, 0.18161555016786726]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.15991854569305866, 0.14545387111756913, 0.17759759239643472, 0.15379887568035153, 0.18161555755629294, 0.18161555755629294]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.278]
 [0.295]
 [0.28 ]
 [0.258]
 [0.238]
 [0.253]] [[1.811]
 [1.787]
 [1.786]
 [1.757]
 [1.76 ]
 [1.798]
 [1.747]] [[1.801]
 [1.628]
 [1.658]
 [1.599]
 [1.563]
 [1.571]
 [1.539]]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.145]
 [0.123]
 [0.105]
 [0.085]
 [0.077]
 [0.105]] [[1.741]
 [1.912]
 [1.806]
 [1.819]
 [1.848]
 [1.953]
 [1.97 ]] [[0.528]
 [0.652]
 [0.467]
 [0.447]
 [0.447]
 [0.57 ]
 [0.649]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.16181247865023518, 0.14717648990825516, 0.17581212005560737, 0.1556203295670898, 0.18376646176320519, 0.17581212005560737]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.90326095
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
probs:  [0.16418211715754158, 0.14933178644567943, 0.17460894510416813, 0.15493568482751421, 0.18233252136092856, 0.17460894510416813]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.639]
 [0.639]
 [0.654]
 [0.639]
 [0.639]
 [0.639]] [[1.952]
 [1.952]
 [1.952]
 [3.616]
 [1.952]
 [1.952]
 [1.952]] [[0.499]
 [0.499]
 [0.499]
 [1.487]
 [0.499]
 [0.499]
 [0.499]]
siam score:  -0.9069203
line 256 mcts: sample exp_bonus 2.0423787787540504
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
siam score:  -0.90690595
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.16189211516894075, 0.14776555652239115, 0.17902432459135198, 0.15310796415599537, 0.18289769367815797, 0.17531234588316286]
first move QE:  0.4543191543310612
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[3.701]
 [3.232]
 [3.232]
 [3.232]
 [3.232]
 [3.232]
 [3.232]] [[1.425]
 [1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]]
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.16189211516894075, 0.14776555652239115, 0.17902432459135198, 0.15310796415599537, 0.18289769367815797, 0.17531234588316286]
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.197]
 [0.197]
 [0.187]
 [0.187]
 [0.197]
 [0.197]] [[ 0.214]
 [ 0.214]
 [ 0.214]
 [-0.   ]
 [ 0.043]
 [ 0.214]
 [ 0.214]] [[0.197]
 [0.197]
 [0.197]
 [0.187]
 [0.187]
 [0.197]
 [0.197]]
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]] [[0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]] [[0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]]
using explorer policy with actor:  0
siam score:  -0.906405
maxi score, test score, baseline:  -0.9930506849315068 -1.0 -0.9930506849315068
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.188]] [[0.299]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.446]] [[0.184]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.188]]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]] [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.5106944364659185
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[1.671]
 [1.671]
 [1.671]
 [1.671]
 [1.671]
 [1.671]
 [1.671]] [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.15757166023694907, 0.14694805920378576, 0.17324147176086488, 0.15757166023694907, 0.18424367985212498, 0.18042346870932632]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.497]
 [0.436]
 [0.431]
 [0.434]
 [0.427]
 [0.434]] [[0.515]
 [0.867]
 [0.547]
 [0.246]
 [0.478]
 [0.159]
 [0.48 ]] [[0.433]
 [0.497]
 [0.436]
 [0.431]
 [0.434]
 [0.427]
 [0.434]]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.586]] [[1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.986]] [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.586]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.933]
 [0.878]
 [0.874]
 [0.94 ]
 [0.877]
 [0.869]] [[2.308]
 [3.012]
 [2.129]
 [1.825]
 [2.404]
 [1.886]
 [1.967]] [[0.79 ]
 [1.247]
 [0.683]
 [0.501]
 [0.891]
 [0.538]
 [0.581]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7300946653644796
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.1558046361435597, 0.1479270646809626, 0.17439570479528885, 0.15862146472715502, 0.18162556482651687, 0.18162556482651687]
using explorer policy with actor:  0
using explorer policy with actor:  0
277 139
siam score:  -0.9067215
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.15402931386147714, 0.1488379490390545, 0.1720464035392969, 0.15959823248916688, 0.18274405053550233, 0.18274405053550233]
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
from probs:  [0.15402930358056907, 0.14883793453479796, 0.17204640791589226, 0.15959822673875984, 0.18274406361499043, 0.18274406361499043]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.1544100536504582, 0.14673384946168574, 0.17247171056521696, 0.15999274760592908, 0.18319581935835494, 0.18319581935835494]
rdn probs:  [0.15441004886373105, 0.14673384167707532, 0.17247171283233279, 0.15999274499948068, 0.18319582581369007, 0.18319582581369007]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.547]
 [0.463]
 [0.513]
 [0.413]
 [0.542]
 [0.63 ]] [[2.419]
 [2.517]
 [2.659]
 [2.664]
 [2.813]
 [2.832]
 [3.163]] [[1.096]
 [1.213]
 [1.235]
 [1.269]
 [1.285]
 [1.379]
 [1.615]]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.15243396957888597, 0.1474658047475231, 0.1728321078768854, 0.16055159604441635, 0.1833582608761446, 0.1833582608761446]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.422]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[0.947]
 [1.294]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]] [[0.41 ]
 [0.422]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
first move QE:  0.49954647047292927
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
siam score:  -0.90131545
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
line 256 mcts: sample exp_bonus 2.1944543446824514
using another actor
from probs:  [0.15507235279973863, 0.15001819084005585, 0.17252351579260553, 0.16333049243029168, 0.17252351579260553, 0.18653193234470275]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.1555667181984338, 0.15049644374192514, 0.1730735149067561, 0.16385118449790773, 0.16988554883949988, 0.1871265898154774]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.15395949743034273, 0.1514508040725128, 0.1741710458038026, 0.16489023277517303, 0.17096286352230103, 0.1845655563958677]
siam score:  -0.8895858
line 256 mcts: sample exp_bonus 1.5180485015433713
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.15183170325798415, 0.15183170325798415, 0.17460908645921963, 0.16530493216050066, 0.17139283559052665, 0.18502973927378485]
line 256 mcts: sample exp_bonus 0.6302927927904772
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
siam score:  -0.8892659
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.15183169758628445, 0.15183169758628445, 0.17460908949576368, 0.16530493163988266, 0.17139283739743444, 0.18502974629435043]
from probs:  [0.15183169758628445, 0.15183169758628445, 0.17460908949576368, 0.16530493163988266, 0.17139283739743444, 0.18502974629435043]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.198]
 [0.267]
 [0.269]
 [0.273]
 [0.27 ]
 [0.27 ]] [[ 0.308]
 [ 1.43 ]
 [ 0.154]
 [ 0.114]
 [-0.019]
 [ 0.079]
 [ 0.362]] [[0.263]
 [0.198]
 [0.267]
 [0.269]
 [0.273]
 [0.27 ]
 [0.27 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5569387864063138
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.1518316809634299, 0.1518316809634299, 0.17460909839539393, 0.1653049301140336, 0.17139284269319527, 0.18502976687051748]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.1522707862948848, 0.1522707862948848, 0.17511408638579531, 0.1628909170389046, 0.1718885278081436, 0.18556489617738686]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.004]
 [ 0.004]
 [ 0.004]
 [ 0.017]
 [ 0.036]
 [ 0.035]] [[3.943]
 [3.992]
 [3.992]
 [3.992]
 [4.108]
 [4.062]
 [3.927]] [[1.064]
 [1.11 ]
 [1.11 ]
 [1.11 ]
 [1.214]
 [1.22 ]
 [1.129]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.15123431895593972, 0.15369811929922317, 0.17349977391005694, 0.16441781202087755, 0.17349977391005694, 0.18365020190384568]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5462193599567458
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.545]
 [0.686]
 [0.694]
 [0.668]
 [0.698]
 [0.695]] [[3.594]
 [2.729]
 [2.264]
 [2.17 ]
 [2.176]
 [2.188]
 [2.439]] [[2.064]
 [1.25 ]
 [0.978]
 [0.905]
 [0.888]
 [0.924]
 [1.132]]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.1480624204110721, 0.15280790801522315, 0.1753050344349022, 0.16612857497424363, 0.17213498480303832, 0.18556107736152058]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.339]
 [0.424]
 [0.422]
 [0.444]
 [0.421]
 [0.415]] [[2.159]
 [1.715]
 [2.009]
 [2.145]
 [2.046]
 [2.291]
 [2.366]] [[0.802]
 [0.352]
 [0.719]
 [0.805]
 [0.783]
 [0.899]
 [0.937]]
siam score:  -0.89666384
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.15108376323046313, 0.15108376323046313, 0.17888228655277574, 0.16661259350016877, 0.16661259350016877, 0.18572499998596034]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.1491187245829604, 0.15143348611347432, 0.1792963563881785, 0.16699826192210215, 0.16699826192210215, 0.1861549090711826]
in main func line 156:  307
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.14772181638913648, 0.15230751229461742, 0.18033120949477865, 0.1679621293512592, 0.1679621293512592, 0.18371520311894907]
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.14772181638913648, 0.15230751229461742, 0.18033120949477865, 0.1679621293512592, 0.1679621293512592, 0.18371520311894907]
line 256 mcts: sample exp_bonus 2.398311275655349
deleting a thread, now have 4 threads
Frames:  22578 train batches done:  2642 episodes:  470
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.8879646
siam score:  -0.8870117
siam score:  -0.8857022
siam score:  -0.8868196
deleting a thread, now have 3 threads
Frames:  22780 train batches done:  2668 episodes:  474
from probs:  [0.1460905350929752, 0.15048728391063956, 0.18370716386632568, 0.17110651974520336, 0.1682233215140991, 0.18038517587075706]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.206]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]] [[2.505]
 [1.991]
 [2.505]
 [2.505]
 [2.505]
 [2.505]
 [2.505]] [[0.073]
 [0.206]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.1460905350929752, 0.15048728391063956, 0.18370716386632568, 0.17110651974520336, 0.1682233215140991, 0.18038517587075706]
siam score:  -0.8862036
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.1460905350929752, 0.15048728391063956, 0.18370716386632568, 0.17110651974520336, 0.1682233215140991, 0.18038517587075706]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.14656001747527686, 0.1509708958606643, 0.18429753255025824, 0.17165639449558467, 0.16876393070341364, 0.1777512289148023]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.112508729719579
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
first move QE:  0.6127582381814043
using explorer policy with actor:  1
from probs:  [0.14780047637362145, 0.15224869136798666, 0.1858574268809684, 0.1701923382944091, 0.16464535610638156, 0.17925571097663268]
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4912808297626376
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.027]] [[9.548]
 [9.548]
 [9.548]
 [9.548]
 [9.548]
 [9.548]
 [9.937]] [[1.578]
 [1.578]
 [1.578]
 [1.578]
 [1.578]
 [1.578]
 [1.684]]
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.14829889019135248, 0.15276210548796182, 0.18311196950490538, 0.1707662621081826, 0.1652005743473649, 0.17986019836023287]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.403]
 [0.437]
 [0.434]
 [0.353]
 [0.353]
 [0.404]] [[3.134]
 [2.897]
 [3.083]
 [3.055]
 [2.992]
 [2.992]
 [3.227]] [[0.41 ]
 [0.403]
 [0.437]
 [0.434]
 [0.353]
 [0.353]
 [0.404]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.567]
 [0.589]
 [0.598]
 [0.6  ]
 [0.599]
 [0.597]] [[2.61 ]
 [2.61 ]
 [2.879]
 [2.766]
 [2.759]
 [2.641]
 [2.821]] [[0.599]
 [0.599]
 [0.733]
 [0.713]
 [0.714]
 [0.673]
 [0.73 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.733]
 [0.733]
 [0.744]
 [0.745]
 [0.745]
 [0.748]] [[0.091]
 [0.634]
 [0.634]
 [0.148]
 [0.118]
 [0.047]
 [0.336]] [[1.293]
 [1.448]
 [1.448]
 [1.308]
 [1.299]
 [1.276]
 [1.379]]
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.034]
 [-0.035]
 [-0.028]] [[5.493]
 [4.974]
 [4.974]
 [4.974]
 [6.671]
 [6.431]
 [4.974]] [[-0.368]
 [-0.523]
 [-0.523]
 [-0.523]
 [ 0.031]
 [-0.051]
 [-0.523]]
Printing some Q and Qe and total Qs values:  [[1.061]
 [1.296]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]] [[1.28 ]
 [1.953]
 [1.28 ]
 [1.28 ]
 [1.28 ]
 [1.28 ]
 [1.28 ]] [[2.154]
 [2.849]
 [2.154]
 [2.154]
 [2.154]
 [2.154]
 [2.154]]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]] [[2.267]
 [2.267]
 [2.267]
 [2.267]
 [2.267]
 [2.267]
 [2.267]] [[1.515]
 [1.515]
 [1.515]
 [1.515]
 [1.515]
 [1.515]
 [1.515]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
UNIT TEST: sample policy line 217 mcts : [0.082 0.02  0.02  0.02  0.02  0.816 0.02 ]
line 256 mcts: sample exp_bonus 5.830702529310357
start point for exploration sampling:  10935
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.499]
 [0.596]
 [0.596]
 [0.592]
 [0.596]
 [0.597]] [[0.999]
 [1.478]
 [0.927]
 [1.025]
 [1.501]
 [1.271]
 [1.453]] [[0.722]
 [0.845]
 [0.673]
 [0.738]
 [1.048]
 [0.903]
 [1.025]]
siam score:  -0.8809261
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.1465886837081895, 0.15527786509818714, 0.1855736269981343, 0.16771600781370796, 0.16250753555158362, 0.18233628083019732]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
using explorer policy with actor:  1
siam score:  -0.8830183
UNIT TEST: sample policy line 217 mcts : [0.061 0.    0.061 0.061 0.061 0.02  0.735]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.543]
 [0.703]
 [0.704]
 [0.691]
 [0.684]
 [0.669]] [[2.296]
 [2.459]
 [2.191]
 [2.11 ]
 [2.256]
 [2.202]
 [2.232]] [[2.26 ]
 [2.294]
 [2.142]
 [2.032]
 [2.217]
 [2.131]
 [2.153]]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.231]
 [0.194]
 [0.197]
 [0.192]
 [0.189]
 [0.209]] [[2.44 ]
 [2.172]
 [2.353]
 [2.309]
 [2.296]
 [2.419]
 [2.263]] [[1.324]
 [0.901]
 [1.143]
 [1.076]
 [1.045]
 [1.247]
 [1.016]]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.14724693991701254, 0.1514845478838471, 0.18640697782481402, 0.16846915400898238, 0.16323728872936483, 0.18315509163597904]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.211]
 [0.211]
 [0.255]
 [0.255]
 [0.255]
 [0.21 ]] [[2.186]
 [1.486]
 [1.996]
 [1.891]
 [1.891]
 [1.891]
 [1.987]] [[1.386]
 [0.49 ]
 [1.123]
 [1.047]
 [1.047]
 [1.047]
 [1.11 ]]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
from probs:  [0.14943721480710032, 0.15373785652070765, 0.1858794945908255, 0.16566541752329045, 0.16566541752329045, 0.1796145990347856]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.1481225170482934, 0.154516084435242, 0.18682042491666645, 0.1665040232857706, 0.1665040232857706, 0.17753292702825693]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5042],
        [-0.3032],
        [-0.4432],
        [-0.4212],
        [-0.3828],
        [-0.5227],
        [-0.4408],
        [-0.0000],
        [-0.3487],
        [-0.5144]], dtype=torch.float64)
-0.024259925299500003 -0.5284742245888622
-0.0727797758985 -0.37597176599543203
-0.0727797758985 -0.515962486039108
-0.0727797758985 -0.4939854446343516
-0.0727797758985 -0.4556037994366452
-0.024259925299500003 -0.5470056689303491
-0.0727797758985 -0.5135314983077143
-0.6665174999999998 -0.6665174999999998
-0.0727797758985 -0.42148165307232016
-0.024259925299500003 -0.538638996746824
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
siam score:  -0.8819986
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.15000657557063626, 0.15210407634157083, 0.1891967215538875, 0.1686218949126806, 0.16604438915762829, 0.1740263424635967]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 5.2385826742515045
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.746]
 [0.736]
 [0.74 ]
 [0.742]
 [0.746]
 [0.742]] [[4.049]
 [4.983]
 [5.41 ]
 [5.574]
 [5.224]
 [5.029]
 [5.161]] [[0.56 ]
 [0.985]
 [1.176]
 [1.256]
 [1.094]
 [1.006]
 [1.064]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.31 ]
 [0.4  ]
 [0.399]
 [0.399]
 [0.4  ]
 [0.399]] [[5.175]
 [5.29 ]
 [5.724]
 [5.77 ]
 [5.857]
 [5.814]
 [5.621]] [[1.4  ]
 [1.409]
 [1.698]
 [1.723]
 [1.771]
 [1.747]
 [1.64 ]]
siam score:  -0.8916383
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.15000656937264942, 0.1521040709239088, 0.18919672993565337, 0.1686218956400763, 0.16604438892612486, 0.1740263452015873]
first move QE:  0.7059060289682855
336 177
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.04 ]
 [-0.041]
 [-0.043]
 [-0.043]
 [-0.037]
 [-0.039]] [[5.456]
 [5.746]
 [5.225]
 [5.477]
 [5.225]
 [4.898]
 [5.94 ]] [[-0.453]
 [-0.356]
 [-0.531]
 [-0.452]
 [-0.535]
 [-0.633]
 [-0.289]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.613650766669559
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.314]
 [0.339]
 [0.334]
 [0.333]
 [0.331]
 [0.344]] [[3.249]
 [3.257]
 [3.459]
 [3.505]
 [3.512]
 [3.476]
 [4.177]] [[1.082]
 [1.075]
 [1.29 ]
 [1.325]
 [1.332]
 [1.295]
 [1.948]]
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.345]
 [0.375]
 [0.414]
 [0.329]
 [0.379]
 [0.307]] [[4.292]
 [4.669]
 [5.764]
 [6.364]
 [4.649]
 [5.564]
 [4.652]] [[0.628]
 [0.859]
 [1.46 ]
 [1.806]
 [0.836]
 [1.357]
 [0.82 ]]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.15146762567290764, 0.15355593320732827, 0.1871458289728399, 0.16252337144336987, 0.1699732432086967, 0.17533399749485779]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.15146762567290764, 0.15355593320732827, 0.1871458289728399, 0.16252337144336987, 0.1699732432086967, 0.17533399749485779]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
Printing some Q and Qe and total Qs values:  [[1.468]
 [1.468]
 [1.468]
 [1.468]
 [1.468]
 [1.468]
 [1.468]] [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[2.878]
 [2.878]
 [2.878]
 [2.878]
 [2.878]
 [2.878]
 [2.878]]
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.018]] [[5.35 ]
 [5.35 ]
 [5.35 ]
 [5.35 ]
 [5.35 ]
 [5.35 ]
 [7.604]] [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [1.583]]
346 183
siam score:  -0.8926228
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
using explorer policy with actor:  1
using explorer policy with actor:  0
siam score:  -0.89402205
Printing some Q and Qe and total Qs values:  [[-0.081]
 [-0.075]
 [-0.082]
 [-0.06 ]
 [-0.083]
 [-0.083]
 [-0.084]] [[3.711]
 [4.025]
 [4.55 ]
 [6.035]
 [4.909]
 [4.865]
 [4.528]] [[0.247]
 [0.428]
 [0.721]
 [1.573]
 [0.924]
 [0.898]
 [0.707]]
line 256 mcts: sample exp_bonus 6.698182698595122
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.021]
 [-0.013]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.014]] [[7.198]
 [7.507]
 [7.62 ]
 [7.788]
 [7.788]
 [7.788]
 [7.815]] [[-0.017]
 [ 0.079]
 [ 0.133]
 [ 0.168]
 [ 0.168]
 [ 0.168]
 [ 0.195]]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
line 256 mcts: sample exp_bonus 1.2043047386889045
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.15054583794984602, 0.15677139241479948, 0.1848374166420487, 0.1659266195691428, 0.16838623283448878, 0.1735325005896742]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.546]
 [0.573]
 [0.576]
 [0.575]
 [0.576]
 [0.576]] [[5.979]
 [5.029]
 [6.015]
 [6.148]
 [6.036]
 [5.949]
 [5.983]] [[1.517]
 [0.866]
 [1.531]
 [1.621]
 [1.547]
 [1.493]
 [1.515]]
siam score:  -0.89303786
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.021]
 [-0.021]
 [-0.034]
 [-0.021]
 [-0.021]
 [-0.023]] [[8.048]
 [8.048]
 [8.048]
 [8.012]
 [8.048]
 [8.048]
 [8.581]] [[1.231]
 [1.231]
 [1.231]
 [1.208]
 [1.231]
 [1.231]
 [1.45 ]]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
using explorer policy with actor:  1
siam score:  -0.89430356
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.15054583794984602, 0.15677139241479948, 0.1848374166420487, 0.1659266195691428, 0.16838623283448878, 0.1735325005896742]
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.115]] [[6.928]
 [6.798]
 [6.798]
 [6.798]
 [6.798]
 [6.798]
 [6.798]] [[1.156]
 [1.077]
 [1.077]
 [1.077]
 [1.077]
 [1.077]
 [1.077]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.758]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[1.771]
 [2.215]
 [1.771]
 [1.771]
 [1.771]
 [1.771]
 [1.771]] [[0.954]
 [1.192]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]]
line 256 mcts: sample exp_bonus 8.93900022713616
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.665]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[2.28 ]
 [2.843]
 [2.28 ]
 [2.28 ]
 [2.28 ]
 [2.28 ]
 [2.28 ]] [[0.766]
 [1.014]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.14967508590763282, 0.1579269862966002, 0.18321506813375824, 0.16474377357444278, 0.16962744207200167, 0.17481164401556418]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.14840003345260186, 0.15862601294665135, 0.18402602652864525, 0.1654729731296236, 0.16788954731184913, 0.1755854066306288]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.743]
 [0.72 ]
 [0.723]
 [0.702]
 [0.724]
 [0.749]] [[3.75 ]
 [3.12 ]
 [3.481]
 [3.521]
 [3.426]
 [3.813]
 [4.581]] [[1.428]
 [1.348]
 [1.423]
 [1.443]
 [1.368]
 [1.542]
 [1.848]]
first move QE:  0.8063951604874033
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.8919767
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
siam score:  -0.88732755
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.1500601830911549, 0.1540308912966074, 0.18608475350352657, 0.16732413181051367, 0.16495033886160185, 0.17754970143659543]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[5.234]
 [5.234]
 [5.234]
 [5.234]
 [5.234]
 [5.234]
 [5.234]] [[1.681]
 [1.681]
 [1.681]
 [1.681]
 [1.681]
 [1.681]
 [1.681]]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.14843454947486645, 0.15432549779680566, 0.18644066768092596, 0.16764416356814651, 0.16526583039469278, 0.17788929108456256]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.8302755708886576
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.14918885383329375, 0.1551097383662447, 0.18738810888459015, 0.16610566678458216, 0.16610566678458216, 0.1761019653467071]
siam score:  -0.8860463
first move QE:  0.8362697877743483
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.1494916590193993, 0.1533948840300176, 0.18776844622030137, 0.16644280763694164, 0.16644280763694164, 0.17645939545639847]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.1494916590193993, 0.1533948840300176, 0.18776844622030137, 0.16644280763694164, 0.16644280763694164, 0.17645939545639847]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.659]
 [0.804]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.647]] [[2.579]
 [2.752]
 [3.325]
 [2.579]
 [2.579]
 [2.579]
 [2.907]] [[1.292]
 [1.41 ]
 [1.912]
 [1.292]
 [1.292]
 [1.292]
 [1.497]]
siam score:  -0.88867396
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
in main func line 156:  369
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.1481701934882666, 0.15389676521287396, 0.18491569538783062, 0.1645015276658506, 0.16916762314516032, 0.1793481951000179]
siam score:  -0.8870919
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.1481701934882666, 0.15389676521287396, 0.18491569538783062, 0.1645015276658506, 0.16916762314516032, 0.1793481951000179]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.14703802925617449, 0.15461435668891943, 0.1829381653990275, 0.16526856714121704, 0.16995641974022796, 0.18018446177443365]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.14703802925617449, 0.15461435668891943, 0.1829381653990275, 0.16526856714121704, 0.16995641974022796, 0.18018446177443365]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.14703802925617449, 0.15461435668891943, 0.1829381653990275, 0.16526856714121704, 0.16995641974022796, 0.18018446177443365]
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.031]
 [-0.033]] [[4.95 ]
 [4.95 ]
 [4.95 ]
 [4.95 ]
 [4.95 ]
 [4.733]
 [4.95 ]] [[1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [0.944]
 [1.12 ]]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
line 256 mcts: sample exp_bonus 3.137551149426506
using explorer policy with actor:  1
siam score:  -0.8878398
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.14817494713179452, 0.15580985575019202, 0.1815776723372835, 0.16428146668293442, 0.1712705457024469, 0.17888551239534858]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.778]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.726]] [[2.992]
 [2.85 ]
 [2.992]
 [2.992]
 [2.992]
 [2.992]
 [3.708]] [[1.609]
 [1.706]
 [1.609]
 [1.609]
 [1.609]
 [1.609]
 [2.184]]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.1492959961905115, 0.1569886683329441, 0.18023890876343057, 0.1633039363417654, 0.17256632942137, 0.17760616094997844]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.228]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.196]] [[3.306]
 [3.362]
 [3.306]
 [3.306]
 [3.306]
 [3.306]
 [4.919]] [[0.565]
 [0.612]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [1.462]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.88466537
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.14965718489421614, 0.15736846775863758, 0.1806749570130456, 0.16369901416422677, 0.17056453632240098, 0.17803583984747293]
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.722]
 [0.683]
 [0.687]
 [0.684]
 [0.685]
 [0.74 ]] [[2.566]
 [2.256]
 [2.748]
 [2.773]
 [2.888]
 [2.749]
 [3.62 ]] [[0.768]
 [0.623]
 [0.886]
 [0.902]
 [0.967]
 [0.887]
 [1.425]]
using explorer policy with actor:  0
UNIT TEST: sample policy line 217 mcts : [0.122 0.469 0.041 0.041 0.102 0.082 0.143]
siam score:  -0.8824295
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.14882010025191128, 0.15840432836629834, 0.18186423001942484, 0.16259583079499026, 0.17168725859806858, 0.17662825196930676]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
from probs:  [0.14882009343348912, 0.1584043252096065, 0.18186423582577438, 0.16259582923969515, 0.1716872605162255, 0.17662825577520938]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8794693
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.14990708227017227, 0.15753145605795754, 0.17791836857747026, 0.16378344256394142, 0.17294128195298827, 0.17791836857747026]
siam score:  -0.8868274
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.025]
 [-0.004]
 [-0.003]
 [-0.004]
 [-0.004]
 [-0.007]] [[5.466]
 [5.515]
 [5.911]
 [5.899]
 [5.876]
 [5.916]
 [6.064]] [[0.639]
 [0.656]
 [0.89 ]
 [0.884]
 [0.871]
 [0.892]
 [0.968]]
Printing some Q and Qe and total Qs values:  [[2.991]
 [2.991]
 [2.991]
 [0.625]
 [0.626]
 [2.991]
 [0.613]] [[0.   ]
 [0.   ]
 [0.   ]
 [2.273]
 [2.321]
 [0.   ]
 [3.441]] [[1.157]
 [1.157]
 [1.157]
 [1.088]
 [1.124]
 [1.157]
 [1.938]]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.632]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]] [[2.456]
 [5.107]
 [2.456]
 [2.456]
 [2.456]
 [2.456]
 [2.456]] [[0.446]
 [1.354]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]]
siam score:  -0.88321203
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.457]
 [0.546]
 [0.565]
 [0.566]
 [0.567]
 [0.528]] [[3.097]
 [3.234]
 [3.738]
 [3.869]
 [3.958]
 [4.458]
 [3.338]] [[0.196]
 [0.102]
 [0.448]
 [0.53 ]
 [0.562]
 [0.731]
 [0.278]]
siam score:  -0.88292605
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.1471782067802602, 0.15826112057024877, 0.1735257819245154, 0.16663487765601787, 0.17595215666694408, 0.17844785640201363]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.14572867689896588, 0.15853011489607105, 0.17382072139261331, 0.16691810474560279, 0.1762512202119832, 0.1787511618547637]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.467]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[2.487]
 [2.829]
 [2.487]
 [2.487]
 [2.487]
 [2.487]
 [2.487]] [[0.815]
 [1.08 ]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.14643072563363949, 0.15929383463153265, 0.1746581037123495, 0.16772223367015215, 0.17228280575269286, 0.17961229659963326]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.14675215316925883, 0.159643497753691, 0.17504149267398494, 0.16589531524012613, 0.17266098073914496, 0.180006560423794]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.14675215316925883, 0.159643497753691, 0.17504149267398494, 0.16589531524012613, 0.17266098073914496, 0.180006560423794]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 5.662646473556353
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.1473282954792028, 0.15634429616840187, 0.17572869765017984, 0.1665466127377587, 0.1733388399332483, 0.18071325803120847]
using explorer policy with actor:  1
siam score:  -0.88974166
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.272]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.252]] [[2.683]
 [3.465]
 [2.683]
 [2.683]
 [2.683]
 [2.683]
 [2.444]] [[0.254]
 [0.272]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.252]]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.14626467599661422, 0.15700469311375095, 0.17647097413856133, 0.16725010417944064, 0.1740710216834477, 0.17893853088818515]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.14626467599661422, 0.15700469311375095, 0.17647097413856133, 0.16725010417944064, 0.1740710216834477, 0.17893853088818515]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.14626467599661422, 0.15700469311375095, 0.17647097413856133, 0.16725010417944064, 0.1740710216834477, 0.17893853088818515]
siam score:  -0.89347535
first move QE:  0.9310059240826697
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9594013351565427
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
404 191
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.146626469729426, 0.15739306229958203, 0.17690751133298982, 0.16766382494874402, 0.17450162035626832, 0.17690751133298982]
using explorer policy with actor:  1
siam score:  -0.8898764
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.1454920716676901, 0.15604403066616654, 0.17753876195935936, 0.16826208845387614, 0.17512428529354868, 0.17753876195935936]
406 193
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.1461619390978103, 0.15676248090616787, 0.1783561771824519, 0.16685823500936559, 0.17593058390210217, 0.17593058390210217]
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.701]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.671]] [[3.106]
 [2.809]
 [3.106]
 [3.106]
 [3.106]
 [3.106]
 [2.636]] [[2.488]
 [2.366]
 [2.488]
 [2.488]
 [2.488]
 [2.488]
 [2.19 ]]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.497]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.61 ]] [[3.165]
 [3.057]
 [3.165]
 [3.165]
 [3.165]
 [3.165]
 [3.145]] [[2.327]
 [1.962]
 [2.327]
 [2.327]
 [2.327]
 [2.327]
 [2.248]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.151]
 [0.137]
 [0.125]] [[7.125]
 [7.125]
 [7.125]
 [7.125]
 [6.801]
 [7.125]
 [6.851]] [[1.735]
 [1.735]
 [1.735]
 [1.735]
 [1.561]
 [1.735]
 [1.567]]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.14670500825044513, 0.15362940612766665, 0.1790188650108122, 0.16747820188210968, 0.17658425936448321, 0.17658425936448321]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.14670500825044513, 0.15362940612766665, 0.1790188650108122, 0.16747820188210968, 0.17658425936448321, 0.17658425936448321]
siam score:  -0.88272935
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.147018242961052, 0.1539574253489802, 0.1794010941047168, 0.1657006570823971, 0.17696129025142698, 0.17696129025142698]
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.736]
 [0.79 ]] [[3.201]
 [3.201]
 [3.201]
 [3.201]
 [3.201]
 [3.134]
 [4.517]] [[1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.61 ]
 [2.332]]
Starting evaluation
from probs:  [0.1482878129443421, 0.15347538519066314, 0.18095030486562275, 0.16503250816980874, 0.17848943218662217, 0.17376455664294102]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.22 ]
 [0.277]
 [0.262]
 [0.277]
 [0.277]
 [0.263]] [[1.986]
 [2.106]
 [1.228]
 [1.879]
 [1.228]
 [1.228]
 [1.985]] [[0.26 ]
 [0.22 ]
 [0.277]
 [0.262]
 [0.277]
 [0.277]
 [0.263]]
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.198]
 [0.244]
 [0.254]
 [0.255]
 [0.25 ]
 [0.255]] [[1.311]
 [2.555]
 [2.104]
 [2.021]
 [2.018]
 [2.077]
 [2.143]] [[0.243]
 [0.198]
 [0.244]
 [0.254]
 [0.255]
 [0.25 ]
 [0.255]]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.335]
 [0.329]
 [0.335]
 [0.335]
 [0.333]
 [0.335]] [[3.056]
 [3.056]
 [3.078]
 [3.056]
 [3.056]
 [2.917]
 [2.882]] [[0.335]
 [0.335]
 [0.329]
 [0.335]
 [0.335]
 [0.333]
 [0.335]]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.421]
 [0.407]
 [0.411]
 [0.411]
 [0.411]
 [0.409]] [[3.21 ]
 [3.21 ]
 [3.162]
 [3.177]
 [3.145]
 [3.053]
 [2.951]] [[0.421]
 [0.421]
 [0.407]
 [0.411]
 [0.411]
 [0.411]
 [0.409]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[1.831]
 [1.613]
 [1.613]
 [1.613]
 [1.613]
 [1.613]
 [1.613]] [[0.332]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.511]
 [0.581]
 [0.588]
 [0.574]
 [0.555]
 [0.554]] [[3.231]
 [2.384]
 [3.256]
 [3.004]
 [3.276]
 [3.091]
 [3.233]] [[0.573]
 [0.511]
 [0.581]
 [0.588]
 [0.574]
 [0.555]
 [0.554]]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.415]
 [0.395]
 [0.396]
 [0.398]
 [0.406]
 [0.409]] [[3.482]
 [3.354]
 [3.87 ]
 [4.028]
 [4.123]
 [4.143]
 [4.136]] [[0.375]
 [0.415]
 [0.395]
 [0.396]
 [0.398]
 [0.406]
 [0.409]]
siam score:  -0.881846
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.693]
 [0.664]
 [0.664]
 [0.664]
 [0.659]
 [0.683]] [[2.899]
 [2.923]
 [2.522]
 [2.522]
 [2.522]
 [2.78 ]
 [4.577]] [[0.665]
 [0.693]
 [0.664]
 [0.664]
 [0.664]
 [0.659]
 [0.683]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[2.78]
 [2.78]
 [2.78]
 [2.78]
 [2.78]
 [2.78]
 [2.78]] [[0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]]
line 256 mcts: sample exp_bonus 5.385181089381766
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.384]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[3.387]
 [3.124]
 [3.387]
 [3.387]
 [3.387]
 [3.387]
 [3.387]] [[0.385]
 [0.384]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]]
siam score:  -0.8817289
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.14721341375937605, 0.15407941425661284, 0.18166261764308153, 0.16568208598295606, 0.1791920462122598, 0.17217042214571376]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.892269370344181
line 256 mcts: sample exp_bonus 2.529893080617841
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.259]
 [0.28 ]] [[2.355]
 [3.322]
 [3.322]
 [3.322]
 [3.322]
 [2.432]
 [2.947]] [[0.233]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.259]
 [0.28 ]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
siam score:  -0.88666344
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.14808942653575105, 0.15146347490887455, 0.18274371503470693, 0.166668047324469, 0.1778403260420089, 0.17319501015418967]
siam score:  -0.88719934
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.597]
 [0.691]
 [0.657]
 [0.36 ]
 [0.662]
 [0.763]] [[2.808]
 [3.047]
 [2.171]
 [2.208]
 [1.777]
 [1.97 ]
 [8.967]] [[0.54 ]
 [0.603]
 [0.435]
 [0.427]
 [0.176]
 [0.371]
 [2.133]]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.781]] [[5.353]
 [5.353]
 [5.353]
 [5.353]
 [5.353]
 [5.353]
 [7.784]] [[0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [1.895]]
rdn probs:  [0.14834251617516772, 0.15001322123913724, 0.18305605472653483, 0.1669529016978664, 0.17814428218111086, 0.1734910239801829]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.14834251043917954, 0.150013216026128, 0.18305605985688655, 0.16695290178746625, 0.17814428577393596, 0.17349102611640382]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
using explorer policy with actor:  1
siam score:  -0.88784605
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.14834251043917954, 0.150013216026128, 0.18305605985688655, 0.16695290178746625, 0.17814428577393596, 0.17349102611640382]
rdn beta is 0 so we're just using the maxi policy
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.2838],
        [-0.4526],
        [-0.0000],
        [-0.2413],
        [-0.5140],
        [-0.2161],
        [-0.0000],
        [-0.2795],
        [-0.3056],
        [-0.3039]], dtype=torch.float64)
-0.0727797758985 -0.35653713408537047
-0.024259925299500003 -0.47685427594565777
-0.014849999999999244 -0.014849999999999244
-0.024259925299500003 -0.2655842824596682
-0.024259925299500003 -0.5382327155831355
-0.0727797758985 -0.28888274013151843
-0.7351490024999999 -0.7351490024999999
-0.043375785898500004 -0.32288213294917273
-0.024259925299500003 -0.3298424137656566
-0.0727797758985 -0.3766494790794227
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]] [[6.008]
 [6.008]
 [6.008]
 [6.008]
 [6.008]
 [6.008]
 [6.008]] [[1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.572]]
line 256 mcts: sample exp_bonus 6.362199090720167
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.14731632678358944, 0.15063429252759317, 0.1813141127632441, 0.16764411691140976, 0.17888183872654387, 0.17420931228761968]
first move QE:  1.0239668377063906
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.054]
 [-0.058]
 [-0.058]
 [-0.057]
 [-0.059]
 [-0.061]] [[4.67 ]
 [4.978]
 [4.744]
 [4.648]
 [4.993]
 [4.964]
 [4.884]] [[0.833]
 [1.028]
 [0.879]
 [0.82 ]
 [1.035]
 [1.016]
 [0.965]]
siam score:  -0.8881436
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.1479989890553121, 0.1513323313813295, 0.18215433234107944, 0.168420985078001, 0.17733240152426522, 0.17276096062001278]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.744]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.752]] [[6.357]
 [6.082]
 [6.357]
 [6.357]
 [6.357]
 [6.357]
 [6.077]] [[2.244]
 [2.088]
 [2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.093]]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.644]
 [0.619]
 [0.636]
 [0.619]
 [0.619]
 [0.638]] [[5.981]
 [6.711]
 [5.981]
 [6.661]
 [5.981]
 [5.981]
 [7.263]] [[1.385]
 [1.774]
 [1.385]
 [1.741]
 [1.385]
 [1.385]
 [2.043]]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.1469309574755662, 0.15189439215326825, 0.18283086856908237, 0.16695014400896444, 0.17799102870314165, 0.1734026090899771]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.14727327901074325, 0.15224827755904013, 0.18325683015458916, 0.16733910648887398, 0.17607590218509359, 0.17380660460165992]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.1471914284657447, 0.15210730273547093, 0.18266544008782307, 0.16906706896602636, 0.17336740855155183, 0.17560135119338321]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
start point for exploration sampling:  10935
siam score:  -0.88322496
using explorer policy with actor:  1
siam score:  -0.88360673
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 7.286690972825214
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.562]] [[3.102]
 [3.068]
 [3.068]
 [3.068]
 [3.068]
 [3.068]
 [3.19 ]] [[1.99 ]
 [1.966]
 [1.966]
 [1.966]
 [1.966]
 [1.966]
 [2.119]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.87852514
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.14516376745654172, 0.15324241880192224, 0.18402863068566974, 0.16824277883087577, 0.17466120211249528, 0.17466120211249528]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.063]
 [0.158]
 [0.158]
 [0.158]
 [0.115]
 [0.272]] [[3.839]
 [3.227]
 [3.839]
 [3.839]
 [3.839]
 [2.584]
 [3.305]] [[1.874]
 [1.344]
 [1.874]
 [1.874]
 [1.874]
 [0.987]
 [1.672]]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.14548289782110385, 0.15357931225489166, 0.18443321590797487, 0.1686126546109618, 0.17504519052293416, 0.1728467288821335]
line 256 mcts: sample exp_bonus 2.524062211467516
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.14548289782110385, 0.15357931225489166, 0.18443321590797487, 0.1686126546109618, 0.17504519052293416, 0.1728467288821335]
using another actor
siam score:  -0.882809
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
using explorer policy with actor:  0
440 204
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[5.943]
 [6.044]
 [5.873]
 [6.044]
 [6.044]
 [5.888]
 [6.023]] [[1.508]
 [1.57 ]
 [1.465]
 [1.57 ]
 [1.57 ]
 [1.475]
 [1.557]]
from probs:  [0.1448496577626107, 0.15452210592116764, 0.18310956381201376, 0.16964774501685342, 0.17611977328375744, 0.17175115420359724]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.14515497807699942, 0.1548478142361792, 0.18349552999553279, 0.17000533580197474, 0.1764910060873392, 0.17000533580197474]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.14550364620836112, 0.15521976495223755, 0.1815342532169029, 0.17041369549821714, 0.1769149446260642, 0.17041369549821714]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.301]] [[7.297]
 [7.297]
 [7.297]
 [7.297]
 [7.297]
 [7.297]
 [6.748]] [[1.875]
 [1.875]
 [1.875]
 [1.875]
 [1.875]
 [1.875]
 [1.56 ]]
from probs:  [0.14550364620836112, 0.15521976495223755, 0.1815342532169029, 0.17041369549821714, 0.1769149446260642, 0.17041369549821714]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.145804202862191, 0.15554039150956447, 0.18190923576286772, 0.17076570708981934, 0.17728038539098606, 0.16870007738457135]
siam score:  -0.8813692
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.316]
 [0.288]
 [0.369]
 [0.219]
 [0.198]
 [0.285]] [[1.234]
 [1.358]
 [1.262]
 [1.423]
 [1.299]
 [1.251]
 [1.134]] [[0.327]
 [0.673]
 [0.49 ]
 [0.866]
 [0.403]
 [0.297]
 [0.314]]
446 206
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.774]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[4.453]
 [4.947]
 [4.453]
 [4.453]
 [4.453]
 [4.453]
 [4.453]] [[1.934]
 [2.238]
 [1.934]
 [1.934]
 [1.934]
 [1.934]
 [1.934]]
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]] [[5.892]
 [5.892]
 [5.892]
 [5.892]
 [5.892]
 [5.892]
 [5.892]] [[1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.699]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.644]] [[7.213]
 [6.549]
 [7.213]
 [7.213]
 [7.213]
 [7.213]
 [7.148]] [[1.999]
 [1.881]
 [1.999]
 [1.999]
 [1.999]
 [1.999]
 [1.97 ]]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.14668503638975472, 0.1547568532786934, 0.1830082123899788, 0.16971924556062853, 0.176111406820316, 0.16971924556062853]
using explorer policy with actor:  1
start point for exploration sampling:  10935
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.1463547223473382, 0.15601812517518096, 0.18212186268415875, 0.16905789766576393, 0.17534493083086644, 0.1711024612966916]
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.454]
 [0.495]
 [0.495]
 [0.495]
 [0.501]
 [0.469]] [[8.01 ]
 [7.325]
 [7.972]
 [7.972]
 [7.972]
 [8.205]
 [9.029]] [[1.48 ]
 [1.194]
 [1.474]
 [1.474]
 [1.474]
 [1.57 ]
 [1.877]]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.1463547162038267, 0.15601812195444364, 0.18212186735870753, 0.16905789838901106, 0.17534493345567748, 0.17110246263833345]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.14664740743970264, 0.15633013880035304, 0.18248608844990225, 0.16739611749823924, 0.1756956015216539, 0.17144464629014883]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[8.098]
 [8.098]
 [8.098]
 [8.098]
 [8.098]
 [8.098]
 [8.098]] [[1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]]
using explorer policy with actor:  1
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.713]
 [1.04 ]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[4.116]
 [4.612]
 [4.116]
 [4.116]
 [4.116]
 [4.116]
 [4.116]] [[1.452]
 [2.229]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.14473093438734416, 0.15746997818272845, 0.18147817610479886, 0.16664958327057888, 0.17697663899441066, 0.17269468906013893]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.625]] [[6.41 ]
 [6.41 ]
 [6.41 ]
 [6.41 ]
 [6.41 ]
 [6.41 ]
 [7.699]] [[1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.881]]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.14376765218713602, 0.15800227436862072, 0.18209163498344094, 0.16528510432193844, 0.17757487986816214, 0.17327845427070182]
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.042]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.044]
 [-0.039]] [[7.748]
 [6.705]
 [7.748]
 [7.748]
 [7.748]
 [9.328]
 [9.558]] [[1.18 ]
 [0.836]
 [1.18 ]
 [1.18 ]
 [1.18 ]
 [1.698]
 [1.775]]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
siam score:  -0.89030105
from probs:  [0.14406594350482055, 0.15833010487749946, 0.18246945489280228, 0.16562804790538171, 0.177943326764933, 0.17156312205456306]
siam score:  -0.8937466
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.318]
 [0.36 ]
 [0.36 ]
 [0.365]
 [0.358]
 [0.309]] [[6.24 ]
 [6.348]
 [5.555]
 [5.772]
 [5.926]
 [6.164]
 [6.508]] [[1.28 ]
 [1.302]
 [1.063]
 [1.132]
 [1.183]
 [1.256]
 [1.35 ]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.336]
 [0.386]
 [0.336]
 [0.336]
 [0.378]
 [0.345]] [[4.318]
 [4.318]
 [3.51 ]
 [4.318]
 [4.318]
 [3.85 ]
 [5.441]] [[0.336]
 [0.336]
 [0.386]
 [0.336]
 [0.336]
 [0.378]
 [0.345]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.89030373
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.14076792407273472, 0.1593458648247932, 0.1836400950390235, 0.16669063209886284, 0.17689169775729285, 0.17266378620729295]
siam score:  -0.8889888
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.61 ]
 [0.565]] [[3.891]
 [3.891]
 [3.891]
 [3.891]
 [3.891]
 [3.654]
 [3.519]] [[1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.292]
 [1.112]]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.608]
 [0.617]
 [0.621]
 [0.608]
 [0.608]
 [0.608]] [[6.225]
 [6.386]
 [6.269]
 [6.267]
 [6.386]
 [6.386]
 [6.368]] [[1.888]
 [1.983]
 [1.917]
 [1.919]
 [1.983]
 [1.983]
 [1.972]]
first move QE:  1.181094629984838
467 211
siam score:  -0.8829434
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.289]
 [0.289]
 [0.222]
 [0.223]
 [0.289]
 [0.272]] [[5.38 ]
 [5.38 ]
 [5.38 ]
 [5.309]
 [5.358]
 [5.38 ]
 [6.16 ]] [[0.841]
 [0.841]
 [0.841]
 [0.774]
 [0.796]
 [0.841]
 [1.178]]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.182]] [[6.049]
 [6.049]
 [6.049]
 [6.049]
 [6.049]
 [6.049]
 [6.521]] [[1.064]
 [1.064]
 [1.064]
 [1.064]
 [1.064]
 [1.064]
 [1.268]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.016]] [[8.165]
 [8.165]
 [8.165]
 [8.165]
 [8.165]
 [8.165]
 [7.981]] [[1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.556]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
siam score:  -0.8872303
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.687]
 [0.629]
 [0.629]
 [0.629]
 [0.665]
 [0.666]] [[7.787]
 [8.087]
 [7.787]
 [7.787]
 [7.787]
 [6.147]
 [7.361]] [[1.67 ]
 [1.798]
 [1.67 ]
 [1.67 ]
 [1.67 ]
 [1.097]
 [1.531]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.1438202906957054, 0.1575966624596702, 0.18762208809908063, 0.16836440130966565, 0.17229145900789927, 0.17030509842797878]
Printing some Q and Qe and total Qs values:  [[0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]] [[8.929]
 [8.929]
 [8.929]
 [8.929]
 [8.929]
 [8.929]
 [8.929]] [[1.999]
 [1.999]
 [1.999]
 [1.999]
 [1.999]
 [1.999]
 [1.999]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.1438202906957054, 0.1575966624596702, 0.18762208809908063, 0.16836440130966565, 0.17229145900789927, 0.17030509842797878]
first move QE:  1.2102708390066743
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.14444759574853197, 0.15828405630508194, 0.18607374079840588, 0.1690987611079026, 0.17104792302003888, 0.17104792302003888]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
siam score:  -0.8847639
start point for exploration sampling:  10935
siam score:  -0.8878411
479 211
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.016]] [[1.29 ]
 [1.29 ]
 [1.29 ]
 [1.29 ]
 [1.29 ]
 [1.29 ]
 [1.706]] [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.016]]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.432]
 [0.433]
 [0.434]
 [0.435]
 [0.428]
 [0.435]] [[8.265]
 [8.138]
 [8.54 ]
 [8.375]
 [8.345]
 [8.313]
 [8.497]] [[1.763]
 [1.691]
 [1.87 ]
 [1.797]
 [1.785]
 [1.766]
 [1.852]]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.14263997686660312, 0.15933624559111167, 0.18731066131364837, 0.1683053155670873, 0.17218495978925352, 0.1702228408722959]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.681]] [[1.693]
 [1.693]
 [1.693]
 [1.693]
 [1.693]
 [1.693]
 [2.441]] [[0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.681]]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.14263997686660312, 0.15933624559111167, 0.18731066131364837, 0.1683053155670873, 0.17218495978925352, 0.1702228408722959]
using explorer policy with actor:  1
using explorer policy with actor:  0
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.143986716920516, 0.15748598509633413, 0.18673439947727338, 0.16615218194994574, 0.1738106814949979, 0.1718300350609327]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.143986716920516, 0.15748598509633413, 0.18673439947727338, 0.16615218194994574, 0.1738106814949979, 0.1718300350609327]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.143986716920516, 0.15748598509633413, 0.18673439947727338, 0.16615218194994574, 0.1738106814949979, 0.1718300350609327]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.143986716920516, 0.15748598509633413, 0.18673439947727338, 0.16615218194994574, 0.1738106814949979, 0.1718300350609327]
using explorer policy with actor:  1
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.102 0.122 0.122 0.102 0.224 0.184 0.143]
first move QE:  1.2484487575986343
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
siam score:  -0.8919048
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[3.32 ]
 [3.208]
 [3.208]
 [3.208]
 [3.208]
 [3.208]
 [3.208]] [[0.609]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
Printing some Q and Qe and total Qs values:  [[1.006]
 [1.026]
 [1.006]
 [1.006]
 [1.006]
 [1.006]
 [1.006]] [[5.719]
 [6.82 ]
 [5.719]
 [5.719]
 [5.719]
 [5.719]
 [5.719]] [[1.5  ]
 [1.898]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.5  ]]
rdn beta is 0 so we're just using the maxi policy
492 213
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.14528583471190978, 0.15566157793039676, 0.18841928152019136, 0.16582557128728195, 0.17338041692420045, 0.17142731762601982]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.624]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.597]] [[8.761]
 [8.027]
 [8.761]
 [8.761]
 [8.761]
 [8.761]
 [9.284]] [[1.684]
 [1.502]
 [1.684]
 [1.684]
 [1.684]
 [1.684]
 [1.879]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.472]] [[2.351]
 [2.124]
 [2.124]
 [2.124]
 [2.124]
 [2.124]
 [2.403]] [[0.468]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.472]]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.14577550381628246, 0.15460791669436372, 0.18905432691888063, 0.16459238342610774, 0.1739647755689326, 0.17200509357543287]
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.776]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.769]] [[7.376]
 [9.561]
 [7.376]
 [7.376]
 [7.376]
 [7.376]
 [9.021]] [[1.191]
 [1.981]
 [1.191]
 [1.191]
 [1.191]
 [1.191]
 [1.777]]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.694]
 [0.663]
 [0.663]
 [0.663]
 [0.673]
 [0.63 ]] [[5.743]
 [5.361]
 [4.818]
 [4.818]
 [4.818]
 [5.543]
 [5.08 ]] [[1.146]
 [1.068]
 [0.825]
 [0.825]
 [0.825]
 [1.087]
 [0.846]]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
from probs:  [0.14493628038314788, 0.15518041742690158, 0.18743047849057068, 0.16520185583926936, 0.17460895319953793, 0.17264201466057266]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.14493627399661, 0.1551804140511049, 0.18743048459303327, 0.16520185540876295, 0.17460895553376762, 0.1726420164167212]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.14493627399661, 0.1551804140511049, 0.18743048459303327, 0.16520185540876295, 0.17460895553376762, 0.1726420164167212]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.559]] [[1.992]
 [1.787]
 [1.787]
 [1.787]
 [1.787]
 [1.787]
 [2.504]] [[0.634]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.559]]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.14493627399661, 0.1551804140511049, 0.18743048459303327, 0.16520185540876295, 0.17460895553376762, 0.1726420164167212]
siam score:  -0.87764925
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.14522189802141153, 0.15548623631049743, 0.18779989388724935, 0.16552743681069018, 0.17298226748507567, 0.17298226748507567]
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.682]
 [0.681]
 [0.704]
 [0.668]
 [0.677]
 [0.692]] [[3.558]
 [6.07 ]
 [4.382]
 [5.447]
 [4.89 ]
 [4.327]
 [6.191]] [[0.566]
 [1.518]
 [0.9  ]
 [1.314]
 [1.071]
 [0.875]
 [1.573]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[3.875]
 [3.875]
 [3.875]
 [3.875]
 [3.875]
 [3.875]
 [3.875]] [[0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.14550221229302998, 0.15578636330010368, 0.18816239424829864, 0.1658469458070236, 0.17138591819847387, 0.1733161661530702]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
from probs:  [0.14572928835478663, 0.1544688526940311, 0.1884560473466485, 0.16610577260226425, 0.17165338931614102, 0.17358664968612839]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.14606225018591829, 0.15482178552213477, 0.18660180701633483, 0.16648529724698824, 0.1720455909585155, 0.17398326907010833]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.747]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]] [[4.921]
 [5.463]
 [4.921]
 [4.921]
 [4.921]
 [4.921]
 [4.921]] [[1.539]
 [1.871]
 [1.539]
 [1.539]
 [1.539]
 [1.539]
 [1.539]]
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.432]
 [0.403]
 [0.403]
 [0.405]
 [0.409]
 [0.402]] [[3.052]
 [5.152]
 [3.625]
 [3.657]
 [3.771]
 [3.908]
 [4.02 ]] [[0.404]
 [0.432]
 [0.403]
 [0.403]
 [0.405]
 [0.409]
 [0.402]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.2960842215262796
from probs:  [0.14654682093329552, 0.1537974146977437, 0.18722088351434613, 0.16525803064800043, 0.17261637154704168, 0.17456047865957247]
siam score:  -0.87155443
first move QE:  1.2870400398508692
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.145597450438074, 0.15272904309440719, 0.1877608177525903, 0.16573461421821484, 0.17311417985588098, 0.17506389464083255]
using explorer policy with actor:  1
siam score:  -0.8677948
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
UNIT TEST: sample policy line 217 mcts : [0.02  0.041 0.02  0.02  0.857 0.02  0.02 ]
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.145597450438074, 0.15272904309440719, 0.1877608177525903, 0.16573461421821484, 0.17311417985588098, 0.17506389464083255]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[5.051]
 [5.051]
 [5.051]
 [5.051]
 [5.051]
 [5.051]
 [5.051]] [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.493]
 [0.43 ]
 [0.435]
 [0.429]
 [0.432]
 [0.417]] [[5.063]
 [5.394]
 [4.936]
 [6.336]
 [4.976]
 [4.935]
 [5.259]] [[0.784]
 [1.025]
 [0.726]
 [1.379]
 [0.742]
 [0.729]
 [0.857]]
first move QE:  1.2938996138644865
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.1458135771583238, 0.15147130498747047, 0.18803954583439408, 0.16598063925899176, 0.17337116158456067, 0.17532377117625927]
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.072]
 [-0.054]
 [-0.05 ]
 [-0.049]
 [-0.048]
 [-0.044]] [[2.971]
 [3.019]
 [2.85 ]
 [2.78 ]
 [2.769]
 [2.773]
 [2.703]] [[0.579]
 [0.594]
 [0.485]
 [0.436]
 [0.429]
 [0.433]
 [0.383]]
siam score:  -0.87465906
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
using explorer policy with actor:  1
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.14628240955476407, 0.15049801933009044, 0.18864414680926322, 0.16475933750619454, 0.1739285995045636, 0.17588748729512427]
using explorer policy with actor:  1
517 217
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.1470271924846538, 0.14982474719282596, 0.18960461048220095, 0.1638714166222799, 0.17288903156464538, 0.17678300165339414]
siam score:  -0.8720084
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
siam score:  -0.8729626
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.656]
 [0.707]
 [0.706]
 [0.705]
 [0.705]
 [0.706]] [[0.881]
 [2.497]
 [0.48 ]
 [0.419]
 [0.52 ]
 [0.576]
 [0.625]] [[0.965]
 [1.832]
 [0.741]
 [0.706]
 [0.762]
 [0.793]
 [0.821]]
using explorer policy with actor:  1
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.68 ]
 [0.659]
 [0.661]
 [0.659]
 [0.664]
 [0.669]] [[5.812]
 [5.398]
 [5.634]
 [5.428]
 [5.641]
 [5.794]
 [5.708]] [[1.9  ]
 [1.662]
 [1.795]
 [1.668]
 [1.799]
 [1.896]
 [1.846]]
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.14789205290651114, 0.14652449625153408, 0.190719924735548, 0.16313461145677596, 0.17390601949896314, 0.17782289515066757]
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.14789205290651114, 0.14652449625153408, 0.190719924735548, 0.16313461145677596, 0.17390601949896314, 0.17782289515066757]
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
line 256 mcts: sample exp_bonus 6.753896402091002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
siam score:  -0.8747837
from probs:  [0.14818552585614309, 0.14681525545909918, 0.19109838414405395, 0.16345833132319468, 0.1742511138532445, 0.17619138936426468]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.14900608165338727, 0.14762822314560306, 0.1921565773605804, 0.1626852336430387, 0.17330787181815263, 0.1752160123792379]
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.783]
 [0.771]
 [0.772]
 [0.775]
 [0.806]
 [0.951]] [[3.653]
 [6.054]
 [3.884]
 [3.996]
 [3.095]
 [2.869]
 [5.109]] [[1.117]
 [1.996]
 [1.248]
 [1.287]
 [0.992]
 [0.978]
 [2.017]]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.14900608165338727, 0.14762822314560306, 0.1921565773605804, 0.1626852336430387, 0.17330787181815263, 0.1752160123792379]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.14900608165338727, 0.14762822314560306, 0.1921565773605804, 0.1626852336430387, 0.17330787181815263, 0.1752160123792379]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.14811492866327627, 0.14811492866327627, 0.19279008540169768, 0.16322157960162906, 0.1738792388350604, 0.1738792388350604]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 6.780572767512068
siam score:  -0.8787908
Printing some Q and Qe and total Qs values:  [[-0.072]
 [-0.072]
 [-0.072]
 [-0.075]
 [-0.072]
 [-0.078]
 [-0.078]] [[5.782]
 [5.782]
 [5.782]
 [5.872]
 [5.782]
 [6.244]
 [6.3  ]] [[0.997]
 [0.997]
 [0.997]
 [1.045]
 [0.997]
 [1.247]
 [1.278]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.663]] [[7.079]
 [7.079]
 [7.079]
 [7.079]
 [7.079]
 [7.079]
 [7.014]] [[2.162]
 [2.162]
 [2.162]
 [2.162]
 [2.162]
 [2.162]
 [2.044]]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.14926407655209434, 0.14926407655209434, 0.1919642405126289, 0.16282573576915185, 0.1733409353070153, 0.1733409353070153]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 5.301971864161827
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.63 ]
 [0.614]
 [0.613]
 [0.612]
 [0.609]
 [0.609]] [[4.721]
 [5.151]
 [4.321]
 [3.787]
 [3.682]
 [3.854]
 [4.124]] [[0.611]
 [0.63 ]
 [0.614]
 [0.613]
 [0.612]
 [0.609]
 [0.609]]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.14926407655209434, 0.14926407655209434, 0.1919642405126289, 0.16282573576915185, 0.1733409353070153, 0.1733409353070153]
line 256 mcts: sample exp_bonus 7.948834227907085
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.873]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]] [[5.1  ]
 [5.525]
 [5.1  ]
 [5.1  ]
 [5.1  ]
 [5.1  ]
 [5.1  ]] [[1.463]
 [1.646]
 [1.463]
 [1.463]
 [1.463]
 [1.463]
 [1.463]]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.03 ]
 [-0.031]
 [-0.033]
 [-0.032]
 [-0.031]
 [-0.03 ]] [[10.   ]
 [ 8.259]
 [ 9.947]
 [10.   ]
 [10.   ]
 [10.   ]
 [10.   ]] [[1.703]
 [1.004]
 [1.684]
 [1.705]
 [1.705]
 [1.706]
 [1.706]]
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]] [[10.]
 [10.]
 [10.]
 [10.]
 [10.]
 [10.]
 [10.]] [[1.7]
 [1.7]
 [1.7]
 [1.7]
 [1.7]
 [1.7]
 [1.7]]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.14926407159344288, 0.14926407159344288, 0.19196424772085768, 0.1628257346747264, 0.1733409372087651, 0.1733409372087651]
first move QE:  1.3308420035957742
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.1498805429755339, 0.1498805429755339, 0.19048138501645653, 0.16349821670354522, 0.17220246454641985, 0.17405684778251054]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.507]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]] [[3.522]
 [5.754]
 [3.522]
 [3.522]
 [3.522]
 [3.522]
 [3.699]] [[0.495]
 [0.507]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.14988053820979647, 0.14988053820979647, 0.19048139177767773, 0.16349821580399268, 0.17220246611808462, 0.17405684988065204]
using another actor
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
using another actor
from probs:  [0.14898249948565453, 0.15035986651277977, 0.191090577172055, 0.16402109865936898, 0.17093245487537392, 0.17461350329476788]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.147]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.189]] [[0.087]
 [1.858]
 [0.105]
 [0.176]
 [0.226]
 [0.154]
 [0.336]] [[0.186]
 [0.147]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.189]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.002]
 [-0.005]
 [-0.007]
 [-0.001]
 [ 0.011]
 [ 0.   ]] [[4.999]
 [5.747]
 [5.237]
 [5.205]
 [5.323]
 [5.251]
 [5.271]] [[0.666]
 [1.142]
 [0.816]
 [0.794]
 [0.874]
 [0.84 ]
 [0.843]]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.538]
 [0.521]
 [0.522]
 [0.519]
 [0.517]
 [0.512]] [[3.362]
 [5.277]
 [3.073]
 [3.063]
 [3.155]
 [3.429]
 [3.437]] [[0.532]
 [0.538]
 [0.521]
 [0.522]
 [0.519]
 [0.517]
 [0.512]]
first move QE:  1.33408114486836
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]] [[6.569]
 [6.569]
 [6.569]
 [6.569]
 [6.569]
 [6.569]
 [6.569]] [[1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.14922730342545384, 0.15060693499290784, 0.19140461134476183, 0.16264735594523366, 0.17121334691530588, 0.17490044737633698]
siam score:  -0.8871869
siam score:  -0.8892925
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.14946800764697177, 0.15084986456551636, 0.19171334772819246, 0.16129670286971354, 0.1714895147105868, 0.17518256247901914]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.655]
 [0.723]
 [0.725]
 [0.725]
 [0.78 ]
 [0.827]] [[3.639]
 [2.755]
 [3.841]
 [3.862]
 [3.884]
 [3.652]
 [3.885]] [[1.089]
 [0.522]
 [1.206]
 [1.22 ]
 [1.233]
 [1.146]
 [1.322]]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.14946800291691445, 0.15084986021550392, 0.19171335461665023, 0.16129670139284047, 0.17148951603698948, 0.17518256482110142]
using another actor
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.142]
 [0.069]
 [0.051]
 [0.097]
 [0.133]
 [0.149]] [[5.8  ]
 [5.8  ]
 [5.182]
 [6.274]
 [6.492]
 [6.449]
 [6.382]] [[ 0.096]
 [ 0.096]
 [-0.256]
 [ 0.072]
 [ 0.237]
 [ 0.294]
 [ 0.305]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.4  ]
 [0.363]
 [0.366]
 [0.365]
 [0.366]
 [0.367]] [[1.564]
 [3.165]
 [1.696]
 [1.428]
 [1.272]
 [1.125]
 [1.052]] [[0.364]
 [0.4  ]
 [0.363]
 [0.366]
 [0.365]
 [0.366]
 [0.367]]
siam score:  -0.8830147
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.578]
 [0.542]
 [0.537]
 [0.532]
 [0.533]
 [0.536]] [[2.186]
 [3.481]
 [2.158]
 [2.093]
 [2.029]
 [2.072]
 [2.245]] [[0.521]
 [0.578]
 [0.542]
 [0.537]
 [0.532]
 [0.533]
 [0.536]]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.14891232077164862, 0.1516397261694208, 0.1944989538486982, 0.16046368480927203, 0.16865086292872164, 0.1758344514722387]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.14891232077164862, 0.1516397261694208, 0.1944989538486982, 0.16046368480927203, 0.16865086292872164, 0.1758344514722387]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.431]
 [0.435]
 [0.42 ]
 [0.435]
 [0.423]
 [0.422]] [[4.137]
 [2.95 ]
 [3.382]
 [3.73 ]
 [3.382]
 [3.845]
 [4.008]] [[0.419]
 [0.431]
 [0.435]
 [0.42 ]
 [0.435]
 [0.423]
 [0.422]]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[3.614]
 [3.614]
 [3.614]
 [3.614]
 [3.614]
 [3.614]
 [3.614]] [[0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]]
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.638]
 [0.638]
 [0.708]
 [0.638]
 [0.745]
 [0.678]] [[2.486]
 [2.486]
 [2.486]
 [2.402]
 [2.486]
 [2.012]
 [3.632]] [[1.203]
 [1.203]
 [1.203]
 [1.203]
 [1.203]
 [0.996]
 [1.917]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.303455812442625
line 256 mcts: sample exp_bonus 2.23154793340085
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 2.295443430584011
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.1483476436724578, 0.15242283935190562, 0.19322273962496525, 0.159742365572273, 0.16952185627494945, 0.1767425555034489]
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.796]
 [0.807]
 [0.748]
 [0.807]
 [0.807]
 [0.79 ]] [[8.71 ]
 [7.404]
 [8.71 ]
 [7.32 ]
 [8.71 ]
 [8.71 ]
 [7.111]] [[2.818]
 [2.12 ]
 [2.818]
 [2.045]
 [2.818]
 [2.818]
 [1.962]]
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.559]
 [0.54 ]
 [0.54 ]
 [0.542]
 [0.541]
 [0.539]] [[4.29 ]
 [5.546]
 [3.365]
 [3.408]
 [3.222]
 [3.744]
 [5.271]] [[0.541]
 [0.559]
 [0.54 ]
 [0.54 ]
 [0.542]
 [0.541]
 [0.539]]
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.678]
 [0.708]
 [0.717]
 [0.673]
 [0.695]
 [0.7  ]] [[5.679]
 [5.956]
 [5.995]
 [5.974]
 [5.217]
 [5.447]
 [5.684]] [[1.436]
 [1.494]
 [1.53 ]
 [1.525]
 [1.138]
 [1.261]
 [1.377]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
rdn probs:  [0.1472305660558446, 0.1526227225624823, 0.19347623774218403, 0.15995186732878588, 0.16974420920211525, 0.1769743971085878]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.1466075875799269, 0.15192713982514788, 0.19435839185173423, 0.16068116026752616, 0.17051815231102335, 0.1759075681646415]
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.78 ]
 [0.838]
 [0.838]
 [0.838]
 [0.84 ]
 [0.848]] [[4.704]
 [6.551]
 [6.747]
 [6.747]
 [6.747]
 [4.954]
 [6.193]] [[0.698]
 [1.181]
 [1.361]
 [1.361]
 [1.361]
 [0.766]
 [1.197]]
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.1466075875799269, 0.15192713982514788, 0.19435839185173423, 0.16068116026752616, 0.17051815231102335, 0.1759075681646415]
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.716]
 [0.794]
 [0.785]
 [0.79 ]
 [0.772]
 [0.742]] [[6.356]
 [6.371]
 [5.905]
 [6.228]
 [5.964]
 [6.944]
 [6.332]] [[1.684]
 [1.646]
 [1.46 ]
 [1.615]
 [1.487]
 [1.964]
 [1.641]]
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.1466075875799269, 0.15192713982514788, 0.19435839185173423, 0.16068116026752616, 0.17051815231102335, 0.1759075681646415]
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.1466075875799269, 0.15192713982514788, 0.19435839185173423, 0.16068116026752616, 0.17051815231102335, 0.1759075681646415]
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.14470071417618466, 0.1525746899640777, 0.1951867942279694, 0.1613660221544437, 0.16951445080027616, 0.1766573286770484]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.345]
 [0.342]
 [0.336]
 [0.337]
 [0.351]
 [0.357]] [[2.334]
 [2.63 ]
 [2.973]
 [2.912]
 [2.847]
 [2.803]
 [2.933]] [[0.348]
 [0.345]
 [0.342]
 [0.336]
 [0.337]
 [0.351]
 [0.357]]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.367]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]] [[1.17 ]
 [1.475]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]] [[0.335]
 [0.367]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]]
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.598]
 [0.623]
 [0.604]
 [0.623]
 [0.611]
 [0.594]] [[5.895]
 [6.112]
 [5.512]
 [5.719]
 [5.512]
 [5.731]
 [5.864]] [[1.11 ]
 [1.18 ]
 [1.03 ]
 [1.061]
 [1.03 ]
 [1.078]
 [1.088]]
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
probs:  [0.14536987241786933, 0.15190193739744423, 0.19608943578868618, 0.16056948515880323, 0.16859499234524675, 0.17747427689195022]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.553]
 [0.647]] [[5.649]
 [5.649]
 [5.649]
 [5.649]
 [5.649]
 [5.033]
 [6.068]] [[1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.229]
 [1.7  ]]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.619]] [[4.942]
 [4.942]
 [4.942]
 [4.942]
 [4.942]
 [4.942]
 [5.081]] [[1.751]
 [1.751]
 [1.751]
 [1.751]
 [1.751]
 [1.751]
 [2.022]]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.757]] [[7.335]
 [7.335]
 [7.335]
 [7.335]
 [7.335]
 [7.335]
 [8.424]] [[1.655]
 [1.655]
 [1.655]
 [1.655]
 [1.655]
 [1.655]
 [2.046]]
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
actor:  1 policy actor:  1  step number:  60 total reward:  0.35499999999999954  reward:  1.0 rdn_beta:  0.833
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.388]
 [0.453]
 [0.453]
 [0.453]
 [0.446]
 [0.452]] [[2.014]
 [2.094]
 [1.82 ]
 [1.855]
 [1.972]
 [1.799]
 [2.107]] [[0.637]
 [0.534]
 [0.574]
 [0.586]
 [0.625]
 [0.554]
 [0.668]]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.447]
 [0.404]
 [0.404]
 [0.404]
 [0.399]
 [0.423]] [[3.159]
 [3.344]
 [3.493]
 [3.493]
 [3.493]
 [3.869]
 [3.934]] [[0.245]
 [0.393]
 [0.356]
 [0.356]
 [0.356]
 [0.471]
 [0.541]]
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
from probs:  [0.11212475341097346, 0.11707202669517812, 0.15207110885924913, 0.12362483017727559, 0.3587408883537475, 0.13636639250357624]
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
maxi score, test score, baseline:  -0.9964753424657534 -1.0 -0.9964753424657534
probs:  [0.11148380433299118, 0.11738601138562914, 0.1507413960700775, 0.12395639282158465, 0.3597002608871083, 0.13673213450260918]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.601]
 [0.551]
 [0.542]
 [0.55 ]
 [0.55 ]
 [0.549]] [[3.703]
 [4.499]
 [3.612]
 [3.325]
 [3.598]
 [3.631]
 [3.657]] [[0.55 ]
 [0.601]
 [0.551]
 [0.542]
 [0.55 ]
 [0.55 ]
 [0.549]]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.1114839536594035, 0.11738617024379093, 0.1507416087954056, 0.12395656229056182, 0.3596993804071105, 0.13673232460372745]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.696]
 [0.698]
 [0.699]
 [0.696]
 [0.701]
 [0.706]] [[4.428]
 [4.428]
 [4.393]
 [4.354]
 [4.428]
 [4.321]
 [4.313]] [[2.344]
 [2.344]
 [2.311]
 [2.273]
 [2.344]
 [2.243]
 [2.241]]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.688]
 [0.662]
 [0.674]
 [0.662]
 [0.65 ]
 [0.674]] [[2.438]
 [2.462]
 [2.324]
 [2.438]
 [2.037]
 [2.343]
 [2.438]] [[2.033]
 [2.099]
 [1.812]
 [2.033]
 [1.309]
 [1.823]
 [2.033]]
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.11077475966546677, 0.11658695490013432, 0.15103701631021363, 0.12419946847006838, 0.3604015281155582, 0.1370002725385587]
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[5.493]
 [5.493]
 [5.493]
 [5.493]
 [5.493]
 [5.493]
 [5.493]] [[1.419]
 [1.419]
 [1.419]
 [1.419]
 [1.419]
 [1.419]
 [1.419]]
siam score:  -0.8853125
start point for exploration sampling:  10935
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9965216216216216 -1.0 -0.9965216216216216
maxi score, test score, baseline:  -0.9965216216216216 -1.0 -0.9965216216216216
probs:  [0.111810943589857, 0.11767750752872079, 0.15073248426758384, 0.12536123042349046, 0.3602862937960596, 0.13413154039428818]
siam score:  -0.87770903
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.049]
 [-0.038]
 [-0.038]
 [-0.035]
 [-0.032]
 [-0.031]] [[6.295]
 [4.899]
 [6.321]
 [6.469]
 [6.586]
 [6.573]
 [6.263]] [[1.086]
 [0.421]
 [1.097]
 [1.166]
 [1.224]
 [1.219]
 [1.074]]
from probs:  [0.11199896152544535, 0.11787539050260831, 0.14930438143664834, 0.12557203411577927, 0.36089214044224116, 0.13435709197727747]
using explorer policy with actor:  1
581 241
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.628]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.696]] [[3.636]
 [4.51 ]
 [3.884]
 [3.884]
 [3.884]
 [3.884]
 [3.999]] [[1.234]
 [1.309]
 [1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.275]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.857 0.02  0.02  0.02  0.041 0.02 ]
maxi score, test score, baseline:  -0.9965329966329967 -1.0 -0.9965329966329967
probs:  [0.1114573225667614, 0.11827932385117984, 0.14981602320772092, 0.12483718890429235, 0.3621275004707331, 0.13348264099931234]
line 256 mcts: sample exp_bonus 6.149856237440954
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9965329966329967 -1.0 -0.9965329966329967
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
probs:  [0.11081661987282962, 0.11849885043065073, 0.14973992136578984, 0.12616313185165448, 0.36249980063586335, 0.1322816758432121]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.624]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.631]] [[5.493]
 [5.618]
 [5.493]
 [5.493]
 [5.493]
 [5.493]
 [5.341]] [[1.373]
 [1.472]
 [1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.33 ]]
first move QE:  1.4390101035910936
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
probs:  [0.11055421642595031, 0.11917540357216258, 0.14578057250724769, 0.12688344317017788, 0.36456944416427156, 0.13303692016019011]
593 244
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.733]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.698]] [[3.819]
 [4.062]
 [3.819]
 [3.819]
 [3.819]
 [3.819]
 [3.862]] [[0.819]
 [0.97 ]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.859]]
maxi score, test score, baseline:  -0.9965555183946488 -1.0 -0.9965555183946488
probs:  [0.11121996661731207, 0.11886235545724273, 0.14511274159227056, 0.12420334311242155, 0.3667635272793815, 0.1338380659413716]
using explorer policy with actor:  1
siam score:  -0.8684867
siam score:  -0.87010765
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
probs:  [0.11162562734245256, 0.11827925212128039, 0.144123331525359, 0.12354563466822904, 0.3680999248136373, 0.1343262295290416]
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.446]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[1.974]
 [3.033]
 [1.974]
 [1.974]
 [1.974]
 [1.974]
 [1.974]] [[0.415]
 [0.446]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]]
siam score:  -0.8745717
Printing some Q and Qe and total Qs values:  [[0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.948]] [[3.872]
 [3.872]
 [3.872]
 [3.872]
 [3.872]
 [3.872]
 [4.202]] [[1.467]
 [1.467]
 [1.467]
 [1.467]
 [1.467]
 [1.467]
 [1.722]]
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
probs:  [0.1124911452004954, 0.11919636056119853, 0.14227454656703534, 0.12450357735248863, 0.3674674974666484, 0.13406687285213373]
using explorer policy with actor:  1
siam score:  -0.8759877
maxi score, test score, baseline:  -0.9965777408637874 -1.0 -0.9965777408637874
probs:  [0.11196927266312072, 0.11858568106823136, 0.14130908952659965, 0.12492108371096278, 0.36869842221074306, 0.13451645082034241]
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]] [[2.856]
 [2.856]
 [2.856]
 [2.856]
 [2.856]
 [2.856]
 [2.856]] [[0.93]
 [0.93]
 [0.93]
 [0.93]
 [0.93]
 [0.93]
 [0.93]]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.669]
 [0.64 ]
 [0.617]
 [0.647]
 [0.664]
 [0.705]] [[2.613]
 [2.705]
 [2.172]
 [2.062]
 [2.559]
 [2.665]
 [2.923]] [[1.051]
 [1.38 ]
 [1.144]
 [1.063]
 [1.289]
 [1.357]
 [1.525]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.772]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]] [[3.053]
 [3.143]
 [3.053]
 [3.053]
 [3.053]
 [3.053]
 [3.053]] [[1.604]
 [1.686]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.512]] [[3.775]
 [3.775]
 [3.775]
 [3.775]
 [3.775]
 [3.775]
 [4.094]] [[1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.847]]
first move QE:  1.4886405174483384
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]] [[1.883]
 [2.088]
 [2.088]
 [2.088]
 [2.088]
 [2.088]
 [2.088]] [[0.523]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.1122714363484441, 0.11884882512894086, 0.13861820441968434, 0.1251409700082426, 0.3691668790996842, 0.13595368499500388]
from probs:  [0.11251584979636416, 0.11910755926298232, 0.13891998163435937, 0.12323770303905886, 0.36996924540937226, 0.1362496608578631]
maxi score, test score, baseline:  -0.9965996699669967 -1.0 -0.9965996699669967
maxi score, test score, baseline:  -0.9965996699669967 -1.0 -0.9965996699669967
start point for exploration sampling:  10935
siam score:  -0.8744093
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
line 256 mcts: sample exp_bonus 2.827109879145417
line 256 mcts: sample exp_bonus 5.269115718051242
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.11354931028461066, 0.11920332881090545, 0.1388353375827623, 0.12436964690876251, 0.3665412530185755, 0.1375011233943837]
siam score:  -0.86917585
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.463]
 [0.428]
 [0.463]
 [0.463]
 [0.43 ]
 [0.433]] [[3.198]
 [3.198]
 [2.597]
 [3.198]
 [3.198]
 [2.772]
 [3.037]] [[0.463]
 [0.463]
 [0.428]
 [0.463]
 [0.463]
 [0.43 ]
 [0.433]]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]] [[5.836]
 [5.836]
 [5.836]
 [5.836]
 [5.836]
 [5.836]
 [5.836]] [[2.44]
 [2.44]
 [2.44]
 [2.44]
 [2.44]
 [2.44]
 [2.44]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.86721957
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.71 ]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]] [[6.838]
 [6.939]
 [6.838]
 [6.838]
 [6.838]
 [6.838]
 [6.838]] [[2.427]
 [2.301]
 [2.427]
 [2.427]
 [2.427]
 [2.427]
 [2.427]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966320261437909 -1.0 -0.9966320261437909
maxi score, test score, baseline:  -0.9966320261437909 -1.0 -0.9966320261437909
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966320261437909 -1.0 -0.9966320261437909
probs:  [0.11121364856795105, 0.11951775269895945, 0.13920155508357196, 0.12469770069491012, 0.367505522013679, 0.1378638209409284]
line 256 mcts: sample exp_bonus 4.582949483761398
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]] [[6.021]
 [6.021]
 [6.021]
 [6.021]
 [6.021]
 [6.021]
 [6.021]] [[1.515]
 [1.515]
 [1.515]
 [1.515]
 [1.515]
 [1.515]
 [1.515]]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]] [[4.514]
 [4.514]
 [4.514]
 [4.514]
 [4.514]
 [4.514]
 [4.514]] [[5.551]
 [5.551]
 [5.551]
 [5.551]
 [5.551]
 [5.551]
 [5.551]]
maxi score, test score, baseline:  -0.9966320261437909 -1.0 -0.9966320261437909
maxi score, test score, baseline:  -0.9966320261437909 -1.0 -0.9966320261437909
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966320261437909 -1.0 -0.9966320261437909
maxi score, test score, baseline:  -0.9966320261437909 -1.0 -0.9966320261437909
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.11276968164864971, 0.11921054806156899, 0.13715734318219303, 0.12644239806905727, 0.3659576604899945, 0.13846236854853647]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.602]] [[5.482]
 [5.482]
 [5.482]
 [5.482]
 [5.482]
 [5.482]
 [5.935]] [[1.787]
 [1.787]
 [1.787]
 [1.787]
 [1.787]
 [1.787]
 [2.064]]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.608]] [[4.191]
 [4.191]
 [4.191]
 [4.191]
 [4.191]
 [4.191]
 [5.028]] [[1.464]
 [1.464]
 [1.464]
 [1.464]
 [1.464]
 [1.464]
 [1.834]]
line 256 mcts: sample exp_bonus 8.065699807698005
siam score:  -0.8672808
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.169]] [[4.853]
 [4.853]
 [4.853]
 [4.853]
 [4.853]
 [4.853]
 [4.774]] [[0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.841]]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.245]] [[5.04]
 [5.04]
 [5.04]
 [5.04]
 [5.04]
 [5.04]
 [5.95]] [[0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [1.31 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.208]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.225]] [[3.535]
 [3.78 ]
 [3.535]
 [3.535]
 [3.535]
 [3.535]
 [4.713]] [[0.497]
 [0.499]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [1.006]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.141]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.157]] [[3.535]
 [3.78 ]
 [3.535]
 [3.535]
 [3.535]
 [3.535]
 [4.715]] [[0.365]
 [0.366]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.897]]
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.741]
 [0.643]
 [0.643]
 [0.643]
 [0.645]
 [0.649]] [[1.226]
 [2.871]
 [0.661]
 [0.716]
 [0.734]
 [0.84 ]
 [0.792]] [[0.967]
 [1.731]
 [0.718]
 [0.742]
 [0.75 ]
 [0.796]
 [0.778]]
625 263
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
line 256 mcts: sample exp_bonus 2.74617132834767
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.232]
 [0.253]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[-0.02 ]
 [ 2.282]
 [-0.022]
 [-0.147]
 [-0.018]
 [ 0.123]
 [ 0.078]] [[0.253]
 [0.232]
 [0.253]
 [0.252]
 [0.252]
 [0.252]
 [0.252]]
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.254]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.254]] [[ 0.072]
 [-0.122]
 [-0.124]
 [-0.143]
 [-0.112]
 [ 0.044]
 [ 0.086]] [[0.253]
 [0.254]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.254]]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.11377660989149431, 0.12114099221045761, 0.137917330437068, 0.12415791353860436, 0.3650898234853078, 0.137917330437068]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.11377660989149431, 0.12114099221045761, 0.137917330437068, 0.12415791353860436, 0.3650898234853078, 0.137917330437068]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.11388741513603641, 0.1202850852945368, 0.13805164594831631, 0.12427882897091161, 0.3654453787018826, 0.13805164594831631]
in main func line 156:  631
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]] [[2.09]
 [2.09]
 [2.09]
 [2.09]
 [2.09]
 [2.09]
 [2.09]] [[2.636]
 [2.636]
 [2.636]
 [2.636]
 [2.636]
 [2.636]
 [2.636]]
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [1.457]] [[1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.499]] [[1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.368]
 [2.461]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
siam score:  -0.86866796
using explorer policy with actor:  1
siam score:  -0.87206846
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.11419877284085236, 0.12056156793600342, 0.13820233317335348, 0.12152965871122386, 0.3660276151877417, 0.13948005215082507]
siam score:  -0.87141764
siam score:  -0.8698662
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.11358701339052162, 0.12081910601128353, 0.13849755463981264, 0.1217892647774833, 0.3668095065410861, 0.13849755463981264]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.458]
 [0.451]
 [0.451]
 [0.451]
 [0.455]
 [0.439]] [[4.393]
 [4.99 ]
 [4.38 ]
 [4.38 ]
 [4.38 ]
 [4.117]
 [4.726]] [[0.443]
 [0.458]
 [0.451]
 [0.451]
 [0.451]
 [0.455]
 [0.439]]
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [1.039]] [[4.649]
 [4.649]
 [4.649]
 [4.649]
 [4.649]
 [4.649]
 [6.388]] [[1.45 ]
 [1.45 ]
 [1.45 ]
 [1.45 ]
 [1.45 ]
 [1.45 ]
 [2.152]]
from probs:  [0.1136956886418326, 0.119979152756555, 0.13863007004946123, 0.12190578983702738, 0.3671592286656624, 0.13863007004946123]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.608]
 [0.641]
 [0.643]
 [0.643]
 [0.644]
 [0.653]] [[4.755]
 [4.116]
 [4.581]
 [4.675]
 [4.725]
 [4.758]
 [4.666]] [[1.447]
 [0.954]
 [1.331]
 [1.397]
 [1.43 ]
 [1.455]
 [1.412]]
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.11430746706603846, 0.11967930052633878, 0.13811109606902067, 0.12256174774893895, 0.3659643653755565, 0.1393760232141067]
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.586]
 [0.585]
 [0.577]
 [0.576]
 [0.576]
 [0.572]] [[4.478]
 [2.743]
 [4.584]
 [4.483]
 [4.502]
 [4.446]
 [4.717]] [[0.581]
 [0.586]
 [0.585]
 [0.577]
 [0.576]
 [0.576]
 [0.572]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.408]
 [0.598]
 [0.716]
 [0.598]
 [0.697]
 [0.465]] [[2.974]
 [3.149]
 [2.974]
 [3.78 ]
 [2.974]
 [2.243]
 [2.867]] [[1.697]
 [1.69 ]
 [1.697]
 [2.151]
 [1.697]
 [1.386]
 [1.578]]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.11430498670478872, 0.11871103670205245, 0.13914055659762217, 0.1224917118609949, 0.36869097946165436, 0.13666072867288728]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.913]
 [0.932]
 [0.8  ]
 [0.8  ]
 [0.921]
 [0.922]] [[4.016]
 [4.304]
 [4.014]
 [5.741]
 [5.741]
 [4.217]
 [4.304]] [[1.241]
 [1.423]
 [1.268]
 [2.153]
 [2.153]
 [1.381]
 [1.44 ]]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.663]] [[7.634]
 [7.634]
 [7.634]
 [7.634]
 [7.634]
 [7.634]
 [6.937]] [[1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.571]]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.61 ]
 [0.626]
 [0.61 ]
 [0.61 ]
 [0.625]
 [0.624]] [[8.097]
 [8.121]
 [8.165]
 [8.121]
 [8.121]
 [8.082]
 [7.98 ]] [[1.954]
 [1.941]
 [1.989]
 [1.941]
 [1.941]
 [1.945]
 [1.889]]
siam score:  -0.85901105
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.803]
 [0.788]
 [0.78 ]
 [0.789]
 [0.795]
 [0.786]] [[2.738]
 [2.88 ]
 [2.769]
 [3.256]
 [2.751]
 [2.844]
 [3.132]] [[1.762]
 [1.945]
 [1.803]
 [2.275]
 [1.787]
 [1.892]
 [2.161]]
using explorer policy with actor:  1
siam score:  -0.865224
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.11448705890588069, 0.11886548357708472, 0.14039608562997782, 0.12262042057510927, 0.36573707078541406, 0.1378938805265335]
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.098]
 [-0.088]
 [-0.086]
 [-0.086]
 [-0.085]
 [-0.09 ]] [[6.142]
 [6.188]
 [5.398]
 [5.842]
 [6.146]
 [5.956]
 [6.12 ]] [[ 0.115]
 [ 0.135]
 [-0.275]
 [-0.031]
 [ 0.132]
 [ 0.033]
 [ 0.113]]
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.243]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]] [[4.774]
 [5.713]
 [4.774]
 [4.774]
 [4.774]
 [4.774]
 [4.774]] [[0.042]
 [0.5  ]
 [0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.042]]
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.015]
 [-0.018]
 [ 0.008]
 [ 0.008]
 [ 0.008]
 [-0.02 ]] [[4.494]
 [6.191]
 [4.935]
 [5.156]
 [5.156]
 [5.156]
 [4.979]] [[-0.713]
 [-0.135]
 [-0.562]
 [-0.435]
 [-0.435]
 [-0.435]
 [-0.552]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
line 256 mcts: sample exp_bonus 4.312871956072645
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.11427347496332266, 0.11857615520398504, 0.13842995117161294, 0.12226223087472572, 0.37042975764129576, 0.13602843014505775]
siam score:  -0.870312
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.11462881127897892, 0.11894487081474593, 0.13886040267264227, 0.12264240843278883, 0.36847209274628523, 0.13645141405455877]
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.11462881127897892, 0.11894487081474593, 0.13886040267264227, 0.12264240843278883, 0.36847209274628523, 0.13645141405455877]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
probs:  [0.11487069505467781, 0.11830461651046725, 0.137935707310821, 0.1229012042859334, 0.3692484240912899, 0.1367393527468105]
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.442]
 [0.428]
 [0.428]
 [0.428]
 [0.424]
 [0.419]] [[3.055]
 [3.61 ]
 [3.254]
 [3.254]
 [3.254]
 [3.21 ]
 [3.394]] [[0.419]
 [0.442]
 [0.428]
 [0.428]
 [0.428]
 [0.424]
 [0.419]]
using another actor
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.2764],
        [-0.4105],
        [-0.5637],
        [-0.7543],
        [-0.5365],
        [-0.5637],
        [-0.0000],
        [-0.5351],
        [-0.0000],
        [-0.5826]], dtype=torch.float64)
-0.043568875399499996 -0.31996634674286456
-0.0727797758985 -0.4833226046978908
-0.024259925299500003 -0.587935472179237
-0.024259925299500003 -0.7786005060226755
-0.024259925299500003 -0.5607729679120061
-0.024259925299500003 -0.5879889896063708
-0.6756881094809998 -0.6756881094809998
-0.024259925299500003 -0.5593879412683189
-0.9560860847999999 -0.9560860847999999
-0.024259925299500003 -0.6068235773602504
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
siam score:  -0.8502655
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.542]
 [0.542]
 [0.609]
 [0.542]
 [0.605]
 [0.617]] [[2.802]
 [2.802]
 [2.802]
 [2.229]
 [2.802]
 [2.55 ]
 [3.814]] [[1.07 ]
 [1.07 ]
 [1.07 ]
 [0.709]
 [1.07 ]
 [0.941]
 [1.878]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
663 298
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
probs:  [0.11590852823094189, 0.11846819911367867, 0.13897550384671617, 0.12394749459703713, 0.36609371763579535, 0.1366065565758308]
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.11590866090543239, 0.11846833540674043, 0.1389756691306703, 0.12394763863610295, 0.3660929774102168, 0.136606718510837]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.589]] [[2.202]
 [2.202]
 [2.202]
 [2.202]
 [2.202]
 [2.202]
 [2.933]] [[0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [1.142]]
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.11604731134289216, 0.11861004773782047, 0.1379457081320187, 0.12409590533321388, 0.36653089940044914, 0.13677012805360553]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.631]
 [0.658]
 [0.689]
 [0.53 ]
 [0.626]
 [0.662]] [[3.393]
 [3.08 ]
 [2.88 ]
 [3.419]
 [4.335]
 [3.485]
 [3.575]] [[1.491]
 [1.234]
 [1.079]
 [1.588]
 [2.263]
 [1.59 ]
 [1.703]]
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.405]
 [0.439]
 [0.434]
 [0.432]
 [0.429]
 [0.421]] [[4.846]
 [3.23 ]
 [4.493]
 [4.828]
 [4.6  ]
 [4.545]
 [4.294]] [[0.432]
 [0.405]
 [0.439]
 [0.434]
 [0.432]
 [0.429]
 [0.421]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [1.027]] [[4.423]
 [4.423]
 [4.423]
 [4.423]
 [4.423]
 [4.423]
 [5.966]] [[1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]
 [2.824]]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.848]] [[4.506]
 [4.506]
 [4.506]
 [4.506]
 [4.506]
 [4.506]
 [5.075]] [[1.633]
 [1.633]
 [1.633]
 [1.633]
 [1.633]
 [1.633]
 [2.17 ]]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.32 ]
 [0.278]
 [0.278]
 [0.275]
 [0.278]
 [0.277]] [[1.334]
 [2.085]
 [1.111]
 [1.092]
 [0.974]
 [0.967]
 [1.274]] [[0.286]
 [0.32 ]
 [0.278]
 [0.278]
 [0.275]
 [0.278]
 [0.277]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.7925207259473823
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.8637482
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.11617112588768157, 0.1187173416366643, 0.13673671462946513, 0.12322218488486451, 0.36956121746275783, 0.1355914154985668]
siam score:  -0.864258
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.11617112588768157, 0.1187173416366643, 0.13673671462946513, 0.12322218488486451, 0.36956121746275783, 0.1355914154985668]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
siam score:  -0.8680077
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.39 ]
 [0.36 ]
 [0.325]
 [0.325]
 [0.325]
 [0.336]] [[2.965]
 [3.216]
 [2.941]
 [4.15 ]
 [4.15 ]
 [4.15 ]
 [2.847]] [[0.507]
 [0.754]
 [0.506]
 [1.35 ]
 [1.35 ]
 [1.35 ]
 [0.4  ]]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8715335
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
probs:  [0.11672498001164643, 0.12011036782192626, 0.134799033940909, 0.12370093065101093, 0.36762738070118195, 0.1370373068733254]
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.172]
 [0.275]
 [0.278]
 [0.26 ]
 [0.286]
 [0.241]] [[4.882]
 [5.017]
 [4.811]
 [5.005]
 [4.924]
 [4.717]
 [4.748]] [[0.876]
 [0.863]
 [0.931]
 [1.066]
 [0.977]
 [0.891]
 [0.822]]
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.026]
 [-0.011]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[4.856]
 [4.579]
 [4.772]
 [4.768]
 [4.781]
 [4.898]
 [5.032]] [[0.484]
 [0.272]
 [0.43 ]
 [0.433]
 [0.442]
 [0.52 ]
 [0.608]]
siam score:  -0.8638162
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
siam score:  -0.8605595
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[4.957]
 [4.957]
 [4.957]
 [4.957]
 [4.957]
 [4.957]
 [4.957]] [[1.341]
 [1.341]
 [1.341]
 [1.341]
 [1.341]
 [1.341]
 [1.341]]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[4.853]
 [4.853]
 [4.853]
 [4.853]
 [4.853]
 [4.853]
 [4.853]] [[1.212]
 [1.212]
 [1.212]
 [1.212]
 [1.212]
 [1.212]
 [1.212]]
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
probs:  [0.11623093415365222, 0.11957705474415782, 0.13407919623784498, 0.12404488621443517, 0.36864958386645963, 0.13741834478345022]
line 256 mcts: sample exp_bonus 2.592395752546607
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
probs:  [0.11623093415365222, 0.11957705474415782, 0.13407919623784498, 0.12404488621443517, 0.36864958386645963, 0.13741834478345022]
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
probs:  [0.11633036352844063, 0.11882389979764053, 0.13419389385041372, 0.12415100000911307, 0.3689649439546044, 0.1375358988597877]
684 310
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
probs:  [0.11680317616923891, 0.11930684715677552, 0.13473931094880442, 0.12465559881196736, 0.36753733929430527, 0.1369577276189086]
siam score:  -0.86022836
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [1.18 ]] [[3.69 ]
 [3.69 ]
 [3.69 ]
 [3.69 ]
 [3.69 ]
 [3.69 ]
 [7.016]] [[0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [2.091]]
using explorer policy with actor:  1
using explorer policy with actor:  0
690 314
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.555]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[4.441]
 [5.167]
 [4.327]
 [4.327]
 [4.327]
 [4.327]
 [4.327]] [[1.468]
 [1.794]
 [1.38 ]
 [1.38 ]
 [1.38 ]
 [1.38 ]
 [1.38 ]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.9584580257036791
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
maxi score, test score, baseline:  -0.9968418960244648 -1.0 -0.9968418960244648
probs:  [0.11727221456876338, 0.11976761006627137, 0.13513221323791952, 0.1250961237601982, 0.368676076661619, 0.13405576170522843]
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
siam score:  -0.8601877
using explorer policy with actor:  1
siam score:  -0.8569391
line 256 mcts: sample exp_bonus 3.966624072279195
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.564]
 [0.623]
 [0.617]
 [0.624]
 [0.631]
 [0.616]] [[4.409]
 [5.491]
 [4.421]
 [4.573]
 [4.505]
 [4.356]
 [5.322]] [[0.601]
 [0.564]
 [0.623]
 [0.617]
 [0.624]
 [0.631]
 [0.616]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.897]
 [0.668]
 [0.654]
 [0.668]
 [0.651]
 [0.63 ]
 [0.718]] [[5.544]
 [5.681]
 [5.598]
 [5.623]
 [5.341]
 [5.444]
 [5.677]] [[1.435]
 [1.148]
 [1.083]
 [1.118]
 [0.944]
 [0.964]
 [1.224]]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.62 ]] [[5.198]
 [5.198]
 [5.198]
 [5.198]
 [5.198]
 [5.198]
 [5.354]] [[1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.517]]
siam score:  -0.85382915
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
probs:  [0.11701484706815726, 0.11948673639722088, 0.13469018474371958, 0.12476255989059543, 0.3704197285408422, 0.13362594335946468]
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.587]
 [0.615]
 [0.614]
 [0.614]
 [0.612]
 [0.612]] [[4.372]
 [4.936]
 [4.23 ]
 [4.081]
 [4.191]
 [4.033]
 [4.137]] [[0.619]
 [0.587]
 [0.615]
 [0.614]
 [0.614]
 [0.612]
 [0.612]]
Printing some Q and Qe and total Qs values:  [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [1.374]] [[4.25 ]
 [4.25 ]
 [4.25 ]
 [4.25 ]
 [4.25 ]
 [4.25 ]
 [7.266]] [[1.105]
 [1.105]
 [1.105]
 [1.105]
 [1.105]
 [1.105]
 [2.365]]
first move QE:  1.6815425162845907
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
siam score:  -0.85702777
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.11719435504025102, 0.11882140289558114, 0.13475345549497358, 0.12489571488881356, 0.37063780267892427, 0.13369726900145643]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.11753051403565078, 0.11916222889710357, 0.13513998082044917, 0.1252539643798606, 0.3688325470936927, 0.13408076477324324]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9968879518072289 -1.0 -0.9968879518072289
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
probs:  [0.11852389660119486, 0.11934089688124007, 0.13628221468825755, 0.12539631072157517, 0.3662939838703965, 0.13416269723733584]
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
probs:  [0.11852389660119486, 0.11934089688124007, 0.13628221468825755, 0.12539631072157517, 0.3662939838703965, 0.13416269723733584]
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
probs:  [0.11852389660119486, 0.11934089688124007, 0.13628221468825755, 0.12539631072157517, 0.3662939838703965, 0.13416269723733584]
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.11852402037117707, 0.119341021714869, 0.136282361577665, 0.12539644343870335, 0.36629331153022665, 0.13416284136735893]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.785]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]] [[4.023]
 [4.749]
 [4.023]
 [4.023]
 [4.023]
 [4.023]
 [4.023]] [[1.365]
 [1.654]
 [1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.365]]
siam score:  -0.8530883
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.11852402037117707, 0.119341021714869, 0.136282361577665, 0.12539644343870335, 0.36629331153022665, 0.13416284136735893]
from probs:  [0.11852402037117707, 0.119341021714869, 0.136282361577665, 0.12539644343870335, 0.36629331153022665, 0.13416284136735893]
Starting evaluation
using explorer policy with actor:  0
siam score:  -0.8493857
siam score:  -0.8508032
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
rdn probs:  [0.11898207726603251, 0.11980224002914096, 0.13573683085524818, 0.1258810934498272, 0.3649163450934299, 0.1346814133063212]
Printing some Q and Qe and total Qs values:  [[1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.426]] [[1.978]
 [1.978]
 [1.978]
 [1.978]
 [1.978]
 [1.978]
 [7.081]] [[1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]
 [3.131]]
siam score:  -0.8475699
line 256 mcts: sample exp_bonus 3.310451223739361
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
probs:  [0.11951504089565931, 0.11951504089565931, 0.13634485271712804, 0.12553446264202406, 0.3638058957589505, 0.13528470709057883]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
probs:  [0.11942218521161936, 0.11861598327665608, 0.13717142463692203, 0.1253927531647532, 0.3632928016867489, 0.13610485202330044]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.573]
 [0.544]
 [0.54 ]
 [0.539]
 [0.534]
 [0.532]] [[1.51 ]
 [1.411]
 [1.376]
 [1.385]
 [1.405]
 [1.502]
 [1.585]] [[1.029]
 [0.941]
 [0.844]
 [0.851]
 [0.88 ]
 [1.021]
 [1.145]]
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
using explorer policy with actor:  1
siam score:  -0.8432234
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
probs:  [0.11983845302715507, 0.11823143579668335, 0.13764956066488326, 0.12582983248423985, 0.3618714477092214, 0.1365792703178171]
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.911]] [[5.547]
 [5.547]
 [5.547]
 [5.547]
 [5.547]
 [5.547]
 [9.499]] [[0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [2.003]]
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
probs:  [0.12032624929573923, 0.11871269078416137, 0.137135209223436, 0.1254455248612488, 0.3633444268527331, 0.13503589898268148]
Printing some Q and Qe and total Qs values:  [[0.935]
 [0.935]
 [0.935]
 [0.935]
 [0.935]
 [0.935]
 [1.369]] [[4.448]
 [4.448]
 [4.448]
 [4.448]
 [4.448]
 [4.448]
 [6.073]] [[1.491]
 [1.491]
 [1.491]
 [1.491]
 [1.491]
 [1.491]
 [2.612]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [1.125]] [[4.654]
 [4.654]
 [4.654]
 [4.654]
 [4.654]
 [4.654]
 [4.091]] [[2.268]
 [2.268]
 [2.268]
 [2.268]
 [2.268]
 [2.268]
 [2.774]]
Printing some Q and Qe and total Qs values:  [[1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.309]] [[2.538]
 [2.538]
 [2.538]
 [2.538]
 [2.538]
 [2.538]
 [3.935]] [[1.116]
 [1.116]
 [1.116]
 [1.116]
 [1.116]
 [1.116]
 [2.207]]
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
from probs:  [0.12042143348222872, 0.11801554767463876, 0.13724369015262144, 0.12554475865522605, 0.36363185078465066, 0.13514271925063442]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.218]
 [0.249]
 [0.258]
 [0.258]
 [0.257]
 [0.205]] [[2.774]
 [3.228]
 [2.858]
 [2.923]
 [2.93 ]
 [3.502]
 [3.735]] [[0.486]
 [0.782]
 [0.513]
 [0.586]
 [0.592]
 [1.088]
 [1.202]]
siam score:  -0.8357536
from probs:  [0.12015310075355524, 0.11855290951453241, 0.13680509083463652, 0.1261164033565324, 0.36261442906090363, 0.13575806647983987]
siam score:  -0.8344391
UNIT TEST: sample policy line 217 mcts : [0.082 0.163 0.102 0.163 0.082 0.286 0.122]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.12065077795649812, 0.11904395831759447, 0.13426581459148323, 0.1266387820784556, 0.36411547023437885, 0.13528519682158982]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.128]
 [0.089]
 [0.128]
 [0.092]
 [0.088]
 [0.1  ]] [[3.249]
 [3.249]
 [3.528]
 [3.249]
 [3.608]
 [3.711]
 [3.613]] [[-0.163]
 [-0.163]
 [-0.055]
 [-0.163]
 [ 0.004]
 [ 0.064]
 [ 0.022]]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]] [[4.051]
 [4.051]
 [4.051]
 [4.051]
 [4.051]
 [4.051]
 [4.051]] [[0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]]
maxi score, test score, baseline:  -0.9971067039106145 -1.0 -0.9971067039106145
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.806]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]] [[3.074]
 [3.475]
 [3.074]
 [3.074]
 [3.074]
 [3.074]
 [3.074]] [[1.259]
 [1.541]
 [1.259]
 [1.259]
 [1.259]
 [1.259]
 [1.259]]
maxi score, test score, baseline:  -0.9971067039106145 -1.0 -0.9971067039106145
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.35 ]
 [0.211]
 [0.269]
 [0.09 ]
 [0.234]
 [0.224]] [[3.698]
 [5.006]
 [3.295]
 [2.684]
 [3.28 ]
 [3.592]
 [3.709]] [[0.682]
 [1.81 ]
 [0.564]
 [0.224]
 [0.437]
 [0.779]
 [0.845]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.491]
 [0.68 ]
 [0.673]
 [0.673]
 [0.666]
 [0.492]] [[2.636]
 [2.708]
 [1.84 ]
 [2.526]
 [2.526]
 [1.685]
 [2.582]] [[2.326]
 [2.395]
 [2.218]
 [2.609]
 [2.609]
 [2.102]
 [2.322]]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[2.185]
 [2.185]
 [2.185]
 [2.185]
 [2.185]
 [2.185]
 [2.185]] [[0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971067039106145 -1.0 -0.9971067039106145
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.73 ]
 [0.738]
 [0.727]
 [0.737]
 [0.725]
 [0.723]] [[3.746]
 [3.857]
 [3.928]
 [3.989]
 [3.387]
 [3.498]
 [4.184]] [[1.   ]
 [1.02 ]
 [1.06 ]
 [1.058]
 [0.877]
 [0.889]
 [1.115]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.337]] [[1.998]
 [1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.948]] [[1.88 ]
 [1.871]
 [1.871]
 [1.871]
 [1.871]
 [1.871]
 [1.801]]
maxi score, test score, baseline:  -0.9971067039106145 -1.0 -0.9971067039106145
from probs:  [0.12124731857776479, 0.11734310607000446, 0.133729144236903, 0.12542455293921465, 0.36550496153318107, 0.1367509166429321]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.12054238208376097, 0.1174374777560438, 0.13383670191978883, 0.1255254278404819, 0.3657971044258332, 0.13686090597409134]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.12066586450383898, 0.11755777954293811, 0.1339738029091888, 0.1256540148431869, 0.3661718233498432, 0.13597671485100407]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.12066586450383898, 0.11755777954293811, 0.1339738029091888, 0.1256540148431869, 0.3661718233498432, 0.13597671485100407]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.12066586450383898, 0.11755777954293811, 0.1339738029091888, 0.1256540148431869, 0.3661718233498432, 0.13597671485100407]
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]] [[0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]] [[0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]]
start point for exploration sampling:  10935
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7022604080845576
from probs:  [0.12066597232157723, 0.1175578838752757, 0.13397392565042435, 0.12565412825462763, 0.36617125005979373, 0.13597683983830133]
Printing some Q and Qe and total Qs values:  [[1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]] [[1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.699]] [[2.922]
 [2.922]
 [2.922]
 [2.922]
 [2.922]
 [2.922]
 [2.922]]
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.707]
 [0.712]
 [0.736]
 [0.712]
 [0.692]
 [0.669]] [[3.236]
 [2.989]
 [3.122]
 [2.221]
 [3.122]
 [2.413]
 [3.504]] [[2.032]
 [1.943]
 [1.997]
 [1.744]
 [1.997]
 [1.719]
 [2.038]]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.447]
 [0.52 ]
 [0.522]
 [0.53 ]
 [0.52 ]
 [0.515]] [[2.714]
 [3.106]
 [2.844]
 [2.856]
 [2.845]
 [2.858]
 [3.   ]] [[ 0.021]
 [-0.079]
 [-0.021]
 [-0.013]
 [-0.001]
 [-0.015]
 [ 0.023]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.1218751317085857, 0.11797656772027844, 0.13432743386975673, 0.12691327593962892, 0.3645801568919934, 0.13432743386975673]
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
first move QE:  1.6960165979830053
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997175204359673 -1.0 -0.997175204359673
from probs:  [0.12013635778693577, 0.11859963604402396, 0.1321406044453338, 0.12758354777181607, 0.3665029868207208, 0.13503686713116952]
maxi score, test score, baseline:  -0.9971826086956522 -1.0 -0.9971826086956522
probs:  [0.120776689166453, 0.1192317762924797, 0.13284492161676612, 0.12651950322901578, 0.3658553274351919, 0.13477178226009343]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.637]
 [0.629]
 [0.629]
 [0.655]
 [0.655]
 [0.65 ]] [[0.709]
 [1.305]
 [1.168]
 [1.168]
 [0.813]
 [0.751]
 [0.953]] [[0.897]
 [1.065]
 [1.003]
 [1.003]
 [0.936]
 [0.916]
 [0.974]]
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.458]
 [0.377]
 [0.388]
 [0.376]
 [0.366]
 [0.371]] [[2.168]
 [4.056]
 [1.776]
 [2.176]
 [1.767]
 [1.665]
 [1.989]] [[0.376]
 [0.458]
 [0.377]
 [0.388]
 [0.376]
 [0.366]
 [0.371]]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.463]
 [0.485]
 [0.479]
 [0.752]
 [0.463]
 [0.497]] [[4.024]
 [4.682]
 [4.026]
 [3.744]
 [5.921]
 [5.195]
 [4.107]] [[0.489]
 [0.692]
 [0.516]
 [0.411]
 [1.685]
 [0.864]
 [0.568]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 5.343402510259796
maxi score, test score, baseline:  -0.9972045822102426 -1.0 -0.9972045822102426
probs:  [0.11882091608939839, 0.11882091608939839, 0.13407991954860793, 0.12683363480085544, 0.36925390387522455, 0.1321907095965153]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.382]
 [0.374]
 [0.372]
 [0.374]
 [0.373]
 [0.376]] [[0.539]
 [1.108]
 [0.349]
 [0.597]
 [0.279]
 [0.298]
 [0.399]] [[0.377]
 [0.382]
 [0.374]
 [0.372]
 [0.374]
 [0.373]
 [0.376]]
maxi score, test score, baseline:  -0.9972045822102426 -1.0 -0.9972045822102426
maxi score, test score, baseline:  -0.9972045822102426 -1.0 -0.9972045822102426
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.677]] [[0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]]
maxi score, test score, baseline:  -0.9972045822102426 -1.0 -0.9972045822102426
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.629]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]] [[4.46]
 [4.43]
 [4.46]
 [4.46]
 [4.46]
 [4.46]
 [4.46]] [[0.396]
 [0.772]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]]
maxi score, test score, baseline:  -0.9972045822102426 -1.0 -0.9972045822102426
siam score:  -0.8111259
siam score:  -0.81186736
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.11946842808100802, 0.1187240507857888, 0.1356613312712118, 0.12746921805683373, 0.3658625451261765, 0.1328144266789811]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.65 ]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]] [[5.017]
 [4.734]
 [5.017]
 [5.017]
 [5.017]
 [5.017]
 [5.017]] [[0.911]
 [0.922]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.435]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.299]] [[3.522]
 [3.531]
 [3.522]
 [3.522]
 [3.522]
 [3.522]
 [3.497]] [[-0.131]
 [ 0.12 ]
 [-0.131]
 [-0.131]
 [-0.131]
 [-0.131]
 [-0.164]]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.11946842808100802, 0.1187240507857888, 0.1356613312712118, 0.12746921805683373, 0.3658625451261765, 0.1328144266789811]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.11827787292135936, 0.1190146862885494, 0.13502845018985246, 0.1277812616573616, 0.366758173636493, 0.1331395553063841]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.874]] [[1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [6.297]] [[0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [2.047]]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.11827787292135936, 0.1190146862885494, 0.13502845018985246, 0.1277812616573616, 0.366758173636493, 0.1331395553063841]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.556]
 [0.716]
 [0.7  ]
 [0.71 ]
 [0.71 ]
 [0.693]] [[0.833]
 [1.123]
 [0.47 ]
 [0.393]
 [0.474]
 [0.476]
 [0.658]] [[0.993]
 [0.825]
 [0.927]
 [0.868]
 [0.916]
 [0.917]
 [0.944]]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.704]] [[1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [4.811]] [[0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [1.971]]
first move QE:  1.7045174636113434
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.695]] [[2.059]
 [2.059]
 [2.059]
 [2.059]
 [2.059]
 [2.059]
 [2.145]] [[1.752]
 [1.752]
 [1.752]
 [1.752]
 [1.752]
 [1.752]
 [1.833]]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
Printing some Q and Qe and total Qs values:  [[0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]] [[1.271]
 [1.271]
 [1.271]
 [1.271]
 [1.271]
 [1.271]
 [1.271]] [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.11772266474185272, 0.11845133501877853, 0.13522647808469604, 0.12796866108474805, 0.36729604805436405, 0.13333481301556066]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
siam score:  -0.8058524
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.11835440928829447, 0.11908698988677653, 0.13405033828130383, 0.12865538954041983, 0.36673335334268786, 0.13311951966051752]
757 385
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.11780245661111892, 0.11852699179678991, 0.13424580834265912, 0.1288429927737245, 0.36726811805867454, 0.13331363241703306]
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
probs:  [0.11791093052104788, 0.11863613303348221, 0.13436942725714607, 0.12896163547242812, 0.3676054660213454, 0.1325164076945504]
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.11791102752333685, 0.11863623079759268, 0.13436954154900127, 0.12896174408342584, 0.3676049360068252, 0.1325165200398181]
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.11791102752333685, 0.11863623079759268, 0.13436954154900127, 0.12896174408342584, 0.3676049360068252, 0.1325165200398181]
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.456]
 [0.413]
 [0.402]
 [0.395]
 [0.397]
 [0.404]] [[0.255]
 [1.234]
 [0.396]
 [0.314]
 [0.286]
 [0.627]
 [0.718]] [[0.402]
 [0.456]
 [0.413]
 [0.402]
 [0.395]
 [0.397]
 [0.404]]
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
Printing some Q and Qe and total Qs values:  [[0.958]
 [1.127]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]] [[2.357]
 [2.382]
 [2.357]
 [2.357]
 [2.357]
 [2.357]
 [2.357]] [[1.557]
 [1.903]
 [1.557]
 [1.557]
 [1.557]
 [1.557]
 [1.557]]
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[2.22 ]
 [3.353]
 [3.353]
 [3.353]
 [3.353]
 [3.353]
 [3.353]] [[-0.143]
 [ 0.428]
 [ 0.428]
 [ 0.428]
 [ 0.428]
 [ 0.428]
 [ 0.428]]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.447]
 [0.423]
 [0.353]
 [0.441]
 [0.272]
 [0.36 ]] [[4.655]
 [4.319]
 [4.335]
 [4.859]
 [4.097]
 [3.624]
 [4.517]] [[ 0.264]
 [ 0.295]
 [ 0.253]
 [ 0.288]
 [ 0.209]
 [-0.287]
 [ 0.187]]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.342]] [[4.171]
 [4.171]
 [4.171]
 [4.171]
 [4.171]
 [4.171]
 [4.301]] [[0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.117]]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.431]
 [0.428]
 [0.424]
 [0.431]
 [0.426]
 [0.434]] [[0.929]
 [0.776]
 [0.925]
 [1.085]
 [0.776]
 [0.936]
 [0.916]] [[0.113]
 [0.037]
 [0.081]
 [0.126]
 [0.037]
 [0.081]
 [0.09 ]]
using explorer policy with actor:  1
first move QE:  1.7026615218462828
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
probs:  [0.11553622638920333, 0.11622494993335147, 0.1356850534786434, 0.1293572326467706, 0.37120219336646043, 0.1319943441855708]
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
probs:  [0.11553622638920333, 0.11622494993335147, 0.1356850534786434, 0.1293572326467706, 0.37120219336646043, 0.1319943441855708]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.408]
 [0.401]
 [0.401]
 [0.4  ]
 [0.384]
 [0.402]] [[1.46 ]
 [2.112]
 [1.641]
 [1.612]
 [1.653]
 [1.319]
 [1.68 ]] [[0.386]
 [0.408]
 [0.401]
 [0.401]
 [0.4  ]
 [0.384]
 [0.402]]
using explorer policy with actor:  1
siam score:  -0.8103457
maxi score, test score, baseline:  -0.9972753280839896 -1.0 -0.9972753280839896
probs:  [0.116874300874395, 0.11618594400890764, 0.13725649884183994, 0.12913623117160988, 0.36792538223756627, 0.13262164286568123]
siam score:  -0.8101126
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.488]
 [0.639]
 [0.641]
 [0.642]
 [0.643]
 [0.644]] [[2.321]
 [3.134]
 [2.151]
 [1.966]
 [1.903]
 [1.941]
 [2.021]] [[0.641]
 [0.488]
 [0.639]
 [0.641]
 [0.642]
 [0.643]
 [0.644]]
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
probs:  [0.11715695238767398, 0.11578512341293988, 0.13758844775605442, 0.1286040021106209, 0.3688143787907807, 0.13205109554193006]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.114]
 [0.242]
 [0.24 ]
 [0.241]
 [0.264]
 [0.247]] [[0.723]
 [1.928]
 [0.814]
 [0.905]
 [0.949]
 [0.779]
 [1.09 ]] [[0.26 ]
 [0.114]
 [0.242]
 [0.24 ]
 [0.241]
 [0.264]
 [0.247]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9972958333333334 -1.0 -0.9972958333333334
siam score:  -0.80195695
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.248]] [[-0.389]
 [-0.389]
 [-0.389]
 [-0.389]
 [-0.389]
 [-0.389]
 [-0.231]] [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.248]]
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.11721678341120431, 0.11518241000272836, 0.13656467037293363, 0.1269483098889789, 0.3711865313736801, 0.1329012949504747]
line 256 mcts: sample exp_bonus 5.795288014373772
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9973093264248705 -1.0 -0.9973093264248705
maxi score, test score, baseline:  -0.9973093264248705 -1.0 -0.9973093264248705
probs:  [0.1166880261157101, 0.11467487150008246, 0.1367491822504609, 0.1271198273057805, 0.37168723627071676, 0.13308085655724933]
using explorer policy with actor:  1
siam score:  -0.7978851
784 411
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
784 412
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
probs:  [0.11783050331876387, 0.11513577449687026, 0.13808808012670515, 0.1275489124248188, 0.36790766694361543, 0.13348906268922658]
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.544]
 [0.698]
 [0.667]
 [0.701]
 [0.667]
 [0.584]] [[2.052]
 [2.659]
 [1.635]
 [2.052]
 [1.597]
 [2.052]
 [2.686]] [[1.036]
 [1.195]
 [0.82 ]
 [1.036]
 [0.801]
 [1.036]
 [1.292]]
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
probs:  [0.11860525954992807, 0.11458335517489088, 0.13710601967509908, 0.12677715442659596, 0.3703267243984287, 0.13260148677505743]
line 256 mcts: sample exp_bonus 2.1701730279428455
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]] [[4.062]
 [4.062]
 [4.062]
 [4.062]
 [4.062]
 [4.062]
 [4.062]] [[2.057]
 [2.057]
 [2.057]
 [2.057]
 [2.057]
 [2.057]
 [2.057]]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.627]
 [0.564]
 [0.603]
 [0.561]
 [0.603]
 [0.566]] [[1.702]
 [1.776]
 [1.316]
 [1.315]
 [1.899]
 [1.315]
 [1.949]] [[0.723]
 [0.904]
 [0.47 ]
 [0.548]
 [0.854]
 [0.548]
 [0.896]]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.732]] [[3.259]
 [3.259]
 [3.259]
 [3.259]
 [3.259]
 [3.259]
 [3.763]] [[1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.563]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.234046959083299
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.682]
 [0.682]
 [0.676]
 [0.676]
 [0.678]
 [0.682]] [[3.677]
 [4.196]
 [4.196]
 [3.725]
 [3.689]
 [3.785]
 [4.07 ]] [[1.167]
 [1.516]
 [1.516]
 [1.19 ]
 [1.165]
 [1.233]
 [1.432]]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.1188935681798411, 0.11486188547502821, 0.1374393086219804, 0.12708533112783674, 0.36879608260272345, 0.13292382399258998]
from probs:  [0.1188935681798411, 0.11486188547502821, 0.1374393086219804, 0.12708533112783674, 0.36879608260272345, 0.13292382399258998]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
first move QE:  1.6833706423647332
siam score:  -0.7772293
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
probs:  [0.11904637252285562, 0.11372505022866443, 0.1376159523908607, 0.1272486654858764, 0.3692692959922223, 0.13309466337952033]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.545]
 [0.522]
 [0.495]
 [0.546]
 [0.511]
 [0.423]] [[4.982]
 [5.242]
 [5.242]
 [5.179]
 [5.653]
 [4.928]
 [5.475]] [[0.957]
 [1.135]
 [1.096]
 [1.015]
 [1.368]
 [0.899]
 [1.059]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973424552429667 -1.0 -0.9973424552429667
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.393]] [[0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.993]
 [1.794]] [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.393]]
Printing some Q and Qe and total Qs values:  [[1.066]
 [0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]] [[2.031]
 [2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]] [[1.744]
 [1.662]
 [1.662]
 [1.662]
 [1.662]
 [1.662]
 [1.662]]
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.744]] [[3.991]
 [3.991]
 [3.991]
 [3.991]
 [3.991]
 [3.991]
 [4.453]] [[1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]
 [2.015]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.656]] [[4.897]
 [4.897]
 [4.897]
 [4.897]
 [4.897]
 [4.897]
 [4.645]] [[1.733]
 [1.733]
 [1.733]
 [1.733]
 [1.733]
 [1.733]
 [1.536]]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.872]] [[3.74 ]
 [3.928]
 [3.928]
 [3.928]
 [3.928]
 [3.928]
 [3.769]] [[1.418]
 [1.547]
 [1.547]
 [1.547]
 [1.547]
 [1.547]
 [1.71 ]]
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.11929862851687366, 0.11336744595634048, 0.13685457024555692, 0.12826466245126777, 0.36979635785974435, 0.1324183349702168]
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.11929862851687366, 0.11336744595634048, 0.13685457024555692, 0.12826466245126777, 0.36979635785974435, 0.1324183349702168]
first move QE:  1.6797770362142797
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.11929862851687366, 0.11336744595634048, 0.13685457024555692, 0.12826466245126777, 0.36979635785974435, 0.1324183349702168]
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.707]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[2.096]
 [2.606]
 [2.096]
 [2.096]
 [2.096]
 [2.096]
 [2.096]] [[0.994]
 [1.41 ]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.11951506729138756, 0.11357312272923331, 0.13528935710590592, 0.12849736989914515, 0.37046650380788293, 0.13265857916644513]
siam score:  -0.782007
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
line 256 mcts: sample exp_bonus 4.685144359448136
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.12026696037959406, 0.11304009536507034, 0.1343639156156307, 0.1293057724417662, 0.37038946798600453, 0.13263378821193414]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
siam score:  -0.77661026
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [1.016]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]] [[3.029]
 [1.624]
 [3.029]
 [3.029]
 [3.029]
 [3.029]
 [3.029]] [[0.111]
 [1.032]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]]
using another actor
from probs:  [0.11933249213511712, 0.11346670511706683, 0.13487100079220404, 0.1297937683456527, 0.369401689667665, 0.13313434394229431]
maxi score, test score, baseline:  -0.9973619289340102 -1.0 -0.9973619289340102
probs:  [0.11953101826251065, 0.11365547140942005, 0.13509538078725397, 0.12919963966633063, 0.3700154850532027, 0.1325030048212821]
maxi score, test score, baseline:  -0.9973619289340102 -1.0 -0.9973619289340102
probs:  [0.11953101826251065, 0.11365547140942005, 0.13509538078725397, 0.12919963966633063, 0.3700154850532027, 0.1325030048212821]
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.122 0.51  0.    0.367 0.   ]
using another actor
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
808 437
from probs:  [0.11960749264931311, 0.11438195594224106, 0.13507783158472375, 0.12842558584249716, 0.3700043652644599, 0.13250276871676508]
maxi score, test score, baseline:  -0.9973811083123426 -1.0 -0.9973811083123426
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973811083123426 -1.0 -0.9973811083123426
siam score:  -0.76057017
siam score:  -0.76075995
maxi score, test score, baseline:  -0.9973811083123426 -1.0 -0.9973811083123426
probs:  [0.12016253570068944, 0.11428915632937611, 0.13483105542323592, 0.12823285890861547, 0.36936674085815996, 0.13311765277992318]
maxi score, test score, baseline:  -0.9973811083123426 -1.0 -0.9973811083123426
probs:  [0.12016253570068944, 0.11428915632937611, 0.13483105542323592, 0.12823285890861547, 0.36936674085815996, 0.13311765277992318]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973811083123426 -1.0 -0.9973811083123426
siam score:  -0.7642975
using another actor
siam score:  -0.76503015
maxi score, test score, baseline:  -0.9973874371859297 -1.0 -0.9973874371859297
from probs:  [0.120441956729925, 0.11393674340557668, 0.1342803196199023, 0.12853104808107116, 0.370224910561483, 0.13258502160204183]
maxi score, test score, baseline:  -0.9973937343358396 -1.0 -0.9973937343358396
maxi score, test score, baseline:  -0.9973937343358396 -1.0 -0.9973937343358396
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973937343358396 -1.0 -0.9973937343358396
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
using explorer policy with actor:  1
using another actor
siam score:  -0.7787366
Printing some Q and Qe and total Qs values:  [[0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.814]] [[1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]
 [0.833]] [[1.786]
 [1.786]
 [1.786]
 [1.786]
 [1.786]
 [1.786]
 [1.514]]
using explorer policy with actor:  0
using another actor
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9974062344139651 -1.0 -0.9974062344139651
probs:  [0.12043346126805093, 0.11275508870345624, 0.135038158002115, 0.12847126382409527, 0.3699687369579226, 0.13333329124435986]
maxi score, test score, baseline:  -0.9974062344139651 -1.0 -0.9974062344139651
maxi score, test score, baseline:  -0.9974062344139651 -1.0 -0.9974062344139651
probs:  [0.12043346126805093, 0.11275508870345624, 0.135038158002115, 0.12847126382409527, 0.3699687369579226, 0.13333329124435986]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.583]
 [0.584]
 [0.578]] [[3.014]
 [3.014]
 [3.014]
 [3.014]
 [3.314]
 [3.334]
 [3.64 ]] [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.583]
 [0.584]
 [0.578]]
siam score:  -0.7758876
start point for exploration sampling:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.703]
 [0.703]
 [0.739]
 [0.74 ]
 [0.747]
 [0.749]] [[1.126]
 [1.891]
 [1.891]
 [1.112]
 [1.138]
 [1.091]
 [1.2  ]] [[0.601]
 [1.293]
 [1.293]
 [0.586]
 [0.614]
 [0.581]
 [0.694]]
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.638]
 [0.538]
 [0.565]
 [0.572]
 [0.564]
 [0.627]] [[4.422]
 [5.067]
 [3.987]
 [3.87 ]
 [3.713]
 [3.74 ]
 [4.956]] [[0.553]
 [0.638]
 [0.538]
 [0.565]
 [0.572]
 [0.564]
 [0.627]]
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
maxi score, test score, baseline:  -0.9974247524752475 -1.0 -0.9974247524752475
probs:  [0.1210936870898176, 0.11277481049984543, 0.13322351400395635, 0.12917555864873642, 0.3696681865645267, 0.13406424319311738]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]]
maxi score, test score, baseline:  -0.9974308641975309 -1.0 -0.9974308641975309
probs:  [0.12146756240341032, 0.11312299963337528, 0.13363484256197194, 0.12879219041297388, 0.3685042373191526, 0.13447816766911586]
from probs:  [0.1217446472554326, 0.1133810493463483, 0.13393968271718418, 0.12908598378723857, 0.3670637053244955, 0.13478493156930077]
maxi score, test score, baseline:  -0.9974369458128078 -1.0 -0.9974369458128078
maxi score, test score, baseline:  -0.9974369458128078 -1.0 -0.9974369458128078
probs:  [0.12184641950120428, 0.1134758283109814, 0.13321640461353232, 0.1291938945431177, 0.36736984549076634, 0.13489760754039792]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.263]
 [0.251]
 [0.25 ]
 [0.263]
 [0.263]
 [0.263]] [[ 0.   ]
 [ 0.   ]
 [-1.699]
 [-1.643]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.263]
 [0.263]
 [0.251]
 [0.25 ]
 [0.263]
 [0.263]
 [0.263]]
siam score:  -0.77182204
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
probs:  [0.12212195637915906, 0.11373243291705239, 0.13351765767384144, 0.12948604964934235, 0.3659392403016522, 0.13520266307895257]
first move QE:  1.6632982304726902
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
probs:  [0.12230742325822559, 0.11276733067557831, 0.13448009835102107, 0.129637006583918, 0.3663280427802359, 0.13448009835102107]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9974609756097561 -1.0 -0.9974609756097561
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.36 ]
 [0.455]
 [0.465]
 [0.405]
 [0.471]
 [0.33 ]] [[ 1.981]
 [ 1.605]
 [-0.227]
 [ 0.759]
 [ 2.762]
 [ 0.407]
 [ 1.929]] [[ 1.114]
 [ 0.95 ]
 [-0.004]
 [ 0.543]
 [ 1.611]
 [ 0.354]
 [ 1.112]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9974609756097561 -1.0 -0.9974609756097561
probs:  [0.12302472491444293, 0.11284281637961936, 0.13526879213986368, 0.13039729588096913, 0.36403502653943254, 0.13443134414567215]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
825 473
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.606]
 [0.525]
 [0.522]
 [0.522]
 [0.521]
 [0.524]] [[0.863]
 [2.786]
 [0.752]
 [0.699]
 [0.722]
 [0.688]
 [0.736]] [[0.522]
 [0.606]
 [0.525]
 [0.522]
 [0.522]
 [0.521]
 [0.524]]
siam score:  -0.7705626
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
maxi score, test score, baseline:  -0.9974728155339806 -1.0 -0.9974728155339806
probs:  [0.12099017701631763, 0.11236879323257631, 0.1359401057697954, 0.13186078069834337, 0.36373629915281946, 0.13510384413014773]
maxi score, test score, baseline:  -0.9974728155339806 -1.0 -0.9974728155339806
probs:  [0.12099017701631763, 0.11236879323257631, 0.1359401057697954, 0.13186078069834337, 0.36373629915281946, 0.13510384413014773]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]] [[-2.38]
 [-2.38]
 [-2.38]
 [-2.38]
 [-2.38]
 [-2.38]
 [-2.38]] [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]]
maxi score, test score, baseline:  -0.9974786924939467 -1.0 -0.9974786924939467
probs:  [0.12107603875001764, 0.11136791130090087, 0.1359416089064777, 0.13267843496969375, 0.36382539177145173, 0.13511061430145818]
first move QE:  1.6457056411728281
maxi score, test score, baseline:  -0.9974786924939467 -1.0 -0.9974786924939467
maxi score, test score, baseline:  -0.9974903614457832 -1.0 -0.9974903614457832
probs:  [0.1210761991603994, 0.11136805481311353, 0.13594179519218083, 0.13267861557544836, 0.36382453611814314, 0.1351107991407148]
830 488
maxi score, test score, baseline:  -0.9974903614457832 -1.0 -0.9974903614457832
siam score:  -0.7711664
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.272]
 [0.31 ]
 [0.162]
 [0.31 ]
 [0.235]
 [0.152]] [[2.338]
 [2.699]
 [2.099]
 [1.922]
 [2.099]
 [2.25 ]
 [2.043]] [[1.383]
 [1.859]
 [1.343]
 [0.965]
 [1.343]
 [1.379]
 [1.066]]
maxi score, test score, baseline:  -0.9974903614457832 -1.0 -0.9974903614457832
probs:  [0.12136667251372188, 0.11163523740192036, 0.13626793252866792, 0.13142053469248066, 0.3646973858917575, 0.13461223697145172]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.486]
 [0.465]
 [0.459]
 [0.46 ]
 [0.464]
 [0.515]] [[3.143]
 [3.714]
 [3.368]
 [3.38 ]
 [3.371]
 [3.343]
 [3.102]] [[0.504]
 [0.486]
 [0.465]
 [0.459]
 [0.46 ]
 [0.464]
 [0.515]]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.244]
 [0.223]
 [0.223]
 [0.244]
 [0.244]
 [0.222]] [[ 0.   ]
 [ 0.   ]
 [-1.873]
 [-2.08 ]
 [ 0.   ]
 [ 0.   ]
 [-1.618]] [[0.244]
 [0.244]
 [0.223]
 [0.223]
 [0.244]
 [0.244]
 [0.222]]
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.075]
 [0.447]
 [0.558]
 [0.252]
 [0.372]
 [0.18 ]] [[3.264]
 [3.216]
 [2.695]
 [2.258]
 [2.783]
 [2.907]
 [2.859]] [[1.827]
 [1.802]
 [1.735]
 [1.46 ]
 [1.602]
 [1.846]
 [1.592]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9975076555023924 -1.0 -0.9975076555023924
siam score:  -0.7635324
maxi score, test score, baseline:  -0.9975076555023924 -1.0 -0.9975076555023924
probs:  [0.12192471005749235, 0.1121485242980862, 0.13523118511890628, 0.13124721054962066, 0.3642171848569881, 0.13523118511890628]
maxi score, test score, baseline:  -0.9975076555023924 -1.0 -0.9975076555023924
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.64 ]] [[1.958]
 [1.978]
 [1.978]
 [1.978]
 [1.978]
 [1.978]
 [2.112]] [[0.653]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.848]]
maxi score, test score, baseline:  -0.9975076555023924 -1.0 -0.9975076555023924
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.557]
 [0.544]
 [0.544]
 [0.544]
 [0.557]
 [0.554]] [[2.74 ]
 [3.415]
 [2.74 ]
 [2.74 ]
 [2.74 ]
 [3.028]
 [2.669]] [[0.544]
 [0.557]
 [0.544]
 [0.544]
 [0.544]
 [0.557]
 [0.554]]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.521]
 [0.511]
 [0.511]
 [0.511]
 [0.516]
 [0.52 ]] [[2.611]
 [3.197]
 [2.611]
 [2.558]
 [2.611]
 [2.506]
 [2.596]] [[0.511]
 [0.521]
 [0.511]
 [0.511]
 [0.511]
 [0.516]
 [0.52 ]]
maxi score, test score, baseline:  -0.9975133651551312 -1.0 -0.9975133651551312
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.262]
 [0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.239]] [[1.053]
 [1.457]
 [1.557]
 [1.557]
 [1.557]
 [1.557]
 [0.867]] [[0.23 ]
 [0.262]
 [0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.239]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9975190476190476 -1.0 -0.9975190476190476
probs:  [0.12163043826721064, 0.11309860258737328, 0.13637682092371964, 0.1315842465603542, 0.36093307073762254, 0.13637682092371964]
maxi score, test score, baseline:  -0.997524703087886 -1.0 -0.997524703087886
maxi score, test score, baseline:  -0.9975359338061466 -1.0 -0.9975359338061466
maxi score, test score, baseline:  -0.9975359338061466 -1.0 -0.9975359338061466
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.788]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]] [[1.148]
 [1.061]
 [1.148]
 [1.148]
 [1.148]
 [1.148]
 [1.148]] [[0.918]
 [0.973]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]]
maxi score, test score, baseline:  -0.9975359338061466 -1.0 -0.9975359338061466
probs:  [0.12315494664947974, 0.11456086802951644, 0.13799310167255402, 0.13164133004864154, 0.35711031069462407, 0.1355394429051842]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.611012643987499
maxi score, test score, baseline:  -0.9975415094339622 -1.0 -0.9975415094339622
probs:  [0.12315502321667582, 0.11456093751876474, 0.13799319046022077, 0.13164141360508924, 0.35710990552719324, 0.13553952967205626]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.296]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.268]] [[2.918]
 [2.986]
 [3.073]
 [3.073]
 [3.073]
 [3.073]
 [2.852]] [[-0.432]
 [-0.333]
 [-0.344]
 [-0.344]
 [-0.344]
 [-0.344]
 [-0.434]]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[2.944]
 [2.944]
 [2.944]
 [2.944]
 [2.944]
 [2.944]
 [2.944]] [[0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]]
siam score:  -0.74038315
Printing some Q and Qe and total Qs values:  [[1.045]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[2.426]
 [2.42 ]
 [2.42 ]
 [2.42 ]
 [2.42 ]
 [2.42 ]
 [2.42 ]] [[1.592]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]]
from probs:  [0.12257835184362485, 0.11463628038424972, 0.13808394392830392, 0.1317279897237237, 0.3573447646699187, 0.13562866945017918]
maxi score, test score, baseline:  -0.9975415094339622 -1.0 -0.9975415094339622
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.6889184727703346
Printing some Q and Qe and total Qs values:  [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]] [[3.721]
 [3.721]
 [3.721]
 [3.721]
 [3.721]
 [3.721]
 [3.721]] [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.474]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[0.043]
 [0.639]
 [0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[0.439]
 [0.474]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
maxi score, test score, baseline:  -0.9975525821596244 -1.0 -0.9975525821596244
probs:  [0.12330923004609555, 0.11418330641295077, 0.13807393576150864, 0.1325134267084525, 0.35548273509962597, 0.13643736597136646]
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.274]
 [0.543]
 [0.535]
 [0.282]
 [0.087]
 [0.159]] [[2.385]
 [2.009]
 [1.503]
 [1.361]
 [2.109]
 [2.473]
 [2.901]] [[1.493]
 [1.136]
 [0.865]
 [0.694]
 [1.258]
 [1.452]
 [2.022]]
maxi score, test score, baseline:  -0.9975525821596244 -1.0 -0.9975525821596244
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9975580796252927 -1.0 -0.9975580796252927
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.7220862
line 256 mcts: sample exp_bonus 2.5668904467528058
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9975635514018691 -1.0 -0.9975635514018691
probs:  [0.12380355330958172, 0.11464104192513205, 0.136984322698778, 0.13228420338170027, 0.3569065649593184, 0.13538031372548956]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.725]
 [0.602]
 [0.575]
 [0.571]
 [0.565]
 [0.578]] [[1.049]
 [3.532]
 [0.943]
 [0.878]
 [0.944]
 [0.98 ]
 [0.89 ]] [[0.315]
 [1.36 ]
 [0.25 ]
 [0.173]
 [0.188]
 [0.188]
 [0.184]]
maxi score, test score, baseline:  -0.9975635514018691 -1.0 -0.9975635514018691
probs:  [0.1239967978247511, 0.11481998470967344, 0.13639007477816736, 0.1317378552077588, 0.3574636591158477, 0.13559162836380162]
maxi score, test score, baseline:  -0.9975635514018691 -1.0 -0.9975635514018691
probs:  [0.1239967978247511, 0.11481998470967344, 0.13639007477816736, 0.1317378552077588, 0.3574636591158477, 0.13559162836380162]
using another actor
line 256 mcts: sample exp_bonus 0.5422013947137526
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9975635514018691 -1.0 -0.9975635514018691
maxi score, test score, baseline:  -0.9975635514018691 -1.0 -0.9975635514018691
probs:  [0.12400487736992752, 0.11544013874267214, 0.13712673098961475, 0.1324493843236376, 0.35544815528318013, 0.13553071329096772]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9975635514018691 -1.0 -0.9975635514018691
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.756]
 [0.728]
 [0.666]
 [0.666]
 [0.666]
 [0.702]] [[4.41 ]
 [3.882]
 [3.22 ]
 [4.41 ]
 [4.41 ]
 [4.41 ]
 [3.226]] [[0.666]
 [0.756]
 [0.728]
 [0.666]
 [0.666]
 [0.666]
 [0.702]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9975635514018691 -1.0 -0.9975635514018691
probs:  [0.12355301326295236, 0.11567019700586581, 0.1380768571267691, 0.13261372160443113, 0.3559569684117788, 0.13412924258820264]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.362]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[-1.278]
 [-0.815]
 [-1.278]
 [-1.278]
 [-1.278]
 [-1.278]
 [-1.278]] [[0.339]
 [0.362]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]]
siam score:  -0.72860724
maxi score, test score, baseline:  -0.9975689976689976 -1.0 -0.9975689976689976
maxi score, test score, baseline:  -0.9975798143851509 -1.0 -0.9975798143851509
probs:  [0.12412742675605158, 0.11508005140535016, 0.1379067230243024, 0.13323026494415194, 0.3556683855372972, 0.1339871483328468]
Starting evaluation
line 256 mcts: sample exp_bonus 2.116860169148794
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]] [[0.074]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]] [[0.389]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.277]
 [0.362]
 [0.359]
 [0.355]
 [0.354]
 [0.354]] [[0.507]
 [2.071]
 [0.831]
 [0.472]
 [0.528]
 [0.694]
 [0.763]] [[0.364]
 [0.277]
 [0.362]
 [0.359]
 [0.355]
 [0.354]
 [0.354]]
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.732]
 [0.735]
 [0.741]
 [0.74 ]
 [0.733]
 [0.726]] [[2.454]
 [3.476]
 [2.501]
 [2.156]
 [2.698]
 [2.484]
 [3.127]] [[0.767]
 [0.732]
 [0.735]
 [0.741]
 [0.74 ]
 [0.733]
 [0.726]]
from probs:  [0.12446117140350842, 0.11538945770076088, 0.13827753536824852, 0.13358849714428045, 0.3546948412389212, 0.13358849714428045]
maxi score, test score, baseline:  -0.9976220956719818 -1.0 -0.9976220956719818
probs:  [0.12455468982050485, 0.115476158010816, 0.13838143787034052, 0.13368887546504238, 0.35496077335310156, 0.13293806548019468]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.42 ]
 [0.682]] [[1.517]
 [1.831]
 [1.831]
 [1.831]
 [1.831]
 [1.529]
 [1.945]] [[0.419]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.42 ]
 [0.682]]
maxi score, test score, baseline:  -0.9976272727272727 -1.0 -0.9976272727272727
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 2.1039452485622516
using explorer policy with actor:  0
siam score:  -0.7221215
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.591]
 [0.604]
 [0.596]
 [0.591]
 [0.59 ]
 [0.577]] [[3.058]
 [2.855]
 [2.72 ]
 [2.703]
 [2.855]
 [2.781]
 [2.855]] [[0.591]
 [0.591]
 [0.604]
 [0.596]
 [0.591]
 [0.59 ]
 [0.577]]
using explorer policy with actor:  0
main train batch thing paused
add a thread
Adding thread: now have 4 threads
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.2651773783789086
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]] [[1.597]
 [1.597]
 [1.597]
 [1.597]
 [1.597]
 [1.597]
 [1.597]] [[0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.075]] [[6.75 ]
 [8.1  ]
 [8.1  ]
 [8.1  ]
 [8.1  ]
 [8.1  ]
 [7.146]] [[-0.981]
 [-0.558]
 [-0.558]
 [-0.558]
 [-0.558]
 [-0.558]
 [-0.866]]
siam score:  -0.7278943
siam score:  -0.728497
maxi score, test score, baseline:  -0.9976827050997783 -1.0 -0.9976827050997783
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.66 ]] [[2.399]
 [2.764]
 [2.764]
 [2.764]
 [2.764]
 [2.764]
 [2.739]] [[1.321]
 [1.516]
 [1.516]
 [1.516]
 [1.516]
 [1.516]
 [1.479]]
867 535
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.34009675960109476
maxi score, test score, baseline:  -0.9976827050997783 -1.0 -0.9976827050997783
probs:  [0.12315143783347368, 0.11597254417051847, 0.13736846253854182, 0.1342635950742166, 0.3564798751639627, 0.1327640852192868]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.614]
 [0.753]
 [0.753]
 [0.624]
 [0.623]
 [0.624]] [[3.624]
 [4.337]
 [3.534]
 [3.534]
 [3.714]
 [3.87 ]
 [3.807]] [[0.82 ]
 [0.657]
 [0.666]
 [0.666]
 [0.469]
 [0.518]
 [0.498]]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.347]
 [0.347]
 [0.347]
 [0.319]
 [0.318]
 [0.314]] [[ 0.396]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.243]
 [ 0.031]
 [-0.039]] [[0.317]
 [0.347]
 [0.347]
 [0.347]
 [0.319]
 [0.318]
 [0.314]]
using explorer policy with actor:  1
first move QE:  1.5954482637229728
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.55 ]
 [0.683]
 [0.626]
 [0.544]
 [0.696]
 [0.476]] [[ 2.349]
 [ 3.028]
 [ 0.513]
 [ 1.224]
 [ 1.373]
 [-1.142]
 [ 1.892]] [[1.135]
 [1.869]
 [0.94 ]
 [1.177]
 [1.134]
 [0.229]
 [1.273]]
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.443]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [1.094]] [[1.563]
 [1.934]
 [1.563]
 [1.563]
 [1.563]
 [1.563]
 [1.788]] [[1.907]
 [1.868]
 [1.907]
 [1.907]
 [1.907]
 [1.907]
 [2.323]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9976876106194691 -1.0 -0.9976876106194691
probs:  [0.12357888046565729, 0.1163750684690858, 0.13705267697776322, 0.13397295206071042, 0.35579552814905024, 0.13322489387773312]
869 538
using another actor
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.717]
 [0.64 ]
 [0.584]
 [0.548]
 [0.496]
 [0.478]] [[2.731]
 [3.466]
 [2.49 ]
 [2.835]
 [3.24 ]
 [3.702]
 [3.342]] [[-0.05 ]
 [ 0.579]
 [ 0.101]
 [ 0.104]
 [ 0.167]
 [ 0.216]
 [ 0.06 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.605894464236425
maxi score, test score, baseline:  -0.9976924944812362 -1.0 -0.9976924944812362
Printing some Q and Qe and total Qs values:  [[0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.975]
 [1.06 ]] [[2.401]
 [2.401]
 [2.401]
 [2.401]
 [2.401]
 [2.391]
 [4.279]] [[0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.348]
 [0.918]]
maxi score, test score, baseline:  -0.9976924944812362 -1.0 -0.9976924944812362
maxi score, test score, baseline:  -0.9976924944812362 -1.0 -0.9976924944812362
probs:  [0.123471131393076, 0.11690718881135495, 0.13677492883800407, 0.1352384902759432, 0.35533964606195256, 0.13226861461966913]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9976924944812362 -1.0 -0.9976924944812362
using explorer policy with actor:  1
rdn probs:  [0.12356038918765395, 0.11699170029291495, 0.13687380645880914, 0.13533625691531334, 0.3555959801053394, 0.13164186703996922]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
probs:  [0.12324134199157427, 0.11728581171042486, 0.13721790438548567, 0.13567648921861428, 0.3546056415845746, 0.13197281110932613]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
probs:  [0.12324134199157427, 0.11728581171042486, 0.13721790438548567, 0.13567648921861428, 0.3546056415845746, 0.13197281110932613]
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
probs:  [0.1234714703158066, 0.11750481928425503, 0.13747413111817625, 0.13592983766968633, 0.3534004979227887, 0.13221924368928697]
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
using explorer policy with actor:  1
siam score:  -0.712467
first move QE:  1.5875161873167072
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [ 0.462]] [[4.726]
 [4.726]
 [4.726]
 [4.726]
 [4.726]
 [4.726]
 [8.716]] [[0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [1.685]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.249]] [[3.422]
 [3.364]
 [3.364]
 [3.364]
 [3.364]
 [3.364]
 [3.296]] [[1.111]
 [1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.006]]
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
siam score:  -0.7120849
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
probs:  [0.12344801917915853, 0.11639434215732389, 0.13814657736627636, 0.1365947295903303, 0.3532713755735751, 0.13214495613333585]
using another actor
using explorer policy with actor:  1
using another actor
from probs:  [0.1235444311802579, 0.11648524528820156, 0.1374734761396794, 0.1367014090762143, 0.35354727793698804, 0.13224816037865883]
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
probs:  [0.12363266307060959, 0.11656843571039763, 0.1375716557545527, 0.13679903730292842, 0.35379977126560724, 0.13162843689590445]
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
probs:  [0.12363266307060959, 0.11656843571039763, 0.1375716557545527, 0.13679903730292842, 0.35379977126560724, 0.13162843689590445]
from probs:  [0.12363266307060959, 0.11656843571039763, 0.1375716557545527, 0.13679903730292842, 0.35379977126560724, 0.13162843689590445]
siam score:  -0.70706964
maxi score, test score, baseline:  -0.9977165938864629 -1.0 -0.9977165938864629
probs:  [0.12372832280783917, 0.1166586282665491, 0.13690488682431987, 0.13690488682431987, 0.3540729905158044, 0.13173028476116752]
line 256 mcts: sample exp_bonus 2.4108822540179466
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.171]] [[2.719]
 [2.376]
 [2.376]
 [2.376]
 [2.376]
 [2.376]
 [2.626]] [[1.356]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [1.24 ]]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.477]] [[2.719]
 [2.719]
 [2.719]
 [2.719]
 [2.719]
 [2.719]
 [5.899]] [[0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [1.375]]
887 552
maxi score, test score, baseline:  -0.9977260869565218 -1.0 -0.9977260869565218
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]] [[2.305]
 [2.305]
 [2.305]
 [2.305]
 [2.305]
 [2.305]
 [2.305]] [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]]
maxi score, test score, baseline:  -0.9977308026030369 -1.0 -0.9977308026030369
probs:  [0.12370801919616503, 0.11667432152534493, 0.13604679392379015, 0.1368062828985019, 0.3558030625369996, 0.13096151991919827]
maxi score, test score, baseline:  -0.9977308026030369 -1.0 -0.9977308026030369
siam score:  -0.70590484
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.3601119704474818
890 556
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.755]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.767]] [[1.752]
 [1.038]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [1.262]] [[1.621]
 [1.365]
 [1.021]
 [1.021]
 [1.021]
 [1.021]
 [1.464]]
line 256 mcts: sample exp_bonus 5.559774104767315
from probs:  [0.12430160162780436, 0.11723414941402734, 0.13446106418510886, 0.1374627235770397, 0.3556489379094234, 0.13089152328659645]
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.27 ]
 [0.302]
 [0.357]
 [0.29 ]
 [0.289]
 [0.257]] [[2.504]
 [2.662]
 [2.22 ]
 [1.96 ]
 [2.299]
 [2.297]
 [2.853]] [[-0.091]
 [-0.053]
 [-0.138]
 [-0.115]
 [-0.134]
 [-0.137]
 [-0.016]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.4702215351251871
siam score:  -0.71438134
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
probs:  [0.12393695254892016, 0.11747809508257762, 0.1347408595835295, 0.1369840436712238, 0.3563884753745408, 0.13047157373920806]
897 560
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]] [[3.889]
 [2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]] [[1.689]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]]
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
probs:  [0.12339611040193697, 0.11755062080473062, 0.1348240425644754, 0.13706861149370775, 0.35660849368114833, 0.13055212105400088]
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.95 ]
 [0.95 ]] [[1.562]
 [3.062]
 [3.062]
 [3.062]
 [3.062]
 [1.562]
 [1.562]] [[0.607]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [0.607]
 [0.607]]
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
898 564
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
probs:  [0.12280667739821886, 0.11759706546031552, 0.1347801501394774, 0.1370111623824647, 0.35658321902531265, 0.1312217255942109]
siam score:  -0.7161288
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.705]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]] [[-1.534]
 [-1.651]
 [-1.534]
 [-1.534]
 [-1.534]
 [-1.534]
 [-1.534]] [[0.272]
 [0.268]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]]
maxi score, test score, baseline:  -0.9977632478632479 -1.0 -0.9977632478632479
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.997772340425532 -1.0 -0.997772340425532
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]] [[4.615]
 [4.143]
 [4.143]
 [4.143]
 [4.143]
 [4.143]
 [4.143]] [[1.17 ]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]]
siam score:  -0.71223867
maxi score, test score, baseline:  -0.9977768577494692 -1.0 -0.9977768577494692
probs:  [0.12362115948968501, 0.11727266408018842, 0.1335014759619747, 0.13641433963306454, 0.3570983350365432, 0.13209202579854412]
maxi score, test score, baseline:  -0.9977768577494692 -1.0 -0.9977768577494692
probs:  [0.12362115948968501, 0.11727266408018842, 0.1335014759619747, 0.13641433963306454, 0.3570983350365432, 0.13209202579854412]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.227]] [[2.179]
 [2.179]
 [2.179]
 [2.179]
 [2.179]
 [2.179]
 [2.241]] [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.227]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9977813559322034 -1.0 -0.9977813559322034
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.7  ]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[1.013]
 [2.137]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]] [[0.624]
 [1.083]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
maxi score, test score, baseline:  -0.9977813559322034 -1.0 -0.9977813559322034
probs:  [0.12400723367253759, 0.11709289745599415, 0.13391840854815354, 0.13609777186334424, 0.35637913209837435, 0.13250455636159614]
siam score:  -0.72655565
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977813559322034 -1.0 -0.9977813559322034
probs:  [0.12360756904469984, 0.1162120147138252, 0.13414437889403966, 0.1363274196086819, 0.35698047672768946, 0.13272814101106387]
maxi score, test score, baseline:  -0.9977813559322034 -1.0 -0.9977813559322034
probs:  [0.12316958280197703, 0.11636777636054274, 0.13432417570253566, 0.13577347962820618, 0.35745894590351474, 0.13290603960322367]
UNIT TEST: sample policy line 217 mcts : [0.796 0.102 0.02  0.02  0.02  0.02  0.02 ]
maxi score, test score, baseline:  -0.9977813559322034 -1.0 -0.9977813559322034
Printing some Q and Qe and total Qs values:  [[1.161]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]] [[2.048]
 [2.035]
 [2.035]
 [2.035]
 [2.035]
 [2.035]
 [2.035]] [[1.444]
 [1.318]
 [1.318]
 [1.318]
 [1.318]
 [1.318]
 [1.318]]
913 583
using explorer policy with actor:  1
using explorer policy with actor:  0
siam score:  -0.71761996
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977813559322034 -1.0 -0.9977813559322034
maxi score, test score, baseline:  -0.9977813559322034 -1.0 -0.9977813559322034
probs:  [0.12328737977507566, 0.11764196642689255, 0.13641910212845815, 0.1342702748342683, 0.35754315670703474, 0.1308381201282706]
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
probs:  [0.12328744078875406, 0.11764202362897754, 0.13641917200823425, 0.13427034326322843, 0.3575428340708506, 0.1308381862399552]
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
siam score:  -0.7086228
line 256 mcts: sample exp_bonus 2.5919193992139045
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.558]
 [0.28 ]
 [0.587]
 [0.28 ]
 [0.613]
 [0.28 ]] [[ 1.31 ]
 [ 2.211]
 [ 1.31 ]
 [-0.538]
 [ 1.31 ]
 [-1.059]
 [ 1.31 ]] [[1.097]
 [1.696]
 [1.097]
 [0.379]
 [1.097]
 [0.142]
 [1.097]]
Printing some Q and Qe and total Qs values:  [[0.958]
 [1.071]
 [1.004]
 [0.999]
 [1.001]
 [1.115]
 [1.111]] [[3.74 ]
 [3.114]
 [3.63 ]
 [3.594]
 [3.531]
 [3.349]
 [3.189]] [[2.278]
 [2.295]
 [2.334]
 [2.311]
 [2.295]
 [2.462]
 [2.4  ]]
from probs:  [0.12304324027208617, 0.11746227169538094, 0.13458515826476164, 0.13529152638909056, 0.3584546875815229, 0.13116311579715778]
Printing some Q and Qe and total Qs values:  [[0.817]
 [0.821]
 [0.774]
 [0.774]
 [0.774]
 [0.785]
 [0.804]] [[3.414]
 [3.15 ]
 [2.667]
 [2.667]
 [2.667]
 [3.144]
 [3.158]] [[1.762]
 [1.682]
 [1.427]
 [1.427]
 [1.427]
 [1.607]
 [1.651]]
maxi score, test score, baseline:  -0.9977947368421053 -1.0 -0.9977947368421053
probs:  [0.1229308144672319, 0.11738118564270127, 0.13370585695864962, 0.13581070168566137, 0.35982973782515065, 0.13034170342060517]
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]] [[3.566]
 [2.65 ]
 [2.65 ]
 [2.65 ]
 [2.65 ]
 [2.65 ]
 [2.65 ]] [[2.452]
 [1.407]
 [1.407]
 [1.407]
 [1.407]
 [1.407]
 [1.407]]
maxi score, test score, baseline:  -0.9977947368421053 -1.0 -0.9977947368421053
probs:  [0.12272329972905684, 0.11773755566948103, 0.13411178879145633, 0.13622302384045729, 0.35912112434302257, 0.1300832076265259]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977947368421053 -1.0 -0.9977947368421053
probs:  [0.1231152107585644, 0.11811354496210925, 0.133848861208316, 0.13594454264781694, 0.3584792182508142, 0.13049862217237912]
maxi score, test score, baseline:  -0.9977991596638656 -1.0 -0.9977991596638656
maxi score, test score, baseline:  -0.9977991596638656 -1.0 -0.9977991596638656
probs:  [0.12277976216698917, 0.11834638608167572, 0.13342736918438586, 0.1355051935229815, 0.3591854082949191, 0.13075588074904865]
siam score:  -0.7103688
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.776]] [[3.065]
 [3.065]
 [3.065]
 [3.065]
 [3.065]
 [3.065]
 [3.987]] [[1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.901]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.2952596337640943
line 256 mcts: sample exp_bonus 2.860970402806092
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[2.255]
 [2.255]
 [2.255]
 [2.255]
 [2.255]
 [2.255]
 [2.255]] [[1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.424]
 [0.598]
 [0.605]
 [0.61 ]
 [0.612]
 [0.606]] [[2.847]
 [2.222]
 [2.668]
 [2.628]
 [2.595]
 [2.575]
 [2.718]] [[0.587]
 [0.424]
 [0.598]
 [0.605]
 [0.61 ]
 [0.612]
 [0.606]]
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.483]
 [0.205]
 [0.439]
 [0.43 ]
 [0.381]
 [0.318]] [[2.73 ]
 [0.455]
 [1.131]
 [0.607]
 [0.948]
 [1.54 ]
 [1.83 ]] [[0.236]
 [0.483]
 [0.205]
 [0.439]
 [0.43 ]
 [0.381]
 [0.318]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9978123173277662 -1.0 -0.9978123173277662
probs:  [0.12310912961085012, 0.11870532072226152, 0.13367316343315386, 0.1357326132051028, 0.3564445514541249, 0.13233522157450672]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.451]
 [0.394]
 [0.407]
 [0.389]
 [0.408]
 [0.398]] [[1.073]
 [1.753]
 [0.493]
 [0.789]
 [0.405]
 [0.869]
 [0.787]] [[0.411]
 [0.451]
 [0.394]
 [0.407]
 [0.389]
 [0.408]
 [0.398]]
Printing some Q and Qe and total Qs values:  [[0.92]
 [0.68]
 [0.68]
 [0.68]
 [0.68]
 [0.68]
 [0.68]] [[2.297]
 [2.272]
 [2.272]
 [2.272]
 [2.272]
 [2.272]
 [2.272]] [[2.305]
 [1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]]
933 623
933 627
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.374]] [[1.505]
 [1.524]
 [1.504]
 [1.523]
 [1.523]
 [1.499]
 [1.46 ]] [[2.618]
 [2.65 ]
 [2.616]
 [2.649]
 [2.649]
 [2.607]
 [2.54 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.12400066127164042, 0.12068998273098805, 0.13185185582078285, 0.13791010930702013, 0.35369553504878576, 0.13185185582078285]
line 256 mcts: sample exp_bonus 2.7488022000288583
in main func line 156:  936
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.02 ]
 [0.016]
 [0.02 ]
 [0.021]
 [0.024]
 [0.021]] [[2.908]
 [2.131]
 [2.865]
 [2.917]
 [2.917]
 [2.946]
 [2.95 ]] [[1.301]
 [0.279]
 [1.25 ]
 [1.327]
 [1.33 ]
 [1.375]
 [1.374]]
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.1301482587455735
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.589]] [[1.289]
 [1.289]
 [1.289]
 [1.289]
 [1.289]
 [1.289]
 [2.494]] [[1.829]
 [1.829]
 [1.829]
 [1.829]
 [1.829]
 [1.829]
 [2.407]]
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
probs:  [0.12477345146402326, 0.12145753952555786, 0.1326336948627123, 0.1366131744483908, 0.35252882086188514, 0.1319933188374307]
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
probs:  [0.12497779100446046, 0.12165644864859708, 0.13285090700148644, 0.13683690371897606, 0.35146846738412973, 0.13220948224235018]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.615]
 [0.635]
 [0.635]
 [2.997]
 [2.997]
 [0.636]] [[-0.826]
 [ 1.726]
 [-0.501]
 [-0.166]
 [ 0.   ]
 [ 0.   ]
 [ 0.49 ]] [[0.742]
 [3.27 ]
 [1.073]
 [1.407]
 [4.401]
 [4.401]
 [2.062]]
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
probs:  [0.12456682532388477, 0.12180841254917918, 0.1330168541604581, 0.13632578495940634, 0.3519074948243403, 0.1323746281827312]
siam score:  -0.7199785
maxi score, test score, baseline:  -0.9978338842975206 -1.0 -0.9978338842975206
from probs:  [0.12485400118592888, 0.12208922867408352, 0.13332351222716599, 0.13596335982443472, 0.35109009254134316, 0.1326798055470438]
maxi score, test score, baseline:  -0.9978381443298969 -1.0 -0.9978381443298969
probs:  [0.12444435830972714, 0.12224001351037578, 0.13348817391597684, 0.1354605437517117, 0.3515232383980564, 0.13284367211415216]
maxi score, test score, baseline:  -0.9978381443298969 -1.0 -0.9978381443298969
using explorer policy with actor:  1
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.486]
 [0.486]
 [0.486]
 [0.451]
 [0.486]
 [0.486]] [[3.017]
 [2.539]
 [2.539]
 [2.539]
 [2.252]
 [2.539]
 [2.539]] [[1.764]
 [1.295]
 [1.295]
 [1.295]
 [0.954]
 [1.295]
 [1.295]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9978423868312757 -1.0 -0.9978423868312757
probs:  [0.12472869250841277, 0.12251931075529805, 0.1337931733395704, 0.13510450156647782, 0.35070712298668416, 0.13314719884355683]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.051421113243288
siam score:  -0.72496736
maxi score, test score, baseline:  -0.997846611909651 -1.0 -0.997846611909651
probs:  [0.12545274107217447, 0.12323053348310536, 0.13392011826500674, 0.13522603216041867, 0.34953088940296023, 0.13263968561633455]
using explorer policy with actor:  1
using another actor
from probs:  [0.12545274107217447, 0.12323053348310536, 0.13392011826500674, 0.13522603216041867, 0.34953088940296023, 0.13263968561633455]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.485]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.496]] [[2.307]
 [3.757]
 [2.307]
 [2.307]
 [2.307]
 [2.307]
 [2.535]] [[0.495]
 [0.485]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.496]]
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
first move QE:  1.509187839560965
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.12551684612646258, 0.12384950207264071, 0.13330591842992634, 0.13590525675699117, 0.34811655818405274, 0.13330591842992634]
line 256 mcts: sample exp_bonus -0.5024882160255199
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.684]] [[2.322]
 [2.322]
 [2.322]
 [2.322]
 [2.322]
 [2.322]
 [1.853]] [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.684]]
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.12601374442163124, 0.12324885807626804, 0.13383365224134622, 0.13578118450109244, 0.3479254805514368, 0.13319708020822527]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.656]
 [0.596]
 [0.636]] [[3.845]
 [3.845]
 [3.845]
 [3.845]
 [3.787]
 [6.91 ]
 [3.845]] [[1.441]
 [1.441]
 [1.441]
 [1.441]
 [1.455]
 [2.071]
 [1.441]]
using explorer policy with actor:  0
using explorer policy with actor:  0
using another actor
from probs:  [0.12601374442163124, 0.12324885807626804, 0.13383365224134622, 0.13578118450109244, 0.3479254805514368, 0.13319708020822527]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.662]
 [0.693]
 [0.695]
 [0.678]
 [0.678]
 [0.678]] [[3.208]
 [3.826]
 [3.148]
 [3.5  ]
 [3.208]
 [3.208]
 [3.585]] [[0.678]
 [0.662]
 [0.693]
 [0.695]
 [0.678]
 [0.678]
 [0.678]]
Printing some Q and Qe and total Qs values:  [[0.676]
 [1.218]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[2.15 ]
 [2.278]
 [2.15 ]
 [2.15 ]
 [2.15 ]
 [2.15 ]
 [2.15 ]] [[1.476]
 [2.604]
 [1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.476]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.875772424040095
Printing some Q and Qe and total Qs values:  [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]] [[2.968]
 [2.968]
 [2.968]
 [2.968]
 [2.968]
 [2.968]
 [2.968]] [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]]
maxi score, test score, baseline:  -0.9978633401221996 -1.0 -0.9978633401221996
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.505415382564159
line 256 mcts: sample exp_bonus 1.6123105682287044
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.499]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[2.234]
 [3.102]
 [2.234]
 [2.234]
 [2.234]
 [2.234]
 [2.234]] [[0.505]
 [0.499]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]]
siam score:  -0.72319484
maxi score, test score, baseline:  -0.9978757085020243 -1.0 -0.9978757085020243
probs:  [0.12645616030366325, 0.12368156437946941, 0.1336647221416658, 0.13494859998536235, 0.3475842310481734, 0.1336647221416658]
maxi score, test score, baseline:  -0.997879797979798 -1.0 -0.997879797979798
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.577]] [[2.562]
 [2.507]
 [2.507]
 [2.507]
 [2.507]
 [2.507]
 [1.998]] [[1.779]
 [1.669]
 [1.669]
 [1.669]
 [1.669]
 [1.669]
 [1.06 ]]
maxi score, test score, baseline:  -0.997879797979798 -1.0 -0.997879797979798
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.077]
 [0.157]
 [0.093]
 [0.157]
 [0.137]
 [0.157]] [[3.68 ]
 [4.459]
 [3.68 ]
 [4.451]
 [3.68 ]
 [4.845]
 [3.68 ]] [[-0.419]
 [-0.059]
 [-0.419]
 [-0.034]
 [-0.419]
 [ 0.318]
 [-0.419]]
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.061]
 [-0.058]
 [-0.059]
 [-0.053]
 [-0.054]
 [-0.052]] [[7.647]
 [7.761]
 [7.143]
 [7.878]
 [7.468]
 [7.274]
 [7.469]] [[ 0.137]
 [ 0.173]
 [-0.025]
 [ 0.216]
 [ 0.092]
 [ 0.027]
 [ 0.094]]
maxi score, test score, baseline:  -0.997879797979798 -1.0 -0.997879797979798
probs:  [0.1262727094445404, 0.12351477781458405, 0.13406989090709662, 0.13406989090709662, 0.3486373932489801, 0.13343533767770238]
using explorer policy with actor:  1
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.359]
 [0.35 ]
 [0.346]
 [0.348]
 [0.336]
 [0.343]] [[1.429]
 [2.202]
 [1.217]
 [1.249]
 [1.118]
 [1.238]
 [1.322]] [[0.352]
 [0.359]
 [0.35 ]
 [0.346]
 [0.348]
 [0.336]
 [0.343]]
first move QE:  1.5044444123482372
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.465]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[2.59 ]
 [3.676]
 [2.59 ]
 [2.59 ]
 [2.59 ]
 [2.59 ]
 [2.59 ]] [[0.471]
 [0.465]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]]
from probs:  [0.12655999463682385, 0.12380834365406322, 0.134336399588104, 0.134336399588104, 0.3478817997790163, 0.1330770627538886]
maxi score, test score, baseline:  -0.9978879275653924 -1.0 -0.9978879275653924
probs:  [0.1266401743786109, 0.12388677965441373, 0.13442150729482027, 0.13378841068181507, 0.348101755580736, 0.13316137240960418]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 2.1457767776540138
maxi score, test score, baseline:  -0.9978919678714859 -1.0 -0.9978919678714859
maxi score, test score, baseline:  -0.9978919678714859 -1.0 -0.9978919678714859
maxi score, test score, baseline:  -0.9978919678714859 -1.0 -0.9978919678714859
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.02  0.776 0.02  0.061 0.082]
maxi score, test score, baseline:  -0.9978919678714859 -1.0 -0.9978919678714859
probs:  [0.12683491468125505, 0.12407728545523479, 0.13462821466783398, 0.13399414440265373, 0.3470992989975088, 0.1333661417955135]
maxi score, test score, baseline:  -0.9978919678714859 -1.0 -0.9978919678714859
probs:  [0.12683491468125505, 0.12407728545523479, 0.13462821466783398, 0.13399414440265373, 0.3470992989975088, 0.1333661417955135]
from probs:  [0.12683491468125505, 0.12407728545523479, 0.13462821466783398, 0.13399414440265373, 0.3470992989975088, 0.1333661417955135]
UNIT TEST: sample policy line 217 mcts : [0.02  0.    0.    0.143 0.143 0.204 0.49 ]
UNIT TEST: sample policy line 217 mcts : [0.102 0.184 0.082 0.061 0.286 0.102 0.184]
maxi score, test score, baseline:  -0.997895991983968 -1.0 -0.997895991983968
probs:  [0.12640058990095834, 0.12475200185210644, 0.13472278918602804, 0.13472278918602804, 0.3459358626216984, 0.13346596725318077]
siam score:  -0.719055
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.004]
 [-0.009]
 [-0.009]
 [-0.004]
 [-0.009]
 [-0.01 ]] [[2.857]
 [2.749]
 [2.699]
 [2.678]
 [2.749]
 [2.681]
 [2.92 ]] [[0.535]
 [0.389]
 [0.312]
 [0.284]
 [0.389]
 [0.287]
 [0.605]]
siam score:  -0.71418226
maxi score, test score, baseline:  -0.9979 -1.0 -0.9979
probs:  [0.1264006437681087, 0.12475205472789525, 0.13472284805764773, 0.13472284805764773, 0.3459355800196771, 0.13346602536902347]
Printing some Q and Qe and total Qs values:  [[0.989]
 [0.846]
 [0.879]
 [0.883]
 [0.884]
 [0.884]
 [0.974]] [[1.572]
 [1.65 ]
 [1.509]
 [1.545]
 [1.588]
 [1.724]
 [1.528]] [[0.964]
 [0.926]
 [0.864]
 [0.887]
 [0.912]
 [0.991]
 [0.93 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.63 ]
 [0.347]] [[2.399]
 [2.399]
 [2.399]
 [2.399]
 [2.399]
 [2.773]
 [2.399]] [[1.82]
 [1.82]
 [1.82]
 [1.82]
 [1.82]
 [2.42]
 [1.82]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12665246853783144, 0.12393560040102852, 0.1355867849107794, 0.13432179916400647, 0.3464226659135211, 0.13308068107283302]
siam score:  -0.7210848
line 256 mcts: sample exp_bonus 0.6935852574634491
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.599]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]] [[1.103]
 [2.464]
 [1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.103]] [[0.6  ]
 [0.599]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12667477762723178, 0.12450111293601769, 0.1355670422731076, 0.133687922196998, 0.34649583324837446, 0.13307331171827044]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.694]
 [0.654]
 [0.654]
 [0.657]
 [0.657]
 [0.656]] [[1.918]
 [2.88 ]
 [1.918]
 [2.027]
 [2.034]
 [2.034]
 [2.042]] [[0.654]
 [0.694]
 [0.654]
 [0.654]
 [0.657]
 [0.657]
 [0.656]]
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]] [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]]
rdn beta is 0 so we're just using the maxi policy
using another actor
from probs:  [0.1269667612435266, 0.12372716630543652, 0.13524572608531232, 0.1339960710148541, 0.34729450136777434, 0.13276977398309603]
Printing some Q and Qe and total Qs values:  [[1.391]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]] [[5.149]
 [1.651]
 [1.651]
 [1.651]
 [1.651]
 [1.651]
 [1.651]] [[2.118]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.773]
 [0.928]
 [0.92 ]
 [0.92 ]
 [0.907]
 [0.894]] [[3.966]
 [3.141]
 [2.785]
 [2.731]
 [2.908]
 [2.983]
 [3.431]] [[1.896]
 [1.028]
 [1.098]
 [1.049]
 [1.163]
 [1.185]
 [1.446]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12739380147202053, 0.12414331047460721, 0.13444675363621925, 0.13444675363621925, 0.3469596751502733, 0.1326097056306605]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.434]
 [0.447]
 [0.447]
 [0.438]
 [0.436]
 [0.439]] [[ 0.178]
 [ 0.484]
 [-0.062]
 [-0.015]
 [ 0.015]
 [-0.058]
 [ 0.187]] [[0.442]
 [0.434]
 [0.447]
 [0.447]
 [0.438]
 [0.436]
 [0.439]]
first move QE:  1.4867895175550234
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.488]
 [0.453]
 [0.454]
 [0.478]
 [0.46 ]
 [0.456]] [[0.474]
 [1.253]
 [0.211]
 [0.189]
 [0.474]
 [0.202]
 [0.16 ]] [[0.478]
 [0.488]
 [0.453]
 [0.454]
 [0.478]
 [0.46 ]
 [0.456]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12727459338018232, 0.1235189309901582, 0.13490739511085134, 0.13428717828549905, 0.34814842840817306, 0.1318634738251361]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.252]
 [0.381]
 [0.378]
 [0.1  ]
 [0.515]
 [0.325]] [[ 2.485]
 [ 1.692]
 [-0.486]
 [-0.141]
 [ 1.833]
 [ 0.016]
 [ 1.104]] [[0.199]
 [0.252]
 [0.381]
 [0.378]
 [0.1  ]
 [0.515]
 [0.325]]
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[5.672]
 [5.672]
 [5.672]
 [5.672]
 [5.672]
 [5.672]
 [5.672]] [[1.841]
 [1.841]
 [1.841]
 [1.841]
 [1.841]
 [1.841]
 [1.841]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
siam score:  -0.7099277
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.556]
 [0.556]
 [0.55 ]
 [0.551]
 [0.542]
 [0.553]] [[4.374]
 [3.879]
 [3.879]
 [3.693]
 [3.58 ]
 [3.953]
 [3.594]] [[0.615]
 [0.469]
 [0.469]
 [0.394]
 [0.359]
 [0.464]
 [0.368]]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.468]
 [0.483]
 [0.513]
 [0.437]
 [0.481]
 [0.516]] [[2.929]
 [3.03 ]
 [2.538]
 [2.023]
 [2.691]
 [3.771]
 [2.7  ]] [[0.506]
 [0.468]
 [0.483]
 [0.513]
 [0.437]
 [0.481]
 [0.516]]
first move QE:  1.48532429725033
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.483]
 [0.475]
 [0.476]
 [0.483]
 [0.475]
 [0.475]] [[4.258]
 [3.76 ]
 [3.781]
 [3.694]
 [3.76 ]
 [3.743]
 [3.656]] [[0.412]
 [0.273]
 [0.264]
 [0.237]
 [0.273]
 [0.253]
 [0.223]]
first move QE:  1.4845532208003764
siam score:  -0.7068768
995 726
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.12809707797184658, 0.12436649464401853, 0.13384207850945862, 0.13384207850945862, 0.3489470979601886, 0.13090517240502914]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12847388865551065, 0.12473233143447222, 0.1342357887010155, 0.1342357887010155, 0.34703195912171314, 0.1312902433862731]
using explorer policy with actor:  1
siam score:  -0.70336837
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
first move QE:  1.476922396793647
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.12881479291444073, 0.12506330750639846, 0.13459198210769066, 0.1333945283476352, 0.3464967683077608, 0.1316386208160741]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.082]
 [0.326]
 [0.274]
 [0.062]
 [0.323]
 [0.13 ]] [[ 2.418]
 [ 2.356]
 [ 0.579]
 [-0.181]
 [ 2.599]
 [ 0.918]
 [ 2.881]] [[0.037]
 [0.082]
 [0.326]
 [0.274]
 [0.062]
 [0.323]
 [0.13 ]]
from probs:  [0.12881479291444073, 0.12506330750639846, 0.13459198210769066, 0.1333945283476352, 0.3464967683077608, 0.1316386208160741]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.713]
 [0.654]
 [0.622]
 [0.695]
 [0.615]
 [0.633]] [[4.269]
 [3.66 ]
 [3.69 ]
 [3.589]
 [3.487]
 [3.647]
 [3.625]] [[0.678]
 [0.549]
 [0.442]
 [0.344]
 [0.456]
 [0.349]
 [0.378]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.573]] [[2.974]
 [2.974]
 [2.974]
 [2.974]
 [2.974]
 [2.974]
 [2.445]] [[0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.096]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 3.67210318729422
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12793669809967376, 0.12527585238642885, 0.13421823797979981, 0.1336212321234242, 0.3470856389806968, 0.13186234042997674]
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.125]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.139]] [[-2.509]
 [-0.221]
 [-2.689]
 [-2.758]
 [-2.723]
 [-2.773]
 [-3.719]] [[0.136]
 [0.125]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.139]]
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.736]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[0.769]
 [1.495]
 [0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]] [[0.567]
 [0.953]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]]
1007 744
siam score:  -0.71213067
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1288013025409308, 0.1261224746610727, 0.13275347461480216, 0.1321737554968236, 0.3479752371895473, 0.1321737554968236]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1283262029250084, 0.1261912543085233, 0.13282587041266758, 0.13224583515058652, 0.34816500205262785, 0.13224583515058652]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.3144133059929417
Starting evaluation
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.12818135408652298, 0.1260580590611775, 0.1326554400327866, 0.1326554400327866, 0.3477942667539397, 0.1326554400327866]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.299]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[0.764]
 [2.406]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]] [[0.324]
 [0.299]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.353]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]] [[-1.022]
 [ 0.275]
 [-1.022]
 [-1.022]
 [-1.022]
 [-1.022]
 [-1.022]] [[0.321]
 [0.353]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.569]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.628]
 [0.633]] [[ 0.371]
 [ 0.822]
 [ 0.554]
 [ 0.554]
 [ 0.554]
 [-0.179]
 [ 0.372]] [[0.853]
 [1.231]
 [1.244]
 [1.244]
 [1.244]
 [1.016]
 [1.209]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]] [[3.16]
 [3.16]
 [3.16]
 [3.16]
 [3.16]
 [3.16]
 [3.16]] [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]]
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.511]] [[1.927]
 [1.927]
 [1.927]
 [1.927]
 [1.927]
 [1.927]
 [3.957]] [[0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.511]]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.914]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]] [[2.678]
 [1.444]
 [2.678]
 [2.678]
 [2.678]
 [2.678]
 [2.678]] [[0.396]
 [0.914]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]]
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.434]
 [0.662]
 [0.635]
 [0.639]
 [0.661]
 [0.782]] [[-0.344]
 [ 1.303]
 [-0.981]
 [-1.142]
 [-1.16 ]
 [-1.124]
 [-0.638]] [[1.565]
 [2.114]
 [1.246]
 [1.143]
 [1.138]
 [1.176]
 [1.523]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1292490799787449, 0.12710809831643463, 0.13203141297602036, 0.131465183804645, 0.34924232505794783, 0.13090389986620743]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.561]
 [0.474]
 [0.463]
 [0.46 ]
 [0.454]
 [0.471]] [[1.408]
 [2.462]
 [1.504]
 [1.333]
 [1.336]
 [1.396]
 [1.253]] [[0.102]
 [0.648]
 [0.153]
 [0.075]
 [0.069]
 [0.078]
 [0.064]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.684]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.612]] [[2.287]
 [3.413]
 [2.287]
 [2.287]
 [2.287]
 [2.287]
 [2.338]] [[0.363]
 [0.787]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.286]]
line 256 mcts: sample exp_bonus -3.5538827401370874
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]] [[0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.611]] [[2.864]
 [2.864]
 [2.864]
 [2.864]
 [2.864]
 [2.864]
 [3.851]] [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.611]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.276]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.465]] [[ 2.126]
 [ 1.856]
 [-0.237]
 [-0.237]
 [-0.237]
 [-0.237]
 [ 0.91 ]] [[1.778]
 [1.737]
 [1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.508]]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.716]] [[2.029]
 [2.029]
 [2.029]
 [2.029]
 [2.029]
 [2.029]
 [1.931]] [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.716]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1293603158695376, 0.12722674162305625, 0.13213255872269597, 0.13213255872269597, 0.34813862735633727, 0.13100919770567704]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
from probs:  [0.12943207119212088, 0.1272973134659357, 0.13220585179028915, 0.13220585179028915, 0.34833173757983765, 0.13052717418152757]
line 256 mcts: sample exp_bonus 1.3426617469574809
using explorer policy with actor:  1
using explorer policy with actor:  0
UNIT TEST: sample policy line 217 mcts : [0.122 0.286 0.102 0.102 0.122 0.082 0.184]
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.688]] [[1.862]
 [1.862]
 [1.862]
 [1.862]
 [1.862]
 [1.862]
 [2.31 ]] [[0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.688]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.474]
 [1.474]
 [1.474]
 [1.474]
 [1.474]
 [1.474]
 [1.474]] [[1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]] [[1.897]
 [1.897]
 [1.897]
 [1.897]
 [1.897]
 [1.897]
 [1.897]]
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.343]
 [0.367]
 [0.368]
 [0.359]
 [0.363]
 [0.453]] [[0.655]
 [1.974]
 [0.236]
 [0.027]
 [0.055]
 [0.161]
 [0.375]] [[0.355]
 [0.343]
 [0.367]
 [0.368]
 [0.359]
 [0.363]
 [0.453]]
Printing some Q and Qe and total Qs values:  [[0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]] [[-0.546]
 [-0.546]
 [-0.546]
 [-0.546]
 [-0.546]
 [-0.546]
 [-0.546]] [[0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1297903990463446, 0.12712580739508342, 0.13200584992145434, 0.13144476290634505, 0.34929608098356585, 0.1303370997472068]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.547]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[2.442]
 [2.456]
 [2.442]
 [2.442]
 [2.442]
 [2.442]
 [2.442]] [[0.516]
 [0.547]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]]
from probs:  [0.1298578653533253, 0.12667207893237326, 0.13207446784126572, 0.13151308916768953, 0.34947764847083723, 0.1304048502345089]
line 256 mcts: sample exp_bonus 5.591931450567988
Printing some Q and Qe and total Qs values:  [[1.   ]
 [1.343]
 [1.021]
 [1.021]
 [1.021]
 [1.021]
 [1.   ]] [[1.546]
 [1.395]
 [1.631]
 [1.631]
 [1.631]
 [1.631]
 [1.64 ]] [[1.485]
 [2.122]
 [1.556]
 [1.556]
 [1.556]
 [1.556]
 [1.517]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]] [[1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.279]] [[0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.668]] [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.344]] [[0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.668]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.611]] [[-0.58 ]
 [-0.58 ]
 [-0.58 ]
 [-0.58 ]
 [-0.58 ]
 [-0.58 ]
 [ 1.326]] [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.611]]
siam score:  -0.7139691
1018 778
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.7617960096161513
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.130115289638442, 0.12745546480041084, 0.13176645362880415, 0.13176645362880415, 0.34878104866509674, 0.130115289638442]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1301875610134144, 0.1275262587994441, 0.1318396421280609, 0.1312842010636539, 0.3489747759820123, 0.1301875610134144]
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.645]] [[-0.243]
 [-0.243]
 [-0.243]
 [-0.243]
 [-0.243]
 [-0.243]
 [ 1.722]] [[0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.645]]
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.56 ]
 [0.53 ]
 [0.517]
 [0.522]
 [0.608]
 [0.523]] [[3.201]
 [3.125]
 [2.704]
 [2.442]
 [2.758]
 [2.745]
 [2.463]] [[0.543]
 [0.56 ]
 [0.53 ]
 [0.517]
 [0.522]
 [0.608]
 [0.523]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.371]] [[0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.625]] [[0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.371]]
rdn probs:  [0.13069978679361888, 0.12802801362841254, 0.1318007415979022, 0.13124790163180283, 0.3475237695546447, 0.13069978679361888]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.658]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[2.733]
 [3.042]
 [2.733]
 [2.733]
 [2.733]
 [2.733]
 [2.733]] [[0.841]
 [1.076]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.57 ]
 [0.57 ]
 [0.529]
 [0.533]
 [0.57 ]
 [0.57 ]] [[4.822]
 [4.353]
 [4.353]
 [4.208]
 [4.153]
 [4.353]
 [4.353]] [[1.734]
 [1.35 ]
 [1.35 ]
 [1.152]
 [1.112]
 [1.35 ]
 [1.35 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13037010985017058, 0.12823829108745663, 0.1314634675417771, 0.1314634675417771, 0.3480945541286479, 0.13037010985017058]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.739]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]] [[1.47 ]
 [1.813]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]] [[0.775]
 [0.999]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.504]
 [0.508]
 [0.515]
 [0.515]
 [0.516]
 [0.512]] [[4.556]
 [3.569]
 [3.317]
 [3.252]
 [3.132]
 [3.267]
 [3.3  ]] [[1.016]
 [0.399]
 [0.283]
 [0.259]
 [0.202]
 [0.268]
 [0.28 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.7014935
from probs:  [0.13058457840616097, 0.12844925264255397, 0.13112981650178285, 0.13058457840616097, 0.34866719563718, 0.13058457840616097]
siam score:  -0.7020997
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.38 ]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[0.606]
 [0.45 ]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[0.366]
 [0.38 ]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.434]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[1.144]
 [1.107]
 [1.144]
 [1.144]
 [1.144]
 [1.144]
 [1.144]] [[0.439]
 [0.434]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.602]
 [0.559]
 [0.559]
 [0.559]
 [0.579]
 [0.614]] [[2.819]
 [3.063]
 [2.819]
 [2.819]
 [2.819]
 [2.97 ]
 [3.381]] [[0.606]
 [0.772]
 [0.606]
 [0.606]
 [0.606]
 [0.695]
 [0.903]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.451636038458212
siam score:  -0.69615126
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1306295542162366, 0.12904798660414857, 0.13009797176884036, 0.13009797176884036, 0.34896091189891126, 0.13116560374302277]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.499]] [[0.862]
 [0.212]
 [0.422]
 [0.212]
 [0.212]
 [0.212]
 [0.753]] [[0.503]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.499]]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.53 ]
 [0.317]] [[2.382]
 [2.382]
 [2.382]
 [2.382]
 [2.382]
 [1.829]
 [2.382]] [[1.496]
 [1.496]
 [1.496]
 [1.496]
 [1.496]
 [1.554]
 [1.496]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13101674645994085, 0.1294304910058406, 0.12995485484198163, 0.1294304910058406, 0.348613031823245, 0.13155438486315132]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6907956
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.542]] [[5.399]
 [5.399]
 [5.399]
 [5.399]
 [5.399]
 [5.399]
 [4.931]] [[1.845]
 [1.845]
 [1.845]
 [1.845]
 [1.845]
 [1.845]
 [1.645]]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.688]] [[1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.264]] [[0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.363]]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.585]] [[1.358]
 [1.654]
 [1.654]
 [1.654]
 [1.654]
 [1.654]
 [1.414]] [[0.326]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.257]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13139151109461392, 0.12928804517652678, 0.12877291066597482, 0.13033108612764435, 0.3482880679703196, 0.13192837896492068]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.467]
 [0.583]
 [0.626]
 [0.614]
 [0.578]
 [0.618]] [[1.867]
 [2.046]
 [1.812]
 [1.741]
 [1.821]
 [1.958]
 [2.158]] [[1.168]
 [1.159]
 [1.157]
 [1.172]
 [1.228]
 [1.293]
 [1.573]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.131508321182181, 0.12738260391172723, 0.12889830187236653, 0.13097768915721048, 0.3486501773383265, 0.13258290653818833]
using explorer policy with actor:  1
from probs:  [0.13171301257661688, 0.1275808736682504, 0.12808281191266388, 0.1311815546281751, 0.34919284787299804, 0.13224889934129566]
Printing some Q and Qe and total Qs values:  [[1.196]
 [1.196]
 [1.196]
 [1.196]
 [1.196]
 [1.196]
 [1.196]] [[1.444]
 [1.444]
 [1.444]
 [1.444]
 [1.444]
 [1.444]
 [1.444]] [[2.129]
 [2.129]
 [2.129]
 [2.129]
 [2.129]
 [2.129]
 [2.129]]
Printing some Q and Qe and total Qs values:  [[1.383]
 [1.383]
 [1.383]
 [1.383]
 [1.383]
 [1.383]
 [1.383]] [[1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]] [[1.589]
 [1.589]
 [1.589]
 [1.589]
 [1.589]
 [1.589]
 [1.589]]
line 256 mcts: sample exp_bonus 0.06447109625738885
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.443]
 [0.421]
 [0.412]
 [0.422]
 [0.427]
 [0.443]] [[2.614]
 [2.671]
 [2.832]
 [2.635]
 [2.506]
 [2.573]
 [2.601]] [[0.113]
 [0.106]
 [0.118]
 [0.033]
 [0.01 ]
 [0.042]
 [0.083]]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.694]
 [0.653]
 [0.652]
 [0.653]
 [0.655]
 [0.653]] [[0.396]
 [1.116]
 [0.326]
 [0.311]
 [0.342]
 [0.384]
 [0.506]] [[0.761]
 [1.078]
 [0.733]
 [0.725]
 [0.739]
 [0.755]
 [0.794]]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]] [[4.453]
 [4.453]
 [4.453]
 [4.453]
 [4.453]
 [4.453]
 [4.453]] [[1.454]
 [1.454]
 [1.454]
 [1.454]
 [1.454]
 [1.454]
 [1.454]]
siam score:  -0.68122023
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.374]
 [0.331]
 [0.331]
 [0.331]
 [0.378]
 [0.414]] [[2.669]
 [2.33 ]
 [3.722]
 [3.722]
 [3.722]
 [1.89 ]
 [1.882]] [[-0.095]
 [-0.194]
 [ 0.184]
 [ 0.184]
 [ 0.184]
 [-0.333]
 [-0.264]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1309687383209169, 0.12741773480077365, 0.12692648925450875, 0.13255348369354283, 0.35011266641818756, 0.13202088751207047]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.441]] [[-1.309]
 [-1.309]
 [-1.309]
 [-1.309]
 [-1.309]
 [-1.309]
 [ 3.224]] [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.441]]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.577]] [[2.575]
 [2.575]
 [2.575]
 [2.575]
 [2.575]
 [2.575]
 [2.815]] [[1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.737]]
1052 835
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.022]] [[2.896]
 [2.896]
 [2.896]
 [2.896]
 [2.896]
 [2.896]
 [6.586]] [[0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.953]]
1052 836
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.581]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.568]] [[4.022]
 [3.754]
 [4.022]
 [4.022]
 [4.022]
 [4.022]
 [4.084]] [[0.578]
 [0.581]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.568]]
using another actor
UNIT TEST: sample policy line 217 mcts : [0.184 0.122 0.061 0.286 0.061 0.184 0.102]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.847]
 [0.837]
 [0.847]
 [0.838]
 [0.847]
 [0.839]] [[9.06 ]
 [9.06 ]
 [9.138]
 [9.06 ]
 [9.155]
 [9.06 ]
 [9.115]] [[1.874]
 [1.874]
 [1.884]
 [1.874]
 [1.889]
 [1.874]
 [1.88 ]]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13099000559976298, 0.12746707852025208, 0.12697959225210684, 0.13309423482538502, 0.3489073792012154, 0.1325617096012778]
siam score:  -0.68719804
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]] [[1.422]
 [1.422]
 [1.422]
 [1.422]
 [1.422]
 [1.422]
 [1.422]] [[0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]]
actor:  1 policy actor:  1  step number:  77 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.738]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]] [[2.393]
 [2.99 ]
 [2.393]
 [2.393]
 [2.393]
 [2.393]
 [2.393]] [[1.066]
 [1.534]
 [1.066]
 [1.066]
 [1.066]
 [1.066]
 [1.066]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27262330788338407, 0.10612745231206124, 0.10653324795316196, 0.11077339832547978, 0.2927260647311951, 0.11121652879471791]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.206]
 [0.429]
 [0.514]
 [0.425]
 [0.454]
 [0.453]] [[3.18 ]
 [2.977]
 [2.384]
 [1.905]
 [1.878]
 [1.748]
 [2.6  ]] [[1.445]
 [1.116]
 [0.881]
 [0.609]
 [0.513]
 [0.443]
 [1.058]]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.508]
 [0.466]
 [0.459]
 [0.456]
 [0.463]
 [0.498]] [[1.268]
 [2.294]
 [1.161]
 [1.095]
 [1.023]
 [1.29 ]
 [1.201]] [[0.475]
 [0.508]
 [0.466]
 [0.459]
 [0.456]
 [0.463]
 [0.498]]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.375]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.37 ]] [[-1.919]
 [-1.276]
 [-2.018]
 [-2.018]
 [-2.018]
 [-2.018]
 [-2.069]] [[0.371]
 [0.375]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.37 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27439129473884155, 0.10521340688839298, 0.10722412571967689, 0.11104939569138776, 0.29018399889983393, 0.11193777806186705]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.259]
 [0.523]
 [0.524]
 [0.533]
 [0.534]
 [0.545]] [[1.097]
 [1.83 ]
 [0.961]
 [0.977]
 [1.013]
 [1.108]
 [1.148]] [[0.169]
 [0.726]
 [0.019]
 [0.039]
 [0.092]
 [0.204]
 [0.263]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]] [[1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.957]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.06 ]
 [0.062]] [[2.79 ]
 [1.811]
 [1.811]
 [1.811]
 [1.811]
 [2.776]
 [2.263]] [[1.796]
 [1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.618]
 [1.252]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]] [[1.826]
 [1.826]
 [1.826]
 [1.826]
 [1.826]
 [1.826]
 [1.826]] [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.22004220996207635
siam score:  -0.7025515
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2745126750683369, 0.104917569816974, 0.10812473607428262, 0.11066535271787349, 0.2893514367499229, 0.11242822957261003]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.686]
 [0.695]] [[2.344]
 [2.393]
 [2.393]
 [2.393]
 [2.393]
 [2.292]
 [2.295]] [[0.675]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.686]
 [0.695]]
Printing some Q and Qe and total Qs values:  [[0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]] [[-0.206]
 [-0.206]
 [-0.206]
 [-0.206]
 [-0.206]
 [-0.206]
 [-0.206]] [[0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.273456050300804, 0.1045636596434158, 0.10855831575392456, 0.11110912025822536, 0.28943378780282897, 0.11287906624080142]
siam score:  -0.69253105
line 256 mcts: sample exp_bonus -0.13995888581572352
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.655]
 [0.682]
 [0.598]
 [0.602]
 [0.602]
 [0.653]] [[3.124]
 [3.329]
 [2.957]
 [2.976]
 [3.275]
 [3.275]
 [2.943]] [[0.616]
 [0.655]
 [0.682]
 [0.598]
 [0.602]
 [0.602]
 [0.653]]
Printing some Q and Qe and total Qs values:  [[0.933]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]] [[5.861]
 [5.693]
 [5.693]
 [5.693]
 [5.693]
 [5.693]
 [5.693]] [[1.897]
 [1.561]
 [1.561]
 [1.561]
 [1.561]
 [1.561]
 [1.561]]
Printing some Q and Qe and total Qs values:  [[0.851]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[6.419]
 [5.205]
 [5.205]
 [5.205]
 [5.205]
 [5.205]
 [5.205]] [[2.057]
 [1.393]
 [1.393]
 [1.393]
 [1.393]
 [1.393]
 [1.393]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2746534692001154, 0.10387626796805212, 0.10820626673303335, 0.11116011316325325, 0.28962658319178375, 0.11247729974376197]
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.156]
 [0.394]
 [0.421]
 [0.376]
 [0.43 ]
 [0.098]] [[2.102]
 [2.469]
 [1.141]
 [0.673]
 [1.702]
 [1.771]
 [3.129]] [[1.076]
 [1.362]
 [0.499]
 [0.138]
 [0.945]
 [1.056]
 [1.851]]
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.351]
 [0.462]
 [0.38 ]
 [0.38 ]
 [0.458]
 [0.45 ]] [[1.902]
 [2.346]
 [1.662]
 [1.892]
 [1.892]
 [1.538]
 [1.49 ]] [[0.876]
 [1.207]
 [0.619]
 [0.759]
 [0.759]
 [0.48 ]
 [0.419]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2746534692001154, 0.10387626796805212, 0.10820626673303335, 0.11116011316325325, 0.28962658319178375, 0.11247729974376197]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27494672679818155, 0.1039871804751073, 0.10832180253882619, 0.11127880289938495, 0.28886809140010933, 0.11259739588839063]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2756777832220277, 0.10351151066914195, 0.10819940117539878, 0.11114100577593841, 0.2885735178189422, 0.11289678133855087]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27464462660557504, 0.10393067531974301, 0.10822865089228344, 0.11115908878265193, 0.2886830072295716, 0.11335395117017487]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.444]
 [0.619]
 [0.347]
 [0.347]
 [0.347]
 [0.427]] [[ 2.047]
 [ 1.835]
 [-0.564]
 [ 2.047]
 [ 2.047]
 [ 2.047]
 [ 2.776]] [[1.314]
 [1.305]
 [0.248]
 [1.314]
 [1.314]
 [1.314]
 [1.773]]
1083 880
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27512676699160965, 0.10414180312255636, 0.10762478560040876, 0.11135556920316766, 0.28820584345990796, 0.11354523162234963]
from probs:  [0.27512676699160965, 0.10414180312255636, 0.10762478560040876, 0.11135556920316766, 0.28820584345990796, 0.11354523162234963]
Printing some Q and Qe and total Qs values:  [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]] [[1.374]
 [1.352]
 [1.4  ]
 [1.399]
 [1.4  ]
 [1.4  ]
 [1.36 ]] [[2.882]
 [2.854]
 [2.918]
 [2.916]
 [2.918]
 [2.918]
 [2.863]]
siam score:  -0.69268346
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.858]
 [0.876]
 [0.798]
 [0.798]
 [0.798]
 [0.858]
 [0.868]] [[1.483]
 [1.461]
 [2.307]
 [2.307]
 [2.307]
 [1.474]
 [1.503]] [[0.858]
 [0.876]
 [0.798]
 [0.798]
 [0.798]
 [0.858]
 [0.868]]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.428]
 [0.413]
 [0.413]
 [0.407]
 [0.413]
 [0.421]] [[-0.262]
 [ 1.488]
 [ 0.458]
 [ 0.458]
 [-0.367]
 [ 0.675]
 [ 0.294]] [[0.404]
 [0.428]
 [0.413]
 [0.413]
 [0.407]
 [0.413]
 [0.421]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2752066288599355, 0.10382897570431492, 0.10807234047812217, 0.11096327455396733, 0.28835819181226946, 0.1135705885913905]
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]] [[3.631]
 [3.631]
 [3.631]
 [3.631]
 [3.631]
 [3.631]
 [3.631]] [[5.126]
 [5.126]
 [5.126]
 [5.126]
 [5.126]
 [5.126]
 [5.126]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.571]
 [0.626]
 [0.653]
 [0.649]
 [0.697]
 [0.601]] [[2.596]
 [3.906]
 [2.389]
 [2.389]
 [2.614]
 [2.91 ]
 [2.763]] [[0.612]
 [0.571]
 [0.626]
 [0.653]
 [0.649]
 [0.697]
 [0.601]]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.261]
 [0.261]
 [0.261]
 [0.028]
 [0.025]
 [0.03 ]] [[1.926]
 [2.894]
 [2.894]
 [2.894]
 [1.86 ]
 [2.08 ]
 [1.654]] [[1.1  ]
 [1.608]
 [1.608]
 [1.608]
 [1.073]
 [1.149]
 [1.   ]]
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.778]
 [0.811]
 [0.803]
 [0.555]
 [0.835]
 [0.812]] [[2.514]
 [3.294]
 [2.223]
 [2.065]
 [3.738]
 [1.87 ]
 [1.734]] [[0.634]
 [1.066]
 [0.419]
 [0.298]
 [0.916]
 [0.231]
 [0.096]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2757379629353949, 0.10375227100842645, 0.10832382955219484, 0.1107684660707737, 0.2880884167046365, 0.11332905372857371]
using another actor
from probs:  [0.2757379629353949, 0.10375227100842645, 0.10832382955219484, 0.1107684660707737, 0.2880884167046365, 0.11332905372857371]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2759476460908744, 0.10346774421437221, 0.10800918469881457, 0.11085269923637081, 0.2883074916830278, 0.11341523407654011]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2753972035046614, 0.10329299795015949, 0.10780934849527819, 0.11105275081725532, 0.2888277890676594, 0.11361991016498627]
using explorer policy with actor:  0
siam score:  -0.6891249
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27525205333846536, 0.10327000401542084, 0.10776806319599569, 0.1114151382072726, 0.2887417725390239, 0.11355296870382163]
1103 918
using another actor
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.607]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.517]] [[-1.362]
 [-0.003]
 [-0.972]
 [-0.972]
 [-0.972]
 [-0.972]
 [-1.268]] [[0.016]
 [0.529]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.054]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.425]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.406]] [[1.636]
 [2.863]
 [1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.708]] [[0.4  ]
 [0.425]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.406]]
from probs:  [0.2747384540383355, 0.10346669854344677, 0.10797332500971113, 0.11162734646884438, 0.28929172823247434, 0.11290244770718776]
UNIT TEST: sample policy line 217 mcts : [0.02  0.122 0.02  0.633 0.02  0.102 0.082]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.345]
 [0.316]
 [0.322]
 [0.306]
 [0.319]
 [0.328]] [[-1.212]
 [-0.704]
 [-1.203]
 [-1.488]
 [-1.459]
 [-1.331]
 [-1.256]] [[0.326]
 [0.345]
 [0.316]
 [0.322]
 [0.306]
 [0.319]
 [0.328]]
using another actor
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27501983175967687, 0.1035726655948112, 0.10808390759365707, 0.11174167137650509, 0.288563845145455, 0.11301807852989476]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27437904460777834, 0.10372019342716464, 0.10784599409999919, 0.11190083505610196, 0.2889748724983323, 0.11317906031062341]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2745918692768994, 0.10380064496412097, 0.10715398651107381, 0.11198763198415901, 0.28919901855770674, 0.11326684870603995]
1108 925
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.307]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[-1.101]
 [ 1.307]
 [-1.101]
 [-1.101]
 [-1.101]
 [-1.101]
 [-1.101]] [[0.294]
 [0.307]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2748941625721665, 0.10319800944915279, 0.10688797604175021, 0.11211091717243057, 0.28951739260329673, 0.11339154216120317]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2748941625721665, 0.10319800944915279, 0.10688797604175021, 0.11211091717243057, 0.28951739260329673, 0.11339154216120317]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [1.071]] [[3.702]
 [3.702]
 [3.702]
 [3.702]
 [3.702]
 [3.702]
 [3.512]] [[1.67 ]
 [1.67 ]
 [1.67 ]
 [1.67 ]
 [1.67 ]
 [1.67 ]
 [1.767]]
1110 926
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.438]
 [0.429]
 [0.429]
 [0.428]
 [0.429]
 [0.416]] [[2.899]
 [3.663]
 [2.698]
 [2.686]
 [2.712]
 [2.727]
 [2.618]] [[0.424]
 [0.438]
 [0.429]
 [0.429]
 [0.428]
 [0.429]
 [0.416]]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.473]
 [0.462]
 [0.479]
 [0.434]
 [0.424]
 [0.494]] [[4.976]
 [5.152]
 [4.154]
 [4.785]
 [4.088]
 [3.885]
 [4.32 ]] [[0.438]
 [0.473]
 [0.462]
 [0.479]
 [0.434]
 [0.424]
 [0.494]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.315]
 [0.315]
 [0.315]
 [0.11 ]
 [0.56 ]
 [0.315]] [[ 2.586]
 [ 2.341]
 [ 2.341]
 [ 2.341]
 [ 1.487]
 [-0.519]
 [ 2.341]] [[1.577]
 [1.45 ]
 [1.45 ]
 [1.45 ]
 [0.913]
 [0.442]
 [1.45 ]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.539]
 [0.537]
 [0.538]
 [0.548]
 [0.591]
 [0.707]] [[2.876]
 [2.783]
 [2.999]
 [2.918]
 [2.919]
 [2.936]
 [2.469]] [[0.063]
 [0.083]
 [0.151]
 [0.126]
 [0.147]
 [0.239]
 [0.315]]
using another actor
from probs:  [0.2754505860485645, 0.10379454030285679, 0.1075058365599833, 0.11150043787490575, 0.28813428671298846, 0.1136143125007013]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.351]
 [0.492]
 [0.496]
 [0.5  ]
 [0.508]
 [0.524]] [[2.198]
 [2.925]
 [1.793]
 [1.9  ]
 [2.045]
 [1.855]
 [2.025]] [[-0.148]
 [-0.215]
 [-0.31 ]
 [-0.267]
 [-0.211]
 [-0.257]
 [-0.169]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.715]
 [0.715]
 [0.715]
 [0.713]
 [0.715]
 [0.716]] [[2.764]
 [2.639]
 [2.639]
 [2.639]
 [2.525]
 [2.585]
 [2.792]] [[0.711]
 [0.715]
 [0.715]
 [0.715]
 [0.713]
 [0.715]
 [0.716]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27567462522956204, 0.10387896213740452, 0.10720960051685056, 0.1115911273417198, 0.28836864224854397, 0.11327704252591897]
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]] [[2.819]
 [2.819]
 [2.819]
 [2.819]
 [2.819]
 [2.819]
 [2.819]] [[0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27493095236229353, 0.10398561581586513, 0.1073196737985444, 0.11170569918542785, 0.2886647135204904, 0.11339334531737877]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.582538724926001
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.877]] [[1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.382]] [[0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.877]]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[3.754]
 [3.754]
 [3.754]
 [3.754]
 [3.754]
 [3.754]
 [3.754]] [[2.084]
 [2.084]
 [2.084]
 [2.084]
 [2.084]
 [2.084]
 [2.084]]
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.289]
 [0.388]
 [0.34 ]
 [0.16 ]
 [0.336]
 [0.262]] [[2.605]
 [2.639]
 [1.907]
 [2.4  ]
 [2.019]
 [1.55 ]
 [2.723]] [[0.676]
 [0.943]
 [0.522]
 [0.835]
 [0.252]
 [0.159]
 [0.966]]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.582]
 [0.596]
 [0.548]
 [0.591]
 [0.512]
 [0.484]] [[3.933]
 [3.568]
 [2.528]
 [2.753]
 [3.853]
 [2.429]
 [3.683]] [[1.486]
 [1.214]
 [0.447]
 [0.563]
 [1.438]
 [0.277]
 [1.19 ]]
siam score:  -0.70967925
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.679]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.693]] [[2.299]
 [3.679]
 [3.479]
 [3.479]
 [3.479]
 [3.479]
 [3.619]] [[0.62 ]
 [1.61 ]
 [1.381]
 [1.381]
 [1.381]
 [1.381]
 [1.598]]
UNIT TEST: sample policy line 217 mcts : [0.184 0.184 0.02  0.02  0.    0.    0.592]
siam score:  -0.7124419
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.554]
 [0.523]
 [0.522]
 [0.511]
 [0.518]
 [0.545]] [[-0.336]
 [-0.111]
 [ 0.   ]
 [-0.413]
 [-0.468]
 [-0.48 ]
 [-0.092]] [[0.339]
 [0.513]
 [0.488]
 [0.349]
 [0.309]
 [0.319]
 [0.501]]
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.222]
 [0.238]
 [0.239]
 [0.23 ]
 [0.228]
 [0.235]] [[2.395]
 [2.63 ]
 [2.29 ]
 [2.419]
 [2.533]
 [2.502]
 [2.348]] [[-0.457]
 [-0.396]
 [-0.478]
 [-0.432]
 [-0.413]
 [-0.426]
 [-0.465]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2737241873257091, 0.10402022735669189, 0.1080653235581757, 0.1116237916451953, 0.28970240125239094, 0.11286406886183711]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2739250894876845, 0.103743954960494, 0.10776329953921414, 0.11170571886283913, 0.2899150307584266, 0.1129469063913416]
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]] [[-2.136]
 [-2.136]
 [-2.136]
 [-2.136]
 [-2.136]
 [-2.136]
 [-2.136]] [[0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.07 ]] [[-2.875]
 [-2.875]
 [-2.875]
 [-2.875]
 [-2.875]
 [-2.875]
 [-3.539]] [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.07 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.927]] [[3.117]
 [3.117]
 [3.117]
 [3.117]
 [3.117]
 [3.117]
 [2.965]] [[2.272]
 [2.272]
 [2.272]
 [2.272]
 [2.272]
 [2.272]
 [2.131]]
Printing some Q and Qe and total Qs values:  [[1.094]
 [0.874]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]] [[1.968]
 [1.87 ]
 [2.291]
 [2.291]
 [2.291]
 [2.291]
 [2.291]] [[2.299]
 [1.858]
 [1.969]
 [1.969]
 [1.969]
 [1.969]
 [1.969]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.479]
 [0.675]
 [0.557]
 [0.642]
 [0.735]
 [0.482]] [[3.99 ]
 [3.894]
 [1.741]
 [2.541]
 [2.17 ]
 [1.654]
 [2.858]] [[0.497]
 [0.479]
 [0.675]
 [0.557]
 [0.642]
 [0.735]
 [0.482]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[ 0.004]
 [-0.   ]
 [ 0.003]
 [ 0.005]
 [ 0.007]
 [ 0.009]
 [ 0.009]] [[1.782]
 [1.476]
 [1.462]
 [1.5  ]
 [1.427]
 [1.468]
 [1.413]] [[0.775]
 [0.264]
 [0.249]
 [0.315]
 [0.2  ]
 [0.271]
 [0.181]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27247141245285517, 0.10397143294683868, 0.1083653596301888, 0.1119199969470563, 0.29052909256493104, 0.11274270545813005]
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.669]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.641]] [[2.83 ]
 [2.634]
 [3.015]
 [3.015]
 [3.015]
 [3.015]
 [2.437]] [[1.547]
 [1.401]
 [1.501]
 [1.501]
 [1.501]
 [1.501]
 [1.223]]
line 256 mcts: sample exp_bonus 2.198605073632238
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2725667262205392, 0.1036579914084787, 0.10840326713255687, 0.11195914790418672, 0.2906307231256746, 0.11278214420856396]
in main func line 156:  1134
deleting a thread, now have 4 threads
Frames:  77863 train batches done:  9110 episodes:  2089
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27217075927385337, 0.10353297023471546, 0.10863660755023831, 0.1117923747602631, 0.2912563121519009, 0.11261097602902893]
siam score:  -0.69658124
1136 962
siam score:  -0.7044568
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0044619520758142
using explorer policy with actor:  1
siam score:  -0.70441407
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2729694930339016, 0.10349003705396648, 0.10819262649252218, 0.112120449473635, 0.29110694447233965, 0.112120449473635]
1137 965
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [1.268]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]] [[1.38 ]
 [1.596]
 [1.38 ]
 [1.38 ]
 [1.38 ]
 [1.38 ]
 [1.38 ]] [[2.375]
 [3.036]
 [2.375]
 [2.375]
 [2.375]
 [2.375]
 [2.375]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.27231008613888835, 0.1043540864106808, 0.10871549192561794, 0.11224096138352546, 0.2905416952418768, 0.11183767889941058]
from probs:  [0.2720284398776806, 0.104268917412628, 0.10898945838243737, 0.11211951336554116, 0.2912738694462504, 0.11131980151546245]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27213606368767235, 0.10431016978374298, 0.10903257836197086, 0.11216387170251595, 0.29138910741032115, 0.11096820905377679]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.206]
 [0.578]
 [0.588]
 [0.548]
 [0.56 ]
 [0.759]] [[2.347]
 [1.844]
 [1.995]
 [2.303]
 [2.519]
 [2.783]
 [2.876]] [[1.043]
 [0.153]
 [0.703]
 [0.953]
 [1.074]
 [1.293]
 [1.596]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.19 ]
 [0.532]
 [0.549]
 [0.395]
 [0.55 ]
 [0.43 ]] [[ 1.932]
 [ 1.749]
 [-0.997]
 [-0.658]
 [ 0.369]
 [-1.045]
 [ 0.315]] [[0.243]
 [0.19 ]
 [0.532]
 [0.549]
 [0.395]
 [0.55 ]
 [0.43 ]]
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.762]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.759]] [[2.194]
 [2.206]
 [2.   ]
 [2.   ]
 [2.   ]
 [2.   ]
 [2.323]] [[2.207]
 [2.221]
 [2.108]
 [2.108]
 [2.108]
 [2.108]
 [2.253]]
siam score:  -0.69219375
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27154241627210074, 0.10445160771513559, 0.10918041957793179, 0.11231595875235471, 0.29178421243860264, 0.11072538524387474]
siam score:  -0.69311273
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27154241627210074, 0.10445160771513559, 0.10918041957793179, 0.11231595875235471, 0.29178421243860264, 0.11072538524387474]
using explorer policy with actor:  1
siam score:  -0.69486916
siam score:  -0.6922051
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]] [[0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]]
siam score:  -0.6919694
line 256 mcts: sample exp_bonus 1.9882405923975055
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.725]
 [0.84 ]
 [0.772]
 [0.732]
 [0.885]
 [0.835]] [[3.251]
 [4.03 ]
 [3.275]
 [4.208]
 [2.761]
 [3.628]
 [3.396]] [[0.653]
 [0.725]
 [0.84 ]
 [0.772]
 [0.732]
 [0.885]
 [0.835]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.69295806
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6889092
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27119151885414977, 0.10472517471403131, 0.10943171175491805, 0.1125508368944406, 0.29152041136510115, 0.11058034641735913]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.744]] [[2.222]
 [2.887]
 [2.887]
 [2.887]
 [2.887]
 [2.887]
 [2.86 ]] [[1.112]
 [1.204]
 [1.204]
 [1.204]
 [1.204]
 [1.204]
 [1.97 ]]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.478]
 [0.104]
 [0.104]
 [0.104]
 [0.45 ]
 [0.783]] [[2.222]
 [2.027]
 [2.887]
 [2.887]
 [2.887]
 [2.377]
 [2.955]] [[1.042]
 [1.229]
 [1.129]
 [1.129]
 [1.129]
 [1.329]
 [1.928]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.634]
 [1.26 ]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[-0.521]
 [ 1.269]
 [-0.521]
 [-0.521]
 [-0.521]
 [-0.521]
 [-0.521]] [[1.393]
 [3.067]
 [1.393]
 [1.393]
 [1.393]
 [1.393]
 [1.393]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2710075994149515, 0.10433053667206448, 0.10936113146985421, 0.1120685061246647, 0.2923407186305782, 0.11089150768788686]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2710075994149515, 0.10433053667206448, 0.10936113146985421, 0.1120685061246647, 0.2923407186305782, 0.11089150768788686]
Printing some Q and Qe and total Qs values:  [[1.125]
 [1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]] [[2.387]
 [2.379]
 [2.379]
 [2.379]
 [2.379]
 [2.379]
 [2.379]] [[2.843]
 [2.739]
 [2.739]
 [2.739]
 [2.739]
 [2.739]
 [2.739]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]] [[2.095]
 [2.095]
 [2.095]
 [2.095]
 [2.095]
 [2.095]
 [2.095]] [[0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]]
Printing some Q and Qe and total Qs values:  [[1.23]
 [1.23]
 [1.23]
 [1.23]
 [1.23]
 [1.23]
 [1.23]] [[1.299]
 [1.299]
 [1.299]
 [1.299]
 [1.299]
 [1.299]
 [1.299]] [[2.041]
 [2.041]
 [2.041]
 [2.041]
 [2.041]
 [2.041]
 [2.041]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
deleting a thread, now have 3 threads
Frames:  79725 train batches done:  9336 episodes:  2162
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26917217740841815, 0.10470632470625255, 0.10975503919737617, 0.11168183069991948, 0.29339370031237116, 0.1112909276756625]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2695428667407359, 0.1048505205810584, 0.10990618788811493, 0.1114441915087611, 0.2928120417725687, 0.1114441915087611]
siam score:  -0.7022162
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.531]
 [0.531]
 [0.526]
 [0.531]
 [0.531]
 [0.523]] [[4.716]
 [4.182]
 [4.182]
 [3.895]
 [4.182]
 [4.182]
 [4.838]] [[0.444]
 [0.291]
 [0.291]
 [0.185]
 [0.291]
 [0.291]
 [0.495]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.7029868
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2695428667407359, 0.1048505205810584, 0.10990618788811493, 0.1114441915087611, 0.2928120417725687, 0.1114441915087611]
1164 1005
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26949453223411635, 0.10519288634204273, 0.10988612087001329, 0.11180808741716228, 0.291810285719503, 0.11180808741716228]
using explorer policy with actor:  1
using explorer policy with actor:  1
Starting evaluation
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26949453223411635, 0.10519288634204273, 0.10988612087001329, 0.11180808741716228, 0.291810285719503, 0.11180808741716228]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.438]
 [0.405]
 [0.428]
 [0.377]
 [0.397]
 [0.427]] [[-1.865]
 [-1.221]
 [-1.908]
 [-2.016]
 [-1.87 ]
 [-1.809]
 [-1.24 ]] [[0.419]
 [0.438]
 [0.405]
 [0.428]
 [0.377]
 [0.397]
 [0.427]]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.65 ]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[2.63 ]
 [3.203]
 [2.197]
 [2.197]
 [2.197]
 [2.197]
 [2.197]] [[0.62 ]
 [0.65 ]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.666]
 [0.689]
 [0.698]
 [0.694]
 [0.693]
 [0.698]] [[2.224]
 [3.394]
 [2.124]
 [1.695]
 [1.925]
 [1.966]
 [2.17 ]] [[0.693]
 [0.666]
 [0.689]
 [0.698]
 [0.694]
 [0.693]
 [0.698]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[ 0.038]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]] [[5.828]
 [3.459]
 [3.459]
 [3.459]
 [3.459]
 [3.459]
 [3.459]] [[1.02 ]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26949453223411635, 0.10519288634204273, 0.10988612087001329, 0.11180808741716228, 0.291810285719503, 0.11180808741716228]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.657]] [[2.839]
 [2.839]
 [2.839]
 [2.839]
 [2.839]
 [2.839]
 [3.973]] [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.657]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2695996503036855, 0.10523391750157318, 0.10992898265575173, 0.11146164246897129, 0.2919241081917475, 0.11185169887827089]
line 256 mcts: sample exp_bonus 2.886442659461655
using explorer policy with actor:  0
1166 1008
line 256 mcts: sample exp_bonus 2.8551723507044517
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.694]
 [0.724]
 [0.694]
 [0.695]
 [0.695]
 [0.794]] [[1.264]
 [1.9  ]
 [0.923]
 [1.081]
 [1.129]
 [1.132]
 [0.351]] [[0.69 ]
 [0.694]
 [0.724]
 [0.694]
 [0.695]
 [0.695]
 [0.794]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.639]] [[2.969]
 [2.969]
 [2.969]
 [2.969]
 [2.969]
 [2.969]
 [2.923]] [[0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.639]]
Printing some Q and Qe and total Qs values:  [[0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]] [[2.003]
 [2.003]
 [2.003]
 [2.003]
 [2.003]
 [2.003]
 [2.003]] [[0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.475091309363831
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.461]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.439]] [[2.513]
 [3.244]
 [1.347]
 [1.347]
 [1.347]
 [1.347]
 [0.584]] [[1.113]
 [1.529]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.127]]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.696]
 [0.707]
 [0.727]
 [0.711]
 [0.708]
 [0.789]] [[ 0.903]
 [ 1.656]
 [ 0.659]
 [ 0.331]
 [ 0.387]
 [ 0.469]
 [-0.332]] [[0.715]
 [0.696]
 [0.707]
 [0.727]
 [0.711]
 [0.708]
 [0.789]]
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.258]
 [0.223]
 [0.216]
 [0.24 ]
 [0.24 ]
 [0.16 ]] [[ 0.783]
 [-0.797]
 [-2.24 ]
 [-0.154]
 [ 0.   ]
 [-2.107]
 [-0.046]] [[0.137]
 [0.258]
 [0.223]
 [0.216]
 [0.24 ]
 [0.24 ]
 [0.16 ]]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]] [[1.696]
 [1.696]
 [1.696]
 [1.696]
 [1.696]
 [1.696]
 [1.696]] [[0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]]
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.719]] [[1.893]
 [1.535]
 [1.535]
 [1.535]
 [1.535]
 [1.535]
 [1.762]] [[0.701]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.719]]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2695131839427884, 0.10453264265069324, 0.1098934138116076, 0.11180843242225678, 0.29283236995427186, 0.11141995721838224]
siam score:  -0.69514537
line 256 mcts: sample exp_bonus 2.535525452907369
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.311]
 [0.294]
 [0.294]
 [0.294]
 [0.282]
 [0.288]] [[-2.716]
 [-2.588]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-2.534]
 [-2.597]] [[0.281]
 [0.311]
 [0.294]
 [0.294]
 [0.294]
 [0.282]
 [0.288]]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.686]
 [0.696]
 [0.696]
 [0.696]
 [0.697]
 [0.705]] [[1.866]
 [1.62 ]
 [1.563]
 [1.679]
 [1.728]
 [1.715]
 [1.771]] [[0.689]
 [0.686]
 [0.696]
 [0.696]
 [0.696]
 [0.697]
 [0.705]]
siam score:  -0.6970187
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.718]
 [0.736]
 [0.733]
 [0.735]
 [0.738]
 [0.782]] [[1.716]
 [1.413]
 [1.018]
 [1.073]
 [0.985]
 [1.066]
 [1.03 ]] [[0.737]
 [0.718]
 [0.736]
 [0.733]
 [0.735]
 [0.738]
 [0.782]]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.437]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[0.11 ]
 [1.178]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]] [[0.458]
 [0.437]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]]
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.778]] [[1.784]
 [1.617]
 [1.617]
 [1.617]
 [1.617]
 [1.617]
 [1.308]] [[0.762]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.778]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.72 ]
 [0.734]
 [0.735]
 [0.735]
 [0.736]
 [0.743]] [[3.066]
 [2.278]
 [0.385]
 [0.564]
 [0.706]
 [0.583]
 [1.557]] [[0.738]
 [0.72 ]
 [0.734]
 [0.735]
 [0.735]
 [0.736]
 [0.743]]
siam score:  -0.698286
rdn probs:  [0.2695131839427884, 0.10453264265069324, 0.1098934138116076, 0.11180843242225678, 0.29283236995427186, 0.11141995721838224]
Printing some Q and Qe and total Qs values:  [[0.024]
 [1.485]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [1.485]] [[2.314]
 [1.236]
 [2.314]
 [2.314]
 [2.314]
 [2.314]
 [1.267]] [[1.455]
 [2.222]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [2.285]]
1170 1013
using explorer policy with actor:  1
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[1.477]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]] [[1.239]
 [2.13 ]
 [2.13 ]
 [2.13 ]
 [2.13 ]
 [2.13 ]
 [2.13 ]] [[2.74 ]
 [1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.609]]
using another actor
from probs:  [0.27008837314104805, 0.10475573395197438, 0.11012794595456107, 0.11165774727720242, 0.29248277788671756, 0.11088742178849649]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.423]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.388]] [[-0.68 ]
 [ 0.444]
 [-1.15 ]
 [-1.15 ]
 [-1.15 ]
 [-1.15 ]
 [-0.763]] [[0.433]
 [0.423]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.388]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[-1.773]
 [-1.773]
 [-1.773]
 [-1.773]
 [-1.773]
 [-1.773]
 [-1.773]] [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.27019133758763453, 0.10479566945918517, 0.11016992948342255, 0.11170031400461014, 0.29259427964079515, 0.11054846982435235]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27019133758763453, 0.10479566945918517, 0.11016992948342255, 0.11170031400461014, 0.29259427964079515, 0.11054846982435235]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27019133758763453, 0.10479566945918517, 0.11016992948342255, 0.11170031400461014, 0.29259427964079515, 0.11054846982435235]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.476]
 [0.457]
 [0.461]
 [0.45 ]
 [0.459]
 [0.444]] [[-2.022]
 [-1.454]
 [-2.41 ]
 [-2.538]
 [-2.62 ]
 [-2.516]
 [-2.714]] [[0.449]
 [0.476]
 [0.457]
 [0.461]
 [0.45 ]
 [0.459]
 [0.444]]
Printing some Q and Qe and total Qs values:  [[-0.095]
 [-0.108]
 [-0.094]
 [-0.093]
 [-0.092]
 [-0.092]
 [-0.092]] [[3.096]
 [4.175]
 [2.468]
 [2.529]
 [2.527]
 [2.591]
 [2.67 ]] [[-0.678]
 [ 0.015]
 [-1.095]
 [-1.051]
 [-1.05 ]
 [-1.008]
 [-0.956]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.041]
 [0.037]
 [0.036]
 [0.033]
 [0.029]
 [0.027]] [[2.47 ]
 [2.309]
 [2.138]
 [2.185]
 [2.143]
 [1.972]
 [2.204]] [[1.171]
 [0.922]
 [0.63 ]
 [0.707]
 [0.63 ]
 [0.338]
 [0.721]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27082915942754726, 0.1040298792099574, 0.1100532287091858, 0.11196399714276456, 0.2923143016262325, 0.11080943388431239]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[5.72 ]
 [5.408]
 [5.408]
 [5.408]
 [5.408]
 [5.408]
 [5.408]] [[1.53 ]
 [1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.323]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.379]] [[4.846]
 [3.634]
 [5.214]
 [5.214]
 [5.214]
 [5.214]
 [4.208]] [[0.962]
 [0.252]
 [1.228]
 [1.228]
 [1.228]
 [1.228]
 [0.66 ]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.31 ]
 [0.386]
 [0.387]
 [0.375]
 [0.375]
 [0.366]] [[4.849]
 [3.635]
 [4.384]
 [4.271]
 [5.219]
 [5.219]
 [4.209]] [[ 0.356]
 [-0.504]
 [ 0.147]
 [ 0.074]
 [ 0.68 ]
 [ 0.68 ]
 [-0.01 ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27082915942754726, 0.1040298792099574, 0.1100532287091858, 0.11196399714276456, 0.2923143016262325, 0.11080943388431239]
using explorer policy with actor:  1
from probs:  [0.2711143537297837, 0.10347438737804959, 0.11016911930913792, 0.11169389811194809, 0.2926221206715017, 0.11092612079957902]
first move QE:  1.2700306979296347
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.335]
 [0.373]] [[-0.598]
 [-0.598]
 [-0.598]
 [-0.598]
 [-0.598]
 [-0.874]
 [-0.755]] [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.335]
 [0.373]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.371]
 [0.44 ]
 [0.432]
 [0.419]
 [0.424]
 [0.438]] [[3.375]
 [2.838]
 [3.158]
 [2.923]
 [2.577]
 [2.839]
 [3.211]] [[0.587]
 [0.284]
 [0.529]
 [0.435]
 [0.294]
 [0.39 ]
 [0.543]]
siam score:  -0.68218446
line 256 mcts: sample exp_bonus 4.096028553422726
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.534]
 [0.56 ]
 [0.512]
 [0.536]
 [0.547]
 [0.544]] [[2.519]
 [2.942]
 [2.538]
 [2.166]
 [2.273]
 [2.302]
 [2.382]] [[0.508]
 [0.534]
 [0.56 ]
 [0.512]
 [0.536]
 [0.547]
 [0.544]]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.263]
 [0.519]
 [0.568]
 [0.57 ]
 [0.597]
 [0.459]] [[5.412]
 [3.753]
 [1.573]
 [0.927]
 [1.073]
 [0.666]
 [1.839]] [[2.065]
 [1.262]
 [0.467]
 [0.213]
 [0.279]
 [0.117]
 [0.545]]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.533]
 [0.619]
 [0.589]
 [0.076]
 [0.557]
 [0.588]] [[ 2.209]
 [-0.052]
 [-1.8  ]
 [-0.985]
 [ 1.683]
 [ 0.646]
 [-0.766]] [[1.697]
 [0.892]
 [0.157]
 [0.505]
 [1.421]
 [1.216]
 [0.602]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27307843813781046, 0.1029659937347987, 0.10912232798976731, 0.11288210659307751, 0.2909816669378214, 0.11096946660672458]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.49 ]
 [1.491]
 [1.491]
 [1.491]
 [1.491]
 [1.491]
 [1.491]] [[1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]] [[2.406]
 [2.406]
 [2.406]
 [2.406]
 [2.406]
 [2.406]
 [2.406]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2732897286950922, 0.1030456622078779, 0.1092067598389095, 0.1121957117412879, 0.29120680986373426, 0.11105532765309822]
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[3.167]
 [3.167]
 [3.167]
 [3.167]
 [3.167]
 [3.167]
 [3.167]] [[1.962]
 [1.962]
 [1.962]
 [1.962]
 [1.962]
 [1.962]
 [1.962]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.7851868831283575
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27401163100365983, 0.1033178601488358, 0.10949523248812075, 0.11210827302929975, 0.2900941586311773, 0.1109728446989065]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.362]
 [0.656]
 [0.526]
 [0.526]
 [0.526]
 [0.547]] [[2.671]
 [2.936]
 [1.515]
 [4.135]
 [4.135]
 [4.135]
 [1.45 ]] [[0.975]
 [1.05 ]
 [0.576]
 [1.918]
 [1.918]
 [1.918]
 [0.414]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2741111605594714, 0.10335538841247319, 0.10917177343043813, 0.11214899424458737, 0.29019952986175734, 0.11101315349127262]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2741111605594714, 0.10335538841247319, 0.10917177343043813, 0.11214899424458737, 0.29019952986175734, 0.11101315349127262]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.48 ]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.354]] [[-0.15 ]
 [ 1.599]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]
 [ 1.76 ]] [[1.23 ]
 [1.652]
 [1.23 ]
 [1.23 ]
 [1.23 ]
 [1.23 ]
 [1.627]]
first move QE:  1.2459164321537672
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.946]] [[2.644]
 [2.644]
 [2.644]
 [2.644]
 [2.644]
 [2.644]
 [3.571]] [[0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.946]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6742567
first move QE:  1.240845552564534
from probs:  [0.27398726867028705, 0.10401059339420324, 0.1091402174795017, 0.11209526224873285, 0.2901692279292008, 0.11059743027807445]
Printing some Q and Qe and total Qs values:  [[1.448]
 [1.448]
 [1.448]
 [1.448]
 [1.448]
 [1.448]
 [1.448]] [[1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]] [[2.207]
 [2.207]
 [2.207]
 [2.207]
 [2.207]
 [2.207]
 [2.207]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27463603018132743, 0.10393172570475666, 0.10939864544497219, 0.11198143283092037, 0.28992817395994247, 0.11012399187808099]
Printing some Q and Qe and total Qs values:  [[0.09 ]
 [0.08 ]
 [0.087]
 [0.088]
 [0.087]
 [0.087]
 [0.093]] [[-3.119]
 [-1.882]
 [-3.805]
 [-4.152]
 [-4.105]
 [-4.117]
 [-4.535]] [[0.09 ]
 [0.08 ]
 [0.087]
 [0.088]
 [0.087]
 [0.087]
 [0.093]]
from probs:  [0.27463603018132743, 0.10393172570475666, 0.10939864544497219, 0.11198143283092037, 0.28992817395994247, 0.11012399187808099]
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.157]
 [0.178]
 [0.178]
 [0.164]
 [0.178]
 [0.166]] [[-3.24 ]
 [-1.877]
 [ 0.   ]
 [ 0.   ]
 [-4.013]
 [ 0.   ]
 [-3.824]] [[0.166]
 [0.157]
 [0.178]
 [0.178]
 [0.164]
 [0.178]
 [0.166]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2748347093672127, 0.10400691274644612, 0.10911853022228427, 0.11206244325003552, 0.2901379158992267, 0.10983948851479478]
siam score:  -0.68183184
from probs:  [0.27544527446890255, 0.10423797159415751, 0.1093609448963799, 0.11193365836326298, 0.2889386458276332, 0.11008350484966388]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[1.442]
 [1.442]
 [1.442]
 [1.442]
 [1.442]
 [1.442]
 [1.442]]
siam score:  -0.68665797
1197 1077
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.487]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.59 ]] [[-3.162]
 [ 0.052]
 [-3.495]
 [-3.675]
 [-3.646]
 [-3.462]
 [-2.962]] [[0.439]
 [1.421]
 [0.332]
 [0.274]
 [0.284]
 [0.343]
 [0.503]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.3252468930783166
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.455]
 [0.414]
 [0.47 ]
 [0.47 ]
 [0.414]
 [0.429]] [[ 0.   ]
 [-1.246]
 [-2.231]
 [ 0.   ]
 [ 0.   ]
 [-2.485]
 [-2.071]] [[0.47 ]
 [0.455]
 [0.414]
 [0.47 ]
 [0.47 ]
 [0.414]
 [0.429]]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[-1.834]
 [-1.834]
 [-1.834]
 [-1.834]
 [-1.834]
 [-1.834]
 [-1.834]] [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]]
line 256 mcts: sample exp_bonus 4.798247993978032
using explorer policy with actor:  1
siam score:  -0.6931765
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.276043657768196, 0.10384564621269406, 0.109251474419101, 0.11217807796532822, 0.288712849090237, 0.10996829454444382]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.276043657768196, 0.10384564621269406, 0.109251474419101, 0.11217807796532822, 0.288712849090237, 0.10996829454444382]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.783]] [[0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [5.404]] [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.783]]
first move QE:  1.222218790520768
from probs:  [0.2762416733591182, 0.10392013825922669, 0.10932984425176127, 0.11225854715116793, 0.2889199527269648, 0.10932984425176127]
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.211]
 [0.26 ]
 [0.256]
 [0.256]
 [0.256]
 [0.259]] [[-0.757]
 [ 1.236]
 [-0.978]
 [-1.07 ]
 [-1.07 ]
 [-1.055]
 [-0.904]] [[0.274]
 [0.211]
 [0.26 ]
 [0.256]
 [0.256]
 [0.256]
 [0.259]]
siam score:  -0.6814467
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.619]
 [0.619]
 [0.619]
 [0.7  ]
 [0.723]
 [0.524]] [[2.002]
 [2.095]
 [2.095]
 [2.095]
 [1.885]
 [1.925]
 [2.339]] [[1.342]
 [1.202]
 [1.202]
 [1.202]
 [1.294]
 [1.352]
 [1.094]]
siam score:  -0.6844417
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 7.483319123769814
line 256 mcts: sample exp_bonus 3.9307705507460917
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[3.753]
 [3.753]
 [3.753]
 [3.753]
 [3.753]
 [3.753]
 [3.753]] [[0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]]
using explorer policy with actor:  1
siam score:  -0.6991101
1213 1091
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.661]] [[0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.628]] [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.661]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.494]
 [0.547]
 [0.548]
 [0.544]
 [0.54 ]
 [0.548]] [[ 1.003]
 [-0.244]
 [-0.484]
 [-0.455]
 [-0.498]
 [-0.539]
 [-0.215]] [[ 0.426]
 [-0.193]
 [-0.167]
 [-0.155]
 [-0.177]
 [-0.199]
 [-0.074]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.1695183977154173
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2745816323498566, 0.10443683251602674, 0.11015343398305469, 0.11307313567591135, 0.28760153149209583, 0.11015343398305469]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[7.814]
 [7.652]
 [7.652]
 [7.652]
 [7.652]
 [7.652]
 [7.652]] [[0.747]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]]
siam score:  -0.6909921
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27494963722954596, 0.1039433457278543, 0.11030106585803987, 0.11322468064756294, 0.28798698614210083, 0.10959428439489607]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[6.62 ]
 [6.587]
 [6.587]
 [6.587]
 [6.587]
 [6.587]
 [6.587]] [[0.887]
 [0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.885]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27494963722954596, 0.1039433457278543, 0.11030106585803987, 0.11322468064756294, 0.28798698614210083, 0.10959428439489607]
using another actor
from probs:  [0.2742966821536469, 0.10403695348075523, 0.11040039915201119, 0.1133266468470741, 0.28824633718038717, 0.10969298118612551]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27439277956314306, 0.10407340190456726, 0.11043907695256842, 0.11336634983255457, 0.28834732172778416, 0.10938107001938263]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2732772857852762, 0.10401367762262352, 0.11070989389943672, 0.11364434499139171, 0.28905440244108555, 0.10930039526018621]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2732772857852762, 0.10401367762262352, 0.11070989389943672, 0.11364434499139171, 0.28905440244108555, 0.10930039526018621]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2733628416097402, 0.1037331682187804, 0.11074455421239202, 0.113679924002664, 0.2891448976596865, 0.10933461429673678]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.275]
 [0.275]
 [0.363]
 [0.275]
 [0.727]
 [0.217]] [[ 0.827]
 [ 0.827]
 [ 0.827]
 [-0.134]
 [ 0.827]
 [ 0.973]
 [ 0.722]] [[0.275]
 [0.275]
 [0.275]
 [0.363]
 [0.275]
 [0.727]
 [0.217]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2734576563306026, 0.10376914762011716, 0.11078296548267601, 0.11371935339328085, 0.28924518631852153, 0.10902569085480178]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2734576563306026, 0.10376914762011716, 0.11078296548267601, 0.11371935339328085, 0.28924518631852153, 0.10902569085480178]
UNIT TEST: sample policy line 217 mcts : [0.061 0.041 0.    0.02  0.02  0.02  0.837]
rdn beta is 0 so we're just using the maxi policy
first move QE:  1.1804059795698492
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27461298662759237, 0.10389500386972375, 0.11125101204655707, 0.11382243048932457, 0.2869322538515562, 0.10948631311524604]
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.477]
 [0.477]
 [0.477]
 [0.591]
 [0.637]
 [0.659]] [[3.037]
 [2.933]
 [2.933]
 [2.933]
 [1.749]
 [1.451]
 [2.448]] [[1.882]
 [1.775]
 [1.775]
 [1.775]
 [1.538]
 [1.484]
 [1.726]]
first move QE:  1.178474292205898
start point for exploration sampling:  10935
from probs:  [0.2750153122090459, 0.10404721669241407, 0.11141400189149703, 0.11287067261038518, 0.28735262794706523, 0.10930016864959256]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.368]] [[3.569]
 [2.954]
 [2.954]
 [2.954]
 [2.954]
 [2.954]
 [3.042]] [[1.905]
 [1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.3  ]]
siam score:  -0.69733477
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2751956704918942, 0.1038041646894636, 0.11148706850697016, 0.11294469452768287, 0.2875410771869718, 0.10902732459701746]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27596133970312986, 0.1037827435758013, 0.11143788548028158, 0.11288983827803827, 0.28659752469219874, 0.10933066827055028]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27619992433250673, 0.10387246979411284, 0.11153423001402842, 0.11298743810943114, 0.2859807467574672, 0.1094251909924537]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27619992433250673, 0.10387246979411284, 0.11153423001402842, 0.11298743810943114, 0.2859807467574672, 0.1094251909924537]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  1.1737447856366823
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4082],
        [-0.4014],
        [-0.0000],
        [-0.4664],
        [-0.4301],
        [-0.0000],
        [-0.5824],
        [-0.0000],
        [-0.5635],
        [-0.0000]], dtype=torch.float64)
-0.0727797758985 -0.4810195048687145
-0.0727797758985 -0.47416898903652155
-0.7308351022499998 -0.7308351022499998
-0.0727797758985 -0.5391881564005915
-0.0439609252995 -0.4740943690190193
-0.9605475 -0.9605475
-0.024259925299500003 -0.6066853548668867
-0.07277977589849927 -0.07277977589849927
-0.024259925299500003 -0.5877571081796713
-0.955892025 -0.955892025
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27639363495364205, 0.1039453199250146, 0.11125489612421798, 0.11306668095089666, 0.28618131708065253, 0.10915815096557624]
using explorer policy with actor:  1
siam score:  -0.71135193
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27721894680982795, 0.1045945735003721, 0.11159238226092796, 0.11340340937287306, 0.2836944236265737, 0.10949626442942505]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27721894680982795, 0.1045945735003721, 0.11159238226092796, 0.11340340937287306, 0.2836944236265737, 0.10949626442942505]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27730511691874743, 0.1043162477076497, 0.11162706938402928, 0.11343865943153592, 0.2837826065580258, 0.1095303000000119]
from probs:  [0.27730511691874743, 0.1043162477076497, 0.11162706938402928, 0.11343865943153592, 0.2837826065580258, 0.1095303000000119]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2766573916664435, 0.10440974258280054, 0.1117271166838973, 0.11354033039395928, 0.28403695063148676, 0.10962846804141266]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2766573916664435, 0.10440974258280054, 0.1117271166838973, 0.11354033039395928, 0.28403695063148676, 0.10962846804141266]
in main func line 156:  1232
from probs:  [0.2766573916664435, 0.10440974258280054, 0.1117271166838973, 0.11354033039395928, 0.28403695063148676, 0.10962846804141266]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27674296431964485, 0.10413272835214177, 0.11176167489957191, 0.11357544945316334, 0.28412480584954675, 0.10966237712593123]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27674296431964485, 0.10413272835214177, 0.11176167489957191, 0.11357544945316334, 0.28412480584954675, 0.10966237712593123]
first move QE:  1.16041951196773
using another actor
from probs:  [0.27730272371204345, 0.10434335438519919, 0.11198773176569325, 0.11380517498469975, 0.28301952689624066, 0.10954148825612371]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27730272371204345, 0.10434335438519919, 0.11198773176569325, 0.11380517498469975, 0.28301952689624066, 0.10954148825612371]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2770463214897814, 0.10458347856780259, 0.11188828978334549, 0.11369789074355952, 0.28367083653600816, 0.10911318287950288]
line 256 mcts: sample exp_bonus 5.460323482283532
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]] [[3.504]
 [2.815]
 [2.815]
 [2.815]
 [2.815]
 [2.815]
 [2.815]] [[2.207]
 [1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.958]]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.147]
 [0.172]
 [0.398]
 [0.201]
 [0.428]
 [0.161]] [[3.317]
 [2.567]
 [2.516]
 [3.571]
 [2.133]
 [1.686]
 [3.308]] [[1.548]
 [1.098]
 [1.098]
 [1.569]
 [0.996]
 [0.998]
 [1.339]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27733593830248493, 0.10469280733839004, 0.11129717949793319, 0.1138167474767989, 0.28396737843951625, 0.10888994894487677]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.386]] [[2.393]
 [2.071]
 [2.071]
 [2.071]
 [2.071]
 [2.071]
 [1.983]] [[0.907]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.389]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27733593830248493, 0.10469280733839004, 0.11129717949793319, 0.1138167474767989, 0.28396737843951625, 0.10888994894487677]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.018]
 [ 0.018]
 [-0.006]
 [-0.005]
 [ 0.013]
 [ 0.022]] [[6.447]
 [5.966]
 [5.966]
 [6.422]
 [6.396]
 [6.278]
 [6.274]] [[0.408]
 [0.169]
 [0.169]
 [0.401]
 [0.386]
 [0.348]
 [0.362]]
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.569]
 [0.631]
 [0.636]
 [0.635]
 [0.637]
 [0.643]] [[5.306]
 [2.52 ]
 [2.123]
 [1.998]
 [1.962]
 [1.969]
 [2.053]] [[1.478]
 [0.385]
 [0.31 ]
 [0.271]
 [0.258]
 [0.262]
 [0.299]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.42 ]
 [0.43 ]
 [0.497]
 [0.521]
 [0.534]
 [0.488]] [[4.742]
 [2.61 ]
 [1.741]
 [0.299]
 [0.334]
 [0.315]
 [2.287]] [[1.619]
 [0.814]
 [0.512]
 [0.046]
 [0.075]
 [0.078]
 [0.748]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27750073340753206, 0.10478189003842804, 0.11102013409192912, 0.11351630145805643, 0.284212079142249, 0.10896886186180546]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2768519553028502, 0.10459016656116567, 0.11147111927270209, 0.11397742655173851, 0.28369781793209053, 0.1094115143794531]
first move QE:  1.1535417477107106
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2769449155902568, 0.10462528544603246, 0.11150854861747153, 0.11401569745475064, 0.2837930768969865, 0.10911247599450223]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[7.819]
 [7.664]
 [7.664]
 [7.664]
 [7.664]
 [7.664]
 [7.664]] [[2.036]
 [1.938]
 [1.938]
 [1.938]
 [1.938]
 [1.938]
 [1.938]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
from probs:  [0.27713408146021434, 0.1046967492354968, 0.11123555448268592, 0.11409357531920566, 0.2839869203700279, 0.10885311913236938]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.892]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [1.285]] [[2.546]
 [3.345]
 [2.546]
 [2.546]
 [2.546]
 [2.546]
 [2.893]] [[1.703]
 [1.908]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [2.542]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.148482841148924
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2778283176255923, 0.10434693981261127, 0.11151420568426625, 0.1136489535335989, 0.28386839797914987, 0.10879318536478146]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]] [[1.149]
 [1.134]
 [1.149]
 [1.149]
 [1.149]
 [1.149]
 [1.148]] [[2.263]
 [2.256]
 [2.263]
 [2.263]
 [2.263]
 [2.263]
 [2.263]]
using explorer policy with actor:  1
start point for exploration sampling:  10935
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[7.434]
 [7.434]
 [7.434]
 [7.434]
 [7.434]
 [7.434]
 [7.434]] [[0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
first move QE:  1.1488099678366062
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[8.956]
 [6.568]
 [6.568]
 [6.568]
 [6.568]
 [6.568]
 [6.568]] [[1.82 ]
 [1.056]
 [1.056]
 [1.056]
 [1.056]
 [1.056]
 [1.056]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.619]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[-0.465]
 [ 2.182]
 [-0.465]
 [-0.465]
 [-0.465]
 [-0.465]
 [-0.465]] [[1.046]
 [1.973]
 [1.046]
 [1.046]
 [1.046]
 [1.046]
 [1.046]]
UNIT TEST: sample policy line 217 mcts : [0.878 0.02  0.02  0.02  0.02  0.02  0.02 ]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.467]
 [0.595]
 [0.597]
 [0.605]
 [0.627]
 [0.607]] [[3.276]
 [3.793]
 [3.212]
 [2.62 ]
 [2.597]
 [2.765]
 [2.685]] [[1.462]
 [1.654]
 [1.425]
 [1.109]
 [1.102]
 [1.206]
 [1.151]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2748465703550677, 0.10522250319722268, 0.11242597220605599, 0.11420757884594124, 0.2845961783137892, 0.10870119708192325]
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.869]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]] [[1.887]
 [2.592]
 [1.887]
 [1.887]
 [1.887]
 [1.887]
 [1.887]] [[1.562]
 [2.03 ]
 [1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]]
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.342]
 [0.468]
 [0.518]
 [0.571]
 [0.567]
 [0.52 ]] [[1.998]
 [2.124]
 [1.814]
 [1.713]
 [1.823]
 [2.082]
 [2.713]] [[1.827]
 [1.256]
 [1.29 ]
 [1.316]
 [1.453]
 [1.58 ]
 [1.83 ]]
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[3.652]
 [3.652]
 [3.652]
 [3.652]
 [3.652]
 [3.652]
 [3.652]] [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.647]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]] [[0.653]
 [1.547]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[0.64 ]
 [0.647]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.387]
 [0.62 ]
 [0.62 ]
 [0.597]
 [0.62 ]
 [0.398]] [[2.486]
 [1.686]
 [2.159]
 [2.159]
 [1.918]
 [2.159]
 [2.299]] [[1.826]
 [1.423]
 [1.72 ]
 [1.72 ]
 [1.614]
 [1.72 ]
 [1.671]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27545084728106084, 0.10514829198711867, 0.11232291430804989, 0.11373750926388873, 0.28440024992014235, 0.10894018723973957]
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.144]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[-1.172]
 [-0.073]
 [-1.172]
 [-1.172]
 [-1.172]
 [-1.172]
 [-1.172]] [[0.222]
 [0.144]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2757662180460954, 0.10526867897308508, 0.11245151570378312, 0.11386773026579795, 0.28390806803818774, 0.1087377889730507]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[4.792]
 [4.792]
 [4.792]
 [4.792]
 [4.792]
 [4.792]
 [4.792]] [[0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2758500976252783, 0.10499652929782319, 0.11248571998697102, 0.11390236531778102, 0.2839944241170519, 0.10877086365509463]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.0737035121232785
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.32 ]
 [0.365]
 [0.355]
 [0.354]
 [0.366]
 [0.361]] [[2.22 ]
 [2.281]
 [2.202]
 [2.042]
 [1.968]
 [2.051]
 [1.74 ]] [[0.958]
 [0.986]
 [0.941]
 [0.72 ]
 [0.622]
 [0.746]
 [0.334]]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.499]
 [0.525]
 [0.555]
 [0.546]
 [0.524]
 [0.524]] [[1.94 ]
 [2.868]
 [1.954]
 [1.89 ]
 [1.821]
 [1.68 ]
 [1.876]] [[0.479]
 [0.499]
 [0.525]
 [0.555]
 [0.546]
 [0.524]
 [0.524]]
siam score:  -0.7194601
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2755135346587344, 0.10519514388504968, 0.1123493383084758, 0.11375955021306405, 0.2845316364963357, 0.10865079643834037]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2755135346587344, 0.10519514388504968, 0.1123493383084758, 0.11375955021306405, 0.2845316364963357, 0.10865079643834037]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27489502906077296, 0.10528495064954266, 0.1124452527223335, 0.11385666854721431, 0.2847745456718539, 0.10874355334828267]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.53 ]
 [0.535]
 [0.53 ]
 [0.53 ]
 [0.539]
 [0.529]] [[4.518]
 [4.92 ]
 [4.028]
 [4.92 ]
 [4.92 ]
 [3.578]
 [3.373]] [[1.474]
 [1.667]
 [1.21 ]
 [1.667]
 [1.667]
 [0.98 ]
 [0.869]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.29 ]] [[4.537]
 [3.405]
 [3.405]
 [3.405]
 [3.405]
 [3.405]
 [3.242]] [[1.692]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.769]]
1261 1194
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.058]
 [-0.02 ]
 [-0.02 ]
 [-0.021]
 [-0.021]
 [-0.023]
 [-0.021]] [[3.555]
 [3.089]
 [3.802]
 [3.726]
 [3.592]
 [3.522]
 [3.38 ]] [[-0.699]
 [-1.011]
 [-0.773]
 [-0.799]
 [-0.845]
 [-0.872]
 [-0.915]]
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.071]
 [-0.071]
 [-0.077]
 [-0.07 ]
 [-0.07 ]
 [-0.071]] [[3.201]
 [2.897]
 [2.81 ]
 [3.313]
 [2.889]
 [3.014]
 [2.818]] [[-0.547]
 [-0.649]
 [-0.676]
 [-0.52 ]
 [-0.65 ]
 [-0.608]
 [-0.675]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[5.006]
 [4.855]
 [4.855]
 [4.855]
 [4.855]
 [4.855]
 [4.855]] [[0.111]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27445082184875286, 0.10576561229214623, 0.11191876522626161, 0.11401856784045039, 0.2852553755947495, 0.1085908571976393]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.69798774
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.721]
 [0.768]] [[3.66 ]
 [3.629]
 [3.629]
 [3.629]
 [3.629]
 [3.544]
 [3.342]] [[2.013]
 [1.934]
 [1.934]
 [1.934]
 [1.934]
 [1.868]
 [1.717]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.27475830402123197, 0.10557919266648741, 0.1120441539018762, 0.11414630903906067, 0.2847595229343305, 0.10871251743701327]
line 256 mcts: sample exp_bonus 2.4844371759308923
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[3.28 ]
 [2.894]
 [2.894]
 [2.894]
 [2.894]
 [2.894]
 [2.894]] [[1.256]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2735378228284513, 0.10575686780528995, 0.1122327086738322, 0.11433840144657109, 0.2852387337190021, 0.10889546552685336]
first move QE:  1.1353485519966353
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.6993877
siam score:  -0.6960103
siam score:  -0.6971867
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27414013406525667, 0.10570511944637222, 0.11247867482354784, 0.1142258220438511, 0.28430558050222327, 0.10914466911874901]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.689]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.656]] [[4.172]
 [4.266]
 [4.172]
 [4.172]
 [4.172]
 [4.172]
 [3.929]] [[1.838]
 [1.974]
 [1.838]
 [1.838]
 [1.838]
 [1.838]
 [1.681]]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.656]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[3.815]
 [3.71 ]
 [4.766]
 [4.766]
 [4.766]
 [4.766]
 [4.766]] [[1.1  ]
 [1.043]
 [1.643]
 [1.643]
 [1.643]
 [1.643]
 [1.643]]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.597]
 [0.629]
 [0.594]
 [0.594]
 [0.638]
 [0.639]] [[3.337]
 [3.374]
 [2.857]
 [4.32 ]
 [4.32 ]
 [2.599]
 [2.481]] [[0.659]
 [0.657]
 [0.299]
 [1.365]
 [1.365]
 [0.114]
 [0.026]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -1.3016248012677936
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.27463468924593903, 0.10559365202487815, 0.11233813404954972, 0.11407735465381236, 0.2840146013843494, 0.10934156864147153]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27490793710965394, 0.10539802909840988, 0.1121082637322753, 0.11383823019839164, 0.2842971817943668, 0.10945035806690265]
from probs:  [0.27490793710965394, 0.10539802909840988, 0.1121082637322753, 0.11383823019839164, 0.2842971817943668, 0.10945035806690265]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.665]
 [0.671]
 [0.671]
 [0.671]
 [0.667]
 [0.659]] [[2.857]
 [3.678]
 [2.832]
 [2.832]
 [2.832]
 [2.664]
 [3.203]] [[0.482]
 [0.921]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.586]
 [0.751]]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.43 ]
 [0.438]] [[1.215]
 [1.215]
 [1.215]
 [1.215]
 [1.215]
 [1.018]
 [0.96 ]] [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.43 ]
 [0.438]]
using explorer policy with actor:  1
from probs:  [0.2746150416661649, 0.1056065808185203, 0.11198991533299898, 0.11406348259566401, 0.2840580514317048, 0.10966692815494711]
from probs:  [0.27358089295217153, 0.10554789099779616, 0.11224601947535254, 0.11432432867539505, 0.2847076495979703, 0.10959321830131452]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27379964212576063, 0.10563228473480278, 0.11233576888633642, 0.11441573985601156, 0.28413571780537816, 0.10968084659171044]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.524]
 [0.492]
 [0.544]
 [0.493]
 [0.468]
 [0.571]] [[3.725]
 [3.584]
 [3.843]
 [3.831]
 [3.705]
 [3.78 ]
 [4.117]] [[1.038]
 [0.961]
 [0.984]
 [1.083]
 [0.94 ]
 [0.914]
 [1.232]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.522]
 [0.549]
 [0.545]
 [0.547]
 [0.549]
 [0.544]] [[3.262]
 [3.395]
 [3.73 ]
 [3.314]
 [3.276]
 [3.73 ]
 [3.418]] [[1.002]
 [0.842]
 [1.119]
 [0.834]
 [0.812]
 [1.119]
 [0.902]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[3.144]
 [3.086]
 [3.086]
 [3.086]
 [3.086]
 [3.086]
 [3.086]] [[0.545]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]]
1294 1220
using another actor
line 256 mcts: sample exp_bonus -2.3922200128593225
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]] [[2.13 ]
 [2.033]
 [2.033]
 [2.033]
 [2.033]
 [2.033]
 [2.033]] [[1.715]
 [1.586]
 [1.586]
 [1.586]
 [1.586]
 [1.586]
 [1.586]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2737485193322669, 0.105631888475927, 0.11197586784406677, 0.11403565355030454, 0.28494033161709537, 0.10966773918033931]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27396695288186806, 0.10571617586643837, 0.11206521731833063, 0.11412664659986273, 0.2843697604157517, 0.10975524691774854]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6841249
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27418449826106434, 0.10580012053685986, 0.11215420348961802, 0.11421726966351987, 0.2838015092080893, 0.10984239884084857]
Printing some Q and Qe and total Qs values:  [[1.227]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[3.252]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[1.318]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]]
Printing some Q and Qe and total Qs values:  [[1.085]
 [0.421]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.555]] [[ 1.001]
 [ 0.822]
 [-0.3  ]
 [-0.3  ]
 [-0.3  ]
 [-0.3  ]
 [-0.191]] [[1.317]
 [0.713]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.544]]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[1.348]
 [1.348]
 [1.348]
 [1.348]
 [1.348]
 [1.348]
 [1.348]] [[0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[ 0.005]
 [-0.012]
 [ 0.005]
 [ 0.005]
 [ 0.005]
 [ 0.005]
 [ 0.005]] [[-1.025]
 [ 0.029]
 [-1.025]
 [-1.025]
 [-1.025]
 [-1.025]
 [-1.025]] [[ 0.005]
 [-0.012]
 [ 0.005]
 [ 0.005]
 [ 0.005]
 [ 0.005]
 [ 0.005]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.497]
 [0.551]
 [0.551]
 [0.488]
 [0.551]
 [0.542]] [[1.917]
 [1.653]
 [0.   ]
 [0.   ]
 [1.559]
 [0.   ]
 [0.177]] [[0.484]
 [0.497]
 [0.551]
 [0.551]
 [0.488]
 [0.551]
 [0.542]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27418449826106434, 0.10580012053685986, 0.11215420348961802, 0.11421726966351987, 0.2838015092080893, 0.10984239884084857]
line 256 mcts: sample exp_bonus -0.020456152139988897
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.443]
 [0.511]
 [0.434]
 [0.418]
 [0.501]
 [0.475]] [[1.566]
 [1.666]
 [1.324]
 [2.684]
 [1.966]
 [1.99 ]
 [2.818]] [[0.355]
 [0.443]
 [0.511]
 [0.434]
 [0.418]
 [0.501]
 [0.475]]
siam score:  -0.6727695
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.82 ]
 [0.821]
 [0.832]
 [0.821]
 [0.821]
 [0.818]] [[2.46 ]
 [3.297]
 [2.248]
 [1.551]
 [2.248]
 [2.248]
 [3.309]] [[0.826]
 [0.82 ]
 [0.821]
 [0.832]
 [0.821]
 [0.821]
 [0.818]]
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.705]
 [0.713]
 [0.712]
 [0.728]
 [0.708]
 [0.706]] [[ 1.682]
 [ 2.097]
 [ 0.345]
 [ 1.416]
 [ 3.532]
 [-0.171]
 [ 0.541]] [[0.707]
 [0.705]
 [0.713]
 [0.712]
 [0.728]
 [0.708]
 [0.706]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2742662989727804, 0.10553334328643048, 0.11218766378268921, 0.11425134545557956, 0.283886179076687, 0.10987516942583334]
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.792]
 [0.796]
 [0.777]
 [0.775]
 [0.788]
 [0.771]] [[2.319]
 [2.372]
 [2.303]
 [2.18 ]
 [2.217]
 [1.997]
 [2.542]] [[0.776]
 [0.792]
 [0.796]
 [0.777]
 [0.775]
 [0.788]
 [0.771]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.723]
 [0.687]
 [0.835]
 [0.835]
 [0.835]
 [0.835]] [[2.766]
 [3.382]
 [3.392]
 [3.472]
 [3.472]
 [3.472]
 [3.472]] [[0.767]
 [0.723]
 [0.687]
 [0.835]
 [0.835]
 [0.835]
 [0.835]]
Printing some Q and Qe and total Qs values:  [[1.   ]
 [1.   ]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]] [[1.107]
 [1.084]
 [1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]] [[1.   ]
 [1.   ]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27436215342050646, 0.10557022656510999, 0.11222687270698596, 0.11394178154479837, 0.28398539561555597, 0.10991357014704324]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.789]
 [0.754]
 [0.754]
 [0.754]
 [0.779]
 [0.781]] [[2.29 ]
 [2.446]
 [2.848]
 [2.848]
 [2.848]
 [2.503]
 [2.408]] [[0.759]
 [0.789]
 [0.754]
 [0.754]
 [0.754]
 [0.779]
 [0.781]]
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.341]] [[1.086]
 [1.086]
 [1.086]
 [1.086]
 [1.086]
 [1.086]
 [1.101]] [[-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.027]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[-1.073]
 [-1.073]
 [-1.073]
 [-1.073]
 [-1.073]
 [-1.073]
 [-1.073]] [[0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[ 0.116]
 [-0.018]
 [ 0.164]
 [ 0.196]
 [ 0.2  ]
 [ 0.197]
 [ 0.226]] [[0.856]
 [1.064]
 [0.833]
 [0.645]
 [0.46 ]
 [0.56 ]
 [0.52 ]] [[-0.584]
 [-0.645]
 [-0.512]
 [-0.636]
 [-0.813]
 [-0.719]
 [-0.702]]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.103]
 [0.127]
 [0.103]
 [0.123]
 [0.123]
 [0.128]] [[0.632]
 [0.632]
 [1.08 ]
 [0.632]
 [0.804]
 [0.797]
 [0.949]] [[-1.11 ]
 [-1.11 ]
 [-0.763]
 [-1.11 ]
 [-0.956]
 [-0.958]
 [-0.849]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.287]
 [0.458]
 [0.593]
 [0.216]
 [0.164]
 [0.153]] [[3.952]
 [4.27 ]
 [4.076]
 [4.389]
 [4.522]
 [1.703]
 [3.385]] [[1.345]
 [1.646]
 [1.801]
 [2.151]
 [1.663]
 [0.213]
 [1.017]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2750222718523386, 0.10523110419881418, 0.11182413371324267, 0.11386779515302509, 0.28387667164758107, 0.11017802343499848]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[5.2]
 [5.2]
 [5.2]
 [5.2]
 [5.2]
 [5.2]
 [5.2]] [[1.725]
 [1.725]
 [1.725]
 [1.725]
 [1.725]
 [1.725]
 [1.725]]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[2.66 ]
 [1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.133]] [[1.687]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]]
using explorer policy with actor:  1
using explorer policy with actor:  1
1303 1232
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27507834244225354, 0.1052744060989634, 0.1118492183314344, 0.11423379383713142, 0.28399904207568544, 0.10956519721453183]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27517384453051136, 0.10531095541627164, 0.1118880503013591, 0.11392627227201764, 0.28409764126506776, 0.1096032362147725]
rdn probs:  [0.27517384453051136, 0.10531095541627164, 0.1118880503013591, 0.11392627227201764, 0.28409764126506776, 0.1096032362147725]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.265]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.463]] [[2.916]
 [2.611]
 [3.332]
 [3.332]
 [3.332]
 [3.332]
 [2.665]] [[0.917]
 [0.312]
 [1.334]
 [1.334]
 [1.334]
 [1.334]
 [0.666]]
siam score:  -0.6614339
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]] [[5.127]
 [5.127]
 [5.127]
 [5.127]
 [5.127]
 [5.127]
 [5.127]] [[0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]]
UNIT TEST: sample policy line 217 mcts : [0.673 0.02  0.041 0.02  0.    0.02  0.224]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6578822
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.1152683413425104
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.378]] [[6.255]
 [5.748]
 [5.748]
 [5.748]
 [5.748]
 [5.748]
 [5.588]] [[0.461]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.232]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.608]
 [0.644]
 [0.758]
 [0.602]
 [0.572]
 [0.536]] [[3.364]
 [3.657]
 [4.647]
 [5.931]
 [2.493]
 [4.984]
 [4.218]] [[0.624]
 [0.683]
 [1.085]
 [1.742]
 [0.282]
 [1.054]
 [0.726]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2748252096948132, 0.10490711173774654, 0.11208158570186726, 0.11412333323315854, 0.2845890516582024, 0.10947370797421194]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.273717744891028, 0.10481838344174627, 0.11229773628340206, 0.11434342134745995, 0.28513788480174823, 0.1096848292346154]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27393382450905274, 0.10490112968921766, 0.11238638691864247, 0.11443368689753688, 0.2845735548097198, 0.1097714171758304]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27393382450905274, 0.10490112968921766, 0.11238638691864247, 0.11443368689753688, 0.2845735548097198, 0.1097714171758304]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27393382450905274, 0.10490112968921766, 0.11238638691864247, 0.11443368689753688, 0.2845735548097198, 0.1097714171758304]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27402878868526104, 0.10493749558664603, 0.11242534771755032, 0.11412668911968939, 0.28467220744325855, 0.10980947144759476]
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]] [[5.463]
 [5.558]
 [5.558]
 [5.558]
 [5.558]
 [5.558]
 [5.558]] [[-0.536]
 [-0.495]
 [-0.495]
 [-0.495]
 [-0.495]
 [-0.495]
 [-0.495]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.23 ]
 [0.335]
 [0.339]
 [0.339]
 [0.329]
 [0.326]] [[6.564]
 [3.962]
 [5.227]
 [5.187]
 [5.187]
 [5.055]
 [5.299]] [[ 0.566]
 [-0.456]
 [ 0.167]
 [ 0.161]
 [ 0.161]
 [ 0.098]
 [ 0.172]]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.303]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[5.425]
 [4.364]
 [4.292]
 [4.292]
 [4.292]
 [4.292]
 [4.292]] [[ 0.643]
 [-0.035]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]]
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[6.299]
 [3.548]
 [3.548]
 [3.548]
 [3.548]
 [3.548]
 [3.548]] [[1.985]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]]
siam score:  -0.6553432
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.5114],
        [-0.0000],
        [-0.4459],
        [-0.0000],
        [-0.5641],
        [-0.0000],
        [-0.5370],
        [-0.3150],
        [-0.0000]], dtype=torch.float64)
-0.2295970745309994 -0.2295970745309994
-0.0530787758985 -0.5644715976041844
-0.9454499999999999 -0.9454499999999999
-0.024259925299500003 -0.47015814814992085
-0.965595015 -0.965595015
-0.024259925299500003 -0.5883240121798381
-0.9181905722055 -0.9181905722055
-0.024259925299500003 -0.5612258797774837
-0.024259925299500003 -0.339232422325937
-0.004949999999999235 -0.004949999999999235
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.023]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]] [[3.575]
 [3.499]
 [3.575]
 [3.575]
 [3.575]
 [3.575]
 [3.575]] [[-0.508]
 [-0.502]
 [-0.508]
 [-0.508]
 [-0.508]
 [-0.508]
 [-0.508]]
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]] [[2.239]
 [2.239]
 [2.239]
 [2.239]
 [2.239]
 [2.239]
 [2.239]] [[0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2731451342911593, 0.10493054249596552, 0.11272911794889265, 0.11443505632922046, 0.28465397528234937, 0.11010617365241265]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27265397749026793, 0.10505137296669434, 0.11285892869858102, 0.11422099449604411, 0.2849817623404361, 0.11023296400797648]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.182]
 [0.197]
 [0.197]
 [0.197]
 [0.199]
 [0.2  ]] [[-2.057]
 [-0.757]
 [-2.034]
 [-2.057]
 [-2.057]
 [-1.99 ]
 [-1.852]] [[0.197]
 [0.182]
 [0.197]
 [0.197]
 [0.197]
 [0.199]
 [0.2  ]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27305342912190084, 0.10520527845057205, 0.1126884876132186, 0.1140441414470314, 0.28461420258695186, 0.11039446078032521]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.43133131433583116
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27305342912190084, 0.10520527845057205, 0.1126884876132186, 0.1140441414470314, 0.28461420258695186, 0.11039446078032521]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.7  ]
 [0.998]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]] [[-0.786]
 [ 1.899]
 [-0.786]
 [-0.786]
 [-0.786]
 [-0.786]
 [-0.786]] [[0.978]
 [2.47 ]
 [0.978]
 [0.978]
 [0.978]
 [0.978]
 [0.978]]
siam score:  -0.6556021
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2721927337118152, 0.10549156349013482, 0.11266049654609236, 0.11435447883719765, 0.28460586077041855, 0.11069486664434137]
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.701]
 [0.699]
 [0.701]
 [0.701]
 [0.701]
 [0.704]] [[2.566]
 [2.787]
 [2.261]
 [2.787]
 [2.787]
 [2.787]
 [2.293]] [[1.428]
 [1.628]
 [1.097]
 [1.628]
 [1.628]
 [1.628]
 [1.138]]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.619]
 [0.628]
 [0.63 ]
 [0.631]
 [0.626]
 [0.64 ]] [[4.538]
 [3.623]
 [4.355]
 [4.294]
 [4.321]
 [4.296]
 [4.241]] [[0.724]
 [0.378]
 [0.642]
 [0.625]
 [0.636]
 [0.617]
 [0.628]]
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[4.316]
 [4.316]
 [4.316]
 [4.316]
 [4.316]
 [4.316]
 [4.316]] [[-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.032]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.042]
 [-0.042]] [[3.091]
 [3.681]
 [3.191]
 [3.191]
 [3.191]
 [2.753]
 [2.755]] [[-0.603]
 [-0.389]
 [-0.574]
 [-0.574]
 [-0.574]
 [-0.718]
 [-0.718]]
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6843634
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2707981111726874, 0.10529178376110485, 0.11273404077348817, 0.11476794952221404, 0.28563490817992104, 0.11077320659058476]
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.514]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]] [[3.167]
 [3.329]
 [3.329]
 [3.329]
 [3.329]
 [3.329]
 [3.329]] [[0.791]
 [0.939]
 [0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26982925702840543, 0.10552401285644819, 0.11298268433681337, 0.1150210790270995, 0.2862648978520079, 0.11037806889922552]
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[3.456]
 [3.456]
 [3.456]
 [3.456]
 [3.456]
 [3.456]
 [3.456]] [[1.846]
 [1.846]
 [1.846]
 [1.846]
 [1.846]
 [1.846]
 [1.846]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.031]
 [0.031]
 [0.104]
 [0.031]
 [0.063]
 [0.031]] [[3.045]
 [3.045]
 [3.045]
 [2.073]
 [3.045]
 [3.343]
 [3.045]] [[-0.6  ]
 [-0.6  ]
 [-0.6  ]
 [-0.778]
 [-0.6  ]
 [-0.435]
 [-0.6  ]]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[3.995]
 [3.995]
 [3.995]
 [3.995]
 [3.995]
 [3.995]
 [3.995]] [[-0.489]
 [-0.489]
 [-0.489]
 [-0.489]
 [-0.489]
 [-0.489]
 [-0.489]]
1327 1254
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2697371337154754, 0.10579231568288831, 0.11326995140262897, 0.11496766023476822, 0.28620772101001823, 0.11002521795422078]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.468]
 [0.59 ]
 [0.611]
 [0.581]
 [0.713]
 [0.615]] [[2.557]
 [2.616]
 [2.361]
 [2.07 ]
 [2.042]
 [1.771]
 [2.101]] [[0.602]
 [0.499]
 [0.658]
 [0.602]
 [0.532]
 [0.707]
 [0.62 ]]
from probs:  [0.27013316025818745, 0.10594763936579503, 0.11343625370824498, 0.11445009688076127, 0.28584609342657485, 0.11018675636043632]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[-1.988]
 [-1.988]
 [-1.988]
 [-1.988]
 [-1.988]
 [-1.988]
 [-1.988]] [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2702181183594653, 0.1059809603777981, 0.1134719299233454, 0.11448609195412718, 0.2859359933164366, 0.1099069060688273]
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [1.115]] [[2.474]
 [2.474]
 [2.474]
 [2.474]
 [2.474]
 [2.474]
 [5.69 ]] [[0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [1.872]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27030838620456604, 0.1060163638991204, 0.11317578035723566, 0.11452433666131945, 0.2860315118039093, 0.10994362107384902]
from probs:  [0.27030838620456604, 0.1060163638991204, 0.11317578035723566, 0.11452433666131945, 0.2860315118039093, 0.10994362107384902]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.562796069230708
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2697422181836703, 0.1060986220391299, 0.11326359349139642, 0.11461319614086682, 0.2862534437706336, 0.1100289263743029]
line 256 mcts: sample exp_bonus 2.552307429714175
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26917103579538937, 0.10618983409367486, 0.11333854381210524, 0.11468476655449306, 0.28650425103530885, 0.11011156870902862]
line 256 mcts: sample exp_bonus 1.0197340478805401
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26945851895593687, 0.10601319736800344, 0.11345959295355791, 0.11480725350431777, 0.28603226587894753, 0.11022917133923649]
from probs:  [0.2688865384102751, 0.10581411609107594, 0.11386434052757019, 0.11453645119815932, 0.2862761588328967, 0.11062239494002275]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.900113563620279
siam score:  -0.7069278
Printing some Q and Qe and total Qs values:  [[0.518]
 [1.284]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[-1.258]
 [ 1.183]
 [-1.258]
 [-1.258]
 [-1.258]
 [-1.258]
 [-1.258]] [[0.786]
 [2.528]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]]
first move QE:  1.0930967821765643
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2675514748146756, 0.10507174874707398, 0.11490450732285859, 0.11490450732285859, 0.28656616960611536, 0.11100159218641778]
siam score:  -0.7125661
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2677980634559421, 0.10518203803608948, 0.11499443309206327, 0.11499443309206327, 0.286862149365429, 0.11016888295841287]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
from probs:  [0.26688870866361486, 0.10540891021199837, 0.11524247009286805, 0.11457250345684034, 0.287480896075681, 0.11040651149899744]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.7068924
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.7056285
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1344 1307
from probs:  [0.26713857056741347, 0.10524043722349068, 0.1153326438654965, 0.11400376384228865, 0.28777726867397524, 0.11050731582733547]
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[6.19 ]
 [2.661]
 [2.661]
 [2.661]
 [2.661]
 [2.661]
 [2.661]] [[1.567]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2674175146669215, 0.10507262430488425, 0.11545307335053344, 0.11412280572073384, 0.2873112752160405, 0.11062270674088653]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.2675051580988732, 0.10510706081294066, 0.1154909119475693, 0.11383246827708837, 0.2874054386302422, 0.11065896223328626]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.2675873928978101, 0.10513937218246425, 0.11552641545707373, 0.11386746195798703, 0.2874937910516575, 0.11038556645300737]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26786576382070043, 0.10497209479868222, 0.11564659748261963, 0.11398591817570777, 0.28702922526430746, 0.11050040045798247]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.344]
 [0.344]
 [0.345]
 [0.344]
 [0.341]
 [0.369]] [[2.994]
 [3.148]
 [2.656]
 [2.489]
 [2.449]
 [2.739]
 [3.653]] [[0.693]
 [0.818]
 [0.418]
 [0.283]
 [0.25 ]
 [0.482]
 [1.252]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[4.133]
 [3.505]
 [3.505]
 [3.505]
 [3.505]
 [3.505]
 [3.505]] [[1.312]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.518]
 [0.573]
 [0.574]
 [0.575]
 [0.575]
 [0.577]] [[-2.347]
 [ 0.869]
 [-2.689]
 [-2.785]
 [-2.741]
 [-2.729]
 [-2.509]] [[0.418]
 [1.445]
 [0.307]
 [0.276]
 [0.291]
 [0.295]
 [0.368]]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.357]
 [0.463]
 [0.489]
 [0.412]
 [0.519]
 [0.552]] [[2.616]
 [2.36 ]
 [1.81 ]
 [2.386]
 [2.309]
 [1.961]
 [2.494]] [[1.741]
 [1.55 ]
 [1.41 ]
 [1.63 ]
 [1.561]
 [1.494]
 [1.702]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.6936637625340536
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.537]
 [0.632]
 [0.648]
 [0.651]
 [0.635]
 [0.638]] [[2.222]
 [2.424]
 [2.325]
 [2.172]
 [2.088]
 [2.134]
 [2.291]] [[1.093]
 [1.202]
 [1.197]
 [1.021]
 [0.917]
 [0.956]
 [1.161]]
line 256 mcts: sample exp_bonus 3.4470369876328775
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.207]
 [0.344]
 [0.404]
 [0.344]
 [0.318]
 [0.31 ]] [[2.014]
 [2.227]
 [2.97 ]
 [2.357]
 [2.97 ]
 [1.483]
 [2.502]] [[ 0.115]
 [ 0.415]
 [ 1.59 ]
 [ 0.852]
 [ 1.59 ]
 [-0.43 ]
 [ 0.919]]
from probs:  [0.26921207858958585, 0.10522315979757316, 0.11589003166298642, 0.11423094085720785, 0.2846955127750187, 0.11074827631762799]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26895754695135216, 0.10541404148370592, 0.11610026375155293, 0.11411162668729619, 0.2844673402000072, 0.11094918092608562]
first move QE:  1.073661764419044
using another actor
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]] [[2.388]
 [1.965]
 [1.965]
 [1.965]
 [1.965]
 [1.965]
 [1.965]] [[1.954]
 [1.816]
 [1.816]
 [1.816]
 [1.816]
 [1.816]
 [1.816]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.426]
 [0.605]
 [0.62 ]
 [0.605]
 [0.605]
 [0.605]] [[2.382]
 [2.416]
 [2.226]
 [1.98 ]
 [2.226]
 [2.226]
 [2.226]] [[1.812]
 [1.417]
 [1.52 ]
 [1.221]
 [1.52 ]
 [1.52 ]
 [1.52 ]]
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]] [[2.481]
 [2.481]
 [2.481]
 [2.481]
 [2.481]
 [2.481]
 [2.481]] [[0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]]
using another actor
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]] [[4.014]
 [3.821]
 [3.821]
 [3.821]
 [3.821]
 [3.821]
 [3.821]] [[0.575]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.309]
 [0.546]
 [0.705]
 [0.359]
 [0.552]
 [0.323]] [[ 2.238]
 [ 2.098]
 [-1.742]
 [ 1.387]
 [ 1.54 ]
 [-1.366]
 [ 1.714]] [[0.283]
 [0.309]
 [0.546]
 [0.705]
 [0.359]
 [0.552]
 [0.323]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26982618861473195, 0.10520314099496705, 0.11580226779773248, 0.1138307502860325, 0.2846429816331318, 0.11069467067340434]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[3.975]
 [3.975]
 [3.975]
 [3.975]
 [3.975]
 [3.975]
 [3.975]] [[1.536]
 [1.536]
 [1.536]
 [1.536]
 [1.536]
 [1.536]
 [1.536]]
line 256 mcts: sample exp_bonus 3.8352082973846677
1363 1341
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.703]
 [0.703]
 [0.783]
 [0.703]
 [0.703]
 [0.703]] [[3.305]
 [3.305]
 [3.305]
 [3.892]
 [3.305]
 [3.305]
 [3.305]] [[1.553]
 [1.553]
 [1.553]
 [2.14 ]
 [1.553]
 [1.553]
 [1.553]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.1063624574291864
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2696233861399785, 0.1051705741359541, 0.11633532894153957, 0.11339916898209183, 0.2845566025857334, 0.11091493921470261]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26895021105599204, 0.10493866942431299, 0.11634374069808004, 0.11373388457135988, 0.2853965164208131, 0.11063697782944197]
start point for exploration sampling:  10935
rdn beta is 0 so we're just using the maxi policy
in main func line 156:  1367
from probs:  [0.268587457524211, 0.10508174471874494, 0.11616892357203847, 0.11388895142241877, 0.2857856312327387, 0.11048729152984813]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[2.032]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]] [[1.24 ]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6730726
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26825411800259436, 0.10523556818648269, 0.11633897695966579, 0.11405566728245423, 0.28546664168437136, 0.11064902788443151]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.433]] [[2.661]
 [2.661]
 [2.661]
 [2.661]
 [2.661]
 [2.661]
 [2.773]] [[1.615]
 [1.615]
 [1.615]
 [1.615]
 [1.615]
 [1.615]
 [1.602]]
siam score:  -0.6715155
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.718]
 [0.732]
 [0.718]
 [0.694]
 [0.732]
 [0.692]] [[ 1.474]
 [-0.012]
 [-0.481]
 [-0.8  ]
 [-1.033]
 [-0.481]
 [ 0.43 ]] [[0.53 ]
 [0.718]
 [0.732]
 [0.718]
 [0.694]
 [0.732]
 [0.692]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.268975735279181, 0.105248167891853, 0.11631908143895989, 0.11404298048161061, 0.2847674451710649, 0.11064658973733074]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26844573606256117, 0.10532447376735585, 0.11640341382711136, 0.1141256626759415, 0.28497390414943696, 0.11072680951759321]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.708]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]] [[3.657]
 [3.964]
 [3.657]
 [3.657]
 [3.657]
 [3.657]
 [3.657]] [[0.706]
 [0.708]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]]
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]] [[2.769]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]] [[1.236]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26820223298906115, 0.10551234672674654, 0.11627932297574906, 0.114329234791922, 0.2847525436011227, 0.1109243189153986]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26820223298906115, 0.10551234672674654, 0.11627932297574906, 0.114329234791922, 0.2847525436011227, 0.1109243189153986]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.418]
 [0.472]
 [0.477]
 [0.478]
 [0.478]
 [0.468]] [[2.805]
 [3.064]
 [2.907]
 [2.839]
 [2.805]
 [2.66 ]
 [2.796]] [[0.238]
 [0.349]
 [0.3  ]
 [0.243]
 [0.21 ]
 [0.065]
 [0.182]]
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.623]
 [0.637]
 [0.659]
 [0.638]
 [0.659]
 [0.64 ]] [[1.917]
 [2.395]
 [1.85 ]
 [2.222]
 [1.677]
 [2.222]
 [1.633]] [[0.609]
 [0.733]
 [0.58 ]
 [0.747]
 [0.524]
 [0.747]
 [0.512]]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.222]
 [0.41 ]
 [0.41 ]
 [0.372]
 [0.363]
 [0.365]] [[3.229]
 [3.18 ]
 [2.904]
 [2.904]
 [2.556]
 [2.578]
 [2.678]] [[ 0.495]
 [ 0.177]
 [ 0.25 ]
 [ 0.25 ]
 [-0.058]
 [-0.054]
 [ 0.021]]
from probs:  [0.26820223298906115, 0.10551234672674654, 0.11627932297574906, 0.114329234791922, 0.2847525436011227, 0.1109243189153986]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[5.137]
 [5.137]
 [5.137]
 [5.137]
 [5.137]
 [5.137]
 [5.137]] [[1.556]
 [1.556]
 [1.556]
 [1.556]
 [1.556]
 [1.556]
 [1.556]]
siam score:  -0.66133964
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.53 ]
 [0.529]] [[2.565]
 [2.565]
 [2.565]
 [2.565]
 [2.565]
 [2.519]
 [2.712]] [[1.628]
 [1.628]
 [1.628]
 [1.628]
 [1.628]
 [1.523]
 [1.817]]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[5.09]
 [5.09]
 [5.09]
 [5.09]
 [5.09]
 [5.09]
 [5.09]] [[0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]]
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.561]
 [0.561]
 [0.561]
 [0.437]
 [0.458]
 [0.452]] [[2.106]
 [2.792]
 [2.792]
 [2.792]
 [1.978]
 [1.142]
 [2.622]] [[1.248]
 [1.964]
 [1.964]
 [1.964]
 [0.909]
 [0.022]
 [1.633]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2681705715518461, 0.10551282439768651, 0.11591854627604956, 0.11430349598862967, 0.28548363759636236, 0.1106109241894257]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2676459430441905, 0.10558846365657731, 0.11600164511244286, 0.11438543703963046, 0.28568829301049337, 0.11069021813666552]
1379 1363
using another actor
siam score:  -0.66103137
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26827527747250235, 0.10556692388425554, 0.11594662768835771, 0.11465439943378361, 0.2849037927728353, 0.11065297874826559]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.572]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[2.356]
 [2.827]
 [2.055]
 [2.055]
 [2.055]
 [2.055]
 [2.055]] [[1.066]
 [0.962]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]]
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.379]
 [0.414]
 [0.412]
 [0.41 ]
 [0.41 ]
 [0.419]] [[-0.757]
 [ 0.302]
 [-0.976]
 [-0.867]
 [-0.771]
 [-0.771]
 [-0.807]] [[0.413]
 [0.379]
 [0.414]
 [0.412]
 [0.41 ]
 [0.41 ]
 [0.419]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2686024839589653, 0.10542695962409064, 0.11608804395682852, 0.11384329342232427, 0.2852512805096533, 0.11078793852813792]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2686024839589653, 0.10542695962409064, 0.11608804395682852, 0.11384329342232427, 0.2852512805096533, 0.11078793852813792]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[1.03]
 [0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]] [[4.612]
 [4.428]
 [4.428]
 [4.428]
 [4.428]
 [4.428]
 [4.428]] [[1.855]
 [1.294]
 [1.294]
 [1.294]
 [1.294]
 [1.294]
 [1.294]]
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.521]
 [0.587]
 [0.585]
 [0.574]
 [0.574]
 [0.606]] [[2.664]
 [2.748]
 [2.577]
 [2.493]
 [2.63 ]
 [2.544]
 [2.474]] [[1.572]
 [1.541]
 [1.557]
 [1.499]
 [1.567]
 [1.51 ]
 [1.528]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.479]
 [0.473]
 [0.45 ]
 [0.457]
 [0.457]
 [0.456]] [[3.032]
 [2.582]
 [2.763]
 [2.916]
 [2.907]
 [2.816]
 [2.81 ]] [[1.015]
 [0.498]
 [0.714]
 [0.86 ]
 [0.863]
 [0.749]
 [0.739]]
siam score:  -0.65820265
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2686024839589653, 0.10542695962409064, 0.11608804395682852, 0.11384329342232427, 0.2852512805096533, 0.11078793852813792]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26884170033926924, 0.10525330799553974, 0.11586486908353358, 0.11394468183903946, 0.28550532424668373, 0.1105901164959343]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.128467294101108
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26892604867674463, 0.10528633092885702, 0.11590122136469631, 0.11366668444167222, 0.28559490074992033, 0.11062481383810953]
siam score:  -0.6504995
first move QE:  1.0527456429090725
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 5.6334758588168015
siam score:  -0.6475714
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65041596
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.026]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]] [[2.405]
 [2.359]
 [2.359]
 [2.359]
 [2.359]
 [2.359]
 [2.359]] [[1.551]
 [1.094]
 [1.094]
 [1.094]
 [1.094]
 [1.094]
 [1.094]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26949217899182143, 0.10525620162607902, 0.11612911934886283, 0.1138967650805333, 0.28407192990508, 0.11115380504762343]
siam score:  -0.6542838
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.552]
 [0.566]
 [0.531]
 [0.531]
 [0.545]
 [0.595]] [[4.994]
 [4.212]
 [4.425]
 [4.81 ]
 [4.81 ]
 [4.933]
 [4.327]] [[1.897]
 [1.284]
 [1.426]
 [1.582]
 [1.582]
 [1.674]
 [1.421]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.471]
 [0.471]
 [0.55 ]
 [0.471]
 [0.471]
 [0.259]] [[2.353]
 [2.353]
 [2.353]
 [2.673]
 [2.353]
 [2.353]
 [2.804]] [[1.514]
 [1.514]
 [1.514]
 [1.742]
 [1.514]
 [1.514]
 [1.527]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2708686578554404, 0.10447727308878037, 0.11607195869280595, 0.11323598327434942, 0.28480626291072497, 0.11053986417789896]
line 256 mcts: sample exp_bonus -1.5968222506182959
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2708686578554404, 0.10447727308878037, 0.11607195869280595, 0.11323598327434942, 0.28480626291072497, 0.11053986417789896]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.52 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.526]
 [0.53 ]] [[2.512]
 [2.953]
 [5.224]
 [5.224]
 [5.224]
 [2.475]
 [2.441]] [[0.966]
 [1.595]
 [4.524]
 [4.524]
 [4.524]
 [0.968]
 [0.929]]
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[4.791]
 [4.791]
 [4.791]
 [4.791]
 [4.791]
 [4.791]
 [4.791]] [[7.621]
 [7.621]
 [7.621]
 [7.621]
 [7.621]
 [7.621]
 [7.621]]
from probs:  [0.2708686578554404, 0.10447727308878037, 0.11607195869280595, 0.11323598327434942, 0.28480626291072497, 0.11053986417789896]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27119814132213826, 0.10460435879297728, 0.11621314812254399, 0.11215732735052662, 0.2851527000200754, 0.11067432439173841]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27119814132213826, 0.10460435879297728, 0.11621314812254399, 0.11215732735052662, 0.2851527000200754, 0.11067432439173841]
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.632]
 [0.622]
 [0.634]
 [0.629]
 [0.627]
 [0.624]] [[3.996]
 [3.295]
 [2.966]
 [3.03 ]
 [2.911]
 [3.82 ]
 [3.218]] [[0.616]
 [0.632]
 [0.622]
 [0.634]
 [0.629]
 [0.627]
 [0.624]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27119814132213826, 0.10460435879297728, 0.11621314812254399, 0.11215732735052662, 0.2851527000200754, 0.11067432439173841]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27119814132213826, 0.10460435879297728, 0.11621314812254399, 0.11215732735052662, 0.2851527000200754, 0.11067432439173841]
first move QE:  1.0412238487057632
using explorer policy with actor:  1
first move QE:  1.04041266400789
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.121]] [[5.402]
 [5.402]
 [5.402]
 [5.402]
 [5.402]
 [5.402]
 [5.64 ]] [[0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.716]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2712794945226691, 0.10463573769305304, 0.11624800939223204, 0.11189099498321875, 0.28523823926703906, 0.11070752414178815]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27134994512080307, 0.10440321386024973, 0.11627819870605387, 0.11192005279142593, 0.2853123149158544, 0.11073627460561311]
siam score:  -0.6360619
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[5.476]
 [5.476]
 [5.476]
 [5.476]
 [5.476]
 [5.476]
 [5.476]] [[0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27134994512080307, 0.10440321386024973, 0.11627819870605387, 0.11192005279142593, 0.2853123149158544, 0.11073627460561311]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.428]
 [0.428]
 [0.395]
 [0.398]
 [0.394]
 [0.4  ]] [[5.527]
 [5.252]
 [5.252]
 [5.458]
 [5.316]
 [5.392]
 [5.295]] [[1.221]
 [1.078]
 [1.078]
 [1.149]
 [1.06 ]
 [1.104]
 [1.051]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27142006982441813, 0.10417176564986343, 0.11630824836837284, 0.11194897618231545, 0.2853860479011781, 0.11076489207385211]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.379]] [[-1.918]
 [-1.918]
 [-1.918]
 [-1.918]
 [-1.918]
 [-1.918]
 [-1.901]] [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.379]]
line 256 mcts: sample exp_bonus 2.9110885451740183
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27131843073318523, 0.10415244799109792, 0.11657439449067758, 0.11190598982521817, 0.28532302867843873, 0.11072570828138253]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27158069426805465, 0.1042531244880894, 0.11572045205311848, 0.11201416109978035, 0.28559882942621184, 0.11083273866474543]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.263]
 [0.269]
 [0.273]
 [0.272]
 [0.272]
 [0.276]] [[-2.521]
 [ 0.12 ]
 [-2.978]
 [-2.904]
 [-3.13 ]
 [-3.15 ]
 [-3.009]] [[0.269]
 [0.263]
 [0.269]
 [0.273]
 [0.272]
 [0.272]
 [0.276]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.424]
 [0.421]
 [0.417]
 [0.42 ]
 [0.42 ]
 [0.414]] [[-0.471]
 [ 0.481]
 [-0.564]
 [-0.636]
 [-0.489]
 [-0.429]
 [-0.553]] [[0.42 ]
 [0.424]
 [0.421]
 [0.417]
 [0.42 ]
 [0.42 ]
 [0.414]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2717464238702192, 0.10431674399128123, 0.11547233812571486, 0.11208251670364938, 0.2857731134654623, 0.11060886384367295]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2717464238702192, 0.10431674399128123, 0.11547233812571486, 0.11208251670364938, 0.2857731134654623, 0.11060886384367295]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.627]
 [0.644]
 [0.654]
 [0.658]
 [0.656]
 [0.664]] [[2.463]
 [2.8  ]
 [2.264]
 [2.135]
 [2.28 ]
 [2.231]
 [2.38 ]] [[1.185]
 [1.449]
 [0.947]
 [0.839]
 [0.992]
 [0.939]
 [1.103]]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.701]
 [0.586]] [[ 2.14 ]
 [ 2.14 ]
 [ 2.14 ]
 [ 2.14 ]
 [ 2.14 ]
 [-0.202]
 [ 2.14 ]] [[2.053]
 [2.053]
 [2.053]
 [2.053]
 [2.053]
 [0.977]
 [2.053]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27146317971047795, 0.1042276466897639, 0.11502534551314571, 0.11226312523403255, 0.28623360510647783, 0.11078709774610197]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.25 ]
 [0.242]
 [0.242]
 [0.242]
 [0.264]
 [0.272]] [[3.184]
 [3.564]
 [3.363]
 [3.363]
 [3.363]
 [3.138]
 [3.254]] [[-0.271]
 [-0.166]
 [-0.248]
 [-0.248]
 [-0.248]
 [-0.28 ]
 [-0.224]]
siam score:  -0.6555221
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using another actor
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.393]
 [0.355]] [[2.956]
 [2.956]
 [2.956]
 [2.956]
 [2.956]
 [2.846]
 [2.956]] [[0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.371]
 [0.367]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27163302430483316, 0.10405702306291131, 0.11508735367879229, 0.11233152409707504, 0.2857409781549267, 0.11115009670146148]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2718259765285904, 0.10413093909005468, 0.11516910500809215, 0.11241131784611584, 0.2852336102936131, 0.11122905123353381]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2719111733833525, 0.10416357625228324, 0.11489177754214347, 0.11244655027990477, 0.28532300942602795, 0.11126391311628797]
line 256 mcts: sample exp_bonus 10.0
from probs:  [0.27226422704799247, 0.10429882384751914, 0.11504095479953062, 0.11229406952066534, 0.28498556573946904, 0.11111635904482334]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2725353455210784, 0.1044026837565964, 0.1151555116340651, 0.11240589102948104, 0.28456428756020336, 0.11093628049857568]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.615]
 [0.654]
 [0.654]
 [0.654]
 [0.685]
 [0.654]] [[1.552]
 [1.658]
 [1.552]
 [1.552]
 [1.552]
 [0.67 ]
 [1.552]] [[0.654]
 [0.615]
 [0.654]
 [0.654]
 [0.654]
 [0.685]
 [0.654]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.398]
 [0.394]
 [0.391]
 [0.391]
 [0.39 ]
 [0.39 ]] [[-0.098]
 [ 0.637]
 [-0.366]
 [-0.409]
 [-0.409]
 [-0.18 ]
 [-0.353]] [[0.394]
 [0.398]
 [0.394]
 [0.391]
 [0.391]
 [0.39 ]
 [0.39 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.0119960037135172
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27270156313429905, 0.10446635830447816, 0.11491317487671743, 0.11217712228760153, 0.2847378415504047, 0.11100393984649913]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.529]
 [0.396]
 [0.396]
 [0.396]
 [0.604]
 [0.578]] [[ 1.403]
 [ 1.718]
 [ 1.249]
 [ 1.249]
 [ 1.249]
 [-0.042]
 [ 1.951]] [[0.807]
 [1.058]
 [0.713]
 [0.713]
 [0.713]
 [0.094]
 [1.219]]
siam score:  -0.647095
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.43 ]
 [0.424]
 [0.421]
 [0.413]
 [0.43 ]
 [0.442]] [[5.151]
 [4.197]
 [4.251]
 [4.136]
 [4.347]
 [4.164]
 [3.981]] [[0.694]
 [0.43 ]
 [0.424]
 [0.421]
 [0.413]
 [0.43 ]
 [0.442]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.6450228
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[3.154]
 [3.154]
 [3.154]
 [3.154]
 [3.154]
 [3.154]
 [3.154]] [[0.838]
 [0.838]
 [0.838]
 [0.838]
 [0.838]
 [0.838]
 [0.838]]
first move QE:  1.0205669788806315
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.325]
 [0.417]
 [0.424]
 [0.418]
 [0.434]
 [0.426]] [[4.773]
 [4.356]
 [4.517]
 [4.571]
 [4.686]
 [4.771]
 [4.65 ]] [[1.454]
 [1.139]
 [1.338]
 [1.372]
 [1.417]
 [1.477]
 [1.412]]
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[2.764]
 [2.764]
 [2.764]
 [2.764]
 [2.764]
 [2.764]
 [2.764]] [[1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]]
siam score:  -0.65007
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2730765330043407, 0.10386956248287738, 0.11537520297828639, 0.11233126103727788, 0.28447661521600315, 0.11087082528121449]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27331524770124493, 0.1037091308331547, 0.11485312358570586, 0.11242945740236414, 0.28472529549706027, 0.11096774498047003]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[1.632]
 [1.632]
 [1.632]
 [1.632]
 [1.632]
 [1.632]
 [1.632]] [[0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]]
siam score:  -0.6520155
siam score:  -0.66319823
using explorer policy with actor:  1
from probs:  [0.2723055582157353, 0.10389167162091667, 0.11471467836359894, 0.11260270618220361, 0.285915593834546, 0.11056979178299951]
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.314]
 [0.482]
 [0.585]
 [0.375]
 [0.32 ]
 [0.581]] [[2.804]
 [3.16 ]
 [3.002]
 [2.995]
 [2.474]
 [2.843]
 [3.079]] [[1.392]
 [1.393]
 [1.472]
 [1.598]
 [0.888]
 [1.131]
 [1.665]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27261521059281774, 0.10375976636718784, 0.11453805252732026, 0.11243526537233343, 0.2862407228692081, 0.11041098227113279]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.245]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]] [[2.392]
 [1.726]
 [2.392]
 [2.392]
 [2.392]
 [2.392]
 [2.392]] [[0.194]
 [0.245]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.43 ]
 [0.215]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[2.392]
 [1.755]
 [1.542]
 [2.392]
 [2.392]
 [2.392]
 [2.392]] [[0.283]
 [0.43 ]
 [0.215]
 [0.283]
 [0.283]
 [0.283]
 [0.283]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [0.192]
 [0.321]
 [0.314]
 [0.145]
 [0.321]
 [0.289]] [[ 1.712]
 [ 2.153]
 [ 0.036]
 [-0.267]
 [ 1.115]
 [ 0.036]
 [ 0.447]] [[0.2  ]
 [0.192]
 [0.321]
 [0.314]
 [0.145]
 [0.321]
 [0.289]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2721750194765327, 0.1038629724195441, 0.11465197934751073, 0.11254710062689292, 0.28652543606841085, 0.11023749206110867]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.432]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.493]] [[3.855]
 [4.24 ]
 [3.413]
 [3.413]
 [3.413]
 [3.413]
 [3.444]] [[0.439]
 [0.432]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.493]]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.445]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[2.623]
 [3.692]
 [2.805]
 [2.805]
 [2.805]
 [2.805]
 [2.805]] [[0.386]
 [0.445]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.392]
 [0.453]
 [0.474]
 [0.469]
 [0.477]
 [0.483]] [[1.131]
 [1.132]
 [1.094]
 [1.196]
 [0.919]
 [1.186]
 [0.805]] [[0.462]
 [0.392]
 [0.453]
 [0.474]
 [0.469]
 [0.477]
 [0.483]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2721750194765327, 0.1038629724195441, 0.11465197934751073, 0.11254710062689292, 0.28652543606841085, 0.11023749206110867]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2722582436180993, 0.10389473105314975, 0.11438126262579185, 0.11258151464421712, 0.2866130481993832, 0.11027119985935878]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2722582436180993, 0.10389473105314975, 0.11438126262579185, 0.11258151464421712, 0.2866130481993832, 0.11027119985935878]
using explorer policy with actor:  0
1443 1443
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[1.122]
 [1.122]
 [1.122]
 [1.122]
 [1.122]
 [1.122]
 [1.122]] [[2.619]
 [2.619]
 [2.619]
 [2.619]
 [2.619]
 [2.619]
 [2.619]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2714787830414565, 0.10440732257285364, 0.11494559216051109, 0.11313696465690855, 0.2866176454537735, 0.10941369211449668]
using explorer policy with actor:  0
from probs:  [0.2714787830414565, 0.10440732257285364, 0.11494559216051109, 0.11313696465690855, 0.2866176454537735, 0.10941369211449668]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.478]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]] [[2.426]
 [3.122]
 [2.426]
 [2.426]
 [2.426]
 [2.426]
 [2.426]] [[0.457]
 [0.478]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.613]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]] [[-0.385]
 [ 1.316]
 [-0.385]
 [-0.385]
 [-0.385]
 [-0.385]
 [-0.385]] [[0.785]
 [1.339]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[2.463]
 [2.463]
 [2.463]
 [2.463]
 [2.463]
 [2.463]
 [2.463]] [[0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
Printing some Q and Qe and total Qs values:  [[0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]] [[3.429]
 [3.429]
 [3.429]
 [3.429]
 [3.429]
 [3.429]
 [3.429]] [[1.564]
 [1.564]
 [1.564]
 [1.564]
 [1.564]
 [1.564]
 [1.564]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2711940928562393, 0.104316785969007, 0.11481590172096526, 0.11331055796505304, 0.2870574213077377, 0.10930524018099765]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2711940928562393, 0.104316785969007, 0.11481590172096526, 0.11331055796505304, 0.2870574213077377, 0.10930524018099765]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.194]
 [0.407]
 [0.29 ]
 [0.292]
 [0.295]
 [0.295]] [[2.317]
 [3.315]
 [3.058]
 [4.028]
 [3.996]
 [3.932]
 [3.703]] [[-0.289]
 [-0.78 ]
 [-0.441]
 [-0.35 ]
 [-0.357]
 [-0.373]
 [-0.45 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2706853406751532, 0.10438960562080375, 0.1148960504133007, 0.11338965583314077, 0.28725780537125284, 0.10938154208634877]
rdn probs:  [0.2706853406751532, 0.10438960562080375, 0.1148960504133007, 0.11338965583314077, 0.28725780537125284, 0.10938154208634877]
siam score:  -0.671427
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2714164878281879, 0.10467157199092819, 0.11490098330246869, 0.11339875158199417, 0.2859352131391836, 0.10967699215723739]
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.219]
 [0.25 ]
 [0.25 ]
 [0.214]
 [0.25 ]
 [0.227]] [[-2.625]
 [-2.488]
 [ 0.025]
 [ 0.025]
 [-2.529]
 [ 0.025]
 [-2.674]] [[0.232]
 [0.219]
 [0.25 ]
 [0.25 ]
 [0.214]
 [0.25 ]
 [0.227]]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[4.328]
 [4.163]
 [4.163]
 [4.163]
 [4.163]
 [4.163]
 [4.163]] [[1.917]
 [1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]]
Printing some Q and Qe and total Qs values:  [[0.824]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]] [[3.079]
 [1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]] [[1.751]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]]
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.639]
 [0.642]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[3.702]
 [2.883]
 [3.363]
 [3.757]
 [3.757]
 [3.757]
 [3.757]] [[1.975]
 [1.491]
 [1.635]
 [1.759]
 [1.759]
 [1.759]
 [1.759]]
Printing some Q and Qe and total Qs values:  [[0.935]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[4.688]
 [3.926]
 [3.926]
 [3.926]
 [3.926]
 [3.926]
 [3.926]] [[2.279]
 [1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]] [[3.398]
 [2.471]
 [2.471]
 [2.471]
 [2.471]
 [2.471]
 [2.471]] [[0.894]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.322]
 [0.302]
 [0.371]
 [0.283]
 [0.371]
 [0.316]] [[1.878]
 [3.017]
 [2.064]
 [1.878]
 [2.   ]
 [1.878]
 [3.557]] [[0.371]
 [0.322]
 [0.302]
 [0.371]
 [0.283]
 [0.371]
 [0.316]]
line 256 mcts: sample exp_bonus 2.7420096208883433
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[4.346]
 [4.417]
 [4.417]
 [4.417]
 [4.417]
 [4.417]
 [4.417]] [[1.761]
 [1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]] [[4.608]
 [4.41 ]
 [4.41 ]
 [4.41 ]
 [4.41 ]
 [4.41 ]
 [4.41 ]] [[1.972]
 [1.186]
 [1.186]
 [1.186]
 [1.186]
 [1.186]
 [1.186]]
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.081]
 [-0.024]
 [-0.024]
 [-0.023]
 [-0.022]
 [-0.02 ]] [[1.335]
 [1.161]
 [1.112]
 [1.11 ]
 [0.953]
 [0.792]
 [0.872]] [[-0.023]
 [-0.181]
 [-0.083]
 [-0.083]
 [-0.134]
 [-0.186]
 [-0.156]]
Printing some Q and Qe and total Qs values:  [[ 0.27 ]
 [-0.059]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]] [[2.267]
 [1.786]
 [1.724]
 [1.724]
 [1.724]
 [1.724]
 [1.724]] [[ 0.389]
 [-0.429]
 [-0.384]
 [-0.384]
 [-0.384]
 [-0.384]
 [-0.384]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2701361754133917, 0.10498160597449037, 0.11493667791627615, 0.11343818548059441, 0.2867821444643186, 0.10972521075092877]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.766]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[1.267]
 [1.868]
 [1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]] [[1.343]
 [1.811]
 [1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.343]]
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.762]] [[3.441]
 [3.405]
 [3.405]
 [3.405]
 [3.405]
 [3.405]
 [3.433]] [[2.246]
 [2.175]
 [2.175]
 [2.175]
 [2.175]
 [2.175]
 [2.284]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26979009381280394, 0.10511465380288493, 0.1147789843946322, 0.11358195070589377, 0.2871455961489173, 0.10958872113486794]
using explorer policy with actor:  1
siam score:  -0.68625367
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26986976619273917, 0.10514569547130273, 0.11481288006042198, 0.1133201803991833, 0.2872303938251766, 0.10962108405117624]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.9511155728840484
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.189]
 [0.39 ]
 [0.491]
 [0.627]
 [0.51 ]
 [0.249]] [[2.722]
 [1.882]
 [2.408]
 [2.373]
 [3.345]
 [2.907]
 [3.712]] [[1.13 ]
 [0.1  ]
 [0.764]
 [0.883]
 [1.771]
 [1.291]
 [1.493]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.446]
 [0.438]
 [0.45 ]
 [0.52 ]
 [0.446]
 [0.449]] [[4.839]
 [5.087]
 [3.637]
 [3.813]
 [3.781]
 [3.113]
 [3.31 ]] [[0.436]
 [0.446]
 [0.438]
 [0.45 ]
 [0.52 ]
 [0.446]
 [0.449]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2704608101180721, 0.10462365512377797, 0.11476187792948275, 0.11298120608823731, 0.2878594593994215, 0.10931299134100844]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2708773162906304, 0.10453609501970959, 0.11463733554327275, 0.11286352325303872, 0.28760439795018594, 0.10948133194316244]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.647]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.688]] [[3.524]
 [3.584]
 [3.924]
 [3.924]
 [3.924]
 [3.924]
 [3.849]] [[2.047]
 [2.046]
 [2.196]
 [2.196]
 [2.196]
 [2.196]
 [2.173]]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.559]
 [0.759]
 [0.763]
 [0.762]
 [0.765]
 [0.766]] [[2.2  ]
 [3.12 ]
 [2.45 ]
 [2.242]
 [2.079]
 [2.401]
 [2.488]] [[1.36 ]
 [1.641]
 [1.496]
 [1.387]
 [1.299]
 [1.475]
 [1.524]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2716072722543859, 0.10456967625314703, 0.11404977120562385, 0.11258741613316732, 0.2876826191061391, 0.10950324504753677]
1466 1477
first move QE:  1.000461596262443
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2716875874153075, 0.1046005978492666, 0.11378779281821343, 0.1126207086012571, 0.28776768779983086, 0.10953562551612449]
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [1.062]] [[4.041]
 [3.677]
 [3.677]
 [3.677]
 [3.677]
 [3.677]
 [1.85 ]] [[0.852]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [1.116]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2711849196891541, 0.10467279124287991, 0.11386632704348768, 0.1126984373266395, 0.287966299723488, 0.10961122497435093]
in main func line 156:  1468
line 256 mcts: sample exp_bonus 1.4630298681232057
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.264]] [[4.133]
 [3.994]
 [3.994]
 [3.994]
 [3.994]
 [3.994]
 [3.78 ]] [[-0.119]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.242]]
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.26 ]
 [0.259]
 [0.26 ]
 [0.26 ]
 [0.259]
 [0.258]] [[4.164]
 [4.487]
 [4.112]
 [4.487]
 [4.487]
 [3.936]
 [3.983]] [[-0.11 ]
 [ 0.008]
 [-0.12 ]
 [ 0.008]
 [ 0.008]
 [-0.179]
 [-0.166]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 4.520268268189595
siam score:  -0.69697833
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.599]
 [0.57 ]
 [0.569]
 [0.564]
 [0.569]
 [0.569]] [[3.716]
 [2.891]
 [2.736]
 [3.046]
 [3.113]
 [3.127]
 [3.283]] [[0.557]
 [0.599]
 [0.57 ]
 [0.569]
 [0.564]
 [0.569]
 [0.569]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27095045429351133, 0.10484776330233384, 0.11405666710711532, 0.11259824342762503, 0.28775241970158466, 0.10979445216782976]
1471 1481
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.27067591722027357, 0.10475877618612423, 0.11393431442638402, 0.11248144947782125, 0.28818864495954744, 0.10996089772984956]
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.096]
 [0.146]
 [0.146]] [[2.772]
 [2.772]
 [2.772]
 [2.772]
 [1.079]
 [2.772]
 [2.772]] [[ 1.042]
 [ 1.042]
 [ 1.042]
 [ 1.042]
 [-0.751]
 [ 1.042]
 [ 1.042]]
from probs:  [0.27075543287134496, 0.10478955085710136, 0.1136740175717396, 0.1125144928153318, 0.28827330527203726, 0.10999320061244502]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27075543287134496, 0.10478955085710136, 0.1136740175717396, 0.1125144928153318, 0.28827330527203726, 0.10999320061244502]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -2.790529668394919
siam score:  -0.6935742
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.091]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[8.139]
 [5.273]
 [7.56 ]
 [7.56 ]
 [7.56 ]
 [7.56 ]
 [7.56 ]] [[1.477]
 [0.63 ]
 [1.305]
 [1.305]
 [1.305]
 [1.305]
 [1.305]]
Printing some Q and Qe and total Qs values:  [[-0.076]
 [-0.1  ]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]] [[2.949]
 [1.808]
 [2.094]
 [2.094]
 [2.094]
 [2.094]
 [2.094]] [[ 0.33 ]
 [-0.577]
 [-0.362]
 [-0.362]
 [-0.362]
 [-0.362]
 [-0.362]]
line 256 mcts: sample exp_bonus 2.1954828603945584
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[3.982]
 [3.982]
 [3.982]
 [3.982]
 [3.982]
 [3.982]
 [3.982]] [[1.534]
 [1.534]
 [1.534]
 [1.534]
 [1.534]
 [1.534]
 [1.534]]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.339]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[1.134]
 [1.418]
 [1.134]
 [1.134]
 [1.134]
 [1.134]
 [1.134]] [[1.452]
 [1.536]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.323]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[1.834]
 [2.163]
 [1.834]
 [1.834]
 [1.834]
 [1.834]
 [1.834]] [[1.746]
 [1.847]
 [1.746]
 [1.746]
 [1.746]
 [1.746]
 [1.746]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]] [[4.372]
 [3.37 ]
 [3.37 ]
 [3.37 ]
 [3.37 ]
 [3.37 ]
 [3.37 ]] [[ 0.123]
 [-0.283]
 [-0.283]
 [-0.283]
 [-0.283]
 [-0.283]
 [-0.283]]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[7.588]
 [5.127]
 [5.127]
 [5.127]
 [5.127]
 [5.127]
 [5.127]] [[1.213]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27014904073692236, 0.10508288162634223, 0.11311782695775077, 0.1125425898865312, 0.2890802505227366, 0.11002741026971674]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6911978
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.638]] [[3.137]
 [3.357]
 [3.357]
 [3.357]
 [3.357]
 [3.357]
 [2.489]] [[1.808]
 [2.032]
 [2.032]
 [2.032]
 [2.032]
 [2.032]
 [1.144]]
line 256 mcts: sample exp_bonus 2.575856114016071
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26937925383075206, 0.10506186717500374, 0.11336218590594811, 0.11249974192235931, 0.2897047263269681, 0.10999222483896877]
using explorer policy with actor:  1
siam score:  -0.6847327
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2692627285883831, 0.10527887371972813, 0.11359633685840431, 0.11273211148643998, 0.2889105342505573, 0.11021941509648718]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7994114953149
using explorer policy with actor:  1
siam score:  -0.68608415
using explorer policy with actor:  1
1485 1490
start point for exploration sampling:  10935
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.779911053515989
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
line 256 mcts: sample exp_bonus 3.734253239870165
siam score:  -0.690625
from probs:  [0.26888777057234475, 0.10565638333565382, 0.11371303350545912, 0.11256569903101565, 0.28856247302025484, 0.11061464053527177]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2687377242504382, 0.10585930473255166, 0.1139314283328792, 0.1122161020678096, 0.28842835596110034, 0.11082708465522109]
using another actor
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.536]
 [0.508]
 [0.511]
 [0.51 ]
 [0.521]
 [0.506]] [[3.738]
 [3.69 ]
 [3.888]
 [3.844]
 [3.974]
 [3.872]
 [3.989]] [[0.517]
 [0.536]
 [0.508]
 [0.511]
 [0.51 ]
 [0.521]
 [0.506]]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.434]
 [0.439]
 [0.454]
 [0.444]
 [0.443]
 [0.443]] [[2.397]
 [2.718]
 [2.424]
 [1.879]
 [2.087]
 [2.136]
 [2.291]] [[0.439]
 [0.434]
 [0.439]
 [0.454]
 [0.444]
 [0.443]
 [0.443]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26905641233506294, 0.10574905335751572, 0.11405797802081397, 0.11206511375676032, 0.28811298892850995, 0.11095845360133712]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26905641233506294, 0.10574905335751572, 0.11405797802081397, 0.11206511375676032, 0.28811298892850995, 0.11095845360133712]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[2.411]
 [1.972]
 [1.972]
 [1.972]
 [1.972]
 [1.972]
 [1.972]] [[0.702]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26930635285712823, 0.10559989823733724, 0.1141639324202086, 0.11216921688069104, 0.2876990709136538, 0.11106152869098093]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.26930635285712823, 0.10559989823733724, 0.1141639324202086, 0.11216921688069104, 0.2876990709136538, 0.11106152869098093]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.683]
 [0.346]
 [0.347]
 [0.151]
 [0.337]
 [0.319]] [[ 1.684]
 [ 0.936]
 [-0.071]
 [ 1.318]
 [ 1.077]
 [-0.459]
 [ 1.406]] [[0.253]
 [0.683]
 [0.346]
 [0.347]
 [0.151]
 [0.337]
 [0.319]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2701334655285136, 0.10569166588417882, 0.1145060459441525, 0.11195381016609515, 0.2865859043480921, 0.1111291081289678]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2702117016047156, 0.10572227631302686, 0.11424958911333957, 0.11198623423767816, 0.28666890538095985, 0.1111612933502799]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.353]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.515]] [[2.023]
 [1.446]
 [2.478]
 [2.478]
 [2.478]
 [2.478]
 [1.945]] [[1.249]
 [0.533]
 [1.653]
 [1.653]
 [1.653]
 [1.653]
 [1.179]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.301]
 [0.271]
 [0.449]
 [0.775]
 [0.446]
 [0.292]] [[1.494]
 [1.788]
 [1.348]
 [0.636]
 [3.482]
 [1.054]
 [2.092]] [[ 0.279]
 [ 0.398]
 [ 0.08 ]
 [-0.146]
 [ 2.076]
 [ 0.116]
 [ 0.579]]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]] [[6.077]
 [4.392]
 [4.392]
 [4.392]
 [4.392]
 [4.392]
 [4.392]] [[1.085]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]]
siam score:  -0.6839916
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.534]] [[4.959]
 [3.302]
 [3.302]
 [3.302]
 [3.302]
 [3.302]
 [3.107]] [[0.704]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.202]]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[7.725]
 [3.918]
 [3.918]
 [3.918]
 [3.918]
 [3.918]
 [3.918]] [[1.531]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]]
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.103]
 [0.105]
 [0.105]
 [0.093]
 [0.105]
 [0.105]] [[4.155]
 [3.807]
 [3.445]
 [3.445]
 [3.695]
 [3.445]
 [3.445]] [[0.664]
 [0.356]
 [0.062]
 [0.062]
 [0.247]
 [0.062]
 [0.062]]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.004]
 [-0.008]
 [-0.005]
 [-0.005]
 [-0.007]
 [-0.006]] [[3.436]
 [3.319]
 [3.355]
 [3.447]
 [3.356]
 [3.308]
 [3.325]] [[-0.089]
 [-0.206]
 [-0.178]
 [-0.082]
 [-0.172]
 [-0.223]
 [-0.203]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.27051618481213924, 0.10587010487423522, 0.11436311832386854, 0.11210953738934308, 0.28639376704996494, 0.11074728755044905]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27084637494689107, 0.10599932917265895, 0.11421552629860014, 0.1122463774948002, 0.28607858704419425, 0.11061380504285521]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27106482826623834, 0.10583997542676667, 0.11402172634571638, 0.11206112243887129, 0.2863093260265648, 0.11070302149584256]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.132]
 [0.126]
 [0.126]
 [0.126]
 [0.13 ]
 [0.123]] [[3.742]
 [3.863]
 [4.48 ]
 [4.48 ]
 [4.48 ]
 [4.206]
 [3.876]] [[0.207]
 [0.26 ]
 [0.454]
 [0.454]
 [0.454]
 [0.371]
 [0.247]]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.52 ]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[2.603]
 [1.911]
 [2.301]
 [2.301]
 [2.301]
 [2.301]
 [2.301]] [[2.127]
 [1.639]
 [1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.708]
 [0.54 ]
 [0.585]
 [0.557]
 [0.695]
 [0.57 ]] [[3.8  ]
 [3.506]
 [3.495]
 [3.435]
 [3.544]
 [3.612]
 [3.65 ]] [[1.229]
 [1.14 ]
 [1.032]
 [1.023]
 [1.072]
 [1.197]
 [1.144]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.133]
 [0.091]
 [0.081]
 [0.067]
 [0.082]
 [0.115]] [[0.612]
 [1.125]
 [0.702]
 [0.536]
 [0.582]
 [0.683]
 [0.69 ]] [[0.065]
 [0.133]
 [0.091]
 [0.081]
 [0.067]
 [0.082]
 [0.115]]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.186]
 [0.203]
 [0.206]
 [0.221]
 [0.204]
 [0.215]] [[2.827]
 [3.193]
 [2.692]
 [2.512]
 [3.042]
 [2.537]
 [2.566]] [[0.584]
 [0.909]
 [0.476]
 [0.316]
 [0.805]
 [0.337]
 [0.373]]
line 256 mcts: sample exp_bonus 2.42737722178988
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65627813
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27080516290829804, 0.10528486217753223, 0.11418478672625677, 0.11195249749934467, 0.2874358125172013, 0.11033687817136706]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]] [[2.812]
 [1.945]
 [1.945]
 [1.945]
 [1.945]
 [1.945]
 [1.945]] [[2.217]
 [1.883]
 [1.883]
 [1.883]
 [1.883]
 [1.883]
 [1.883]]
from probs:  [0.2704677624746559, 0.10540987797008085, 0.11432037033547028, 0.11208543047672823, 0.2877771152948798, 0.10993944344818486]
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.502]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[3.833]
 [3.662]
 [3.833]
 [3.833]
 [3.833]
 [3.833]
 [3.833]] [[0.493]
 [0.502]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]]
using explorer policy with actor:  0
siam score:  -0.65504164
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.631]
 [0.791]
 [0.799]
 [0.795]
 [0.792]
 [1.268]] [[1.216]
 [1.262]
 [1.014]
 [1.097]
 [1.165]
 [1.079]
 [1.385]] [[1.197]
 [0.981]
 [1.109]
 [1.169]
 [1.2  ]
 [1.147]
 [2.112]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27024289268343493, 0.1055781980753514, 0.11450291885122606, 0.11199097212503052, 0.28757002183207153, 0.11011499643288557]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27024289268343493, 0.1055781980753514, 0.11450291885122606, 0.11199097212503052, 0.28757002183207153, 0.11011499643288557]
line 256 mcts: sample exp_bonus 4.604438867267092
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27024289268343493, 0.1055781980753514, 0.11450291885122606, 0.11199097212503052, 0.28757002183207153, 0.11011499643288557]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27024289268343493, 0.1055781980753514, 0.11450291885122606, 0.11199097212503052, 0.28757002183207153, 0.11011499643288557]
using explorer policy with actor:  1
using another actor
line 256 mcts: sample exp_bonus 2.5693324644297477
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.834]
 [0.678]
 [0.678]
 [0.678]
 [0.692]
 [0.678]] [[2.958]
 [3.189]
 [2.617]
 [2.617]
 [2.617]
 [3.255]
 [2.617]] [[1.542]
 [1.817]
 [1.195]
 [1.195]
 [1.195]
 [1.704]
 [1.195]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27044980654302697, 0.10517846904155893, 0.11430549609810925, 0.11207671900278096, 0.28779020236125913, 0.11019930695326466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27044980654302697, 0.10517846904155893, 0.11430549609810925, 0.11207671900278096, 0.28779020236125913, 0.11019930695326466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27044980654302697, 0.10517846904155893, 0.11430549609810925, 0.11207671900278096, 0.28779020236125913, 0.11019930695326466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27044980654302697, 0.10517846904155893, 0.11430549609810925, 0.11207671900278096, 0.28779020236125913, 0.11019930695326466]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.617]
 [0.662]
 [0.672]
 [0.673]
 [0.667]
 [0.716]] [[2.658]
 [2.87 ]
 [2.373]
 [2.282]
 [2.52 ]
 [2.424]
 [2.417]] [[0.932]
 [1.037]
 [0.628]
 [0.559]
 [0.799]
 [0.689]
 [0.781]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27044980654302697, 0.10517846904155893, 0.11430549609810925, 0.11207671900278096, 0.28779020236125913, 0.11019930695326466]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6509296
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27074129548488, 0.10505299371090515, 0.11386229544285723, 0.11192495569030894, 0.2881003806624909, 0.1103180790085578]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.577]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.561]] [[3.391]
 [3.782]
 [3.391]
 [3.391]
 [3.391]
 [3.391]
 [3.434]] [[1.386]
 [1.663]
 [1.386]
 [1.386]
 [1.386]
 [1.386]
 [1.4  ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27092138280496214, 0.10512287116373374, 0.11393803252466433, 0.1119994041237645, 0.28762685077712796, 0.11039145860574735]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27092138280496214, 0.10512287116373374, 0.11393803252466433, 0.1119994041237645, 0.28762685077712796, 0.11039145860574735]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.828]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.696]] [[2.161]
 [2.978]
 [2.871]
 [2.871]
 [2.871]
 [2.871]
 [2.692]] [[0.708]
 [0.828]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.696]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27092138280496214, 0.10512287116373374, 0.11393803252466433, 0.1119994041237645, 0.28762685077712796, 0.11039145860574735]
Printing some Q and Qe and total Qs values:  [[ 0.063]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]] [[3.357]
 [3.585]
 [3.585]
 [3.585]
 [3.585]
 [3.585]
 [3.585]] [[-0.111]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]]
line 256 mcts: sample exp_bonus 2.94957598768624
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]] [[3.356]
 [2.743]
 [2.743]
 [2.743]
 [2.743]
 [2.743]
 [2.743]] [[0.636]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]]
first move QE:  0.9822561508598276
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27130338810245075, 0.1047958119705179, 0.11409868773599988, 0.11188557707250711, 0.28736942205114585, 0.11054711306737855]
using another actor
start point for exploration sampling:  10935
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.23138751479773617
first move QE:  0.9885890564709134
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27160138531413947, 0.10516524030273326, 0.1131022714638665, 0.11173868578859703, 0.28772023805384067, 0.110672179076823]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2717387294309432, 0.10498150007975493, 0.11315946532377531, 0.11152642788911, 0.2878657331953192, 0.11072814408109742]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.182]
 [0.182]
 [0.126]
 [0.182]
 [0.182]
 [0.182]] [[3.726]
 [3.726]
 [3.726]
 [3.786]
 [3.726]
 [3.726]
 [3.726]] [[-0.234]
 [-0.234]
 [-0.234]
 [-0.305]
 [-0.234]
 [-0.234]
 [-0.234]]
first move QE:  0.988088674530196
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2717387294309432, 0.10498150007975493, 0.11315946532377531, 0.11152642788911, 0.2878657331953192, 0.11072814408109742]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2717387294309432, 0.10498150007975493, 0.11315946532377531, 0.11152642788911, 0.2878657331953192, 0.11072814408109742]
from probs:  [0.27187446840574936, 0.10479801189773896, 0.11321599075850018, 0.11158213758870698, 0.2880095279336975, 0.11051986341560711]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.644]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[1.741]
 [1.755]
 [1.537]
 [1.537]
 [1.537]
 [1.537]
 [1.537]] [[1.62 ]
 [1.72 ]
 [1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.583]]
from probs:  [0.27215360797875626, 0.10467054316156703, 0.11333223213821929, 0.111428871409596, 0.2883052337317475, 0.11010951158011396]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27156402385478434, 0.10494888347105102, 0.11308145753220271, 0.11119209711911257, 0.28907189611410355, 0.11014164190874585]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27156402385478434, 0.10494888347105102, 0.11308145753220271, 0.11119209711911257, 0.28907189611410355, 0.11014164190874585]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27156402385478434, 0.10494888347105102, 0.11308145753220271, 0.11119209711911257, 0.28907189611410355, 0.11014164190874585]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27156402385478434, 0.10494888347105102, 0.11308145753220271, 0.11119209711911257, 0.28907189611410355, 0.11014164190874585]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.5777821798311222
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.644]
 [0.576]
 [0.668]
 [0.576]
 [0.576]
 [0.59 ]] [[3.244]
 [3.217]
 [3.342]
 [3.386]
 [3.342]
 [3.342]
 [3.312]] [[1.823]
 [1.943]
 [1.905]
 [2.058]
 [1.905]
 [1.905]
 [1.911]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2710888880454675, 0.1050173383172117, 0.11315521699988801, 0.11126462421659876, 0.2892604485934, 0.11021348382743407]
Printing some Q and Qe and total Qs values:  [[0.885]
 [0.902]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]] [[1.59 ]
 [1.643]
 [1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.572]] [[0.885]
 [0.902]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]]
rdn beta is 0 so we're just using the maxi policy
first move QE:  0.9806849115531763
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.63875395
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.6354796043890976
siam score:  -0.64365256
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27116781188319744, 0.10459718421431174, 0.11291140065822215, 0.11103455883171429, 0.290039131638561, 0.11024991277399357]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27167230007165855, 0.10479177974398007, 0.11284880572489467, 0.11124113063333253, 0.2892504316142088, 0.1101955522119254]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27167230007165855, 0.10479177974398007, 0.11284880572489467, 0.11124113063333253, 0.2892504316142088, 0.1101955522119254]
1551 1588
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[1.304]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]] [[0.586]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27094325111171913, 0.10501098601854605, 0.11281300322970472, 0.11120995090902648, 0.2898554934777029, 0.1101673152533008]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.4997070065093716
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.429]
 [0.533]
 [0.446]
 [0.533]
 [0.533]
 [0.533]] [[4.992]
 [3.894]
 [5.69 ]
 [4.065]
 [5.69 ]
 [5.69 ]
 [5.69 ]] [[0.437]
 [0.072]
 [0.881]
 [0.164]
 [0.881]
 [0.881]
 [0.881]]
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.918]] [[1.   ]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.002]] [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.918]]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.81 ]] [[-0.384]
 [-0.384]
 [-0.384]
 [-0.384]
 [-0.384]
 [-0.384]
 [ 0.465]] [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.81 ]]
siam score:  -0.6449318
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2692794746598945, 0.10512680122487324, 0.11291719832214275, 0.11158021614113989, 0.29082054571173954, 0.11027576394021006]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26934872180959873, 0.10515383533637225, 0.11294623579017211, 0.111608909794605, 0.2908953323024792, 0.11004696496677274]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.277]
 [0.272]
 [0.266]
 [0.287]
 [0.268]
 [0.267]] [[-0.139]
 [ 1.122]
 [ 0.17 ]
 [ 0.131]
 [ 0.317]
 [-0.157]
 [-0.093]] [[0.268]
 [0.277]
 [0.272]
 [0.266]
 [0.287]
 [0.268]
 [0.267]]
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.622]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[2.519]
 [3.405]
 [2.519]
 [2.519]
 [2.519]
 [2.519]
 [2.519]] [[1.335]
 [1.765]
 [1.335]
 [1.335]
 [1.335]
 [1.335]
 [1.335]]
line 256 mcts: sample exp_bonus 0.9282417506309147
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.261]
 [0.303]
 [0.305]
 [0.295]
 [0.294]
 [0.297]] [[5.263]
 [5.389]
 [5.473]
 [5.312]
 [5.327]
 [5.3  ]
 [5.119]] [[0.401]
 [0.356]
 [0.469]
 [0.421]
 [0.405]
 [0.395]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.197]
 [0.103]
 [0.089]
 [0.094]
 [0.093]
 [0.108]] [[3.275]
 [3.74 ]
 [3.36 ]
 [3.29 ]
 [3.326]
 [3.515]
 [3.482]] [[-0.398]
 [-0.039]
 [-0.354]
 [-0.404]
 [-0.382]
 [-0.321]
 [-0.301]]
using explorer policy with actor:  1
from probs:  [0.26928529521308536, 0.10468045764338631, 0.11291534048178614, 0.11158179345077433, 0.2915129543552543, 0.11002415885571355]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2688229947078906, 0.10474668571863308, 0.11298677851171354, 0.11165238778690154, 0.2916973855500645, 0.11009376772479666]
Printing some Q and Qe and total Qs values:  [[1.195]
 [0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]] [[3.74]
 [2.64]
 [2.64]
 [2.64]
 [2.64]
 [2.64]
 [2.64]] [[1.931]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]]
using explorer policy with actor:  1
first move QE:  0.9751038490246028
from probs:  [0.2688229947078906, 0.10474668571863308, 0.11298677851171354, 0.11165238778690154, 0.2916973855500645, 0.11009376772479666]
line 256 mcts: sample exp_bonus -0.5567286425308231
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2686472508232012, 0.10492396758251542, 0.11237205664696877, 0.11184135742616007, 0.2921910780792853, 0.11002428944186936]
Printing some Q and Qe and total Qs values:  [[1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]] [[0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]] [[1.782]
 [1.782]
 [1.782]
 [1.782]
 [1.782]
 [1.782]
 [1.782]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.269145165598027, 0.10511843521072525, 0.1123138324781536, 0.1120486458466759, 0.2914007597885125, 0.10997316107790583]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2692134981237378, 0.10514512344107393, 0.11234234752812093, 0.11207709356913885, 0.2914747427258101, 0.1097471946121184]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26927572869403277, 0.10493827151987542, 0.11236831624218493, 0.11210300096790754, 0.2915421191374418, 0.10977256343855757]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26927572869403277, 0.10493827151987542, 0.11236831624218493, 0.11210300096790754, 0.2915421191374418, 0.10977256343855757]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]] [[3.181]
 [3.007]
 [3.007]
 [3.007]
 [3.007]
 [3.007]
 [3.007]] [[ 0.146]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]]
Printing some Q and Qe and total Qs values:  [[ 0.027]
 [-0.019]
 [-0.015]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[4.799]
 [4.139]
 [3.548]
 [3.49 ]
 [3.499]
 [3.484]
 [3.542]] [[ 0.76 ]
 [ 0.348]
 [ 0.029]
 [-0.002]
 [ 0.004]
 [-0.004]
 [ 0.027]]
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.029]
 [0.056]
 [0.069]
 [0.073]
 [0.083]
 [0.096]] [[3.335]
 [1.881]
 [1.867]
 [1.804]
 [1.838]
 [1.949]
 [1.819]] [[ 1.286]
 [-0.139]
 [-0.115]
 [-0.154]
 [-0.118]
 [-0.006]
 [-0.104]]
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.045]
 [-0.039]
 [-0.039]
 [-0.038]
 [-0.038]
 [-0.037]] [[5.309]
 [3.968]
 [3.725]
 [3.483]
 [3.459]
 [3.517]
 [3.411]] [[ 1.186]
 [ 0.328]
 [ 0.182]
 [ 0.03 ]
 [ 0.014]
 [ 0.051]
 [-0.014]]
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]] [[4.158]
 [4.158]
 [4.158]
 [4.158]
 [4.158]
 [4.158]
 [4.158]] [[0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.46 ]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[0.94]
 [1.54]
 [1.15]
 [1.15]
 [1.15]
 [1.15]
 [1.15]] [[0.644]
 [0.652]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]]
line 256 mcts: sample exp_bonus 1.7749452882489918
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2699855365896256, 0.10521488766361747, 0.11213379261152866, 0.11187037639101789, 0.29098687817992425, 0.10980852856428613]
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.212]
 [0.206]
 [0.197]
 [0.195]
 [0.197]
 [0.201]] [[3.473]
 [3.544]
 [3.392]
 [3.284]
 [3.307]
 [3.355]
 [3.429]] [[-0.462]
 [-0.425]
 [-0.487]
 [-0.541]
 [-0.536]
 [-0.517]
 [-0.484]]
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.68 ]
 [0.677]
 [0.668]
 [0.669]
 [0.67 ]
 [0.671]] [[2.373]
 [2.263]
 [2.265]
 [2.364]
 [2.349]
 [2.315]
 [2.361]] [[0.934]
 [0.883]
 [0.877]
 [0.926]
 [0.917]
 [0.896]
 [0.93 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[1.244]
 [1.244]
 [1.244]
 [1.244]
 [1.244]
 [1.244]
 [1.244]] [[0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2694408117495931, 0.10526311864326993, 0.11243172522708263, 0.11216760912581578, 0.2911009487851501, 0.10959578646908842]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[3.92 ]
 [4.151]
 [4.151]
 [4.151]
 [4.151]
 [4.151]
 [4.151]] [[0.938]
 [1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.097]]
line 256 mcts: sample exp_bonus 0.4879071106578192
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.365]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.302]] [[-0.3  ]
 [ 0.435]
 [-0.602]
 [-0.602]
 [-0.602]
 [-0.602]
 [-0.459]] [[0.305]
 [0.365]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.302]]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.393]
 [0.391]
 [0.379]
 [0.365]
 [0.386]
 [0.391]] [[2.763]
 [2.77 ]
 [3.21 ]
 [3.096]
 [2.984]
 [2.945]
 [2.99 ]] [[0.62 ]
 [0.621]
 [1.057]
 [0.918]
 [0.78 ]
 [0.782]
 [0.839]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26946872767579017, 0.10528857532491499, 0.11270545789585439, 0.11217723113714358, 0.2904973933661295, 0.10986261460016736]
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.392]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[0.14 ]
 [1.054]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]] [[0.379]
 [0.392]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.429]
 [0.476]
 [0.472]
 [0.472]
 [0.472]
 [0.471]] [[4.116]
 [4.227]
 [3.704]
 [3.77 ]
 [3.809]
 [3.699]
 [3.555]] [[0.472]
 [0.429]
 [0.476]
 [0.472]
 [0.472]
 [0.472]
 [0.471]]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.448]
 [0.566]
 [0.572]
 [0.572]
 [0.573]
 [0.575]] [[-3.468]
 [ 0.469]
 [-3.588]
 [-3.732]
 [-3.769]
 [-3.724]
 [-3.817]] [[0.3  ]
 [1.6  ]
 [0.259]
 [0.212]
 [0.199]
 [0.215]
 [0.184]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26959426159087013, 0.10535214379368717, 0.11249044661612237, 0.11249044661612237, 0.2906550264020314, 0.1094176749811666]
from probs:  [0.26959426159087013, 0.10535214379368717, 0.11249044661612237, 0.11249044661612237, 0.2906550264020314, 0.1094176749811666]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2698407411531919, 0.10544846316687669, 0.11233013428155156, 0.1125932922623305, 0.29026965782443404, 0.10951771131161546]
line 256 mcts: sample exp_bonus -1.2293130440749294
line 256 mcts: sample exp_bonus 4.050108838511911
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.391]
 [0.375]
 [0.369]
 [0.372]
 [0.371]
 [0.369]] [[-2.049]
 [-1.122]
 [-1.948]
 [-2.214]
 [-2.151]
 [-2.271]
 [-2.177]] [[0.374]
 [0.391]
 [0.375]
 [0.369]
 [0.372]
 [0.371]
 [0.369]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2698407411531919, 0.10544846316687669, 0.11233013428155156, 0.1125932922623305, 0.29026965782443404, 0.10951771131161546]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.504]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.521]] [[3.759]
 [4.354]
 [3.759]
 [3.759]
 [3.759]
 [3.759]
 [4.317]] [[1.111]
 [1.357]
 [1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.379]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]] [[1.999]
 [3.321]
 [3.321]
 [3.321]
 [3.321]
 [3.321]
 [3.321]] [[1.79 ]
 [2.165]
 [2.165]
 [2.165]
 [2.165]
 [2.165]
 [2.165]]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.65 ]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[0.415]
 [1.992]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[0.638]
 [1.304]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]]
from probs:  [0.2698407411531919, 0.10544846316687669, 0.11233013428155156, 0.1125932922623305, 0.29026965782443404, 0.10951771131161546]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2693844840051464, 0.10551435511371891, 0.11240032640208675, 0.1126636488233057, 0.2904510396320855, 0.1095861460236568]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.006]
 [ 0.008]
 [-0.001]
 [ 0.003]
 [ 0.021]
 [-0.005]] [[2.436]
 [4.862]
 [4.71 ]
 [4.795]
 [4.845]
 [6.067]
 [4.253]] [[-0.83 ]
 [ 0.473]
 [ 0.413]
 [ 0.446]
 [ 0.478]
 [ 1.167]
 [ 0.147]]
from probs:  [0.2694554376634026, 0.1055421467273358, 0.11242993172392576, 0.11242993172392576, 0.29052754204788833, 0.10961501011352183]
line 256 mcts: sample exp_bonus 4.509557406655877
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.031]] [[-0.724]
 [-0.724]
 [-0.724]
 [-0.724]
 [-0.724]
 [-0.724]
 [-0.662]] [[0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.031]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6316019
1584 1633
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.441]
 [0.285]
 [0.574]
 [0.574]
 [0.574]
 [0.457]] [[3.87 ]
 [2.763]
 [3.12 ]
 [3.249]
 [3.249]
 [3.249]
 [3.72 ]] [[2.407]
 [1.265]
 [1.311]
 [2.013]
 [2.013]
 [2.013]
 [2.249]]
Printing some Q and Qe and total Qs values:  [[1.007]
 [0.755]
 [0.806]
 [0.807]
 [0.807]
 [0.807]
 [0.806]] [[2.468]
 [2.736]
 [2.898]
 [3.181]
 [3.181]
 [3.181]
 [2.853]] [[1.044]
 [0.889]
 [1.063]
 [1.249]
 [1.249]
 [1.249]
 [1.033]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.269371193121196, 0.105523067639093, 0.11265487204884457, 0.11265487204884457, 0.29045878096759004, 0.10933721417443185]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.499]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[0.263]
 [1.117]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[0.491]
 [0.499]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]]
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.72 ]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]] [[2.352]
 [2.782]
 [2.352]
 [2.352]
 [2.352]
 [2.352]
 [2.352]] [[1.147]
 [1.498]
 [1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]]
Printing some Q and Qe and total Qs values:  [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[2.317]
 [2.317]
 [2.317]
 [2.317]
 [2.317]
 [2.317]
 [2.317]] [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26898829899993115, 0.1056163338518882, 0.11275444168327615, 0.11249157196066062, 0.2907155019962718, 0.10943385150797208]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.516]
 [0.621]
 [0.911]
 [0.621]
 [0.621]
 [0.528]] [[2.072]
 [2.518]
 [1.404]
 [1.266]
 [1.404]
 [1.404]
 [1.576]] [[0.516]
 [0.516]
 [0.621]
 [0.911]
 [0.621]
 [0.621]
 [0.528]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6135498622353779
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.575]
 [0.599]
 [0.598]
 [0.602]
 [0.601]
 [0.612]] [[0.41 ]
 [2.11 ]
 [1.633]
 [1.079]
 [1.334]
 [1.33 ]
 [1.432]] [[0.606]
 [0.575]
 [0.599]
 [0.598]
 [0.602]
 [0.601]
 [0.612]]
siam score:  -0.63625354
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[1.821]
 [2.008]
 [2.008]
 [2.008]
 [2.008]
 [2.008]
 [2.008]] [[0.68 ]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26933241734270774, 0.10575144946067258, 0.11211288565580715, 0.11263548310275621, 0.2910874160800381, 0.10908034835801819]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[1.953]
 [2.365]
 [2.365]
 [2.365]
 [2.365]
 [2.365]
 [2.365]] [[0.602]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2690776091838215, 0.10566472136587346, 0.11200501685215167, 0.11278808359303535, 0.2914817863191871, 0.10898278268593078]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -3.133247364760714
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]] [[2.603]
 [2.612]
 [2.612]
 [2.612]
 [2.612]
 [2.612]
 [2.612]] [[0.64 ]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]]
line 256 mcts: sample exp_bonus 2.1074530530825712
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn probs:  [0.26892855248540287, 0.10561946844427815, 0.11220017126492453, 0.11298460239942444, 0.2913391627675109, 0.10892804263845912]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]]
from probs:  [0.26892855248540287, 0.10561946844427815, 0.11220017126492453, 0.11298460239942444, 0.2913391627675109, 0.10892804263845912]
using another actor
siam score:  -0.63595366
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2689899695380134, 0.10541521264602709, 0.11222579518532198, 0.11301040546572445, 0.2914056978844475, 0.10895291928046556]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2689899695380134, 0.10541521264602709, 0.11222579518532198, 0.11301040546572445, 0.2914056978844475, 0.10895291928046556]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.637]
 [0.507]
 [0.478]
 [0.613]
 [0.659]
 [0.516]] [[2.014]
 [2.143]
 [1.803]
 [1.569]
 [1.573]
 [1.492]
 [1.808]] [[0.41 ]
 [0.637]
 [0.507]
 [0.478]
 [0.613]
 [0.659]
 [0.516]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2689899695380134, 0.10541521264602709, 0.11222579518532198, 0.11301040546572445, 0.2914056978844475, 0.10895291928046556]
siam score:  -0.63565147
siam score:  -0.6364178
Printing some Q and Qe and total Qs values:  [[ 0.04 ]
 [-0.014]
 [ 0.04 ]
 [ 0.04 ]
 [ 0.04 ]
 [ 0.04 ]
 [ 0.02 ]] [[3.296]
 [2.137]
 [3.296]
 [3.296]
 [3.296]
 [3.296]
 [2.267]] [[ 0.887]
 [-0.049]
 [ 0.887]
 [ 0.887]
 [ 0.887]
 [ 0.887]
 [ 0.081]]
from probs:  [0.2689899695380134, 0.10541521264602709, 0.11222579518532198, 0.11301040546572445, 0.2914056978844475, 0.10895291928046556]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.6322162
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26916430844411016, 0.10548353479532593, 0.11229853143791182, 0.11308365024321666, 0.2909464407764244, 0.10902353430301094]
Printing some Q and Qe and total Qs values:  [[0.879]
 [0.842]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.624]] [[1.513]
 [1.861]
 [1.857]
 [1.857]
 [1.857]
 [1.857]
 [1.837]] [[0.879]
 [0.842]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.624]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26916430844411016, 0.10548353479532593, 0.11229853143791182, 0.11308365024321666, 0.2909464407764244, 0.10902353430301094]
first move QE:  0.9576177415407842
line 256 mcts: sample exp_bonus 3.254145522811267
using explorer policy with actor:  1
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.029]
 [-0.028]
 [-0.022]
 [-0.021]
 [-0.026]
 [-0.027]] [[2.722]
 [3.783]
 [3.822]
 [4.246]
 [4.303]
 [4.249]
 [3.86 ]] [[-0.372]
 [ 0.383]
 [ 0.412]
 [ 0.72 ]
 [ 0.762]
 [ 0.718]
 [ 0.44 ]]
line 256 mcts: sample exp_bonus 2.346034493277119
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.37 ]
 [0.631]
 [0.656]
 [0.654]
 [0.606]
 [0.657]] [[1.188]
 [1.996]
 [1.551]
 [1.387]
 [1.344]
 [1.369]
 [1.236]] [[0.851]
 [0.818]
 [1.043]
 [0.984]
 [0.951]
 [0.873]
 [0.886]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2696812898614941, 0.10545820364038597, 0.11251422219679622, 0.11225453451119888, 0.2908588149851752, 0.10923293480494961]
line 256 mcts: sample exp_bonus 4.707977710053692
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2696812898614941, 0.10545820364038597, 0.11251422219679622, 0.11225453451119888, 0.2908588149851752, 0.10923293480494961]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2696812898614941, 0.10545820364038597, 0.11251422219679622, 0.11225453451119888, 0.2908588149851752, 0.10923293480494961]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]] [[3.647]
 [3.647]
 [3.647]
 [3.647]
 [3.647]
 [3.647]
 [3.647]] [[0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.145]
 [-0.003]
 [-0.005]
 [-0.006]
 [-0.015]
 [-0.013]] [[2.245]
 [2.24 ]
 [2.252]
 [2.145]
 [2.199]
 [2.284]
 [2.3  ]] [[-0.246]
 [ 0.062]
 [-0.229]
 [-0.27 ]
 [-0.253]
 [-0.243]
 [-0.233]]
siam score:  -0.62241226
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.39 ]
 [0.392]] [[4.88 ]
 [4.88 ]
 [4.88 ]
 [4.88 ]
 [4.88 ]
 [4.854]
 [4.897]] [[0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.931]
 [0.964]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2687814426518095, 0.10558814180158431, 0.11265285428644159, 0.11239284663199225, 0.2912171907044922, 0.10936752392368006]
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]] [[6.983]
 [5.192]
 [5.192]
 [5.192]
 [5.192]
 [5.192]
 [5.192]] [[1.624]
 [1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2690391376336661, 0.10523558370411984, 0.11250060359723468, 0.11250060359723468, 0.2914963960243779, 0.1092276754433668]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.441]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[ 0.613]
 [ 0.037]
 [-0.62 ]
 [-0.62 ]
 [-0.62 ]
 [-0.62 ]
 [-0.62 ]] [[0.654]
 [0.441]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2692734922751262, 0.10510168328271009, 0.11259860063533098, 0.11259860063533098, 0.2911048016726532, 0.10932282149884853]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26958233378402074, 0.10522222898580968, 0.11246821323251772, 0.11272774487981568, 0.290795329021568, 0.10920415009626808]
siam score:  -0.62576044
from probs:  [0.26958233378402074, 0.10522222898580968, 0.11246821323251772, 0.11272774487981568, 0.290795329021568, 0.10920415009626808]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
from probs:  [0.26964783825236094, 0.10524779641105798, 0.11249554132338167, 0.11275513603298173, 0.2908659879299467, 0.10898770005027096]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]] [[2.209]
 [2.35 ]
 [2.35 ]
 [2.35 ]
 [2.35 ]
 [2.35 ]
 [2.35 ]] [[1.975]
 [2.066]
 [2.066]
 [2.066]
 [2.066]
 [2.066]
 [2.066]]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.549]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[2.484]
 [3.255]
 [2.484]
 [2.484]
 [2.484]
 [2.484]
 [2.484]] [[0.517]
 [1.428]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2693719769637871, 0.10537998391248207, 0.11263683173549453, 0.11289675248684763, 0.2905898701632963, 0.10912458473809222]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.497]
 [0.457]
 [0.469]
 [0.453]
 [0.45 ]
 [0.455]] [[2.522]
 [3.078]
 [2.289]
 [2.435]
 [2.334]
 [2.283]
 [2.492]] [[0.475]
 [0.497]
 [0.457]
 [0.469]
 [0.453]
 [0.45 ]
 [0.455]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.9502837184567486
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]] [[2.36]
 [2.36]
 [2.36]
 [2.36]
 [2.36]
 [2.36]
 [2.36]] [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[3.511]
 [3.511]
 [3.511]
 [3.511]
 [3.511]
 [3.511]
 [3.511]] [[0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.457]
 [0.503]
 [0.493]
 [0.492]
 [0.514]
 [0.551]] [[4.8  ]
 [4.048]
 [3.549]
 [3.523]
 [3.378]
 [3.289]
 [3.298]] [[0.489]
 [0.457]
 [0.503]
 [0.493]
 [0.492]
 [0.514]
 [0.551]]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]] [[7.733]
 [5.099]
 [5.099]
 [5.099]
 [5.099]
 [5.099]
 [5.099]] [[1.478]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]]
siam score:  -0.6225547
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
1619 1660
Printing some Q and Qe and total Qs values:  [[1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.233]] [[0.949]
 [0.949]
 [0.949]
 [0.949]
 [0.949]
 [0.949]
 [0.949]] [[1.436]
 [1.436]
 [1.436]
 [1.436]
 [1.436]
 [1.436]
 [1.436]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2697403918588317, 0.10529864549387515, 0.112790882973525, 0.112790882973525, 0.29034785941543123, 0.10903133728481182]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2697403918588317, 0.10529864549387515, 0.112790882973525, 0.112790882973525, 0.29034785941543123, 0.10903133728481182]
siam score:  -0.6391041
using explorer policy with actor:  1
siam score:  -0.6366806
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26935786968507375, 0.10538863711759137, 0.11288727770539991, 0.11288727770539991, 0.2905960000746845, 0.10888293771185066]
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.727]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[3.251]
 [2.74 ]
 [3.251]
 [3.251]
 [3.251]
 [3.251]
 [3.251]] [[1.928]
 [1.688]
 [1.928]
 [1.928]
 [1.928]
 [1.928]
 [1.928]]
first move QE:  0.9464550783697792
using explorer policy with actor:  1
siam score:  -0.6344373
start point for exploration sampling:  10935
line 256 mcts: sample exp_bonus 3.3352092727663583
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.091]
 [0.091]
 [0.091]
 [0.091]
 [0.091]
 [0.091]] [[1.577]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]] [[-0.456]
 [-0.475]
 [-0.475]
 [-0.475]
 [-0.475]
 [-0.475]
 [-0.475]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.63202906
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 3.557013790898652
from probs:  [0.26948827625159993, 0.10521489893248225, 0.11294193080429496, 0.11268255301493761, 0.29073668884186366, 0.10893565215482162]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26990161519278816, 0.10537627669453886, 0.11285538460450217, 0.11285538460450217, 0.28990860212555425, 0.10910273677811441]
Printing some Q and Qe and total Qs values:  [[0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]] [[4.279]
 [4.279]
 [4.279]
 [4.279]
 [4.279]
 [4.279]
 [4.279]] [[0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]]
siam score:  -0.63100505
from probs:  [0.27009705586528004, 0.10522831412707608, 0.11293710524276392, 0.11267838239058169, 0.2901185302214648, 0.10894061215283359]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.347]
 [0.278]
 [0.278]
 [0.278]
 [0.3  ]
 [0.278]] [[4.92 ]
 [4.942]
 [5.02 ]
 [5.02 ]
 [5.02 ]
 [4.721]
 [5.02 ]] [[-0.146]
 [-0.028]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.197]
 [-0.141]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2693296947487088, 0.10518219747329978, 0.11312764385793521, 0.11286848450887392, 0.29060799542304844, 0.10888398398813375]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.6438899
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26990562265928014, 0.10540711646441442, 0.11336955319368722, 0.11310983966472925, 0.2893308823365895, 0.10887698568129935]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]] [[5.819]
 [4.706]
 [4.706]
 [4.706]
 [4.706]
 [4.706]
 [4.706]] [[2.712]
 [2.417]
 [2.417]
 [2.417]
 [2.417]
 [2.417]
 [2.417]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.346]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[4.748]
 [4.692]
 [4.748]
 [4.748]
 [4.748]
 [4.748]
 [4.748]] [[0.534]
 [0.577]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]]
in main func line 156:  1633
UNIT TEST: sample policy line 217 mcts : [0.082 0.265 0.122 0.061 0.306 0.082 0.082]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6287389
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2691368298184407, 0.10559544184282461, 0.11381303524774761, 0.11303456265748105, 0.2898329692712151, 0.10858716116229075]
line 256 mcts: sample exp_bonus -0.154324835129735
siam score:  -0.6267366
first move QE:  0.9392176125599112
from probs:  [0.26890523162419044, 0.10574135566368606, 0.11344936805380976, 0.11293336959404436, 0.29023346606558365, 0.10873720899868573]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.26896529915319994, 0.10554159790242477, 0.11347471015357302, 0.11295859643107263, 0.29029829785422956, 0.10876149850550006]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.481]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[-0.02 ]
 [ 1.657]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[0.482]
 [1.138]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.622]
 [0.658]
 [0.658]
 [0.656]
 [0.658]
 [0.643]] [[2.27 ]
 [3.608]
 [2.966]
 [2.928]
 [2.826]
 [2.966]
 [3.45 ]] [[1.048]
 [2.026]
 [1.759]
 [1.74 ]
 [1.685]
 [1.759]
 [1.98 ]]
first move QE:  0.9352080233254993
siam score:  -0.6315413
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2683161381785703, 0.10553548918396892, 0.11370774373649724, 0.11319057011413204, 0.2902652058295187, 0.10898485295731265]
line 256 mcts: sample exp_bonus 0.9345944314246972
from probs:  [0.2686712779641692, 0.10545300976841768, 0.11385824584198234, 0.11334038769526983, 0.2900220742670519, 0.10865500446310898]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2689685721507575, 0.10534826204214225, 0.11372440268055277, 0.11346580280708525, 0.2897177252853172, 0.10877523503414499]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2689685721507575, 0.10534826204214225, 0.11372440268055277, 0.11346580280708525, 0.2897177252853172, 0.10877523503414499]
UNIT TEST: sample policy line 217 mcts : [0.041 0.224 0.041 0.02  0.612 0.041 0.02 ]
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.658]
 [0.306]
 [0.306]
 [0.306]
 [0.451]
 [0.306]] [[3.973]
 [3.018]
 [4.865]
 [4.865]
 [4.865]
 [3.478]
 [4.865]] [[0.39 ]
 [0.604]
 [0.517]
 [0.517]
 [0.517]
 [0.344]
 [0.517]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.188]
 [0.193]
 [0.194]
 [0.194]
 [0.199]
 [0.194]] [[5.296]
 [5.57 ]
 [5.298]
 [5.102]
 [5.122]
 [5.255]
 [5.277]] [[-0.014]
 [ 0.087]
 [ 0.006]
 [-0.058]
 [-0.052]
 [ 0.003]
 [ 0.001]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2691606302609998, 0.10520286287908243, 0.11380560805593404, 0.11328926256790921, 0.2899245994133131, 0.10861703682276129]
from probs:  [0.2694375666129149, 0.10531110492409661, 0.11340582461881644, 0.11289381259106737, 0.29022289958007275, 0.10872879167303176]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.455]
 [0.443]
 [0.432]
 [0.433]
 [0.432]
 [0.435]] [[1.958]
 [3.222]
 [2.034]
 [1.396]
 [1.432]
 [1.247]
 [1.515]] [[0.435]
 [0.455]
 [0.443]
 [0.432]
 [0.433]
 [0.432]
 [0.435]]
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[1.58 ]
 [0.245]
 [0.245]
 [0.245]
 [0.245]
 [0.245]
 [0.245]] [[1.865]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2691675310456345, 0.10544003660733929, 0.11354466661338261, 0.11303202773272351, 0.2899538303935578, 0.10886190760736224]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.429]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[-1.669]
 [-1.554]
 [-1.669]
 [-1.669]
 [-1.669]
 [-1.669]
 [-1.669]] [[0.41 ]
 [0.429]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 3.7657056712585084
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26989912300968355, 0.10572662051726074, 0.11308404038380374, 0.11333924652262803, 0.28949791698283417, 0.10845305258378975]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.385]
 [0.426]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[2.618]
 [2.891]
 [2.677]
 [2.916]
 [2.916]
 [2.916]
 [2.916]] [[0.849]
 [0.96 ]
 [0.901]
 [1.032]
 [1.032]
 [1.032]
 [1.032]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.63800675
1653 1708
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26952119477757497, 0.10559240769108014, 0.1131767972191751, 0.1134322126900477, 0.2897353767564219, 0.1085420108657002]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2695804949602914, 0.10539561970306807, 0.11320169843245406, 0.11345717009996123, 0.2897991244731655, 0.10856589233105975]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2692128563703077, 0.10548538976310967, 0.11304362561513283, 0.11355380653514438, 0.290045959065317, 0.10865836265098848]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26928170881221625, 0.10551236814285271, 0.11307253704889844, 0.11332709379243387, 0.29012013967027256, 0.10868615253332611]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[3.062]
 [3.442]
 [3.442]
 [3.442]
 [3.442]
 [3.442]
 [3.442]] [[-0.153]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2688468775183873, 0.10557515578633697, 0.11313982355065219, 0.11339453177423059, 0.29029278255771634, 0.10875082881267659]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26901357178529334, 0.10564061599684692, 0.11320997411412066, 0.11346484026551655, 0.2898527397890539, 0.10881825804916871]
from probs:  [0.26901357178529334, 0.10564061599684692, 0.11320997411412066, 0.11346484026551655, 0.2898527397890539, 0.10881825804916871]
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.053]
 [-0.048]
 [-0.049]
 [-0.051]
 [-0.052]
 [-0.053]] [[8.159]
 [2.69 ]
 [2.235]
 [1.414]
 [1.478]
 [1.434]
 [0.83 ]] [[ 1.317]
 [ 0.028]
 [-0.074]
 [-0.267]
 [-0.254]
 [-0.265]
 [-0.407]]
siam score:  -0.6395909
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.484]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[ 0.312]
 [ 0.18 ]
 [-2.102]
 [-2.102]
 [-2.102]
 [-2.102]
 [-2.102]] [[1.357]
 [1.4  ]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]]
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.217]
 [0.234]
 [0.239]
 [0.239]
 [0.245]
 [0.232]] [[3.798]
 [3.896]
 [3.563]
 [3.731]
 [3.74 ]
 [3.546]
 [3.691]] [[0.719]
 [0.737]
 [0.551]
 [0.662]
 [0.668]
 [0.555]
 [0.628]]
siam score:  -0.6428413
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.244]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]] [[4.415]
 [2.209]
 [2.209]
 [2.209]
 [2.209]
 [2.209]
 [2.209]] [[2.705]
 [1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]]
siam score:  -0.64053154
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.6430893
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.175]] [[-3.553]
 [-3.602]
 [-3.602]
 [-3.602]
 [-3.602]
 [-3.602]
 [-3.706]] [[0.177]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.175]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2678908276459688, 0.10546999136919794, 0.11322472716986777, 0.11398927858683522, 0.2905722454223985, 0.10885292980573175]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.216]
 [0.21 ]
 [0.201]
 [0.199]
 [0.198]
 [0.205]] [[1.237]
 [1.235]
 [0.941]
 [0.833]
 [0.818]
 [0.725]
 [0.633]] [[0.199]
 [0.216]
 [0.21 ]
 [0.201]
 [0.199]
 [0.198]
 [0.205]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2678908276459688, 0.10546999136919794, 0.11322472716986777, 0.11398927858683522, 0.2905722454223985, 0.10885292980573175]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 4.866793265108805
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2678908276459688, 0.10546999136919794, 0.11322472716986777, 0.11398927858683522, 0.2905722454223985, 0.10885292980573175]
siam score:  -0.648555
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
1671 1720
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26820613978184654, 0.10515908731066403, 0.11310521768805328, 0.11386710041882377, 0.2909142539791529, 0.1087482008214596]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26820613978184654, 0.10515908731066403, 0.11310521768805328, 0.11386710041882377, 0.2909142539791529, 0.1087482008214596]
siam score:  -0.6556265
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.268264116664075, 0.10496565368438521, 0.11312966712045874, 0.11389171454389954, 0.29097713956952476, 0.10877170841765674]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.268264116664075, 0.10496565368438521, 0.11312966712045874, 0.11389171454389954, 0.29097713956952476, 0.10877170841765674]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1675 1722
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26790707358637234, 0.10528472645860892, 0.11296987866931694, 0.11372715685918329, 0.291241382148215, 0.10886978227830346]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26754837132436865, 0.10537281422375076, 0.11306439631185683, 0.11356849577217669, 0.29148505284322634, 0.10896086952462065]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26754837132436865, 0.10537281422375076, 0.11306439631185683, 0.11356849577217669, 0.29148505284322634, 0.10896086952462065]
line 256 mcts: sample exp_bonus 1.3411578364725811
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]] [[5.039]
 [3.395]
 [3.395]
 [3.395]
 [3.395]
 [3.395]
 [3.395]] [[ 0.394]
 [-0.25 ]
 [-0.25 ]
 [-0.25 ]
 [-0.25 ]
 [-0.25 ]
 [-0.25 ]]
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.183]
 [0.213]
 [0.219]
 [0.218]
 [0.218]
 [0.223]] [[7.326]
 [4.267]
 [3.516]
 [3.223]
 [3.344]
 [3.542]
 [3.415]] [[ 0.719]
 [-0.197]
 [-0.355]
 [-0.425]
 [-0.394]
 [-0.339]
 [-0.367]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.454]
 [0.452]
 [0.422]
 [0.421]
 [0.427]
 [0.419]] [[4.368]
 [5.726]
 [5.148]
 [3.886]
 [4.093]
 [4.093]
 [4.191]] [[1.025]
 [1.574]
 [1.378]
 [0.896]
 [0.964]
 [0.975]
 [0.992]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.544]
 [0.564]
 [0.563]
 [0.563]
 [0.563]
 [0.564]] [[2.125]
 [2.344]
 [2.181]
 [2.344]
 [2.089]
 [1.898]
 [2.106]] [[0.568]
 [0.544]
 [0.564]
 [0.563]
 [0.563]
 [0.563]
 [0.564]]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.13 ]] [[4.413]
 [4.413]
 [4.413]
 [4.413]
 [4.413]
 [4.413]
 [4.258]] [[-0.261]
 [-0.261]
 [-0.261]
 [-0.261]
 [-0.261]
 [-0.261]
 [-0.292]]
1682 1731
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[1.392]
 [1.392]
 [1.392]
 [1.392]
 [1.392]
 [1.392]
 [1.392]] [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[0.281]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[0.603]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]]
from probs:  [0.26653999792176947, 0.10566013685352738, 0.11312170583097912, 0.11337269176663559, 0.292279852266023, 0.1090256153610655]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2663454117271138, 0.10581095375965843, 0.11328317321307561, 0.11353451740041928, 0.29207636802764253, 0.10894957587209017]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.599]
 [0.781]
 [0.518]
 [0.781]
 [0.781]
 [0.532]] [[2.491]
 [2.491]
 [1.45 ]
 [2.507]
 [1.45 ]
 [1.45 ]
 [2.377]] [[0.571]
 [0.694]
 [0.365]
 [0.542]
 [0.365]
 [0.365]
 [0.484]]
from probs:  [0.2663454117271138, 0.10581095375965843, 0.11328317321307561, 0.11353451740041928, 0.29207636802764253, 0.10894957587209017]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26651011720547774, 0.10587638625065618, 0.11335322645376653, 0.11360472606997357, 0.2916385947628694, 0.10901694925725644]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26651011720547774, 0.10587638625065618, 0.11335322645376653, 0.11360472606997357, 0.2916385947628694, 0.10901694925725644]
Printing some Q and Qe and total Qs values:  [[0.749]
 [1.016]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]] [[2.107]
 [1.957]
 [1.963]
 [1.963]
 [1.963]
 [1.963]
 [1.963]] [[0.725]
 [1.16 ]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[2.607]
 [2.401]
 [2.401]
 [2.401]
 [2.401]
 [2.401]
 [2.401]] [[1.895]
 [1.544]
 [1.544]
 [1.544]
 [1.544]
 [1.544]
 [1.544]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26651011720547774, 0.10587638625065618, 0.11335322645376653, 0.11360472606997357, 0.2916385947628694, 0.10901694925725644]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.64464086
from probs:  [0.26657716125922404, 0.10590302081974753, 0.1133817419183439, 0.1133817419183439, 0.291711960208905, 0.10904437387543567]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26664392986136914, 0.10592954596027224, 0.1131596739007808, 0.11341014022666447, 0.29178502423179487, 0.10907168581911854]
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.012]] [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.974]] [[-0.497]
 [-0.497]
 [-0.497]
 [-0.497]
 [-0.497]
 [-0.497]
 [-0.045]]
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.56 ]
 [0.564]
 [0.564]
 [0.598]
 [0.564]
 [0.609]] [[2.476]
 [2.691]
 [2.459]
 [2.459]
 [1.926]
 [2.459]
 [1.988]] [[1.331]
 [1.458]
 [1.234]
 [1.234]
 [0.768]
 [1.234]
 [0.854]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.657]
 [0.657]
 [0.649]
 [0.652]
 [0.657]
 [0.659]] [[2.296]
 [1.97 ]
 [1.97 ]
 [2.162]
 [2.073]
 [1.97 ]
 [2.183]] [[1.111]
 [0.777]
 [0.777]
 [0.954]
 [0.87 ]
 [0.777]
 [0.996]]
siam score:  -0.64339966
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.71 ]
 [0.662]
 [0.661]
 [0.663]
 [0.779]
 [0.648]] [[3.083]
 [3.161]
 [3.102]
 [2.939]
 [3.091]
 [3.224]
 [3.409]] [[1.421]
 [1.543]
 [1.427]
 [1.305]
 [1.421]
 [1.693]
 [1.636]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2668174852519175, 0.10557615552341534, 0.11347725324415359, 0.1132272387929497, 0.2919861358842496, 0.10891573130331446]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.592]
 [0.597]
 [0.626]
 [0.604]
 [0.239]
 [0.613]] [[2.363]
 [2.448]
 [2.224]
 [2.136]
 [2.172]
 [3.475]
 [2.381]] [[0.983]
 [1.122]
 [0.909]
 [0.851]
 [0.865]
 [1.776]
 [1.077]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.64606357
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.266988422496895, 0.10587086218085082, 0.11255187537949976, 0.11279803458096155, 0.29280118979199354, 0.10898961556979934]
first move QE:  0.9179566290495514
siam score:  -0.64505064
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26702526798624904, 0.10611255559032683, 0.11280882092563996, 0.11280882092563996, 0.2922357617579728, 0.10900877281417161]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2671486199381891, 0.10594533868084091, 0.1126152196490716, 0.11286093280389843, 0.29237075966252085, 0.1090591292654791]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[1.23]
 [1.23]
 [1.23]
 [1.23]
 [1.23]
 [1.23]
 [1.23]] [[0.915]
 [0.903]
 [0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]] [[1.943]
 [1.936]
 [1.943]
 [1.943]
 [1.943]
 [1.943]
 [1.943]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26733272355536597, 0.10580288648620267, 0.11244806027301582, 0.11293871013238307, 0.2925722449422242, 0.10890537461080825]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26733272355536597, 0.10580288648620267, 0.11244806027301582, 0.11293871013238307, 0.2925722449422242, 0.10890537461080825]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2673936652973876, 0.10582700553952182, 0.11247369417442846, 0.1129644558833918, 0.2926389403397679, 0.10870223876550233]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[5.844]
 [5.576]
 [5.576]
 [5.576]
 [5.576]
 [5.576]
 [5.576]] [[1.742]
 [1.67 ]
 [1.67 ]
 [1.67 ]
 [1.67 ]
 [1.67 ]
 [1.67 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]] [[4.446]
 [3.522]
 [3.522]
 [3.522]
 [3.522]
 [3.522]
 [3.522]] [[1.909]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]] [[3.905]
 [3.925]
 [3.925]
 [3.925]
 [3.925]
 [3.925]
 [3.925]] [[1.545]
 [1.552]
 [1.552]
 [1.552]
 [1.552]
 [1.552]
 [1.552]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26778757854384083, 0.10598290549051032, 0.1126393857511663, 0.11288456958521868, 0.29184318623855426, 0.10886237439070957]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26778757854384083, 0.10598290549051032, 0.1126393857511663, 0.11288456958521868, 0.29184318623855426, 0.10886237439070957]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.211]
 [0.337]
 [0.335]
 [0.335]
 [0.339]
 [0.328]] [[4.172]
 [3.884]
 [3.768]
 [3.991]
 [3.991]
 [4.037]
 [4.044]] [[1.416]
 [0.998]
 [1.134]
 [1.353]
 [1.353]
 [1.408]
 [1.392]]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.279]
 [0.602]
 [0.355]
 [0.572]
 [0.618]] [[2.759]
 [2.759]
 [2.88 ]
 [3.395]
 [2.383]
 [2.759]
 [3.338]] [[1.416]
 [1.416]
 [0.869]
 [1.688]
 [0.855]
 [1.416]
 [1.7  ]]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.063]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.22 ]] [[-1.394]
 [ 0.734]
 [-1.394]
 [-1.394]
 [-1.394]
 [-1.394]
 [ 0.149]] [[0.737]
 [1.236]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [1.121]]
Printing some Q and Qe and total Qs values:  [[1.251]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]] [[3.184]
 [2.831]
 [2.831]
 [2.831]
 [2.831]
 [2.831]
 [2.831]] [[1.459]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]]
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[3.05 ]
 [3.507]
 [3.507]
 [3.507]
 [3.507]
 [3.507]
 [3.507]] [[0.712]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.111]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]] [[2.376]
 [2.349]
 [2.349]
 [2.349]
 [2.349]
 [2.349]
 [2.349]] [[0.909]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[1.971]
 [4.175]
 [4.175]
 [4.175]
 [4.175]
 [4.175]
 [4.175]] [[-0.002]
 [-0.424]
 [-0.424]
 [-0.424]
 [-0.424]
 [-0.424]
 [-0.424]]
first move QE:  0.909303352521039
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2676787496302908, 0.10595124346950606, 0.11259004551939762, 0.11283454280343068, 0.2923488286971839, 0.10859658988019082]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.038]
 [-0.036]
 [-0.038]
 [-0.038]
 [-0.035]
 [-0.038]] [[5.751]
 [5.966]
 [5.812]
 [6.635]
 [6.635]
 [5.621]
 [6.635]] [[-0.27 ]
 [-0.202]
 [-0.251]
 [ 0.022]
 [ 0.022]
 [-0.312]
 [ 0.022]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2666146876190048, 0.10620597698322566, 0.1128607403877713, 0.1128607403877713, 0.2930517090214463, 0.10840614560078057]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26631763327114666, 0.10609755600234182, 0.11297410934407914, 0.11297410934407914, 0.2933460803527156, 0.10829051168563755]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26631763327114666, 0.10609755600234182, 0.11297410934407914, 0.11297410934407914, 0.2933460803527156, 0.10829051168563755]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
1708 1775
siam score:  -0.6400827
siam score:  -0.64100856
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26662392981653626, 0.10600513862660879, 0.11285953823227678, 0.11285953823227678, 0.29368346278553453, 0.10796839230676672]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.33 ]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]] [[2.087]
 [2.532]
 [2.242]
 [2.242]
 [2.242]
 [2.242]
 [2.242]] [[0.312]
 [0.33 ]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26662392981653626, 0.10600513862660879, 0.11285953823227678, 0.11285953823227678, 0.29368346278553453, 0.10796839230676672]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.229]
 [0.364]
 [0.477]
 [0.124]
 [0.364]
 [0.327]] [[2.528]
 [3.366]
 [2.528]
 [3.145]
 [2.13 ]
 [2.528]
 [3.471]] [[0.364]
 [0.229]
 [0.364]
 [0.477]
 [0.124]
 [0.364]
 [0.327]]
siam score:  -0.6391522
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]] [[4.035]
 [4.035]
 [4.035]
 [4.035]
 [4.035]
 [4.035]
 [4.035]] [[4.489]
 [4.489]
 [4.489]
 [4.489]
 [4.489]
 [4.489]
 [4.489]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.051]
 [-0.041]
 [-0.037]
 [-0.044]
 [-0.046]
 [-0.051]] [[6.059]
 [3.277]
 [3.366]
 [4.003]
 [4.078]
 [4.033]
 [4.03 ]] [[1.402]
 [0.062]
 [0.112]
 [0.413]
 [0.442]
 [0.419]
 [0.413]]
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]] [[5.12 ]
 [4.253]
 [4.253]
 [4.253]
 [4.253]
 [4.253]
 [4.253]] [[0.809]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.118]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.101]] [[2.825]
 [3.058]
 [3.051]
 [3.051]
 [3.051]
 [3.051]
 [1.933]] [[-0.107]
 [ 0.012]
 [ 0.022]
 [ 0.022]
 [ 0.022]
 [ 0.022]
 [-0.969]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2668184713638967, 0.10608248503625316, 0.11269830511123463, 0.11245581902867234, 0.2938977482599246, 0.10804717120001847]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.239]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[4.499]
 [4.799]
 [4.499]
 [4.499]
 [4.499]
 [4.499]
 [4.499]] [[-0.291]
 [-0.143]
 [-0.291]
 [-0.291]
 [-0.291]
 [-0.291]
 [-0.291]]
siam score:  -0.6361377
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2668184713638967, 0.10608248503625316, 0.11269830511123463, 0.11245581902867234, 0.2938977482599246, 0.10804717120001847]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2670015386541733, 0.10615526947648372, 0.11253297628828851, 0.11253297628828851, 0.29409939496045967, 0.10767784433230622]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.26658797904663306, 0.10621516255116008, 0.11259646768146782, 0.11259646768146782, 0.294265326591659, 0.1077385964476123]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.151]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[6.239]
 [2.419]
 [2.419]
 [2.419]
 [2.419]
 [2.419]
 [2.419]] [[1.622]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[9.448]
 [5.291]
 [5.291]
 [5.291]
 [5.291]
 [5.291]
 [5.291]] [[1.927]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2669396265426837, 0.10635526750295608, 0.11274499000443951, 0.11226203398922673, 0.29403809137124653, 0.10765999058944743]
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]] [[3.861]
 [3.528]
 [3.528]
 [3.528]
 [3.528]
 [3.528]
 [3.528]] [[1.909]
 [1.41 ]
 [1.41 ]
 [1.41 ]
 [1.41 ]
 [1.41 ]
 [1.41 ]]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]] [[3.489]
 [3.395]
 [3.395]
 [3.395]
 [3.395]
 [3.395]
 [3.395]] [[1.513]
 [1.32 ]
 [1.32 ]
 [1.32 ]
 [1.32 ]
 [1.32 ]
 [1.32 ]]
Printing some Q and Qe and total Qs values:  [[0.954]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[6.256]
 [3.322]
 [3.322]
 [3.322]
 [3.322]
 [3.322]
 [3.322]] [[1.608]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.228]
 [0.21 ]
 [0.217]
 [0.211]
 [0.21 ]
 [0.22 ]] [[4.1  ]
 [2.433]
 [2.415]
 [2.398]
 [2.467]
 [2.482]
 [2.614]] [[ 0.958]
 [-0.111]
 [-0.132]
 [-0.132]
 [-0.116]
 [-0.112]
 [-0.061]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2669396265426837, 0.10635526750295608, 0.11274499000443951, 0.11226203398922673, 0.29403809137124653, 0.10765999058944743]
Printing some Q and Qe and total Qs values:  [[ 0.068]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]] [[2.425]
 [2.488]
 [2.488]
 [2.488]
 [2.488]
 [2.488]
 [2.488]] [[-0.235]
 [-0.35 ]
 [-0.35 ]
 [-0.35 ]
 [-0.35 ]
 [-0.35 ]
 [-0.35 ]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2665841775842696, 0.10622353568325447, 0.11283263440279712, 0.11234930295281437, 0.2942666673071833, 0.10774368206968107]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2662297055017191, 0.10609215053829314, 0.1129200406928352, 0.11243633482804663, 0.2944946222584023, 0.10782714618070355]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.46 ]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]] [[2.166]
 [3.452]
 [2.166]
 [2.166]
 [2.166]
 [2.166]
 [2.166]] [[0.282]
 [1.241]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.041 0.204 0.02  0.592 0.041 0.02  0.082]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2662297055017191, 0.10609215053829314, 0.1129200406928352, 0.11243633482804663, 0.2944946222584023, 0.10782714618070355]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26629425395160805, 0.10611787300171896, 0.11270496463213439, 0.11246359546416036, 0.2945660236496617, 0.1078532893007164]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.534]
 [0.491]
 [0.487]
 [0.487]
 [0.544]
 [0.544]] [[2.322]
 [2.595]
 [2.002]
 [1.96 ]
 [1.906]
 [2.322]
 [2.322]] [[ 0.367]
 [ 0.524]
 [ 0.055]
 [ 0.02 ]
 [-0.015]
 [ 0.367]
 [ 0.367]]
siam score:  -0.64247674
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26629425395160805, 0.10611787300171896, 0.11270496463213439, 0.11246359546416036, 0.2945660236496617, 0.1078532893007164]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.211]
 [0.244]
 [0.231]
 [0.225]
 [0.229]
 [0.246]] [[6.079]
 [4.529]
 [5.588]
 [5.738]
 [5.708]
 [5.381]
 [5.059]] [[ 1.052]
 [-0.237]
 [ 0.181]
 [ 0.207]
 [ 0.184]
 [ 0.084]
 [ 0.01 ]]
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[1.114]
 [1.114]
 [1.099]
 [1.114]
 [1.114]
 [1.114]
 [1.114]] [[0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.458]
 [0.9  ]
 [0.588]
 [0.749]
 [0.472]
 [0.478]] [[1.368]
 [1.98 ]
 [0.554]
 [0.436]
 [0.299]
 [0.621]
 [0.621]] [[0.468]
 [0.458]
 [0.9  ]
 [0.588]
 [0.749]
 [0.472]
 [0.478]]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.057]
 [-0.069]
 [-0.069]
 [-0.071]
 [-0.071]
 [-0.072]] [[4.328]
 [4.209]
 [4.543]
 [4.369]
 [4.395]
 [4.321]
 [4.261]] [[-0.911]
 [-0.992]
 [-0.905]
 [-0.963]
 [-0.958]
 [-0.984]
 [-1.004]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2663313188780571, 0.10614248927418204, 0.11295765196346547, 0.11247491127297457, 0.2939985295559265, 0.1080950990553943]
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]] [[-1.546]
 [-1.546]
 [-1.546]
 [-1.546]
 [-1.546]
 [-1.546]
 [-1.546]] [[0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]]
from probs:  [0.2663901019305919, 0.10616591640828502, 0.11298258329943281, 0.11249973606130985, 0.2940634191493879, 0.10789824315099265]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64057195
line 256 mcts: sample exp_bonus 1.9961945132684624
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26603728566087675, 0.10603512564592353, 0.11306966545080054, 0.1125864460546418, 0.2942900706777054, 0.10798140651005203]
line 256 mcts: sample exp_bonus 3.369289482604031
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]] [[0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]] [[0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]]
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[1.253]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[0.378]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2663148789381032, 0.10593381644581733, 0.11318764657855643, 0.11270392297316452, 0.2939858283898089, 0.10787390667454966]
using explorer policy with actor:  0
from probs:  [0.2663148789381032, 0.10593381644581733, 0.11318764657855643, 0.11270392297316452, 0.2939858283898089, 0.10787390667454966]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.357]
 [0.377]
 [0.377]
 [0.374]
 [0.389]
 [0.421]] [[1.258]
 [1.545]
 [0.908]
 [0.656]
 [0.717]
 [0.652]
 [0.712]] [[0.363]
 [0.357]
 [0.377]
 [0.377]
 [0.374]
 [0.389]
 [0.421]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2663711006225642, 0.10574507030042907, 0.11321154160153149, 0.11272771587733939, 0.2940478916832518, 0.10789667991488387]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[0.998]
 [1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]] [[0.435]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]]
using explorer policy with actor:  0
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.316]
 [0.366]
 [0.379]
 [0.379]
 [0.379]
 [0.375]] [[1.327]
 [1.617]
 [1.174]
 [1.242]
 [1.242]
 [1.242]
 [1.164]] [[0.358]
 [0.316]
 [0.366]
 [0.379]
 [0.379]
 [0.379]
 [0.375]]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[1.171]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]] [[0.386]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]]
siam score:  -0.64706343
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.54 ]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[0.288]
 [1.756]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[1.063]
 [1.785]
 [1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]]
line 256 mcts: sample exp_bonus 0.25645635172803183
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 3.5040461227018227
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.332]
 [0.417]
 [0.407]
 [0.417]
 [0.417]
 [0.403]] [[1.198]
 [2.364]
 [1.024]
 [0.907]
 [1.024]
 [1.024]
 [1.237]] [[0.409]
 [0.332]
 [0.417]
 [0.407]
 [0.417]
 [0.417]
 [0.403]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[1.348]
 [1.29 ]
 [1.29 ]
 [1.29 ]
 [1.29 ]
 [1.29 ]
 [1.29 ]] [[0.387]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]]
line 256 mcts: sample exp_bonus 1.78951035093557
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 6.118615018760467
using explorer policy with actor:  0
siam score:  -0.65581083
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[0.542]
 [1.308]
 [1.308]
 [1.308]
 [1.308]
 [1.308]
 [1.308]] [[0.438]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.365]
 [0.599]
 [0.403]
 [0.401]
 [0.391]
 [0.401]] [[1.5  ]
 [2.626]
 [2.219]
 [1.097]
 [1.208]
 [1.338]
 [1.25 ]] [[0.42 ]
 [0.365]
 [0.599]
 [0.403]
 [0.401]
 [0.391]
 [0.401]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2670534622224582, 0.10559529585842564, 0.11325847992939618, 0.11277557785444049, 0.2935828578786249, 0.10773432625665458]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[1.814]
 [1.996]
 [1.996]
 [1.996]
 [1.996]
 [1.996]
 [1.996]] [[0.448]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 0.5204341235216606
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 3.54187416175885
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.361]
 [0.392]
 [0.398]
 [0.522]
 [0.391]
 [0.394]] [[1.202]
 [2.822]
 [1.635]
 [1.054]
 [2.04 ]
 [1.22 ]
 [0.809]] [[0.397]
 [0.361]
 [0.392]
 [0.398]
 [0.522]
 [0.391]
 [0.394]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.526]
 [0.575]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[4.441]
 [3.849]
 [3.415]
 [3.861]
 [3.861]
 [3.861]
 [4.197]] [[2.031]
 [1.735]
 [1.686]
 [1.857]
 [1.857]
 [1.857]
 [1.969]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[1.472]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]] [[0.372]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
start point for exploration sampling:  10935
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.8973143496588142
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]] [[7.862]
 [5.276]
 [5.276]
 [5.276]
 [5.276]
 [5.276]
 [5.276]] [[1.076]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.409]
 [0.391]
 [0.421]
 [0.421]
 [0.419]
 [0.425]] [[1.386]
 [2.427]
 [2.721]
 [1.53 ]
 [1.718]
 [1.543]
 [1.129]] [[0.419]
 [0.409]
 [0.391]
 [0.421]
 [0.421]
 [0.419]
 [0.425]]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]] [[4.813]
 [4.262]
 [4.262]
 [4.262]
 [4.262]
 [4.262]
 [4.262]] [[0.465]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2673433813677816, 0.10508478137461903, 0.11313918313990542, 0.11289800951614902, 0.2939015779226544, 0.10763306667889043]
rdn probs:  [0.2673433813677816, 0.10508478137461903, 0.11313918313990542, 0.11289800951614902, 0.2939015779226544, 0.10763306667889043]
1732 1812
line 256 mcts: sample exp_bonus 3.798333168485102
from probs:  [0.26686543543418445, 0.1051289244836257, 0.11316799666243428, 0.11316799666243428, 0.2939969989415708, 0.10767264781575046]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26692333010099056, 0.10515173149887203, 0.11319254770048547, 0.11319254770048547, 0.2940607796191547, 0.10747906338001177]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.443]
 [0.443]
 [0.448]
 [0.443]
 [0.443]
 [0.494]] [[4.21 ]
 [4.21 ]
 [4.21 ]
 [4.473]
 [4.21 ]
 [4.21 ]
 [3.984]] [[1.402]
 [1.402]
 [1.402]
 [1.599]
 [1.402]
 [1.402]
 [1.313]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]] [[0.856]
 [0.874]
 [0.885]
 [0.885]
 [0.896]
 [0.874]
 [0.896]] [[-0.286]
 [-0.268]
 [-0.257]
 [-0.257]
 [-0.246]
 [-0.268]
 [-0.246]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26700216680235017, 0.10540202780972956, 0.11298045161065422, 0.11298045161065422, 0.29476074268450236, 0.10687415948210952]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26700216680235017, 0.10540202780972956, 0.11298045161065422, 0.11298045161065422, 0.29476074268450236, 0.10687415948210952]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.886]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]] [[2.349]
 [0.874]
 [2.349]
 [2.349]
 [2.349]
 [2.349]
 [2.349]] [[0.275]
 [0.886]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]]
first move QE:  0.89582975664111
Printing some Q and Qe and total Qs values:  [[1.1  ]
 [0.891]
 [0.891]
 [0.891]
 [0.891]
 [0.891]
 [0.891]] [[2.5  ]
 [3.071]
 [3.071]
 [3.071]
 [3.071]
 [3.071]
 [3.071]] [[1.826]
 [1.79 ]
 [1.79 ]
 [1.79 ]
 [1.79 ]
 [1.79 ]
 [1.79 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2670660404830011, 0.10542724264429403, 0.11300747939613026, 0.11276825406917551, 0.2948312569195445, 0.10689972648785463]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.642]
 [0.32 ]
 [0.307]
 [0.322]
 [0.326]
 [0.498]] [[5.27 ]
 [9.022]
 [8.025]
 [8.035]
 [7.693]
 [8.08 ]
 [6.707]] [[0.31 ]
 [1.636]
 [0.659]
 [0.636]
 [0.551]
 [0.689]
 [0.574]]
using explorer policy with actor:  1
using another actor
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26676976247735446, 0.10511598072783235, 0.11311689106198805, 0.11287743412167746, 0.2951167068662107, 0.10700322474493705]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.487]
 [0.441]
 [0.429]
 [0.539]
 [0.539]
 [0.431]] [[4.195]
 [4.952]
 [3.944]
 [3.887]
 [4.351]
 [4.351]
 [4.401]] [[0.415]
 [0.487]
 [0.441]
 [0.429]
 [0.539]
 [0.539]
 [0.431]]
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.473]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.471]] [[6.163]
 [6.462]
 [6.163]
 [6.163]
 [6.163]
 [6.163]
 [6.534]] [[0.467]
 [0.473]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.471]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.561]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.577]] [[5.164]
 [5.802]
 [5.164]
 [5.164]
 [5.164]
 [5.164]
 [5.037]] [[0.241]
 [0.384]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.162]]
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.193]
 [0.373]
 [0.373]
 [0.442]
 [0.373]
 [0.276]] [[2.118]
 [2.623]
 [2.328]
 [2.328]
 [1.888]
 [2.328]
 [2.41 ]] [[1.036]
 [1.429]
 [1.297]
 [1.297]
 [0.869]
 [1.297]
 [1.278]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2668333754834325, 0.10514104632544298, 0.11314386452933996, 0.11266589390568311, 0.2951870793877937, 0.10702874036830778]
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.587]
 [0.728]
 [0.599]
 [0.599]
 [0.599]
 [0.599]] [[3.383]
 [2.924]
 [3.269]
 [3.383]
 [3.383]
 [3.383]
 [3.383]] [[0.599]
 [0.587]
 [0.728]
 [0.599]
 [0.599]
 [0.599]
 [0.599]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 3.6981179498781103
Printing some Q and Qe and total Qs values:  [[-0.109]
 [-0.118]
 [-0.113]
 [-0.114]
 [-0.116]
 [-0.114]
 [-0.112]] [[4.632]
 [3.905]
 [4.744]
 [4.755]
 [4.928]
 [4.92 ]
 [4.962]] [[-1.432]
 [-1.694]
 [-1.402]
 [-1.401]
 [-1.348]
 [-1.346]
 [-1.329]]
siam score:  -0.65303123
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]] [[4.703]
 [4.703]
 [4.703]
 [4.703]
 [4.703]
 [4.703]
 [4.703]] [[1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]]
Printing some Q and Qe and total Qs values:  [[1.252]
 [0.951]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]] [[2.149]
 [2.016]
 [2.852]
 [2.852]
 [2.852]
 [2.852]
 [2.852]] [[2.623]
 [1.932]
 [1.965]
 [1.965]
 [1.965]
 [1.965]
 [1.965]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2666612972124893, 0.10529131001046788, 0.11330556554541613, 0.11258916641811806, 0.2956089498168101, 0.10654371099669853]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26671617122943736, 0.10510719532891778, 0.1133288817581138, 0.11261233520898038, 0.2956697807311265, 0.10656563574342426]
line 256 mcts: sample exp_bonus 1.3574562728265267
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2665918340398146, 0.10507049642627368, 0.11351063745673186, 0.11255581294223929, 0.295534674517514, 0.1067365446174266]
line 256 mcts: sample exp_bonus 2.110497660614124
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.571]
 [0.574]
 [0.578]
 [0.569]
 [0.592]
 [0.583]] [[5.586]
 [5.875]
 [5.711]
 [5.514]
 [6.732]
 [5.782]
 [5.591]] [[1.688]
 [1.937]
 [1.813]
 [1.665]
 [2.605]
 [1.897]
 [1.733]]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[2.718]
 [2.718]
 [2.718]
 [2.718]
 [2.718]
 [2.718]
 [2.718]] [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2671588479533436, 0.10488507381257203, 0.11351126158667794, 0.11255861691796763, 0.29555520209204866, 0.10633099763739037]
siam score:  -0.65849364
Printing some Q and Qe and total Qs values:  [[ 0.09 ]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[4.835]
 [5.152]
 [5.152]
 [5.152]
 [5.152]
 [5.152]
 [5.152]] [[0.321]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]]
from probs:  [0.2671588479533436, 0.10488507381257203, 0.11351126158667794, 0.11255861691796763, 0.29555520209204866, 0.10633099763739037]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.470560648496055
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.66139436
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.424]
 [0.429]
 [0.408]
 [0.412]
 [0.465]
 [0.435]] [[5.154]
 [5.294]
 [5.397]
 [5.178]
 [5.523]
 [5.594]
 [5.296]] [[1.505]
 [1.622]
 [1.698]
 [1.525]
 [1.768]
 [1.872]
 [1.634]]
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.358]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]] [[3.983]
 [4.555]
 [3.983]
 [3.983]
 [3.983]
 [3.983]
 [3.983]] [[1.63 ]
 [1.831]
 [1.63 ]
 [1.63 ]
 [1.63 ]
 [1.63 ]
 [1.63 ]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2669455691876854, 0.1050179130268209, 0.11317599945353872, 0.1124653210392886, 0.2959295291472332, 0.10646566814543319]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2669455691876854, 0.1050179130268209, 0.11317599945353872, 0.1124653210392886, 0.2959295291472332, 0.10646566814543319]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.612]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.617]] [[3.265]
 [3.298]
 [3.265]
 [3.265]
 [3.265]
 [3.265]
 [4.159]] [[0.994]
 [1.094]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [1.391]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.0341108719756225
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26705582050790044, 0.10485774329785559, 0.11322274232847902, 0.11251177039658912, 0.2960517511469404, 0.10630017232223547]
siam score:  -0.6612579
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.023]
 [-0.024]
 [-0.012]
 [-0.015]
 [-0.011]
 [-0.046]] [[3.905]
 [3.447]
 [3.767]
 [3.996]
 [4.097]
 [4.873]
 [4.309]] [[-0.268]
 [-0.394]
 [-0.288]
 [-0.188]
 [-0.16 ]
 [ 0.106]
 [-0.151]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.199]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[6.144]
 [5.276]
 [5.276]
 [5.276]
 [5.276]
 [5.276]
 [5.276]] [[ 0.081]
 [-0.613]
 [-0.613]
 [-0.613]
 [-0.613]
 [-0.613]
 [-0.613]]
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[3.683]
 [4.185]
 [4.185]
 [4.185]
 [4.185]
 [4.185]
 [4.185]] [[-1.085]
 [-0.905]
 [-0.905]
 [-0.905]
 [-0.905]
 [-0.905]
 [-0.905]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6550925
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[5.254]
 [5.254]
 [5.254]
 [5.254]
 [5.254]
 [5.254]
 [5.254]] [[-0.137]
 [-0.137]
 [-0.137]
 [-0.137]
 [-0.137]
 [-0.137]
 [-0.137]]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[1.695]
 [1.36 ]
 [1.36 ]
 [1.36 ]
 [1.36 ]
 [1.36 ]
 [1.36 ]] [[0.603]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.221]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]] [[2.158]
 [3.431]
 [2.158]
 [2.158]
 [2.158]
 [2.158]
 [2.158]] [[0.198]
 [0.221]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]]
line 256 mcts: sample exp_bonus 10.0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65508413
1774 1841
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.266668272828034, 0.10493412128992732, 0.11304839880986982, 0.1123417500168418, 0.2968412661003482, 0.1061661909549788]
1775 1842
Printing some Q and Qe and total Qs values:  [[1.169]
 [1.148]
 [1.17 ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]] [[1.104]
 [1.143]
 [0.978]
 [2.43 ]
 [2.43 ]
 [2.43 ]
 [2.43 ]] [[1.305]
 [1.314]
 [1.17 ]
 [1.821]
 [1.821]
 [1.821]
 [1.821]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.335]
 [0.336]
 [0.32 ]
 [0.32 ]
 [0.348]
 [0.34 ]] [[6.68 ]
 [7.195]
 [6.436]
 [6.848]
 [6.848]
 [6.197]
 [5.96 ]] [[0.456]
 [0.633]
 [0.382]
 [0.488]
 [0.488]
 [0.325]
 [0.23 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6570935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26650309612616513, 0.1050844503963509, 0.11297343309216486, 0.11226885547096263, 0.2972665222680653, 0.10590364264629115]
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.408]
 [0.423]
 [0.455]
 [0.094]
 [0.878]
 [0.33 ]] [[2.201]
 [1.551]
 [1.087]
 [1.259]
 [1.653]
 [2.264]
 [1.815]] [[0.613]
 [0.701]
 [0.575]
 [0.698]
 [0.107]
 [1.879]
 [0.634]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.407]
 [1.431]
 [0.893]
 [0.893]
 [0.893]
 [0.893]
 [1.319]] [[0.939]
 [0.941]
 [1.761]
 [1.761]
 [1.761]
 [1.761]
 [0.969]] [[1.601]
 [1.652]
 [1.397]
 [1.397]
 [1.397]
 [1.397]
 [1.456]]
from probs:  [0.26617859582140413, 0.10538657533523738, 0.11235814530240934, 0.11235814530240934, 0.29751041541995227, 0.10620812281858766]
siam score:  -0.66157305
using explorer policy with actor:  1
1784 1859
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2666323225257909, 0.10497024418693096, 0.11231360424913338, 0.11208226234048008, 0.29801506960877305, 0.10598649708889167]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2662883112881342, 0.10484729611130358, 0.11239706398454044, 0.11216555016675277, 0.2982365232699136, 0.10606525517935539]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.49 ]
 [0.465]
 [0.498]
 [0.465]
 [0.465]
 [0.518]] [[2.539]
 [2.691]
 [2.539]
 [2.062]
 [2.539]
 [2.539]
 [2.578]] [[1.257]
 [1.444]
 [1.257]
 [0.808]
 [1.257]
 [1.257]
 [1.363]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2666351975620489, 0.10498387772730522, 0.11208084671713411, 0.11208084671713411, 0.29801580788008863, 0.10620342339628913]
line 256 mcts: sample exp_bonus 8.405073559253195
Printing some Q and Qe and total Qs values:  [[ 0.112]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[6.984]
 [4.251]
 [4.251]
 [4.251]
 [4.251]
 [4.251]
 [4.251]] [[ 0.281]
 [-0.755]
 [-0.755]
 [-0.755]
 [-0.755]
 [-0.755]
 [-0.755]]
first move QE:  0.8824647147047533
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.723]
 [0.723]
 [0.522]
 [0.723]
 [0.723]
 [0.723]] [[3.393]
 [3.393]
 [3.393]
 [2.638]
 [3.393]
 [3.393]
 [3.393]] [[1.111]
 [1.111]
 [1.111]
 [0.207]
 [1.111]
 [1.111]
 [1.111]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[3.692]
 [3.486]
 [3.486]
 [3.486]
 [3.486]
 [3.486]
 [3.486]] [[0.397]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[4.749]
 [4.021]
 [4.021]
 [4.021]
 [4.021]
 [4.021]
 [4.021]] [[1.093]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2664002322079227, 0.10510446456522915, 0.11220958529290441, 0.11220958529290441, 0.2977507216093028, 0.10632541103173654]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2660046111482112, 0.10516114607124549, 0.11227009850050393, 0.11227009850050393, 0.2979112948008231, 0.1063827509787123]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.81]
 [0.81]
 [0.81]
 [1.37]
 [0.81]
 [0.81]
 [0.81]] [[1.216]
 [1.216]
 [1.216]
 [1.582]
 [1.216]
 [1.216]
 [1.216]] [[1.27 ]
 [1.27 ]
 [1.27 ]
 [2.635]
 [1.27 ]
 [1.27 ]
 [1.27 ]]
line 256 mcts: sample exp_bonus 5.515571067005953
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
first move QE:  0.8824428667339658
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26622732839256824, 0.10484817227978258, 0.11213369016636748, 0.11236409870168011, 0.29816072650185443, 0.10626598395774701]
siam score:  -0.644483
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.961]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.795]] [[1.712]
 [1.821]
 [1.712]
 [1.712]
 [1.712]
 [1.712]
 [1.784]] [[1.221]
 [1.908]
 [1.221]
 [1.221]
 [1.221]
 [1.221]
 [1.687]]
from probs:  [0.2958357121829822, 0.005549096293659285, 0.1243496643855547, 0.12486063457089619, 0.33132057254319713, 0.11808432002371032]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.485]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.556]] [[-0.285]
 [ 2.356]
 [ 1.546]
 [ 1.546]
 [ 1.546]
 [ 1.546]
 [ 0.848]] [[0.16 ]
 [1.679]
 [1.403]
 [1.403]
 [1.403]
 [1.403]
 [0.967]]
from probs:  [0.2968771604738377, 0.005568631111846649, 0.124026449012072, 0.12478742000500706, 0.330468926072625, 0.11827141332461166]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.472]
 [0.551]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[2.201]
 [2.207]
 [2.372]
 [2.519]
 [2.519]
 [2.519]
 [2.519]] [[0.907]
 [0.757]
 [1.08 ]
 [1.256]
 [1.256]
 [1.256]
 [1.256]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29645545568670895, 0.005571970952718839, 0.1241008350856551, 0.12486226247837204, 0.33066712803718645, 0.1183423477593586]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.322]
 [0.497]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[2.53 ]
 [2.53 ]
 [1.455]
 [2.53 ]
 [2.53 ]
 [2.53 ]
 [2.53 ]] [[0.322]
 [0.322]
 [0.497]
 [0.322]
 [0.322]
 [0.322]
 [0.322]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]] [[0.881]
 [0.87 ]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]] [[1.395]
 [1.376]
 [1.395]
 [1.395]
 [1.395]
 [1.395]
 [1.395]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[9.163]
 [4.924]
 [4.924]
 [4.924]
 [4.924]
 [4.924]
 [4.924]] [[1.913]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2971440307819402, 0.005584912932216253, 0.12388567576015987, 0.12489679339116794, 0.3300997144956543, 0.11838887263886148]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2971440307819402, 0.005584912932216253, 0.12388567576015987, 0.12489679339116794, 0.3300997144956543, 0.11838887263886148]
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]] [[3.651]
 [1.916]
 [1.916]
 [1.916]
 [1.916]
 [1.916]
 [1.916]] [[0.884]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]]
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]] [[1.976]
 [1.976]
 [1.976]
 [1.976]
 [1.976]
 [1.976]
 [1.976]] [[0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.372]
 [0.396]
 [0.38 ]
 [0.358]
 [0.355]
 [0.374]] [[3.841]
 [3.486]
 [3.378]
 [2.995]
 [3.385]
 [3.484]
 [3.422]] [[0.878]
 [0.735]
 [0.707]
 [0.511]
 [0.674]
 [0.718]
 [0.706]]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.586]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.544]] [[1.406]
 [1.818]
 [1.889]
 [1.889]
 [1.889]
 [1.889]
 [2.111]] [[1.457]
 [1.71 ]
 [1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.776]]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.484]
 [0.552]
 [0.553]
 [0.55 ]
 [0.623]
 [0.55 ]] [[-3.799]
 [-1.605]
 [-3.961]
 [-4.044]
 [-4.036]
 [ 0.537]
 [-3.894]] [[0.327]
 [1.504]
 [0.242]
 [0.197]
 [0.199]
 [2.786]
 [0.278]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.473]
 [0.466]
 [0.478]
 [0.478]
 [0.463]
 [0.483]] [[6.258]
 [6.437]
 [6.902]
 [6.531]
 [6.531]
 [6.998]
 [6.666]] [[1.459]
 [1.57 ]
 [1.822]
 [1.626]
 [1.626]
 [1.873]
 [1.703]]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.487]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[7.495]
 [7.385]
 [7.097]
 [7.097]
 [7.097]
 [7.097]
 [7.097]] [[1.723]
 [1.666]
 [1.535]
 [1.535]
 [1.535]
 [1.535]
 [1.535]]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[6.103]
 [6.103]
 [6.103]
 [6.103]
 [6.103]
 [6.103]
 [6.103]] [[1.732]
 [1.732]
 [1.732]
 [1.732]
 [1.732]
 [1.732]
 [1.732]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6515397
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29748350828495873, 0.005591293515705733, 0.12377681810165767, 0.12503948396261869, 0.32981245518154045, 0.11829644095351878]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29733481983504595, 0.005599781276720832, 0.12396471524000413, 0.12497422427167394, 0.3296504406711388, 0.11847601870541641]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.14384533688591
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2975311847568603, 0.005603479466570211, 0.12404658362537975, 0.1250567593538131, 0.32920773053419977, 0.1185542622631769]
siam score:  -0.6539959
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2971107075884037, 0.005606833544542758, 0.12412083426129085, 0.12513161465119915, 0.32940478459747674, 0.11862522535708682]
siam score:  -0.65635484
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.641]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[3.251]
 [3.827]
 [2.557]
 [2.557]
 [2.557]
 [2.557]
 [2.557]] [[1.459]
 [1.909]
 [1.051]
 [1.051]
 [1.051]
 [1.051]
 [1.051]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2973346940842883, 0.005611060436965599, 0.12371502424725633, 0.12497144939417433, 0.32965311702557565, 0.11871465481174001]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.413]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]] [[1.709]
 [2.985]
 [1.709]
 [1.709]
 [1.709]
 [1.709]
 [1.709]] [[0.39 ]
 [0.413]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]] [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]]
first move QE:  0.8776784083844642
start point for exploration sampling:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2974084863555265, 0.005612452985167799, 0.12349754858801014, 0.12500246470208898, 0.32973493005550475, 0.11874411731370198]
first move QE:  0.8774753943765398
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.631]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[3.14 ]
 [3.617]
 [3.14 ]
 [3.14 ]
 [3.14 ]
 [3.14 ]
 [3.14 ]] [[1.056]
 [1.535]
 [1.056]
 [1.056]
 [1.056]
 [1.056]
 [1.056]]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[7.188]
 [7.188]
 [7.188]
 [7.188]
 [7.188]
 [7.188]
 [7.188]] [[0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]]
Printing some Q and Qe and total Qs values:  [[1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]] [[0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]] [[2.46]
 [2.46]
 [2.46]
 [2.46]
 [2.46]
 [2.46]
 [2.46]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29755744972769377, 0.0056152641017335895, 0.1233121426146452, 0.1248114659078033, 0.3299000848152236, 0.1188035928329006]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6585747
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.2507120264809912
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2976234882121458, 0.0056278019394110776, 0.12309495090185814, 0.12483704845003002, 0.32997610301788405, 0.11884060747867092]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.248]
 [0.403]
 [0.399]
 [0.387]
 [0.386]
 [0.34 ]] [[6.715]
 [5.473]
 [5.925]
 [4.387]
 [4.491]
 [4.595]
 [5.038]] [[0.797]
 [0.151]
 [0.611]
 [0.091]
 [0.1  ]
 [0.134]
 [0.19 ]]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.28 ]
 [0.356]
 [0.369]
 [0.356]
 [0.369]
 [0.367]] [[4.651]
 [4.452]
 [4.462]
 [4.739]
 [4.835]
 [4.969]
 [4.6  ]] [[1.234]
 [1.006]
 [1.064]
 [1.205]
 [1.241]
 [1.314]
 [1.137]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[7.081]
 [7.421]
 [7.421]
 [7.421]
 [7.421]
 [7.421]
 [7.421]] [[1.619]
 [1.738]
 [1.738]
 [1.738]
 [1.738]
 [1.738]
 [1.738]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29769634142148604, 0.005629179530391146, 0.12288029933824245, 0.12486760645362975, 0.3300568756015764, 0.11886969765467409]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.349]
 [0.333]
 [0.329]
 [0.334]
 [0.315]
 [0.34 ]] [[2.024]
 [2.279]
 [2.216]
 [2.08 ]
 [2.068]
 [2.214]
 [1.979]] [[0.339]
 [0.349]
 [0.333]
 [0.329]
 [0.334]
 [0.315]
 [0.34 ]]
line 256 mcts: sample exp_bonus 4.963081641527146
actor:  1 policy actor:  1  step number:  143 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.357]
 [0.367]
 [0.337]
 [0.32 ]
 [0.312]
 [0.317]] [[4.39 ]
 [5.388]
 [4.655]
 [4.361]
 [4.619]
 [4.568]
 [4.72 ]] [[0.772]
 [1.357]
 [0.974]
 [0.783]
 [0.904]
 [0.869]
 [0.955]]
line 256 mcts: sample exp_bonus 1.6923048935265224
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]] [[6.259]
 [6.259]
 [6.259]
 [6.259]
 [6.259]
 [6.259]
 [6.259]] [[1.107]
 [1.107]
 [1.107]
 [1.107]
 [1.107]
 [1.107]
 [1.107]]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.205]
 [0.202]
 [0.2  ]
 [0.198]
 [0.203]
 [0.196]] [[4.338]
 [3.563]
 [3.351]
 [3.403]
 [3.325]
 [3.655]
 [3.458]] [[0.94 ]
 [0.326]
 [0.231]
 [0.253]
 [0.218]
 [0.365]
 [0.273]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.66236204
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.007]
 [-0.001]
 [ 0.059]
 [ 0.055]
 [-0.003]
 [ 0.055]] [[2.018]
 [2.717]
 [1.923]
 [1.688]
 [3.254]
 [1.807]
 [3.254]] [[-0.309]
 [ 0.851]
 [-0.462]
 [-0.733]
 [ 1.868]
 [-0.659]
 [ 1.868]]
siam score:  -0.6623813
start point for exploration sampling:  10935
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2612080584148292, 0.004959035534428745, 0.10803686873426688, 0.2320455412888646, 0.28903202836215136, 0.10471846766545921]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]] [[7.535]
 [3.795]
 [3.795]
 [3.795]
 [3.795]
 [3.795]
 [3.795]] [[2.43 ]
 [1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]]
first move QE:  0.8742627585495287
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]] [[5.998]
 [6.054]
 [6.054]
 [6.054]
 [6.054]
 [6.054]
 [6.054]] [[0.452]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]]
siam score:  -0.67085576
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2606756422884487, 0.004968701938194287, 0.10781980437077537, 0.2320178369228324, 0.28959542426200424, 0.10492259021774492]
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.625]
 [0.771]
 [0.792]
 [0.807]
 [0.753]
 [0.816]] [[2.826]
 [3.446]
 [3.006]
 [2.906]
 [2.965]
 [2.693]
 [2.803]] [[1.355]
 [1.471]
 [1.346]
 [1.296]
 [1.372]
 [1.05 ]
 [1.251]]
Printing some Q and Qe and total Qs values:  [[0.87 ]
 [0.694]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]] [[4.804]
 [5.078]
 [4.905]
 [4.905]
 [4.905]
 [4.905]
 [4.905]] [[1.685]
 [1.521]
 [1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.476]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2606756422884487, 0.004968701938194287, 0.10781980437077537, 0.2320178369228324, 0.28959542426200424, 0.10492259021774492]
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.016]
 [-0.012]
 [ 0.111]
 [-0.01 ]
 [-0.015]
 [-0.018]] [[2.826]
 [3.471]
 [3.065]
 [2.956]
 [2.575]
 [2.862]
 [3.327]] [[0.23 ]
 [0.613]
 [0.377]
 [0.422]
 [0.091]
 [0.256]
 [0.527]]
using explorer policy with actor:  1
using explorer policy with actor:  1
1833 1907
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.602]
 [0.583]
 [0.583]
 [0.567]
 [0.583]
 [0.583]] [[1.123]
 [2.236]
 [1.237]
 [1.237]
 [0.991]
 [1.237]
 [1.237]] [[-0.024]
 [ 0.435]
 [ 0.037]
 [ 0.037]
 [-0.069]
 [ 0.037]
 [ 0.037]]
siam score:  -0.67721236
using explorer policy with actor:  1
start point for exploration sampling:  10935
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 7.61950685282793
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]] [[3.96 ]
 [4.217]
 [4.217]
 [4.217]
 [4.217]
 [4.217]
 [4.217]] [[-0.063]
 [-0.228]
 [-0.228]
 [-0.228]
 [-0.228]
 [-0.228]
 [-0.228]]
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.194]
 [0.215]
 [0.211]
 [0.211]
 [0.215]
 [0.214]] [[5.831]
 [6.025]
 [5.45 ]
 [5.282]
 [5.222]
 [5.386]
 [5.259]] [[0.977]
 [1.055]
 [0.777]
 [0.685]
 [0.654]
 [0.744]
 [0.676]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.309]
 [0.294]
 [0.311]
 [0.29 ]
 [0.295]
 [0.314]] [[2.142]
 [2.546]
 [2.067]
 [1.83 ]
 [1.909]
 [1.834]
 [1.91 ]] [[0.307]
 [0.309]
 [0.294]
 [0.311]
 [0.29 ]
 [0.295]
 [0.314]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4234],
        [-0.4370],
        [-0.4750],
        [-0.4225],
        [-0.4667],
        [-0.0000],
        [-0.4162],
        [-0.0000],
        [-0.0000],
        [-0.4507]], dtype=torch.float64)
-0.024259925299500003 -0.4476732131403876
-0.024259925299500003 -0.46128745818519157
-0.0631738157985 -0.5381819320672879
-0.024259925299500003 -0.4467863457035144
-0.024259925299500003 -0.49093409357208523
-0.9108 -0.9108
-0.024259925299500003 -0.4404117165178002
-0.004949999999999235 -0.004949999999999235
-0.9605475 -0.9605475
-0.024259925299500003 -0.4749578779158758
siam score:  -0.67057335
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.60095506411922
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.963]
 [0.472]
 [0.472]
 [0.459]
 [0.456]
 [0.452]] [[5.075]
 [3.677]
 [4.741]
 [4.741]
 [3.948]
 [4.055]
 [5.284]] [[0.415]
 [1.009]
 [0.384]
 [0.384]
 [0.093]
 [0.123]
 [0.526]]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.413]
 [0.475]
 [0.436]
 [0.476]
 [0.475]
 [0.471]] [[3.295]
 [3.778]
 [3.44 ]
 [3.285]
 [3.272]
 [3.211]
 [3.023]] [[0.721]
 [1.059]
 [0.862]
 [0.648]
 [0.708]
 [0.65 ]
 [0.469]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32658866578260015, 0.006249831377101399, 0.13455745916610123, 0.29064132067324044, 0.2357128916238555, 0.006249831377101399]
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.5  ]
 [0.255]
 [0.264]
 [0.264]
 [0.274]
 [0.341]] [[1.153]
 [1.813]
 [1.516]
 [1.503]
 [1.484]
 [1.466]
 [1.472]] [[-0.023]
 [ 1.392]
 [ 0.494]
 [ 0.486]
 [ 0.456]
 [ 0.441]
 [ 0.562]]
line 256 mcts: sample exp_bonus 3.2963473158579775
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32658866578260015, 0.006249831377101399, 0.13455745916610123, 0.29064132067324044, 0.2357128916238555, 0.006249831377101399]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -0.7087820805110344
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32667455684294344, 0.00625147504909675, 0.13432985243847548, 0.29071775777540493, 0.23577488284498266, 0.00625147504909675]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.473]
 [0.496]
 [0.527]
 [0.51 ]
 [0.476]
 [0.497]] [[4.917]
 [5.24 ]
 [5.152]
 [4.912]
 [5.035]
 [5.211]
 [5.355]] [[1.187]
 [1.331]
 [1.311]
 [1.226]
 [1.269]
 [1.321]
 [1.408]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32667455684294344, 0.00625147504909675, 0.13432985243847548, 0.29071775777540493, 0.23577488284498266, 0.00625147504909675]
first move QE:  0.8707072619594014
siam score:  -0.6573939
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.489]
 [0.512]
 [0.512]
 [0.512]
 [0.459]
 [0.512]] [[2.489]
 [2.737]
 [2.489]
 [2.489]
 [2.489]
 [2.66 ]
 [2.489]] [[0.154]
 [0.191]
 [0.154]
 [0.154]
 [0.154]
 [0.105]
 [0.154]]
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.373]
 [0.385]
 [0.411]
 [0.382]
 [0.436]
 [0.354]] [[6.156]
 [5.764]
 [5.864]
 [5.227]
 [5.92 ]
 [5.046]
 [6.816]] [[0.603]
 [0.509]
 [0.565]
 [0.405]
 [0.578]
 [0.395]
 [0.821]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3268454214602405, 0.006254744835094561, 0.13387707031825763, 0.2908698154651859, 0.23589820308612694, 0.006254744835094561]
Printing some Q and Qe and total Qs values:  [[0.33]
 [0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]] [[5.353]
 [4.707]
 [4.707]
 [4.707]
 [4.707]
 [4.707]
 [4.707]] [[0.436]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]]
using explorer policy with actor:  1
siam score:  -0.66312045
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]] [[4.967]
 [4.054]
 [4.054]
 [4.054]
 [4.054]
 [4.054]
 [4.054]] [[ 0.554]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]]
using explorer policy with actor:  1
first move QE:  0.8696632785660374
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.515]
 [0.469]
 [0.484]
 [0.469]
 [0.469]
 [0.496]] [[2.349]
 [2.6  ]
 [2.349]
 [2.114]
 [2.349]
 [2.349]
 [2.702]] [[0.669]
 [0.879]
 [0.669]
 [0.529]
 [0.669]
 [0.669]
 [0.928]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.156]
 [0.16 ]
 [0.174]
 [0.168]
 [0.179]
 [0.257]] [[3.142]
 [2.649]
 [3.015]
 [2.917]
 [3.015]
 [2.964]
 [3.253]] [[-0.029]
 [-0.342]
 [-0.09 ]
 [-0.127]
 [-0.073]
 [-0.086]
 [ 0.263]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.32597586067598466, 0.0062628245258483884, 0.13405000868036807, 0.29124555232105176, 0.23620292927089873, 0.0062628245258483884]
siam score:  -0.66690385
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3261707307484703, 0.006266568475068389, 0.13413014447586555, 0.2908218552095316, 0.23634413261599582, 0.006266568475068389]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[4.139]
 [3.275]
 [3.275]
 [3.275]
 [3.275]
 [3.275]
 [3.275]] [[1.557]
 [1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]]
line 256 mcts: sample exp_bonus 1.9255581468318106
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.695]
 [0.691]
 [0.689]
 [0.688]
 [0.693]
 [0.692]] [[2.134]
 [2.405]
 [2.257]
 [2.226]
 [2.238]
 [2.211]
 [2.302]] [[1.326]
 [1.5  ]
 [1.393]
 [1.369]
 [1.375]
 [1.365]
 [1.424]]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.407]
 [0.63 ]
 [0.723]
 [0.63 ]
 [0.63 ]
 [0.526]] [[2.639]
 [2.485]
 [2.639]
 [2.431]
 [2.639]
 [2.639]
 [2.62 ]] [[1.858]
 [1.206]
 [1.858]
 [1.767]
 [1.858]
 [1.858]
 [1.625]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.143 0.102 0.224 0.041 0.061 0.224 0.204]
Printing some Q and Qe and total Qs values:  [[1.188]
 [1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]] [[1.216]
 [1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.198]] [[1.48 ]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.508]
 [0.608]
 [0.508]
 [0.552]
 [0.61 ]] [[2.703]
 [2.703]
 [2.703]
 [2.922]
 [2.703]
 [2.619]
 [2.674]] [[1.626]
 [1.626]
 [1.626]
 [1.879]
 [1.626]
 [1.602]
 [1.695]]
from probs:  [0.3260314974571307, 0.006288614066257094, 0.13356303524702598, 0.2906526553075949, 0.23717558385573423, 0.006288614066257094]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]] [[4.493]
 [4.091]
 [4.091]
 [4.091]
 [4.091]
 [4.091]
 [4.091]] [[ 0.001]
 [-0.221]
 [-0.221]
 [-0.221]
 [-0.221]
 [-0.221]
 [-0.221]]
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5737],
        [-0.5091],
        [-0.0000],
        [-0.4888],
        [-0.0000],
        [-0.4889],
        [-0.0000],
        [-0.0000],
        [-0.5614],
        [-0.6322]], dtype=torch.float64)
-0.0628797758985 -0.6365421519439088
-0.043375785898500004 -0.5524850196455273
-0.9507464999999999 -0.9507464999999999
-0.024259925299500003 -0.5130302447004305
-0.9608890648499999 -0.9608890648499999
-0.024259925299500003 -0.5131707081129754
-0.5532179399999997 -0.5532179399999997
-0.838035 -0.838035
-0.0727797758985 -0.634167685857023
-0.024259925299500003 -0.6564666851208087
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.428]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[0.049]
 [2.234]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]] [[0.445]
 [1.025]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22852772947252437, 0.007198394812015407, 0.15288574714069864, 0.33270169611632155, 0.27148803764642476, 0.007198394812015407]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.211]
 [0.164]
 [0.324]
 [0.207]
 [0.324]
 [0.324]] [[2.236]
 [1.602]
 [0.217]
 [1.302]
 [1.846]
 [1.302]
 [1.302]] [[0.329]
 [0.211]
 [0.164]
 [0.324]
 [0.207]
 [0.324]
 [0.324]]
using explorer policy with actor:  1
using another actor
using explorer policy with actor:  1
from probs:  [0.22852772947252437, 0.007198394812015407, 0.15288574714069864, 0.33270169611632155, 0.27148803764642476, 0.007198394812015407]
Starting evaluation
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22859500907177127, 0.007200514052071268, 0.1526363528655257, 0.332799645003466, 0.27156796495509467, 0.007200514052071268]
first move QE:  0.8655576974934098
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.454]
 [0.44 ]] [[0.765]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.993]
 [0.197]] [[0.419]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.454]
 [0.44 ]]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 2.5007135307396084
line 256 mcts: sample exp_bonus 7.9420651830058935
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[7.786]
 [5.745]
 [5.745]
 [5.745]
 [5.745]
 [5.745]
 [5.745]] [[1.642]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[8.076]
 [5.611]
 [5.611]
 [5.611]
 [5.611]
 [5.611]
 [5.611]] [[1.597]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.445]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.434]] [[0.658]
 [2.398]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.709]] [[0.438]
 [0.445]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.434]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22859500907177127, 0.007200514052071268, 0.1526363528655257, 0.332799645003466, 0.27156796495509467, 0.007200514052071268]
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.667]
 [0.702]
 [0.703]
 [0.705]
 [0.705]
 [0.709]] [[1.393]
 [2.203]
 [1.488]
 [1.259]
 [1.577]
 [1.577]
 [1.352]] [[0.743]
 [0.667]
 [0.702]
 [0.703]
 [0.705]
 [0.705]
 [0.709]]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.251]
 [0.303]
 [0.303]
 [0.311]
 [0.326]
 [0.31 ]] [[-0.494]
 [ 0.02 ]
 [-0.889]
 [-0.637]
 [-0.956]
 [-1.42 ]
 [-0.835]] [[0.301]
 [0.251]
 [0.303]
 [0.303]
 [0.311]
 [0.326]
 [0.31 ]]
line 256 mcts: sample exp_bonus 2.030659733528729
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22859500907177127, 0.007200514052071268, 0.1526363528655257, 0.332799645003466, 0.27156796495509467, 0.007200514052071268]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.6958636016038113
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]] [[1.821]
 [1.713]
 [1.713]
 [1.713]
 [1.713]
 [1.713]
 [1.713]] [[0.766]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]]
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.606]
 [0.637]
 [0.635]
 [0.641]
 [0.638]
 [0.637]] [[0.257]
 [1.281]
 [0.533]
 [0.038]
 [0.071]
 [0.262]
 [0.369]] [[0.638]
 [0.606]
 [0.637]
 [0.635]
 [0.641]
 [0.638]
 [0.637]]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[-0.761]
 [-0.433]
 [-0.433]
 [-0.433]
 [-0.433]
 [-0.433]
 [-0.433]] [[0.546]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]]
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[-0.878]
 [-0.868]
 [-0.868]
 [-0.868]
 [-0.868]
 [-0.868]
 [-0.868]] [[0.31 ]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.2537788756786465
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]] [[2.194]
 [2.321]
 [2.321]
 [2.321]
 [2.321]
 [2.321]
 [2.321]] [[0.698]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.8  ]
 [0.574]
 [0.675]
 [0.676]
 [0.683]
 [0.676]
 [0.685]] [[0.331]
 [3.233]
 [2.604]
 [2.403]
 [2.116]
 [2.317]
 [2.213]] [[0.8  ]
 [0.574]
 [0.675]
 [0.676]
 [0.683]
 [0.676]
 [0.685]]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]] [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.552]
 [0.618]
 [0.621]
 [0.616]
 [0.617]
 [0.624]] [[-0.626]
 [ 1.521]
 [ 0.559]
 [ 0.428]
 [ 0.528]
 [ 0.55 ]
 [ 0.661]] [[0.647]
 [0.552]
 [0.618]
 [0.621]
 [0.616]
 [0.617]
 [0.624]]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.52 ]
 [0.402]
 [0.442]
 [0.495]
 [0.435]
 [0.463]] [[3.066]
 [4.32 ]
 [1.61 ]
 [2.84 ]
 [3.765]
 [2.895]
 [3.99 ]] [[1.084]
 [1.623]
 [0.562]
 [1.018]
 [1.405]
 [1.023]
 [1.414]]
line 256 mcts: sample exp_bonus 1.7558978254297224
Printing some Q and Qe and total Qs values:  [[0.941]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]] [[2.097]
 [1.847]
 [1.847]
 [1.847]
 [1.847]
 [1.847]
 [1.847]] [[0.941]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5023726961871273
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]] [[1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]] [[0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.936]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]] [[0.852]
 [2.56 ]
 [2.56 ]
 [2.56 ]
 [2.56 ]
 [2.56 ]
 [2.56 ]] [[0.936]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]]
siam score:  -0.6777739
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[2.437]
 [3.138]
 [3.138]
 [3.138]
 [3.138]
 [3.138]
 [3.138]] [[0.83 ]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.94 ]
 [0.649]
 [0.709]
 [0.706]
 [0.701]
 [0.705]
 [0.765]] [[0.77 ]
 [2.791]
 [2.445]
 [2.459]
 [2.453]
 [2.361]
 [2.022]] [[0.94 ]
 [0.649]
 [0.709]
 [0.706]
 [0.701]
 [0.705]
 [0.765]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22912652946840678, 0.007217256412722435, 0.15269735789684452, 0.3315421953795132, 0.27219940442979074, 0.007217256412722435]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[-1.339]
 [-1.339]
 [-1.339]
 [-1.339]
 [-1.339]
 [-1.339]
 [-1.339]] [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22912652946840678, 0.007217256412722435, 0.15269735789684452, 0.3315421953795132, 0.27219940442979074, 0.007217256412722435]
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]] [[2.196]
 [2.321]
 [2.321]
 [2.321]
 [2.321]
 [2.321]
 [2.321]] [[0.872]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]]
from probs:  [0.22912652946840678, 0.007217256412722435, 0.15269735789684452, 0.3315421953795132, 0.27219940442979074, 0.007217256412722435]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.886]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.652]] [[-0.038]
 [ 2.392]
 [ 2.392]
 [ 2.392]
 [ 2.392]
 [ 2.392]
 [ 2.37 ]] [[0.886]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.652]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.952]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]] [[0.32 ]
 [2.673]
 [2.673]
 [2.673]
 [2.673]
 [2.673]
 [2.673]] [[0.952]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2293271058048329, 0.00722357436662184, 0.15195563296350076, 0.331832425930616, 0.27243768656780665, 0.00722357436662184]
Printing some Q and Qe and total Qs values:  [[0.96 ]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]] [[0.973]
 [2.279]
 [2.279]
 [2.279]
 [2.279]
 [2.279]
 [2.279]] [[0.96 ]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.609]
 [0.629]
 [0.629]
 [0.605]
 [0.629]
 [0.618]] [[1.703]
 [3.197]
 [1.371]
 [1.371]
 [1.431]
 [1.371]
 [1.669]] [[0.62 ]
 [0.609]
 [0.629]
 [0.629]
 [0.605]
 [0.629]
 [0.618]]
UNIT TEST: sample policy line 217 mcts : [0.082 0.245 0.102 0.061 0.306 0.122 0.082]
Printing some Q and Qe and total Qs values:  [[0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]] [[2.442]
 [2.442]
 [2.442]
 [2.442]
 [2.442]
 [2.442]
 [2.442]] [[0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]]
siam score:  -0.6752093
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]] [[1.973]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]] [[0.82 ]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]]
using explorer policy with actor:  0
siam score:  -0.6758643
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[-0.899]
 [-0.843]
 [-0.843]
 [-0.843]
 [-0.843]
 [-0.843]
 [-0.843]] [[0.547]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[0.382]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[0.645]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.603]
 [0.653]
 [0.651]
 [0.65 ]
 [0.649]
 [0.652]] [[1.882]
 [2.888]
 [2.324]
 [2.294]
 [2.319]
 [2.253]
 [2.324]] [[0.688]
 [0.603]
 [0.653]
 [0.651]
 [0.65 ]
 [0.649]
 [0.652]]
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.683]] [[1.851]
 [2.643]
 [2.643]
 [2.643]
 [2.643]
 [2.643]
 [2.375]] [[0.724]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.683]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]] [[0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]] [[2.642]
 [2.609]
 [2.609]
 [2.609]
 [2.609]
 [2.609]
 [2.609]] [[0.662]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.411]
 [0.382]
 [0.395]
 [0.382]
 [0.382]
 [0.393]] [[4.941]
 [4.239]
 [4.941]
 [4.214]
 [4.941]
 [4.941]
 [4.905]] [[1.398]
 [0.99 ]
 [1.398]
 [0.941]
 [1.398]
 [1.398]
 [1.398]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2294813328688452, 0.007228432364817599, 0.1520578261649971, 0.3313830698771514, 0.27262090635937114, 0.007228432364817599]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.568]
 [0.552]
 [0.549]
 [0.548]
 [0.548]
 [0.549]] [[-0.7  ]
 [ 1.871]
 [-0.728]
 [-0.868]
 [-0.706]
 [-0.805]
 [-0.638]] [[0.553]
 [0.568]
 [0.552]
 [0.549]
 [0.548]
 [0.548]
 [0.549]]
line 256 mcts: sample exp_bonus -0.7284130235017087
siam score:  -0.67794013
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[6.455]
 [6.71 ]
 [6.71 ]
 [6.71 ]
 [6.71 ]
 [6.71 ]
 [6.71 ]] [[1.523]
 [1.695]
 [1.695]
 [1.695]
 [1.695]
 [1.695]
 [1.695]]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.576]
 [0.576]
 [0.698]
 [0.576]
 [0.576]
 [0.6  ]] [[2.276]
 [2.276]
 [2.276]
 [2.108]
 [2.276]
 [2.276]
 [2.658]] [[1.479]
 [1.479]
 [1.479]
 [1.46 ]
 [1.479]
 [1.479]
 [1.831]]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[2.11 ]
 [2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]] [[0.618]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.582]] [[0.869]
 [2.865]
 [2.865]
 [2.865]
 [2.865]
 [2.865]
 [2.521]] [[0.722]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.582]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[0.847]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]] [[-0.251]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.595]
 [0.673]
 [0.698]
 [0.698]
 [0.67 ]
 [0.664]] [[1.735]
 [3.876]
 [3.346]
 [2.307]
 [2.307]
 [3.195]
 [3.239]] [[0.801]
 [0.595]
 [0.673]
 [0.698]
 [0.698]
 [0.67 ]
 [0.664]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[1.902]
 [2.256]
 [2.256]
 [2.256]
 [2.256]
 [2.256]
 [2.256]] [[0.58 ]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.943]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]] [[2.129]
 [2.5  ]
 [2.5  ]
 [2.5  ]
 [2.5  ]
 [2.5  ]
 [2.5  ]] [[0.943]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.878 0.02  0.02  0.02  0.02  0.02 ]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.516]
 [0.605]
 [0.599]
 [0.597]
 [0.595]
 [0.602]] [[1.955]
 [3.748]
 [3.203]
 [3.308]
 [3.265]
 [3.234]
 [3.142]] [[0.73 ]
 [0.516]
 [0.605]
 [0.599]
 [0.597]
 [0.595]
 [0.602]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22963512892664956, 0.007233276786749076, 0.15215973377521586, 0.3309349696035969, 0.2728036141210395, 0.007233276786749076]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[2.486]
 [2.486]
 [2.486]
 [2.486]
 [2.486]
 [2.486]
 [2.486]] [[1.782]
 [1.782]
 [1.782]
 [1.782]
 [1.782]
 [1.782]
 [1.782]]
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]] [[2.452]
 [2.52 ]
 [2.52 ]
 [2.52 ]
 [2.52 ]
 [2.52 ]
 [2.52 ]] [[0.787]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.573]
 [0.656]
 [0.652]
 [0.65 ]
 [0.649]
 [0.652]] [[3.193]
 [3.629]
 [3.316]
 [3.307]
 [3.264]
 [3.222]
 [3.196]] [[0.688]
 [0.573]
 [0.656]
 [0.652]
 [0.65 ]
 [0.649]
 [0.652]]
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.209]
 [0.21 ]] [[2.327]
 [1.64 ]
 [1.64 ]
 [1.64 ]
 [1.64 ]
 [0.683]
 [1.64 ]] [[0.243]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.209]
 [0.21 ]]
Printing some Q and Qe and total Qs values:  [[0.927]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]] [[1.837]
 [2.496]
 [2.496]
 [2.496]
 [2.496]
 [2.496]
 [2.496]] [[0.927]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.809]
 [0.651]
 [0.662]
 [0.663]
 [0.662]
 [0.661]
 [0.662]] [[1.553]
 [3.048]
 [2.855]
 [3.082]
 [3.029]
 [2.832]
 [2.973]] [[0.809]
 [0.651]
 [0.662]
 [0.663]
 [0.662]
 [0.661]
 [0.662]]
using another actor
from probs:  [0.22963512892664956, 0.007233276786749076, 0.15215973377521586, 0.3309349696035969, 0.2728036141210395, 0.007233276786749076]
rdn probs:  [0.22963512892664956, 0.007233276786749076, 0.15215973377521586, 0.3309349696035969, 0.2728036141210395, 0.007233276786749076]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.334]
 [0.352]
 [0.361]
 [0.365]
 [0.37 ]
 [0.357]] [[4.499]
 [4.528]
 [4.462]
 [4.239]
 [4.257]
 [3.934]
 [3.293]] [[0.872]
 [0.879]
 [0.861]
 [0.774]
 [0.783]
 [0.652]
 [0.382]]
line 256 mcts: sample exp_bonus 2.71840544687664
first move QE:  0.8593348052635523
start point for exploration sampling:  10935
using explorer policy with actor:  1
from probs:  [0.22963512892664956, 0.007233276786749076, 0.15215973377521586, 0.3309349696035969, 0.2728036141210395, 0.007233276786749076]
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.185]
 [0.201]
 [0.201]
 [0.192]
 [0.186]
 [0.196]] [[-1.159]
 [ 0.032]
 [ 0.   ]
 [ 0.   ]
 [-0.706]
 [-0.684]
 [-0.815]] [[0.192]
 [0.185]
 [0.201]
 [0.201]
 [0.192]
 [0.186]
 [0.196]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.70303667
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.671]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]] [[1.128]
 [3.23 ]
 [1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.128]] [[1.241]
 [1.919]
 [1.241]
 [1.241]
 [1.241]
 [1.241]
 [1.241]]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.597]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.578]] [[4.855]
 [5.114]
 [4.968]
 [4.968]
 [4.968]
 [4.968]
 [6.324]] [[1.276]
 [1.481]
 [1.54 ]
 [1.54 ]
 [1.54 ]
 [1.54 ]
 [2.175]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2687657829226933, 0.008465853233231284, 0.008465853233231284, 0.386546323724866, 0.31929033365274695, 0.008465853233231284]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2687657829226933, 0.008465853233231284, 0.008465853233231284, 0.386546323724866, 0.31929033365274695, 0.008465853233231284]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2687657829226933, 0.008465853233231284, 0.008465853233231284, 0.386546323724866, 0.31929033365274695, 0.008465853233231284]
using explorer policy with actor:  1
using another actor
from probs:  [0.26897503026012387, 0.008472444314986064, 0.008472444314986064, 0.38606871996518466, 0.31953891682973323, 0.008472444314986064]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[5.837]
 [5.222]
 [5.222]
 [5.222]
 [5.222]
 [5.222]
 [5.222]] [[1.147]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.619]
 [0.571]
 [0.587]
 [0.587]
 [0.611]
 [0.595]] [[5.652]
 [6.072]
 [4.967]
 [4.705]
 [5.257]
 [5.693]
 [5.202]] [[0.595]
 [0.619]
 [0.571]
 [0.587]
 [0.587]
 [0.611]
 [0.595]]
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.774]
 [0.789]
 [0.783]
 [0.783]
 [0.783]
 [0.783]] [[3.074]
 [3.024]
 [2.98 ]
 [3.158]
 [3.158]
 [3.158]
 [3.158]] [[0.904]
 [0.774]
 [0.789]
 [0.783]
 [0.783]
 [0.783]
 [0.783]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2691837423311091, 0.00847901853638341, 0.00847901853638341, 0.38559233794275394, 0.3197868641169867, 0.00847901853638341]
line 256 mcts: sample exp_bonus 8.42553060860317
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2691837423311091, 0.00847901853638341, 0.00847901853638341, 0.38559233794275394, 0.3197868641169867, 0.00847901853638341]
1879 1991
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.552]
 [0.567]
 [0.567]
 [0.379]
 [0.567]
 [0.567]] [[ 1.06 ]
 [ 1.958]
 [ 1.06 ]
 [ 1.06 ]
 [-0.934]
 [ 1.06 ]
 [ 1.06 ]] [[1.358]
 [1.822]
 [1.358]
 [1.358]
 [0.156]
 [1.358]
 [1.358]]
start point for exploration sampling:  10935
siam score:  -0.68186915
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.577]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.581]] [[2.061]
 [2.959]
 [1.796]
 [1.796]
 [1.796]
 [1.796]
 [2.542]] [[0.567]
 [0.577]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.581]]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.586]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.59 ]] [[4.049]
 [3.719]
 [4.049]
 [4.049]
 [4.049]
 [4.049]
 [3.706]] [[0.585]
 [0.586]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.59 ]]
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]] [[2.588]
 [2.588]
 [2.588]
 [2.588]
 [2.588]
 [2.588]
 [2.588]] [[0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2693919211868879, 0.00848557596203529, 0.00848557596203529, 0.3851171729756533, 0.3200341779513529, 0.00848557596203529]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2693919211868879, 0.00848557596203529, 0.00848557596203529, 0.3851171729756533, 0.3200341779513529, 0.00848557596203529]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[8.123]
 [4.324]
 [4.324]
 [4.324]
 [4.324]
 [4.324]
 [4.324]] [[1.639]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2693919211868879, 0.00848557596203529, 0.00848557596203529, 0.3851171729756533, 0.3200341779513529, 0.00848557596203529]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[7.579]
 [5.347]
 [5.347]
 [5.347]
 [5.347]
 [5.347]
 [5.347]] [[1.308]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2693919211868879, 0.00848557596203529, 0.00848557596203529, 0.3851171729756533, 0.3200341779513529, 0.00848557596203529]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]] [[4.584]
 [4.326]
 [4.326]
 [4.326]
 [4.326]
 [4.326]
 [4.326]] [[-0.02 ]
 [-0.177]
 [-0.177]
 [-0.177]
 [-0.177]
 [-0.177]
 [-0.177]]
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.225]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]] [[0.968]
 [0.775]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]] [[0.168]
 [0.225]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.26959956886823155, 0.008492116656223963, 0.008492116656223963, 0.3846432204058544, 0.3202808607572423, 0.008492116656223963]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.607]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[2.034]
 [2.517]
 [2.034]
 [2.034]
 [2.034]
 [2.034]
 [2.034]] [[1.891]
 [2.293]
 [1.891]
 [1.891]
 [1.891]
 [1.891]
 [1.891]]
from probs:  [0.2698066874055103, 0.008498640682904054, 0.008498640682904054, 0.3841704755990686, 0.32052691494670893, 0.008498640682904054]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.542]
 [0.542]
 [0.538]
 [0.539]
 [0.535]
 [0.536]] [[1.241]
 [3.622]
 [1.804]
 [1.094]
 [1.24 ]
 [1.287]
 [2.042]] [[0.533]
 [0.542]
 [0.542]
 [0.538]
 [0.539]
 [0.535]
 [0.536]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2698066874055103, 0.008498640682904054, 0.008498640682904054, 0.3841704755990686, 0.32052691494670893, 0.008498640682904054]
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]] [[1.28]
 [1.28]
 [1.28]
 [1.28]
 [1.28]
 [1.28]
 [1.28]] [[0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.56 ]
 [0.769]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]] [[2.81 ]
 [2.81 ]
 [1.291]
 [2.81 ]
 [2.81 ]
 [2.81 ]
 [2.81 ]] [[0.56 ]
 [0.56 ]
 [0.769]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.0105973184571635
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2698066874055103, 0.008498640682904054, 0.008498640682904054, 0.3841704755990686, 0.32052691494670893, 0.008498640682904054]
UNIT TEST: sample policy line 217 mcts : [0.082 0.224 0.061 0.102 0.286 0.163 0.082]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.672]
 [0.812]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[3.359]
 [3.359]
 [3.694]
 [3.359]
 [3.359]
 [3.359]
 [3.359]] [[0.672]
 [0.672]
 [0.812]
 [0.672]
 [0.672]
 [0.672]
 [0.672]]
in main func line 156:  1892
start point for exploration sampling:  10935
1893 2004
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2698066874055103, 0.008498640682904054, 0.008498640682904054, 0.3841704755990686, 0.32052691494670893, 0.008498640682904054]
1894 2007
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.842]
 [0.842]
 [0.842]
 [0.842]
 [0.842]
 [0.842]] [[3.191]
 [3.299]
 [3.299]
 [3.299]
 [3.299]
 [3.299]
 [3.299]] [[1.852]
 [1.823]
 [1.823]
 [1.823]
 [1.823]
 [1.823]
 [1.823]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6763977
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27021934511774703, 0.008511638987931456, 0.008511638987931456, 0.3832285908551773, 0.32101714706328144, 0.008511638987931456]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.494]
 [0.541]
 [0.692]
 [0.384]
 [0.344]
 [0.581]] [[2.202]
 [2.898]
 [2.15 ]
 [2.494]
 [2.29 ]
 [2.69 ]
 [2.922]] [[1.062]
 [1.685]
 [1.211]
 [1.572]
 [1.179]
 [1.42 ]
 [1.774]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]] [[3.475]
 [3.244]
 [3.244]
 [3.244]
 [3.244]
 [3.244]
 [3.244]] [[0.927]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.488]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[-0.096]
 [ 1.778]
 [ 0.901]
 [ 0.901]
 [ 0.901]
 [ 0.901]
 [ 0.901]] [[0.478]
 [0.488]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]]
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.554]
 [0.554]
 [0.554]
 [0.691]
 [0.554]
 [0.637]] [[2.109]
 [2.315]
 [2.315]
 [2.315]
 [2.295]
 [2.315]
 [2.283]] [[1.911]
 [1.946]
 [1.946]
 [1.946]
 [2.012]
 [1.946]
 [1.977]]
using explorer policy with actor:  0
from probs:  [0.27021934511774703, 0.008511638987931456, 0.008511638987931456, 0.3832285908551773, 0.32101714706328144, 0.008511638987931456]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27021934511774703, 0.008511638987931456, 0.008511638987931456, 0.3832285908551773, 0.32101714706328144, 0.008511638987931456]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.592]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.299]] [[2.406]
 [1.929]
 [2.406]
 [2.406]
 [2.406]
 [2.406]
 [2.507]] [[0.337]
 [0.592]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.299]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27021934511774703, 0.008511638987931456, 0.008511638987931456, 0.3832285908551773, 0.32101714706328144, 0.008511638987931456]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]] [[0.866]
 [1.281]
 [1.281]
 [1.281]
 [1.281]
 [1.281]
 [1.281]] [[0.702]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.424]
 [0.448]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[0.315]
 [1.621]
 [1.904]
 [1.621]
 [1.621]
 [1.621]
 [1.621]] [[0.426]
 [0.424]
 [0.448]
 [0.424]
 [0.424]
 [0.424]
 [0.424]]
first move QE:  0.8575395295472079
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]] [[3.203]
 [2.788]
 [2.788]
 [2.788]
 [2.788]
 [2.788]
 [2.788]] [[ 0.008]
 [-0.138]
 [-0.138]
 [-0.138]
 [-0.138]
 [-0.138]
 [-0.138]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27021934511774703, 0.008511638987931456, 0.008511638987931456, 0.3832285908551773, 0.32101714706328144, 0.008511638987931456]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.669]
 [0.602]
 [0.397]
 [0.602]
 [0.602]
 [0.665]] [[2.1  ]
 [1.997]
 [2.1  ]
 [1.742]
 [2.1  ]
 [2.1  ]
 [2.379]] [[2.303]
 [2.383]
 [2.303]
 [1.879]
 [2.303]
 [2.303]
 [2.48 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27021934511774703, 0.008511638987931456, 0.008511638987931456, 0.3832285908551773, 0.32101714706328144, 0.008511638987931456]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.378]
 [0.545]
 [0.459]
 [0.545]
 [0.545]
 [0.463]] [[4.132]
 [3.237]
 [4.132]
 [3.461]
 [4.132]
 [4.132]
 [3.641]] [[2.352]
 [1.308]
 [2.352]
 [1.636]
 [2.352]
 [2.352]
 [1.796]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.127409878606154
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.337022116169666
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27021934511774703, 0.008511638987931456, 0.008511638987931456, 0.3832285908551773, 0.32101714706328144, 0.008511638987931456]
from probs:  [0.27021934511774703, 0.008511638987931456, 0.008511638987931456, 0.3832285908551773, 0.32101714706328144, 0.008511638987931456]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.247]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]] [[4.512]
 [3.674]
 [3.487]
 [3.487]
 [3.487]
 [3.487]
 [3.487]] [[1.375]
 [0.818]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.8685430876774527
first move QE:  0.8560766673304103
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1913 2022
from probs:  [0.27042488830203415, 0.00851811339256862, 0.00851811339256862, 0.38275944176684057, 0.3212613297534193, 0.00851811339256862]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27042488830203415, 0.00851811339256862, 0.00851811339256862, 0.38275944176684057, 0.3212613297534193, 0.00851811339256862]
line 256 mcts: sample exp_bonus -0.6868236744509681
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65400445
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27042488830203415, 0.00851811339256862, 0.00851811339256862, 0.38275944176684057, 0.3212613297534193, 0.00851811339256862]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.619]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.498]] [[1.965]
 [1.941]
 [1.572]
 [1.572]
 [1.572]
 [1.572]
 [2.406]] [[0.577]
 [0.846]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [1.068]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27083441327412777, 0.008531013019416153, 0.008531013019416153, 0.38182470745309954, 0.32174784021452413, 0.008531013019416153]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27083441327412777, 0.008531013019416153, 0.008531013019416153, 0.38182470745309954, 0.32174784021452413, 0.008531013019416153]
line 256 mcts: sample exp_bonus 6.576858130814667
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[4.191]
 [4.191]
 [4.191]
 [4.191]
 [4.191]
 [4.191]
 [4.191]] [[0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]]
1921 2031
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27083441327412777, 0.008531013019416153, 0.008531013019416153, 0.38182470745309954, 0.32174784021452413, 0.008531013019416153]
UNIT TEST: sample policy line 217 mcts : [0.122 0.286 0.143 0.082 0.265 0.102 0.   ]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27083441327412777, 0.008531013019416153, 0.008531013019416153, 0.38182470745309954, 0.32174784021452413, 0.008531013019416153]
from probs:  [0.27083441327412777, 0.008531013019416153, 0.008531013019416153, 0.38182470745309954, 0.32174784021452413, 0.008531013019416153]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27083441327412777, 0.008531013019416153, 0.008531013019416153, 0.38182470745309954, 0.32174784021452413, 0.008531013019416153]
siam score:  -0.6676445
start point for exploration sampling:  10935
using another actor
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]] [[5.671]
 [3.73 ]
 [3.73 ]
 [3.73 ]
 [3.73 ]
 [3.73 ]
 [3.73 ]] [[1.115]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.527]
 [0.733]
 [0.582]
 [0.419]
 [0.591]
 [0.573]] [[2.654]
 [3.334]
 [3.048]
 [0.585]
 [1.568]
 [0.995]
 [1.839]] [[1.426]
 [1.832]
 [1.909]
 [0.227]
 [0.626]
 [0.487]
 [0.978]]
siam score:  -0.6713646
using explorer policy with actor:  1
start point for exploration sampling:  10935
using another actor
using explorer policy with actor:  1
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.8587494827085187
siam score:  -0.66862386
from probs:  [0.2712418695299102, 0.008543847483769854, 0.008543847483769854, 0.3808946949518323, 0.322231893066948, 0.008543847483769854]
line 256 mcts: sample exp_bonus 4.634334539412652
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.54 ]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[3.01 ]
 [3.512]
 [3.01 ]
 [3.01 ]
 [3.01 ]
 [3.01 ]
 [3.01 ]] [[0.832]
 [1.122]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2712418695299102, 0.008543847483769854, 0.008543847483769854, 0.3808946949518323, 0.322231893066948, 0.008543847483769854]
siam score:  -0.66640013
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.433]
 [0.456]
 [0.439]
 [0.456]
 [0.456]
 [0.456]] [[1.921]
 [2.539]
 [2.339]
 [1.833]
 [2.339]
 [2.339]
 [2.339]] [[0.433]
 [0.433]
 [0.456]
 [0.439]
 [0.456]
 [0.456]
 [0.456]]
line 256 mcts: sample exp_bonus 2.101204507150527
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[4.059]
 [4.059]
 [4.059]
 [4.059]
 [4.059]
 [4.059]
 [4.059]] [[0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2714448267815024, 0.00855024043411463, 0.00855024043411463, 0.38043144821423797, 0.32247300370191584, 0.00855024043411463]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.234]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[2.81]
 [2.99]
 [2.81]
 [2.81]
 [2.81]
 [2.81]
 [2.81]] [[-0.006]
 [ 0.055]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
first move QE:  0.8608762112442302
Printing some Q and Qe and total Qs values:  [[-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]] [[4.394]
 [4.554]
 [4.554]
 [4.554]
 [4.554]
 [4.554]
 [4.554]] [[1.202]
 [1.302]
 [1.302]
 [1.302]
 [1.302]
 [1.302]
 [1.302]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27164727270507705, 0.00855661727813849, 0.00855661727813849, 0.379969368574809, 0.3227135068856986, 0.00855661727813849]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[4.999]
 [4.999]
 [4.999]
 [4.999]
 [4.999]
 [4.999]
 [4.999]] [[1.244]
 [1.244]
 [1.244]
 [1.244]
 [1.244]
 [1.244]
 [1.244]]
siam score:  -0.67530024
siam score:  -0.67568535
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27164727270507705, 0.00855661727813849, 0.00855661727813849, 0.379969368574809, 0.3227135068856986, 0.00855661727813849]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.416]
 [0.392]
 [0.389]
 [0.373]
 [0.354]
 [0.355]] [[5.695]
 [5.677]
 [5.637]
 [5.575]
 [5.451]
 [5.718]
 [5.753]] [[1.136]
 [1.192]
 [1.117]
 [1.069]
 [0.956]
 [1.095]
 [1.119]]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.397]
 [0.368]] [[6.185]
 [6.185]
 [6.185]
 [6.185]
 [6.185]
 [5.922]
 [6.185]] [[1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.51 ]
 [1.585]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.16798549978061
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.663]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]] [[2.721]
 [2.475]
 [2.721]
 [2.721]
 [2.721]
 [2.721]
 [2.721]] [[1.568]
 [1.419]
 [1.568]
 [1.568]
 [1.568]
 [1.568]
 [1.568]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27184920923055333, 0.00856297807663196, 0.00856297807663196, 0.37950845162853497, 0.3229534049110159, 0.00856297807663196]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27184920923055333, 0.00856297807663196, 0.00856297807663196, 0.37950845162853497, 0.3229534049110159, 0.00856297807663196]
1951 2046
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.191]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[0.051]
 [0.903]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]] [[0.334]
 [0.191]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]]
start point for exploration sampling:  10935
start point for exploration sampling:  10935
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.67759824
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.27184920923055333, 0.00856297807663196, 0.00856297807663196, 0.37950845162853497, 0.3229534049110159, 0.00856297807663196]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1952 2050
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.794816611158861
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.376]
 [0.466]
 [0.551]
 [0.551]
 [0.551]
 [0.412]] [[3.126]
 [2.856]
 [2.937]
 [3.511]
 [3.511]
 [3.511]
 [2.472]] [[1.52 ]
 [1.324]
 [1.411]
 [1.725]
 [1.725]
 [1.725]
 [1.166]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.780183579476609
UNIT TEST: sample policy line 217 mcts : [0.857 0.041 0.02  0.02  0.02  0.02  0.02 ]
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[7.731]
 [4.25 ]
 [4.25 ]
 [4.25 ]
 [4.25 ]
 [4.25 ]
 [4.25 ]] [[1.622]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.378]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.443]] [[4.134]
 [4.884]
 [4.134]
 [4.134]
 [4.134]
 [4.134]
 [3.535]] [[1.105]
 [1.355]
 [1.105]
 [1.105]
 [1.105]
 [1.105]
 [0.876]]
line 256 mcts: sample exp_bonus 10.0
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
line 256 mcts: sample exp_bonus 2.4135421293481283
using another actor
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[5.875]
 [5.875]
 [5.875]
 [5.875]
 [5.875]
 [5.875]
 [5.875]] [[1.649]
 [1.649]
 [1.649]
 [1.649]
 [1.649]
 [1.649]
 [1.649]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.521]
 [0.543]
 [0.543]
 [0.543]
 [0.516]
 [0.543]] [[6.076]
 [5.556]
 [6.076]
 [6.076]
 [6.076]
 [5.354]
 [6.076]] [[2.181]
 [1.729]
 [2.181]
 [2.181]
 [2.181]
 [1.56 ]
 [2.181]]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.524]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.454]
 [0.41 ]] [[6.985]
 [4.743]
 [5.458]
 [5.458]
 [5.458]
 [5.374]
 [5.458]] [[1.709]
 [0.748]
 [0.968]
 [0.968]
 [0.968]
 [0.971]
 [0.968]]
using explorer policy with actor:  0
using explorer policy with actor:  1
first move QE:  0.8595122476769254
line 256 mcts: sample exp_bonus 2.301418743250582
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.718]
 [0.586]
 [0.635]
 [0.568]
 [0.573]
 [0.635]] [[ 0.601]
 [ 1.539]
 [-0.009]
 [ 0.83 ]
 [-0.079]
 [ 0.066]
 [ 0.83 ]] [[0.889]
 [1.421]
 [0.64 ]
 [1.019]
 [0.581]
 [0.64 ]
 [1.019]]
siam score:  -0.6702928
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.641]
 [0.701]
 [0.703]
 [0.722]
 [0.691]
 [0.73 ]] [[2.711]
 [2.271]
 [2.63 ]
 [3.183]
 [2.789]
 [2.548]
 [2.681]] [[1.865]
 [1.481]
 [1.824]
 [2.25 ]
 [1.97 ]
 [1.75 ]
 [1.897]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.78581736161468
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[2.152]
 [2.711]
 [2.711]
 [2.711]
 [2.711]
 [2.711]
 [2.711]] [[0.678]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.061]
 [-0.045]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[2.207]
 [1.6  ]
 [1.994]
 [1.6  ]
 [1.6  ]
 [1.6  ]
 [1.6  ]] [[ 0.176]
 [-0.271]
 [ 0.014]
 [-0.271]
 [-0.271]
 [-0.271]
 [-0.271]]
using explorer policy with actor:  1
siam score:  -0.67796826
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.406]
 [0.419]
 [0.437]
 [0.42 ]
 [0.374]
 [0.386]] [[5.317]
 [3.657]
 [3.122]
 [3.442]
 [4.202]
 [3.613]
 [3.338]] [[1.861]
 [0.881]
 [0.563]
 [0.768]
 [1.222]
 [0.834]
 [0.674]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.074]
 [0.022]
 [0.066]
 [0.02 ]
 [0.049]
 [0.097]] [[2.367]
 [2.413]
 [2.489]
 [2.168]
 [2.316]
 [2.52 ]
 [2.462]] [[0.481]
 [0.568]
 [0.601]
 [0.216]
 [0.358]
 [0.681]
 [0.667]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.473]] [[1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.494]] [[ 0.128]
 [ 0.128]
 [ 0.128]
 [ 0.128]
 [ 0.128]
 [ 0.128]
 [-0.099]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.505]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.566]] [[1.614]
 [0.825]
 [1.614]
 [1.614]
 [1.614]
 [1.614]
 [1.401]] [[1.758]
 [1.385]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.703]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.2465637888585563
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.67263746
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.66488206
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.8428856604483843
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.687]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[3.443]
 [1.962]
 [3.443]
 [3.443]
 [3.443]
 [3.443]
 [3.443]] [[0.388]
 [0.687]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[4.13 ]
 [3.415]
 [3.415]
 [3.415]
 [3.415]
 [3.415]
 [3.415]] [[1.419]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[3.681]
 [3.681]
 [3.681]
 [3.681]
 [3.681]
 [3.681]
 [3.681]] [[1.142]
 [1.142]
 [1.142]
 [1.142]
 [1.142]
 [1.142]
 [1.142]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
siam score:  -0.66830707
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
line 256 mcts: sample exp_bonus 1.877948919317069
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.113]
 [0.174]
 [0.267]
 [0.28 ]
 [0.376]
 [0.202]] [[2.319]
 [2.66 ]
 [2.022]
 [1.799]
 [1.724]
 [2.211]
 [2.109]] [[-0.468]
 [-0.207]
 [-0.51 ]
 [-0.473]
 [-0.498]
 [ 0.019]
 [-0.396]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.01 ]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[2.503]
 [3.021]
 [2.503]
 [2.503]
 [2.503]
 [2.503]
 [2.503]] [[-0.483]
 [-0.129]
 [-0.483]
 [-0.483]
 [-0.483]
 [-0.483]
 [-0.483]]
1992 2073
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
1996 2073
using another actor
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.533]
 [0.518]
 [0.519]
 [0.529]
 [0.524]
 [0.509]] [[4.855]
 [4.386]
 [4.394]
 [3.987]
 [4.853]
 [3.152]
 [3.537]] [[ 0.421]
 [ 0.329]
 [ 0.302]
 [ 0.167]
 [ 0.475]
 [-0.101]
 [-0.002]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[6.194]
 [5.825]
 [5.825]
 [5.825]
 [5.825]
 [5.825]
 [5.825]] [[-0.253]
 [-0.375]
 [-0.375]
 [-0.375]
 [-0.375]
 [-0.375]
 [-0.375]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.09 ]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]] [[8.645]
 [6.601]
 [6.601]
 [6.601]
 [6.601]
 [6.601]
 [6.601]] [[ 0.571]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]]
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.6  ]
 [0.602]
 [0.602]
 [0.602]
 [0.617]
 [0.602]] [[6.019]
 [5.64 ]
 [6.007]
 [6.007]
 [6.007]
 [5.065]
 [6.007]] [[1.909]
 [1.64 ]
 [1.891]
 [1.891]
 [1.891]
 [1.274]
 [1.891]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
using explorer policy with actor:  1
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
siam score:  -0.6632764
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.262149492224172
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
first move QE:  0.8369312307725103
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.604]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.671]] [[2.113]
 [2.103]
 [2.113]
 [2.113]
 [2.113]
 [2.113]
 [2.086]] [[1.205]
 [1.169]
 [1.205]
 [1.205]
 [1.205]
 [1.205]
 [1.239]]
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.87 ]
 [0.629]] [[1.983]
 [1.983]
 [1.983]
 [1.983]
 [1.983]
 [1.216]
 [1.983]] [[0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.87 ]
 [0.629]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]] [[1.777]
 [1.511]
 [1.511]
 [1.511]
 [1.511]
 [1.511]
 [1.511]] [[0.047]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]]
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]] [[3.809]
 [3.809]
 [3.809]
 [3.809]
 [3.809]
 [3.809]
 [3.809]] [[-0.705]
 [-0.705]
 [-0.705]
 [-0.705]
 [-0.705]
 [-0.705]
 [-0.705]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.5665569818396925
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.447]
 [0.392]
 [0.397]
 [0.423]
 [0.423]
 [0.423]] [[2.3  ]
 [2.963]
 [2.616]
 [2.446]
 [2.712]
 [2.712]
 [2.712]] [[0.435]
 [1.17 ]
 [0.766]
 [0.604]
 [0.896]
 [0.896]
 [0.896]]
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
using explorer policy with actor:  0
siam score:  -0.6588054
siam score:  -0.65942794
first move QE:  0.8337940235933771
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.625]
 [0.625]
 [0.65 ]
 [0.625]
 [0.625]
 [0.636]] [[2.728]
 [2.728]
 [2.728]
 [2.89 ]
 [2.728]
 [2.728]
 [2.942]] [[1.513]
 [1.513]
 [1.513]
 [1.722]
 [1.513]
 [1.513]
 [1.746]]
Printing some Q and Qe and total Qs values:  [[0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]] [[1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]] [[0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
line 256 mcts: sample exp_bonus 2.083713507483098
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
2016 2098
using another actor
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.107]
 [0.205]
 [0.208]
 [0.208]
 [0.209]
 [0.17 ]] [[-3.474]
 [-0.793]
 [-3.558]
 [-3.683]
 [-3.609]
 [-3.536]
 [-1.861]] [[0.212]
 [0.107]
 [0.205]
 [0.208]
 [0.208]
 [0.209]
 [0.17 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.416]
 [0.396]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[-1.245]
 [ 0.95 ]
 [-1.059]
 [-1.194]
 [-1.194]
 [-1.194]
 [-1.194]] [[0.401]
 [0.416]
 [0.396]
 [0.399]
 [0.399]
 [0.399]
 [0.399]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]] [[0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]] [[2.9]
 [2.9]
 [2.9]
 [2.9]
 [2.9]
 [2.9]
 [2.9]]
line 256 mcts: sample exp_bonus 1.5080066653785842
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.443]
 [0.524]
 [0.531]
 [0.532]
 [0.527]
 [0.526]] [[-0.434]
 [ 1.406]
 [ 0.467]
 [ 0.7  ]
 [ 0.761]
 [ 0.711]
 [ 0.571]] [[0.516]
 [0.443]
 [0.524]
 [0.531]
 [0.532]
 [0.527]
 [0.526]]
line 256 mcts: sample exp_bonus 0.6228592915304688
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]] [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]] [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]] [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]]
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.401]
 [0.535]
 [0.521]
 [0.526]
 [0.535]
 [0.536]] [[0.612]
 [1.633]
 [0.733]
 [0.723]
 [0.513]
 [0.911]
 [0.89 ]] [[0.521]
 [0.401]
 [0.535]
 [0.521]
 [0.526]
 [0.535]
 [0.536]]
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.457]
 [0.612]
 [0.615]
 [0.615]
 [0.615]
 [0.611]] [[-1.559]
 [ 1.772]
 [ 1.482]
 [ 1.071]
 [ 1.071]
 [ 1.071]
 [ 1.32 ]] [[0.767]
 [0.457]
 [0.612]
 [0.615]
 [0.615]
 [0.615]
 [0.611]]
siam score:  -0.66524446
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.43 ]
 [0.511]
 [0.516]
 [0.497]
 [0.518]
 [0.514]] [[-0.049]
 [ 2.566]
 [ 2.055]
 [ 1.962]
 [ 1.754]
 [ 1.324]
 [ 1.448]] [[0.515]
 [0.43 ]
 [0.511]
 [0.516]
 [0.497]
 [0.518]
 [0.514]]
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[2.131]
 [1.971]
 [1.971]
 [1.971]
 [1.971]
 [1.971]
 [1.971]] [[0.714]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]]
line 256 mcts: sample exp_bonus 1.9752306050162056
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[1.439]
 [1.278]
 [1.278]
 [1.278]
 [1.278]
 [1.278]
 [1.278]] [[0.486]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]] [[0.943]
 [0.945]
 [0.945]
 [0.945]
 [0.945]
 [0.945]
 [0.945]] [[0.706]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.372]
 [0.508]
 [0.504]
 [0.502]
 [0.508]
 [0.513]] [[0.861]
 [1.704]
 [0.77 ]
 [0.792]
 [0.927]
 [0.739]
 [0.765]] [[0.491]
 [0.372]
 [0.508]
 [0.504]
 [0.502]
 [0.508]
 [0.513]]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.406]
 [0.51 ]
 [0.505]
 [0.505]
 [0.515]
 [0.503]] [[0.76 ]
 [2.026]
 [1.126]
 [1.075]
 [1.045]
 [1.009]
 [0.991]] [[0.5  ]
 [0.406]
 [0.51 ]
 [0.505]
 [0.505]
 [0.515]
 [0.503]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.412]
 [0.517]
 [0.505]
 [0.505]
 [0.511]
 [0.514]] [[0.86 ]
 [1.581]
 [0.713]
 [1.046]
 [1.046]
 [0.725]
 [0.741]] [[0.5  ]
 [0.412]
 [0.517]
 [0.505]
 [0.505]
 [0.511]
 [0.514]]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[-2.378]
 [-2.378]
 [-2.378]
 [-2.378]
 [-2.378]
 [-2.378]
 [-2.378]] [[0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.359]
 [0.591]
 [0.595]
 [0.8  ]
 [0.595]
 [0.597]] [[0.883]
 [2.275]
 [1.479]
 [1.404]
 [0.739]
 [1.164]
 [1.01 ]] [[0.621]
 [0.359]
 [0.591]
 [0.595]
 [0.8  ]
 [0.595]
 [0.597]]
Printing some Q and Qe and total Qs values:  [[0.896]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[1.443]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]] [[0.896]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[1.156]
 [1.658]
 [1.658]
 [1.658]
 [1.658]
 [1.658]
 [1.658]] [[0.902]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]]
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]] [[1.521]
 [1.946]
 [1.946]
 [1.946]
 [1.946]
 [1.946]
 [1.946]] [[0.859]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[1.078]
 [1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]] [[0.588]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.538]
 [0.561]
 [0.585]
 [0.573]
 [0.58 ]
 [0.586]] [[-0.716]
 [ 1.908]
 [ 1.625]
 [ 1.491]
 [ 0.887]
 [ 0.909]
 [ 0.569]] [[0.579]
 [0.538]
 [0.561]
 [0.585]
 [0.573]
 [0.58 ]
 [0.586]]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[0.533]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]] [[0.668]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]]
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.265]
 [0.255]
 [0.255]] [[2.626]
 [2.626]
 [2.626]
 [2.626]
 [3.077]
 [2.626]
 [2.626]] [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.781]
 [0.544]
 [0.544]]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.558]
 [0.604]
 [0.62 ]
 [0.62 ]
 [0.626]
 [0.621]] [[0.822]
 [1.457]
 [1.286]
 [1.146]
 [1.182]
 [0.558]
 [1.012]] [[0.618]
 [0.558]
 [0.604]
 [0.62 ]
 [0.62 ]
 [0.626]
 [0.621]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.144974662098828
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
line 256 mcts: sample exp_bonus 1.4464160148510694
line 256 mcts: sample exp_bonus 10.0
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[3.487]
 [1.549]
 [1.549]
 [1.549]
 [1.549]
 [1.549]
 [1.549]] [[0.86 ]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[1.236]
 [1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]] [[0.476]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]]
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.502]
 [0.584]
 [0.582]
 [0.569]
 [0.571]
 [0.567]] [[4.151]
 [4.032]
 [3.86 ]
 [3.751]
 [4.897]
 [3.854]
 [3.936]] [[1.077]
 [0.97 ]
 [0.966]
 [0.912]
 [1.449]
 [0.951]
 [0.987]]
line 256 mcts: sample exp_bonus 3.9917016230209406
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[2.756]
 [1.881]
 [1.881]
 [1.881]
 [1.881]
 [1.881]
 [1.881]] [[0.638]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]]
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.553]
 [0.594]
 [0.612]
 [0.618]
 [0.619]
 [0.621]] [[0.721]
 [2.015]
 [1.437]
 [1.017]
 [1.085]
 [1.014]
 [1.022]] [[0.615]
 [0.553]
 [0.594]
 [0.612]
 [0.618]
 [0.619]
 [0.621]]
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]] [[1.444]
 [1.353]
 [1.353]
 [1.353]
 [1.353]
 [1.353]
 [1.353]] [[0.734]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]]
using explorer policy with actor:  0
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.491]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[-1.733]
 [ 1.814]
 [ 0.852]
 [ 0.852]
 [ 0.852]
 [ 0.852]
 [ 0.852]] [[0.687]
 [0.491]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]]
line 256 mcts: sample exp_bonus -1.5818951579214242
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.401]
 [0.477]
 [0.469]
 [0.47 ]
 [0.47 ]
 [0.475]] [[0.025]
 [1.709]
 [0.628]
 [1.05 ]
 [1.026]
 [1.066]
 [0.576]] [[0.48 ]
 [0.401]
 [0.477]
 [0.469]
 [0.47 ]
 [0.47 ]
 [0.475]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[2.86]
 [2.86]
 [2.86]
 [2.86]
 [2.86]
 [2.86]
 [2.86]] [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
using explorer policy with actor:  0
siam score:  -0.68276477
line 256 mcts: sample exp_bonus 0.026032878933234138
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.412]
 [0.4  ]
 [0.423]
 [0.33 ]
 [0.45 ]
 [0.353]] [[2.762]
 [2.248]
 [2.628]
 [2.736]
 [3.427]
 [3.175]
 [2.447]] [[1.005]
 [1.09 ]
 [1.238]
 [1.301]
 [1.511]
 [1.507]
 [1.124]]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 1.612286016197952
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[0.634]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[0.552]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
rdn probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]] [[6.179]
 [3.307]
 [3.307]
 [3.307]
 [3.307]
 [3.307]
 [3.307]] [[1.361]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]]
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.172]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]] [[5.822]
 [5.549]
 [4.764]
 [4.764]
 [4.764]
 [4.764]
 [4.764]] [[1.283]
 [1.123]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.008]
 [-0.008]
 [-0.009]
 [-0.01 ]
 [-0.01 ]
 [-0.011]] [[5.184]
 [3.123]
 [2.61 ]
 [3.269]
 [3.274]
 [3.348]
 [3.551]] [[ 0.529]
 [-0.72 ]
 [-1.029]
 [-0.634]
 [-0.632]
 [-0.587]
 [-0.466]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
first move QE:  0.8286786549769298
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.462]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]] [[2.607]
 [3.216]
 [2.607]
 [2.607]
 [2.607]
 [2.607]
 [2.607]] [[0.085]
 [0.676]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
2034 2111
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[5.007]
 [5.007]
 [5.007]
 [5.007]
 [5.007]
 [5.007]
 [5.007]] [[-0.765]
 [-0.765]
 [-0.765]
 [-0.765]
 [-0.765]
 [-0.765]
 [-0.765]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.416]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[-0.277]
 [ 1.481]
 [-0.29 ]
 [-0.29 ]
 [-0.29 ]
 [-0.29 ]
 [-0.29 ]] [[0.336]
 [0.416]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
siam score:  -0.6875357
first move QE:  0.8286094181323002
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[3.56]
 [2.94]
 [2.94]
 [2.94]
 [2.94]
 [2.94]
 [2.94]] [[0.616]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[5.218]
 [3.208]
 [3.208]
 [3.208]
 [3.208]
 [3.208]
 [3.208]] [[0.578]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.8282675173737173
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.   ]
 [ 0.019]
 [ 0.018]
 [ 0.024]
 [ 0.024]
 [ 0.009]] [[2.237]
 [1.807]
 [2.28 ]
 [2.193]
 [2.243]
 [2.161]
 [2.099]] [[ 0.183]
 [-0.236]
 [ 0.276]
 [ 0.186]
 [ 0.248]
 [ 0.167]
 [ 0.075]]
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]] [[-2.21]
 [-2.21]
 [-2.21]
 [-2.21]
 [-2.21]
 [-2.21]
 [-2.21]] [[0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.413]
 [0.433]
 [0.474]
 [0.433]
 [0.433]
 [0.48 ]] [[1.537]
 [2.327]
 [1.537]
 [1.158]
 [1.537]
 [1.537]
 [2.017]] [[0.433]
 [0.413]
 [0.433]
 [0.474]
 [0.433]
 [0.433]
 [0.48 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.35 ]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]] [[-1.335]
 [-0.117]
 [-1.335]
 [-1.335]
 [-1.335]
 [-1.335]
 [-1.335]] [[0.344]
 [0.35 ]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 0.6945551708371944
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.489]
 [0.508]
 [0.494]
 [0.706]
 [0.706]
 [0.706]] [[2.08 ]
 [2.501]
 [2.219]
 [2.291]
 [1.008]
 [1.008]
 [1.008]] [[0.516]
 [0.489]
 [0.508]
 [0.494]
 [0.706]
 [0.706]
 [0.706]]
2046 2132
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.647]
 [0.6  ]
 [0.557]
 [0.6  ]
 [0.6  ]
 [0.624]] [[0.41 ]
 [0.714]
 [0.41 ]
 [0.594]
 [0.41 ]
 [0.41 ]
 [0.634]] [[1.877]
 [2.064]
 [1.877]
 [1.902]
 [1.877]
 [1.877]
 [2.003]]
using another actor
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
first move QE:  0.823731500518678
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]] [[2.678]
 [2.336]
 [2.336]
 [2.336]
 [2.336]
 [2.336]
 [2.336]] [[0.536]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]]
in main func line 156:  2048
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]] [[1.792]
 [1.792]
 [1.792]
 [1.792]
 [1.792]
 [1.792]
 [1.792]] [[0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[ 0.293]
 [-0.825]
 [-0.825]
 [-0.825]
 [-0.825]
 [-0.825]
 [-0.825]] [[0.58 ]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]]
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]] [[6.488]
 [3.378]
 [3.378]
 [3.378]
 [3.378]
 [3.378]
 [3.378]] [[1.778]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
first move QE:  0.8145444192227564
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]] [[1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]] [[0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.471]
 [0.489]
 [0.489]
 [0.61 ]
 [0.354]
 [0.489]] [[3.638]
 [3.814]
 [3.429]
 [3.429]
 [3.844]
 [3.466]
 [3.994]] [[0.408]
 [0.781]
 [0.562]
 [0.562]
 [1.076]
 [0.319]
 [0.934]]
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[3.395]
 [3.296]
 [3.296]
 [3.296]
 [3.296]
 [3.296]
 [3.296]] [[1.013]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
siam score:  -0.6848213
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
siam score:  -0.6860908
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.736]
 [0.549]
 [0.59 ]
 [0.549]
 [0.549]
 [0.738]] [[3.516]
 [3.096]
 [3.516]
 [3.53 ]
 [3.516]
 [3.516]
 [3.306]] [[1.393]
 [1.625]
 [1.393]
 [1.479]
 [1.393]
 [1.393]
 [1.7  ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.68244874
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 7.581863387999875
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.6828943
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.433]
 [0.383]
 [0.398]
 [0.398]
 [0.38 ]
 [0.398]] [[5.099]
 [4.935]
 [5.304]
 [5.025]
 [5.025]
 [5.017]
 [5.025]] [[0.284]
 [0.325]
 [0.348]
 [0.285]
 [0.285]
 [0.246]
 [0.285]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 7.70487944197915
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]] [[4.824]
 [3.722]
 [3.722]
 [3.722]
 [3.722]
 [3.722]
 [3.722]] [[-0.126]
 [-0.788]
 [-0.788]
 [-0.788]
 [-0.788]
 [-0.788]
 [-0.788]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.816]
 [0.609]
 [0.615]
 [0.325]
 [0.682]
 [0.564]] [[2.363]
 [1.32 ]
 [1.675]
 [1.873]
 [1.831]
 [2.041]
 [2.105]] [[2.096]
 [1.841]
 [1.805]
 [1.93 ]
 [1.562]
 [2.11 ]
 [2.007]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[1.336]
 [1.443]
 [1.336]
 [1.338]
 [1.336]
 [1.336]
 [1.327]] [[1.107]
 [1.097]
 [1.107]
 [1.013]
 [1.107]
 [1.107]
 [1.075]] [[2.146]
 [2.352]
 [2.146]
 [2.087]
 [2.146]
 [2.146]
 [2.106]]
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.353]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[2.25 ]
 [2.032]
 [2.076]
 [2.076]
 [2.076]
 [2.076]
 [2.076]] [[0.343]
 [0.353]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.712]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[4.073]
 [5.23 ]
 [4.073]
 [4.073]
 [4.073]
 [4.073]
 [4.073]] [[0.848]
 [1.94 ]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.473]
 [0.443]
 [0.443]
 [0.391]
 [0.383]
 [0.395]] [[2.448]
 [2.167]
 [2.134]
 [2.134]
 [2.045]
 [1.989]
 [2.025]] [[ 0.396]
 [ 0.264]
 [ 0.181]
 [ 0.181]
 [ 0.019]
 [-0.035]
 [ 0.013]]
siam score:  -0.67660254
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[4.665]
 [4.665]
 [4.665]
 [4.665]
 [4.665]
 [4.665]
 [4.665]] [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.691081360785111
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.6709041
siam score:  -0.67318565
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]] [[0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]] [[0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.53 ]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.449]] [[1.201]
 [1.222]
 [1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.222]] [[1.73 ]
 [1.806]
 [1.73 ]
 [1.73 ]
 [1.73 ]
 [1.73 ]
 [1.763]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.377]
 [0.38 ]
 [0.371]
 [0.375]
 [0.372]
 [0.366]] [[1.98 ]
 [2.226]
 [2.136]
 [1.84 ]
 [2.101]
 [2.792]
 [1.453]] [[0.372]
 [0.377]
 [0.38 ]
 [0.371]
 [0.375]
 [0.372]
 [0.366]]
line 256 mcts: sample exp_bonus 5.423421735572513
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.135]
 [1.054]
 [1.054]
 [1.054]
 [1.054]
 [1.054]
 [1.054]] [[1.519]
 [1.64 ]
 [1.64 ]
 [1.64 ]
 [1.64 ]
 [1.64 ]
 [1.64 ]] [[1.755]
 [1.632]
 [1.632]
 [1.632]
 [1.632]
 [1.632]
 [1.632]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.028]
 [0.35 ]
 [0.029]
 [0.031]
 [0.04 ]
 [0.029]] [[1.348]
 [1.034]
 [0.531]
 [0.887]
 [1.022]
 [0.904]
 [0.816]] [[1.685]
 [1.432]
 [1.285]
 [1.314]
 [1.425]
 [1.336]
 [1.255]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.985]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]] [[2.036]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]] [[2.   ]
 [1.329]
 [1.329]
 [1.329]
 [1.329]
 [1.329]
 [1.329]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]] [[4.753]
 [4.625]
 [4.625]
 [4.625]
 [4.625]
 [4.625]
 [4.625]] [[1.186]
 [1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.242]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[-0.279]
 [ 0.62 ]
 [-0.279]
 [-0.279]
 [-0.279]
 [-0.279]
 [-0.279]] [[0.191]
 [0.242]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]]
siam score:  -0.692213
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.653]
 [0.571]
 [0.693]
 [0.571]
 [0.571]
 [0.558]] [[1.018]
 [1.673]
 [1.018]
 [1.551]
 [1.018]
 [1.018]
 [1.069]] [[0.611]
 [0.993]
 [0.611]
 [1.032]
 [0.611]
 [0.611]
 [0.601]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.186]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]] [[3.006]
 [2.994]
 [2.994]
 [2.994]
 [2.994]
 [2.994]
 [2.994]] [[1.098]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.4051550450888524
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[2.766]
 [3.108]
 [3.108]
 [3.108]
 [3.108]
 [3.108]
 [3.108]] [[-0.468]
 [-0.506]
 [-0.506]
 [-0.506]
 [-0.506]
 [-0.506]
 [-0.506]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.373]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]] [[-0.613]
 [-0.845]
 [-0.613]
 [-0.613]
 [-0.613]
 [-0.613]
 [-0.613]] [[0.38 ]
 [0.373]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[6.904]
 [3.734]
 [3.734]
 [3.734]
 [3.734]
 [3.734]
 [3.734]] [[1.183]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]]
siam score:  -0.6908779
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[-0.108]
 [ 1.797]
 [ 1.797]
 [ 1.797]
 [ 1.797]
 [ 1.797]
 [ 1.797]] [[0.629]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.511]
 [0.511]
 [0.645]
 [0.511]
 [0.511]
 [0.511]] [[2.222]
 [2.222]
 [2.222]
 [4.112]
 [2.222]
 [2.222]
 [2.222]] [[0.889]
 [0.889]
 [0.889]
 [2.029]
 [0.889]
 [0.889]
 [0.889]]
2089 2200
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
siam score:  -0.6872641
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.234]
 [0.221]
 [0.221]
 [0.221]
 [0.212]
 [0.221]] [[-0.164]
 [ 0.46 ]
 [-0.263]
 [-0.263]
 [-0.263]
 [-0.377]
 [-0.263]] [[0.216]
 [0.234]
 [0.221]
 [0.221]
 [0.221]
 [0.212]
 [0.221]]
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.707]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[2.607]
 [3.002]
 [2.607]
 [2.607]
 [2.607]
 [2.607]
 [2.607]] [[0.398]
 [0.981]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.71]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]] [[3.74 ]
 [1.494]
 [1.494]
 [1.494]
 [1.494]
 [1.494]
 [1.494]] [[ 1.092]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3631158939158161
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.59 ]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[1.993]
 [2.333]
 [1.993]
 [1.993]
 [1.993]
 [1.993]
 [1.993]] [[1.083]
 [1.435]
 [1.083]
 [1.083]
 [1.083]
 [1.083]
 [1.083]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.355]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[2.393]
 [3.129]
 [2.393]
 [2.393]
 [2.393]
 [2.393]
 [2.393]] [[0.158]
 [0.682]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.091]
 [0.091]
 [0.091]
 [0.091]
 [0.091]
 [0.091]] [[7.852]
 [5.563]
 [5.563]
 [5.563]
 [5.563]
 [5.563]
 [5.563]] [[ 0.981]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.003]
 [0.019]
 [0.016]
 [0.013]
 [0.012]
 [0.014]] [[7.671]
 [4.842]
 [4.498]
 [4.792]
 [4.9  ]
 [4.999]
 [5.051]] [[ 0.813]
 [-0.642]
 [-0.724]
 [-0.633]
 [-0.605]
 [-0.574]
 [-0.554]]
Printing some Q and Qe and total Qs values:  [[ 0.189]
 [-0.007]
 [-0.004]
 [-0.004]
 [-0.005]
 [-0.005]
 [-0.005]] [[6.411]
 [4.842]
 [4.498]
 [5.076]
 [4.9  ]
 [4.999]
 [5.051]] [[ 0.235]
 [-0.681]
 [-0.789]
 [-0.598]
 [-0.658]
 [-0.625]
 [-0.607]]
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]] [[6.338]
 [6.255]
 [6.255]
 [6.255]
 [6.255]
 [6.255]
 [6.255]] [[1.23 ]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]]
using explorer policy with actor:  1
from probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]] [[0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]] [[1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.321]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.306]
 [0.328]
 [0.328]
 [0.329]
 [0.325]
 [0.327]] [[5.95 ]
 [5.824]
 [5.807]
 [5.936]
 [6.082]
 [6.303]
 [6.465]] [[1.184]
 [1.078]
 [1.095]
 [1.175]
 [1.267]
 [1.397]
 [1.499]]
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.283]
 [0.308]
 [0.308]
 [0.298]
 [0.299]
 [0.288]] [[5.893]
 [5.989]
 [6.074]
 [6.074]
 [6.243]
 [6.426]
 [5.944]] [[1.062]
 [1.11 ]
 [1.198]
 [1.198]
 [1.294]
 [1.415]
 [1.086]]
2101 2212
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.523]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[0.755]
 [1.222]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[0.638]
 [0.872]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.057]
 [0.046]
 [0.056]
 [0.082]
 [0.069]
 [0.059]] [[2.022]
 [1.353]
 [1.59 ]
 [1.906]
 [1.671]
 [1.386]
 [1.265]] [[ 1.109]
 [ 0.034]
 [ 0.408]
 [ 0.954]
 [ 0.613]
 [ 0.113]
 [-0.107]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
using explorer policy with actor:  1
siam score:  -0.6969428
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.378]
 [0.378]
 [0.378]
 [0.17 ]
 [0.378]
 [0.378]] [[1.29 ]
 [0.924]
 [0.924]
 [0.924]
 [1.273]
 [0.924]
 [0.924]] [[2.094]
 [1.952]
 [1.952]
 [1.952]
 [2.007]
 [1.952]
 [1.952]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2105 2224
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.489]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.489]] [[3.899]
 [5.418]
 [3.899]
 [3.899]
 [3.899]
 [3.899]
 [4.843]] [[0.662]
 [1.164]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [1.025]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.3280134069583758, 0.01033209410678782, 0.01033209410678782, 0.2513145528498113, 0.3896757578714497, 0.01033209410678782]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.7923103634654955
siam score:  -0.69863397
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.025]
 [-0.03 ]
 [-0.022]
 [-0.012]
 [-0.022]
 [-0.022]] [[2.524]
 [2.813]
 [2.528]
 [2.524]
 [3.019]
 [2.524]
 [2.524]] [[-0.537]
 [-0.382]
 [-0.548]
 [-0.537]
 [-0.247]
 [-0.537]
 [-0.537]]
actor:  1 policy actor:  1  step number:  126 total reward:  0.15499999999999936  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.647]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.571]] [[4.268]
 [4.5  ]
 [4.268]
 [4.268]
 [4.268]
 [4.268]
 [4.285]] [[1.173]
 [1.456]
 [1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.231]]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.438]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]] [[4.769]
 [5.413]
 [4.489]
 [4.489]
 [4.489]
 [4.489]
 [4.489]] [[0.581]
 [0.658]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]]
using explorer policy with actor:  1
first move QE:  0.7909422334697563
Printing some Q and Qe and total Qs values:  [[0.33]
 [0.33]
 [0.33]
 [0.33]
 [0.33]
 [0.33]
 [0.33]] [[2.812]
 [2.812]
 [2.812]
 [2.812]
 [2.812]
 [2.812]
 [2.812]] [[-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
first move QE:  0.7903482914050265
first move QE:  0.7903482914050265
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.274]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[3.158]
 [3.502]
 [3.158]
 [3.158]
 [3.158]
 [3.158]
 [3.158]] [[0.258]
 [0.274]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.285]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]] [[2.481]
 [3.546]
 [2.481]
 [2.481]
 [2.481]
 [2.481]
 [2.481]] [[0.262]
 [0.285]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]]
first move QE:  0.7901266872410725
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.695063
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.6938692
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.37 ]
 [0.428]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]] [[2.991]
 [3.291]
 [0.903]
 [2.182]
 [2.057]
 [1.923]
 [1.882]] [[0.374]
 [0.37 ]
 [0.428]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.366]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[1.724]
 [2.04 ]
 [1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.396]] [[0.474]
 [0.366]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.665]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[3.242]
 [2.674]
 [3.242]
 [3.242]
 [3.242]
 [3.242]
 [3.242]] [[0.767]
 [0.67 ]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.601]
 [0.573]] [[4.595]
 [4.595]
 [4.595]
 [4.595]
 [4.595]
 [4.372]
 [4.595]] [[0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.413]
 [0.432]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.32175127471302
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[4.101]
 [4.101]
 [4.101]
 [4.101]
 [4.101]
 [4.101]
 [4.101]] [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
siam score:  -0.6867406
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.261]
 [0.227]
 [0.261]
 [0.335]
 [0.261]
 [0.261]] [[5.075]
 [4.299]
 [4.671]
 [4.299]
 [4.407]
 [4.299]
 [4.299]] [[0.961]
 [0.546]
 [0.728]
 [0.546]
 [0.67 ]
 [0.546]
 [0.546]]
first move QE:  0.783057603692494
line 256 mcts: sample exp_bonus 1.970631535477378
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.69237816
siam score:  -0.69025195
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.233]
 [0.182]
 [0.182]
 [0.182]
 [0.258]
 [0.182]] [[4.169]
 [4.153]
 [4.108]
 [4.108]
 [4.108]
 [4.352]
 [4.108]] [[1.026]
 [1.034]
 [0.911]
 [0.911]
 [0.911]
 [1.241]
 [0.911]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6852245
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.47139340306833927
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]] [[6.113]
 [6.113]
 [6.113]
 [6.113]
 [6.113]
 [6.113]
 [6.113]] [[0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]]
Printing some Q and Qe and total Qs values:  [[0.874]
 [0.68 ]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[2.356]
 [3.232]
 [2.55 ]
 [2.55 ]
 [2.55 ]
 [2.55 ]
 [2.55 ]] [[0.874]
 [0.68 ]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.126]
 [0.073]
 [0.082]
 [0.126]
 [0.088]
 [0.101]] [[4.294]
 [4.303]
 [4.567]
 [4.338]
 [4.303]
 [4.432]
 [4.206]] [[0.759]
 [0.699]
 [0.84 ]
 [0.65 ]
 [0.699]
 [0.746]
 [0.564]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[3.75 ]
 [1.202]
 [1.202]
 [1.202]
 [1.202]
 [1.202]
 [1.202]] [[2.337]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.458]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[4.858]
 [4.697]
 [4.961]
 [4.961]
 [4.961]
 [4.961]
 [4.961]] [[0.187]
 [0.134]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.287]
 [0.283]
 [0.285]
 [0.284]
 [0.283]
 [0.285]] [[3.486]
 [3.574]
 [3.054]
 [2.979]
 [2.921]
 [2.908]
 [3.016]] [[1.298]
 [1.349]
 [1.031]
 [0.987]
 [0.951]
 [0.942]
 [1.009]]
from probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
siam score:  -0.7021534
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.589]
 [0.63 ]
 [0.787]
 [0.689]
 [0.708]
 [0.851]] [[5.415]
 [4.934]
 [4.124]
 [3.242]
 [3.884]
 [4.401]
 [3.353]] [[0.551]
 [0.589]
 [0.63 ]
 [0.787]
 [0.689]
 [0.708]
 [0.851]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.567]
 [0.545]
 [0.533]
 [0.535]
 [0.532]
 [0.573]] [[3.857]
 [4.499]
 [4.316]
 [4.252]
 [4.254]
 [4.261]
 [4.058]] [[0.593]
 [0.567]
 [0.545]
 [0.533]
 [0.535]
 [0.532]
 [0.573]]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[2.821]
 [2.821]
 [2.821]
 [2.821]
 [2.821]
 [2.821]
 [2.821]] [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
siam score:  -0.6970387
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 5.026326062940888
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.691]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]] [[2.881]
 [2.945]
 [2.881]
 [2.881]
 [2.881]
 [2.881]
 [2.881]] [[2.044]
 [2.101]
 [2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.044]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.7041479
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]] [[1.526]
 [1.526]
 [1.526]
 [1.526]
 [1.526]
 [1.526]
 [1.526]] [[0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.531]
 [0.579]
 [0.636]
 [0.579]
 [0.579]
 [0.466]] [[2.475]
 [3.432]
 [2.475]
 [2.387]
 [2.475]
 [2.475]
 [2.62 ]] [[0.866]
 [1.518]
 [0.866]
 [0.864]
 [0.866]
 [0.866]
 [0.848]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.206]
 [0.234]
 [0.31 ]
 [0.234]
 [0.234]
 [0.215]] [[3.52 ]
 [3.703]
 [3.52 ]
 [3.891]
 [3.52 ]
 [3.52 ]
 [3.516]] [[1.143]
 [1.251]
 [1.143]
 [1.514]
 [1.143]
 [1.143]
 [1.12 ]]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.46 ]
 [0.472]
 [0.566]
 [0.562]
 [0.566]
 [0.629]] [[2.433]
 [2.807]
 [2.813]
 [2.502]
 [2.624]
 [2.502]
 [2.56 ]] [[1.053]
 [1.202]
 [1.234]
 [1.006]
 [1.161]
 [1.006]
 [1.21 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]] [[1.576]
 [1.566]
 [1.566]
 [1.566]
 [1.566]
 [1.566]
 [1.566]] [[0.724]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]]
siam score:  -0.70760435
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.635]
 [0.667]
 [0.643]
 [0.667]
 [0.667]
 [0.567]] [[1.91 ]
 [2.37 ]
 [1.91 ]
 [1.6  ]
 [1.91 ]
 [1.91 ]
 [1.766]] [[1.208]
 [1.418]
 [1.208]
 [1.039]
 [1.208]
 [1.208]
 [1.077]]
2156 2290
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.718]
 [0.8  ]
 [0.718]
 [0.718]
 [0.718]
 [0.718]] [[2.868]
 [2.868]
 [1.773]
 [2.868]
 [2.868]
 [2.868]
 [2.868]] [[0.718]
 [0.718]
 [0.8  ]
 [0.718]
 [0.718]
 [0.718]
 [0.718]]
siam score:  -0.7226473
line 256 mcts: sample exp_bonus -3.254885720594678
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.607]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[4.342]
 [3.82 ]
 [3.776]
 [3.776]
 [3.776]
 [3.776]
 [3.776]] [[0.591]
 [0.607]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
Printing some Q and Qe and total Qs values:  [[1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]] [[0.762]
 [0.752]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.762]] [[0.714]
 [0.705]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.714]]
Starting evaluation
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2571408391193715, 0.22416574941040324, 0.00809967913542289, 0.19701400501263863, 0.30548004818674074, 0.00809967913542289]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.574]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]] [[3.631]
 [3.272]
 [2.055]
 [2.055]
 [2.055]
 [2.055]
 [2.055]] [[0.569]
 [0.574]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.393]
 [0.471]
 [0.521]
 [0.05 ]
 [0.24 ]
 [0.596]] [[1.544]
 [1.831]
 [0.906]
 [1.465]
 [1.161]
 [1.63 ]
 [1.322]] [[1.187]
 [1.308]
 [0.494]
 [1.101]
 [0.248]
 [0.931]
 [1.049]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.502]
 [0.563]
 [0.428]
 [0.546]
 [0.546]
 [0.552]] [[-0.744]
 [ 0.814]
 [-0.538]
 [ 0.598]
 [-0.636]
 [-0.889]
 [-1.092]] [[0.564]
 [0.502]
 [0.563]
 [0.428]
 [0.546]
 [0.546]
 [0.552]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.3410594399059488
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.504]
 [0.513]
 [0.513]
 [0.355]
 [0.395]
 [0.522]] [[3.278]
 [3.875]
 [1.614]
 [1.614]
 [2.105]
 [1.55 ]
 [3.275]] [[0.513]
 [0.504]
 [0.513]
 [0.513]
 [0.355]
 [0.395]
 [0.522]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.641]
 [0.518]
 [0.63 ]
 [0.633]
 [0.423]
 [0.631]] [[5.552]
 [4.76 ]
 [4.238]
 [3.588]
 [4.578]
 [4.848]
 [4.65 ]] [[0.635]
 [0.641]
 [0.518]
 [0.63 ]
 [0.633]
 [0.423]
 [0.631]]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.583]
 [0.549]
 [0.65 ]
 [0.549]
 [0.549]
 [0.612]] [[2.238]
 [2.205]
 [2.238]
 [1.908]
 [2.238]
 [2.238]
 [1.961]] [[1.303]
 [1.349]
 [1.303]
 [1.284]
 [1.303]
 [1.303]
 [1.243]]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.756]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]] [[4.462]
 [5.34 ]
 [4.462]
 [4.462]
 [4.462]
 [4.462]
 [4.462]] [[1.622]
 [2.237]
 [1.622]
 [1.622]
 [1.622]
 [1.622]
 [1.622]]
Printing some Q and Qe and total Qs values:  [[0.805]
 [0.618]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.626]] [[-0.76 ]
 [ 1.477]
 [ 0.349]
 [ 0.349]
 [ 0.349]
 [ 0.349]
 [ 0.944]] [[0.805]
 [0.618]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.626]]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.524]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[4.545]
 [5.276]
 [4.046]
 [4.046]
 [4.046]
 [4.046]
 [4.046]] [[1.336]
 [1.711]
 [0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.629]
 [0.647]
 [0.651]
 [0.651]
 [0.648]
 [0.651]] [[2.945]
 [3.839]
 [3.244]
 [3.759]
 [3.759]
 [3.168]
 [3.759]] [[0.567]
 [1.067]
 [0.706]
 [1.058]
 [1.058]
 [0.658]
 [1.058]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[0.18]
 [0.11]
 [0.11]
 [0.11]
 [0.11]
 [0.11]
 [0.11]] [[0.655]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[2.325]
 [2.325]
 [2.325]
 [2.325]
 [2.325]
 [2.325]
 [2.325]] [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.3818211975131908
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[0.607]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]] [[0.557]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.587]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[-2.505]
 [ 0.9  ]
 [-1.477]
 [-1.477]
 [-1.477]
 [-1.477]
 [-1.477]] [[0.626]
 [0.587]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[-0.294]
 [-1.164]
 [-1.164]
 [-1.164]
 [-1.164]
 [-1.164]
 [-1.164]] [[0.674]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[-0.682]
 [-0.964]
 [-0.964]
 [-0.964]
 [-0.964]
 [-0.964]
 [-0.964]] [[0.653]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]]
line 256 mcts: sample exp_bonus -0.7615011614462959
first move QE:  0.7697157001003152
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.120957682032032
Printing some Q and Qe and total Qs values:  [[0.809]
 [0.548]
 [0.631]
 [0.628]
 [0.628]
 [0.628]
 [0.632]] [[-1.048]
 [ 2.513]
 [ 1.337]
 [ 0.707]
 [ 0.707]
 [ 0.707]
 [ 1.246]] [[0.809]
 [0.548]
 [0.631]
 [0.628]
 [0.628]
 [0.628]
 [0.632]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]] [[1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]] [[0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]]
Printing some Q and Qe and total Qs values:  [[0.893]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[1.361]
 [1.53 ]
 [1.53 ]
 [1.53 ]
 [1.53 ]
 [1.53 ]
 [1.53 ]] [[0.893]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]]
line 256 mcts: sample exp_bonus -2.5713400027799276
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]] [[1.681]
 [1.369]
 [1.369]
 [1.369]
 [1.369]
 [1.369]
 [1.369]] [[0.751]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]]
Printing some Q and Qe and total Qs values:  [[0.841]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[ 0.231]
 [-0.635]
 [-0.635]
 [-0.635]
 [-0.635]
 [-0.635]
 [-0.635]] [[0.841]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 0.4792168836951323
line 256 mcts: sample exp_bonus 0.9952305235723218
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.851]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]] [[2.28 ]
 [2.241]
 [2.241]
 [2.241]
 [2.241]
 [2.241]
 [2.241]] [[0.851]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]]
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[2.723]
 [2.654]
 [2.654]
 [2.654]
 [2.654]
 [2.654]
 [2.654]] [[0.797]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[3.517]
 [3.51 ]
 [3.51 ]
 [3.51 ]
 [3.51 ]
 [3.51 ]
 [3.51 ]] [[0.695]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.592]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[-0.807]
 [ 1.092]
 [-0.893]
 [-0.893]
 [-0.893]
 [-0.893]
 [-0.893]] [[0.755]
 [0.592]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[ 0.315]
 [-0.181]
 [-0.181]
 [-0.181]
 [-0.181]
 [-0.181]
 [-0.181]] [[0.766]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[0.63 ]
 [1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]] [[0.728]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]]
line 256 mcts: sample exp_bonus 4.854101662420388
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[0.446]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]] [[0.688]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
line 256 mcts: sample exp_bonus 0.28191273945516526
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[0.224]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]] [[0.661]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.677]
 [1.677]] [[0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]]
line 256 mcts: sample exp_bonus 1.0264912810233096
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]] [[2.984]
 [3.037]
 [3.037]
 [3.037]
 [3.037]
 [3.037]
 [3.037]] [[0.71 ]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]]
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]] [[1.656]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]] [[0.757]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]]
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]] [[2.941]
 [3.209]
 [3.209]
 [3.209]
 [3.209]
 [3.209]
 [3.209]] [[0.782]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
line 256 mcts: sample exp_bonus 3.30131901674242
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[3.559]
 [3.504]
 [3.504]
 [3.504]
 [3.504]
 [3.504]
 [3.504]] [[0.733]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.538]
 [0.535]
 [0.533]
 [0.53 ]
 [0.531]
 [0.532]] [[-0.02 ]
 [ 1.207]
 [ 0.485]
 [ 0.501]
 [ 0.453]
 [ 0.331]
 [ 0.091]] [[0.67 ]
 [0.538]
 [0.535]
 [0.533]
 [0.53 ]
 [0.531]
 [0.532]]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]] [[2.147]
 [1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]] [[0.708]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]]
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.592]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[-0.829]
 [ 1.079]
 [ 0.054]
 [ 0.054]
 [ 0.054]
 [ 0.054]
 [ 0.054]] [[0.645]
 [0.592]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]]
siam score:  -0.73463607
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.607]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]] [[-0.076]
 [ 1.007]
 [ 0.092]
 [ 0.092]
 [ 0.092]
 [ 0.092]
 [ 0.092]] [[0.669]
 [0.607]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]]
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]] [[3.316]
 [3.206]
 [3.206]
 [3.206]
 [3.206]
 [3.206]
 [3.206]] [[0.852]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]]
line 256 mcts: sample exp_bonus 3.822565761110143
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[2.259]
 [2.259]
 [2.259]
 [2.259]
 [2.259]
 [2.259]
 [2.259]] [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.89 ]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]] [[3.107]
 [3.232]
 [3.232]
 [3.232]
 [3.232]
 [3.232]
 [3.232]] [[0.89 ]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.647]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]] [[-0.575]
 [ 1.458]
 [ 0.539]
 [ 0.539]
 [ 0.539]
 [ 0.539]
 [ 0.539]] [[0.73 ]
 [0.647]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]]
line 256 mcts: sample exp_bonus 1.4766658416546599
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5299013467443453
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.543]
 [0.606]
 [0.61 ]
 [0.609]
 [0.61 ]
 [0.611]] [[-0.645]
 [ 2.767]
 [ 1.425]
 [ 1.134]
 [ 1.139]
 [ 0.608]
 [ 0.527]] [[0.721]
 [0.543]
 [0.606]
 [0.61 ]
 [0.609]
 [0.61 ]
 [0.611]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]] [[0.37 ]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[0.717]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.554]
 [0.568]
 [0.559]
 [0.555]
 [0.533]
 [0.59 ]] [[2.714]
 [4.574]
 [4.667]
 [3.177]
 [2.582]
 [4.857]
 [2.444]] [[0.533]
 [0.554]
 [0.568]
 [0.559]
 [0.555]
 [0.533]
 [0.59 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]]
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[1.412]
 [1.123]
 [1.123]
 [1.123]
 [1.123]
 [1.123]
 [1.123]] [[0.687]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[1.483]
 [1.483]
 [1.483]
 [1.483]
 [1.483]
 [1.483]
 [1.483]] [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]]
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]] [[5.119]
 [4.795]
 [4.795]
 [4.795]
 [4.795]
 [4.795]
 [4.795]] [[0.365]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]]
rdn probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[ 0.115]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[9.522]
 [6.933]
 [6.933]
 [6.933]
 [6.933]
 [6.933]
 [6.933]] [[1.636]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]]
line 256 mcts: sample exp_bonus 8.42258117961507
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]] [[4.975]
 [3.389]
 [3.389]
 [3.389]
 [3.389]
 [3.389]
 [3.389]] [[1.541]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
siam score:  -0.73802847
Printing some Q and Qe and total Qs values:  [[ 0.008]
 [-0.026]
 [-0.026]
 [-0.025]
 [-0.029]
 [-0.019]
 [-0.009]] [[5.07 ]
 [4.842]
 [5.219]
 [4.437]
 [4.297]
 [4.88 ]
 [5.099]] [[1.289]
 [1.161]
 [1.341]
 [0.97 ]
 [0.901]
 [1.184]
 [1.294]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]] [[3.561]
 [3.149]
 [3.149]
 [3.149]
 [3.149]
 [3.149]
 [3.149]] [[0.877]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]]
siam score:  -0.74121994
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.327]
 [0.376]] [[4.651]
 [4.651]
 [4.651]
 [4.651]
 [4.651]
 [5.202]
 [4.651]] [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [1.003]
 [0.735]]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.819184057367112
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
line 256 mcts: sample exp_bonus 1.377547922883721
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.529]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[3.599]
 [3.917]
 [3.599]
 [3.599]
 [3.599]
 [3.599]
 [3.599]] [[1.287]
 [1.435]
 [1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.121]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.104]] [[0.89 ]
 [1.439]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [1.786]] [[0.022]
 [0.121]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.104]]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.624]
 [0.602]
 [0.602]
 [0.602]
 [0.629]
 [0.602]] [[4.314]
 [5.127]
 [4.251]
 [4.251]
 [4.251]
 [4.058]
 [4.251]] [[0.864]
 [1.436]
 [0.809]
 [0.809]
 [0.809]
 [0.733]
 [0.809]]
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.896]
 [0.753]] [[2.598]
 [2.598]
 [2.598]
 [2.598]
 [2.598]
 [3.198]
 [2.598]] [[1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.764]
 [1.303]]
siam score:  -0.73451126
first move QE:  0.7610303545620364
siam score:  -0.73640424
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.501]
 [0.516]
 [0.524]
 [0.516]
 [0.524]
 [0.516]] [[5.386]
 [5.114]
 [5.118]
 [5.088]
 [5.118]
 [5.255]
 [5.118]] [[0.573]
 [0.426]
 [0.458]
 [0.463]
 [0.458]
 [0.52 ]
 [0.458]]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.528]
 [0.503]
 [0.512]
 [0.484]
 [0.486]
 [0.535]] [[2.99 ]
 [4.377]
 [1.731]
 [3.31 ]
 [2.181]
 [1.616]
 [4.051]] [[0.488]
 [0.528]
 [0.503]
 [0.512]
 [0.484]
 [0.486]
 [0.535]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.604]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[3.071]
 [4.088]
 [3.071]
 [3.071]
 [3.071]
 [3.071]
 [3.071]] [[0.894]
 [1.258]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.678]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[0.504]
 [0.963]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[1.335]
 [1.559]
 [1.335]
 [1.335]
 [1.335]
 [1.335]
 [1.335]]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.612]
 [0.558]
 [0.567]
 [0.575]
 [0.581]
 [0.582]] [[0.73 ]
 [1.314]
 [0.682]
 [1.388]
 [1.748]
 [1.861]
 [1.075]] [[0.687]
 [0.965]
 [0.647]
 [0.9  ]
 [1.036]
 [1.087]
 [0.825]]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
UNIT TEST: sample policy line 217 mcts : [0.367 0.041 0.041 0.061 0.204 0.204 0.082]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2190 2336
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
siam score:  -0.73490465
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.632]
 [0.583]] [[3.632]
 [2.699]
 [2.699]
 [2.699]
 [2.699]
 [0.956]
 [2.699]] [[2.028]
 [1.334]
 [1.334]
 [1.334]
 [1.334]
 [0.181]
 [1.334]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.253961014476746
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[3.4  ]
 [3.442]
 [3.442]
 [3.442]
 [3.442]
 [3.442]
 [3.442]] [[0.504]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.522]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]] [[3.311]
 [3.294]
 [2.954]
 [2.954]
 [2.954]
 [2.954]
 [2.954]] [[0.517]
 [0.522]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]]
siam score:  -0.7133738
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 4.870330404926584
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.7214696
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[4.605]
 [4.605]
 [4.605]
 [4.605]
 [4.605]
 [4.605]
 [4.605]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.196]
 [0.213]
 [0.196]
 [0.196]
 [0.196]
 [0.196]] [[4.459]
 [4.72 ]
 [4.218]
 [4.72 ]
 [4.72 ]
 [4.72 ]
 [4.72 ]] [[-0.002]
 [ 0.035]
 [-0.099]
 [ 0.035]
 [ 0.035]
 [ 0.035]
 [ 0.035]]
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.696]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]] [[2.74 ]
 [3.297]
 [2.74 ]
 [2.74 ]
 [2.74 ]
 [2.74 ]
 [2.74 ]] [[0.649]
 [0.993]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.583]
 [0.574]
 [0.627]
 [0.579]
 [0.627]
 [0.576]] [[2.215]
 [2.634]
 [2.786]
 [3.544]
 [2.093]
 [3.544]
 [2.334]] [[1.008]
 [1.578]
 [1.763]
 [2.879]
 [0.847]
 [2.879]
 [1.163]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[5.063]
 [5.063]
 [5.063]
 [5.063]
 [5.063]
 [5.063]
 [5.063]] [[1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.464]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[5.82 ]
 [5.264]
 [5.309]
 [5.309]
 [5.309]
 [5.309]
 [5.309]] [[1.597]
 [1.293]
 [1.311]
 [1.311]
 [1.311]
 [1.311]
 [1.311]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.473]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[2.752]
 [3.063]
 [2.752]
 [2.752]
 [2.752]
 [2.752]
 [2.752]] [[0.463]
 [0.473]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.867]
 [0.608]
 [0.602]
 [0.62 ]
 [0.661]
 [0.57 ]] [[3.433]
 [2.135]
 [4.034]
 [3.866]
 [3.612]
 [4.253]
 [4.467]] [[0.676]
 [0.867]
 [0.608]
 [0.602]
 [0.62 ]
 [0.661]
 [0.57 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.7609303482458483
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.472]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[3.207]
 [5.019]
 [3.207]
 [3.207]
 [3.207]
 [3.207]
 [3.207]] [[0.19 ]
 [1.453]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.479]
 [0.487]
 [0.487]
 [0.507]
 [0.487]
 [0.482]] [[4.524]
 [4.773]
 [4.524]
 [4.524]
 [3.183]
 [4.524]
 [3.429]] [[0.817]
 [0.953]
 [0.817]
 [0.817]
 [0.056]
 [0.817]
 [0.171]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]] [[4.855]
 [4.077]
 [4.077]
 [4.077]
 [4.077]
 [4.077]
 [4.077]] [[2.18 ]
 [1.818]
 [1.818]
 [1.818]
 [1.818]
 [1.818]
 [1.818]]
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[1.363]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]] [[1.303]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]]
Printing some Q and Qe and total Qs values:  [[0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.942]
 [0.942]] [[3.076]
 [3.288]
 [3.288]
 [3.288]
 [3.288]
 [3.141]
 [3.134]] [[0.865]
 [0.937]
 [0.937]
 [0.937]
 [0.937]
 [0.892]
 [0.889]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.441]
 [0.44 ]
 [0.415]
 [0.414]
 [0.419]
 [0.424]] [[4.409]
 [4.773]
 [4.413]
 [4.062]
 [3.979]
 [3.889]
 [3.813]] [[0.954]
 [1.228]
 [0.983]
 [0.71 ]
 [0.653]
 [0.6  ]
 [0.555]]
Printing some Q and Qe and total Qs values:  [[0.903]
 [1.169]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]] [[0.951]
 [1.182]
 [1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]] [[2.029]
 [2.715]
 [2.163]
 [2.163]
 [2.163]
 [2.163]
 [2.163]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.473]
 [0.461]
 [0.461]
 [0.461]
 [0.459]
 [0.461]] [[0.908]
 [1.006]
 [0.922]
 [0.922]
 [0.922]
 [0.911]
 [0.922]] [[0.6  ]
 [0.69 ]
 [0.612]
 [0.612]
 [0.612]
 [0.599]
 [0.612]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]] [[2.672]
 [2.675]
 [2.675]
 [2.675]
 [2.675]
 [2.675]
 [2.675]] [[0.809]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]]
siam score:  -0.72747976
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]] [[6.053]
 [6.096]
 [6.096]
 [6.096]
 [6.096]
 [6.096]
 [6.096]] [[0.803]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 0.743097549731639
siam score:  -0.7176152
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.535]
 [0.527]
 [0.526]
 [0.533]
 [0.533]
 [0.531]] [[1.836]
 [2.328]
 [2.029]
 [1.427]
 [1.966]
 [2.013]
 [1.974]] [[0.531]
 [0.535]
 [0.527]
 [0.526]
 [0.533]
 [0.533]
 [0.531]]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.537]
 [0.532]
 [0.533]
 [0.533]
 [0.533]
 [0.533]] [[1.448]
 [2.446]
 [1.192]
 [1.425]
 [1.425]
 [1.425]
 [1.425]] [[0.53 ]
 [0.537]
 [0.532]
 [0.533]
 [0.533]
 [0.533]
 [0.533]]
siam score:  -0.72198224
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.618]
 [0.636]
 [0.609]
 [0.609]
 [0.609]
 [0.609]] [[2.485]
 [3.27 ]
 [2.158]
 [2.485]
 [2.485]
 [2.485]
 [2.485]] [[0.609]
 [0.618]
 [0.636]
 [0.609]
 [0.609]
 [0.609]
 [0.609]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
2212 2363
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.234]
 [0.247]
 [0.264]
 [0.264]
 [0.239]
 [0.264]] [[3.504]
 [3.684]
 [3.615]
 [3.504]
 [3.504]
 [3.474]
 [3.504]] [[0.951]
 [1.094]
 [1.042]
 [0.951]
 [0.951]
 [0.891]
 [0.951]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.326126806064225
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.02 ]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.018]] [[2.144]
 [2.22 ]
 [2.219]
 [2.222]
 [2.198]
 [2.2  ]
 [1.984]] [[0.263]
 [0.391]
 [0.397]
 [0.403]
 [0.363]
 [0.366]
 [0.002]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.7321336
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]] [[2.597]
 [2.597]
 [2.597]
 [2.597]
 [2.597]
 [2.597]
 [2.597]] [[0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]] [[2.664]
 [2.37 ]
 [2.37 ]
 [2.37 ]
 [2.37 ]
 [2.37 ]
 [2.37 ]] [[0.237]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.708]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]] [[2.832]
 [3.398]
 [2.832]
 [2.832]
 [2.832]
 [2.832]
 [2.832]] [[1.591]
 [2.003]
 [1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]]
using explorer policy with actor:  1
siam score:  -0.72845215
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.571]
 [0.513]
 [0.511]
 [0.511]
 [0.513]
 [0.515]] [[0.877]
 [1.239]
 [0.758]
 [0.946]
 [0.917]
 [0.763]
 [1.052]] [[0.149]
 [0.467]
 [0.063]
 [0.202]
 [0.18 ]
 [0.066]
 [0.284]]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]] [[4.716]
 [4.716]
 [4.716]
 [4.716]
 [4.716]
 [4.716]
 [4.716]] [[1.571]
 [1.571]
 [1.571]
 [1.571]
 [1.571]
 [1.571]
 [1.571]]
2222 2368
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.194]] [[4.167]
 [4.02 ]
 [4.02 ]
 [4.02 ]
 [4.02 ]
 [4.02 ]
 [4.145]] [[0.81 ]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.737]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.658]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[1.737]
 [3.708]
 [1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]] [[0.498]
 [1.376]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.7279947
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus -1.1949781564996425
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.743972
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using another actor
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.4420],
        [-0.2806],
        [-0.4134],
        [-0.3326],
        [-0.5978],
        [-0.3451],
        [-0.3927],
        [-0.4935],
        [-0.4552]], dtype=torch.float64)
-0.9702 -0.9702
-0.024259925299500003 -0.46628603393225293
-0.024259925299500003 -0.3049088960806058
-0.0337698257985 -0.44713232535261066
-0.024259925299500003 -0.3568119605245941
-0.024259925299500003 -0.622072457876277
-0.024259925299500003 -0.36939220404396456
-0.0337698257985 -0.4264416626793608
-0.024259925299500003 -0.5177315631623398
-0.024259925299500003 -0.47950503632725716
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.77]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]] [[6.251]
 [4.842]
 [4.842]
 [4.842]
 [4.842]
 [4.842]
 [4.842]] [[0.77]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.564]
 [0.555]
 [0.615]
 [0.591]
 [0.606]
 [0.584]] [[5.301]
 [6.777]
 [7.165]
 [5.748]
 [5.981]
 [5.403]
 [5.902]] [[0.606]
 [0.564]
 [0.555]
 [0.615]
 [0.591]
 [0.606]
 [0.584]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[2.449]
 [2.449]
 [2.449]
 [2.449]
 [2.449]
 [2.449]
 [2.449]] [[1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.253]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]] [[3.903]
 [3.823]
 [3.903]
 [3.903]
 [3.903]
 [3.903]
 [3.903]] [[1.329]
 [1.336]
 [1.329]
 [1.329]
 [1.329]
 [1.329]
 [1.329]]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.39 ]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[3.09 ]
 [3.178]
 [3.09 ]
 [3.09 ]
 [3.09 ]
 [3.09 ]
 [3.09 ]] [[1.036]
 [1.103]
 [1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
line 256 mcts: sample exp_bonus 3.30775594630012
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.387]
 [0.382]
 [0.382]
 [0.299]
 [0.382]
 [0.382]] [[4.479]
 [4.804]
 [5.202]
 [5.202]
 [5.175]
 [5.202]
 [5.202]] [[1.079]
 [1.341]
 [1.623]
 [1.623]
 [1.543]
 [1.623]
 [1.623]]
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.778]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[4.158]
 [5.853]
 [4.158]
 [4.158]
 [4.158]
 [4.158]
 [4.158]] [[1.994]
 [2.607]
 [1.994]
 [1.994]
 [1.994]
 [1.994]
 [1.994]]
Printing some Q and Qe and total Qs values:  [[0.173]
 [0.184]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]] [[4.769]
 [4.974]
 [4.769]
 [4.769]
 [4.769]
 [4.769]
 [4.769]] [[1.661]
 [1.784]
 [1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.661]]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[6.08 ]
 [5.608]
 [6.08 ]
 [6.08 ]
 [6.08 ]
 [6.08 ]
 [6.08 ]] [[-0.034]
 [-0.191]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 5.635889634454984
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.311]
 [0.462]
 [0.306]
 [0.306]
 [0.498]
 [0.306]] [[4.998]
 [4.89 ]
 [4.441]
 [5.945]
 [5.945]
 [4.452]
 [5.945]] [[0.792]
 [0.716]
 [0.564]
 [1.251]
 [1.251]
 [0.588]
 [1.251]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
2249 2370
siam score:  -0.7489613
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.748]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[1.464]
 [1.969]
 [1.464]
 [1.464]
 [1.464]
 [1.464]
 [1.464]] [[1.358]
 [1.676]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]] [[4.539]
 [4.539]
 [4.539]
 [4.539]
 [4.539]
 [4.539]
 [4.539]] [[1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]]
siam score:  -0.75177956
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.684]
 [0.571]
 [0.618]
 [0.618]
 [0.592]
 [0.618]] [[1.228]
 [1.449]
 [1.553]
 [1.228]
 [1.228]
 [2.452]
 [1.228]] [[0.169]
 [0.408]
 [0.36 ]
 [0.169]
 [0.169]
 [1.058]
 [0.169]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.75351906
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.533]
 [0.534]
 [0.534]
 [0.506]
 [0.488]
 [0.482]] [[3.031]
 [3.042]
 [3.161]
 [3.161]
 [3.018]
 [4.113]
 [3.161]] [[0.54 ]
 [0.664]
 [0.706]
 [0.706]
 [0.602]
 [0.932]
 [0.602]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.45 ]
 [0.52 ]
 [0.52 ]
 [0.554]
 [0.52 ]
 [0.52 ]] [[3.004]
 [1.749]
 [1.555]
 [1.555]
 [1.051]
 [1.555]
 [1.555]] [[2.067]
 [1.359]
 [1.278]
 [1.278]
 [0.969]
 [1.278]
 [1.278]]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
first move QE:  0.7628283121999219
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.538]
 [0.493]
 [0.499]
 [0.489]
 [0.49 ]
 [0.495]] [[6.732]
 [7.666]
 [6.694]
 [6.78 ]
 [6.976]
 [7.528]
 [7.356]] [[1.152]
 [1.604]
 [1.119]
 [1.165]
 [1.237]
 [1.479]
 [1.412]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.209]
 [0.175]
 [0.125]
 [0.122]
 [0.105]
 [0.121]] [[1.455]
 [1.229]
 [1.626]
 [1.181]
 [1.374]
 [1.747]
 [1.475]] [[0.13 ]
 [0.209]
 [0.175]
 [0.125]
 [0.122]
 [0.105]
 [0.121]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
first move QE:  0.7634781337021632
siam score:  -0.75858504
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[9.097]
 [6.329]
 [6.329]
 [6.329]
 [6.329]
 [6.329]
 [6.329]] [[2.118]
 [1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.192]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[4.378]
 [4.378]
 [4.378]
 [4.378]
 [4.378]
 [4.378]
 [4.378]] [[2.17]
 [2.17]
 [2.17]
 [2.17]
 [2.17]
 [2.17]
 [2.17]]
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]] [[2.269]
 [2.941]
 [2.941]
 [2.941]
 [2.941]
 [2.941]
 [2.941]] [[1.157]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[0.826]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]] [[ 0.819]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.735943700033164
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.71 ]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]] [[2.791]
 [2.843]
 [2.791]
 [2.791]
 [2.791]
 [2.791]
 [2.791]] [[2.139]
 [2.238]
 [2.139]
 [2.139]
 [2.139]
 [2.139]
 [2.139]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.604]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[1.154]
 [1.375]
 [1.154]
 [1.154]
 [1.154]
 [1.154]
 [1.154]] [[0.672]
 [0.604]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.8244457029331544
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.638]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[5.061]
 [5.066]
 [5.061]
 [5.061]
 [5.061]
 [5.061]
 [5.061]] [[1.576]
 [1.595]
 [1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]]
line 256 mcts: sample exp_bonus 5.063080966778789
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.7648037245508827
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.12 ]
 [0.086]
 [0.094]
 [0.136]
 [0.124]
 [0.136]] [[5.389]
 [5.777]
 [5.655]
 [5.532]
 [5.085]
 [5.362]
 [5.085]] [[-0.015]
 [ 0.117]
 [ 0.008]
 [-0.017]
 [-0.082]
 [-0.014]
 [-0.082]]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]] [[2.164]
 [2.164]
 [2.164]
 [2.164]
 [2.164]
 [2.164]
 [2.164]] [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.466]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[3.99 ]
 [4.103]
 [3.99 ]
 [3.99 ]
 [3.99 ]
 [3.99 ]
 [3.99 ]] [[0.441]
 [0.462]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.552]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.548]] [[5.128]
 [5.362]
 [5.128]
 [5.128]
 [5.128]
 [5.128]
 [5.396]] [[1.552]
 [1.661]
 [1.552]
 [1.552]
 [1.552]
 [1.552]
 [1.676]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.588]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]] [[3.046]
 [3.443]
 [3.046]
 [3.046]
 [3.046]
 [3.046]
 [3.046]] [[0.911]
 [1.088]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]]
2283 2390
first move QE:  0.7703763088715799
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.56 ]
 [0.496]
 [0.497]
 [0.509]
 [0.506]
 [0.513]] [[3.889]
 [4.849]
 [5.033]
 [5.075]
 [4.949]
 [4.934]
 [4.892]] [[0.335]
 [0.71 ]
 [0.704]
 [0.735]
 [0.674]
 [0.659]
 [0.643]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.35 ]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[5.163]
 [5.361]
 [5.163]
 [5.163]
 [5.163]
 [5.163]
 [5.163]] [[0.467]
 [0.606]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]]
using explorer policy with actor:  1
first move QE:  0.7703763088715799
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.74245995
using explorer policy with actor:  1
siam score:  -0.7444465
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.044]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.063]] [[2.906]
 [4.039]
 [2.906]
 [2.906]
 [2.906]
 [2.906]
 [3.843]] [[-1.201]
 [-0.885]
 [-1.201]
 [-1.201]
 [-1.201]
 [-1.201]
 [-0.911]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.629]
 [0.587]
 [0.591]
 [0.546]
 [0.568]
 [0.628]] [[0.798]
 [2.521]
 [0.243]
 [0.077]
 [0.736]
 [0.139]
 [0.848]] [[0.388]
 [1.341]
 [0.234]
 [0.159]
 [0.438]
 [0.172]
 [0.549]]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.67 ]
 [0.587]
 [0.591]
 [0.546]
 [0.568]
 [0.628]] [[0.798]
 [2.79 ]
 [0.243]
 [0.077]
 [0.736]
 [0.139]
 [0.848]] [[0.385]
 [1.485]
 [0.232]
 [0.157]
 [0.434]
 [0.17 ]
 [0.544]]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.505]
 [0.617]
 [0.62 ]
 [0.597]
 [0.643]
 [0.609]] [[2.289]
 [1.939]
 [1.904]
 [2.004]
 [1.183]
 [0.896]
 [1.397]] [[1.688]
 [1.087]
 [1.289]
 [1.361]
 [0.769]
 [0.67 ]
 [0.934]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.841]
 [0.685]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]] [[3.666]
 [3.136]
 [3.352]
 [3.352]
 [3.352]
 [3.352]
 [3.352]] [[2.064]
 [1.399]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.175]
 [0.15 ]
 [0.158]
 [0.151]
 [0.149]
 [0.152]] [[-0.728]
 [-0.098]
 [-0.581]
 [-0.386]
 [-0.611]
 [-0.385]
 [-0.61 ]] [[0.157]
 [0.175]
 [0.15 ]
 [0.158]
 [0.151]
 [0.149]
 [0.152]]
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.105]
 [0.129]
 [0.131]
 [0.131]
 [0.131]
 [0.125]] [[-1.825]
 [-0.177]
 [-2.002]
 [-1.707]
 [-1.707]
 [-1.707]
 [-1.947]] [[0.125]
 [0.105]
 [0.129]
 [0.131]
 [0.131]
 [0.131]
 [0.125]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9395386851186098
first move QE:  0.7691127904667918
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
siam score:  -0.72785956
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.307]
 [0.297]
 [0.312]
 [0.314]
 [0.294]
 [0.315]] [[ 0.846]
 [ 0.652]
 [-1.238]
 [-1.236]
 [-0.825]
 [-0.57 ]
 [-0.576]] [[0.13 ]
 [0.307]
 [0.297]
 [0.312]
 [0.314]
 [0.294]
 [0.315]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]] [[-0.81]
 [-0.81]
 [-0.81]
 [-0.81]
 [-0.81]
 [-0.81]
 [-0.81]] [[0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.48 ]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[2.618]
 [2.952]
 [2.618]
 [2.618]
 [2.618]
 [2.618]
 [2.618]] [[0.727]
 [0.924]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.648]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]] [[2.137]
 [2.776]
 [2.137]
 [2.137]
 [2.137]
 [2.137]
 [2.137]] [[1.506]
 [1.769]
 [1.506]
 [1.506]
 [1.506]
 [1.506]
 [1.506]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.831]
 [0.831]
 [0.831]
 [0.494]
 [0.664]
 [0.831]] [[ 2.152]
 [ 2.406]
 [ 2.406]
 [ 2.406]
 [ 1.838]
 [-1.015]
 [ 2.406]] [[2.152]
 [2.303]
 [2.303]
 [2.303]
 [1.843]
 [0.91 ]
 [2.303]]
UNIT TEST: sample policy line 217 mcts : [0.102 0.184 0.082 0.082 0.082 0.327 0.143]
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[ 0.647]
 [-1.916]
 [-1.916]
 [-1.916]
 [-1.916]
 [-1.916]
 [-1.916]] [[0.685]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  0
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]] [[4.4]
 [4.4]
 [4.4]
 [4.4]
 [4.4]
 [4.4]
 [4.4]] [[0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]]
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]] [[5.944]
 [3.994]
 [3.994]
 [3.994]
 [3.994]
 [3.994]
 [3.994]] [[0.892]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.615]
 [0.628]
 [0.639]
 [0.628]
 [0.628]
 [0.613]] [[0.983]
 [2.086]
 [0.983]
 [1.062]
 [0.983]
 [0.983]
 [1.573]] [[0.628]
 [0.615]
 [0.628]
 [0.639]
 [0.628]
 [0.628]
 [0.613]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.512]
 [0.496]
 [0.512]
 [0.512]
 [0.502]
 [0.504]] [[3.355]
 [3.222]
 [3.354]
 [3.222]
 [3.222]
 [3.319]
 [3.339]] [[0.494]
 [0.512]
 [0.496]
 [0.512]
 [0.512]
 [0.502]
 [0.504]]
2308 2418
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[4.752]
 [4.752]
 [4.752]
 [4.752]
 [4.752]
 [4.752]
 [4.752]] [[4.554]
 [4.554]
 [4.554]
 [4.554]
 [4.554]
 [4.554]
 [4.554]]
line 256 mcts: sample exp_bonus 1.0773170508331715
rdn probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[2.608]
 [2.608]
 [2.608]
 [2.608]
 [2.608]
 [2.608]
 [2.608]] [[1.887]
 [1.887]
 [1.887]
 [1.887]
 [1.887]
 [1.887]
 [1.887]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
siam score:  -0.7264064
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.388]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.433]] [[3.654]
 [5.016]
 [3.654]
 [3.654]
 [3.654]
 [3.654]
 [3.914]] [[0.696]
 [1.52 ]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.868]]
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.299]
 [0.293]
 [0.272]
 [0.262]
 [0.284]
 [0.277]] [[1.212]
 [0.856]
 [0.758]
 [1.344]
 [1.241]
 [0.97 ]
 [0.774]] [[1.036]
 [1.033]
 [0.953]
 [1.354]
 [1.263]
 [1.096]
 [0.942]]
2313 2422
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
first move QE:  0.7668353357514243
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.688]
 [0.584]
 [0.584]
 [0.491]
 [0.584]
 [0.61 ]] [[1.13 ]
 [0.581]
 [0.502]
 [0.502]
 [0.672]
 [0.502]
 [0.446]] [[0.872]
 [0.703]
 [0.47 ]
 [0.47 ]
 [0.537]
 [0.47 ]
 [0.441]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  0
2318 2429
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]] [[2.277]
 [2.458]
 [2.458]
 [2.458]
 [2.458]
 [2.458]
 [2.458]] [[0.853]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]]
start point for exploration sampling:  10935
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9388708155573346
siam score:  -0.69913846
Printing some Q and Qe and total Qs values:  [[1.027]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.837]] [[6.894]
 [3.235]
 [3.235]
 [3.235]
 [3.235]
 [3.235]
 [3.235]] [[2.014]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[5.535]
 [2.859]
 [2.859]
 [2.859]
 [2.859]
 [2.859]
 [2.859]] [[1.861]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]]
siam score:  -0.693928
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.687]
 [0.69 ]] [[4.632]
 [4.691]
 [4.691]
 [4.691]
 [4.691]
 [6.032]
 [4.691]] [[1.516]
 [1.539]
 [1.539]
 [1.539]
 [1.539]
 [2.401]
 [1.539]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
line 256 mcts: sample exp_bonus 4.357984798624375
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.331]
 [0.316]
 [0.326]
 [0.321]
 [0.32 ]
 [0.333]] [[2.615]
 [2.717]
 [2.627]
 [2.661]
 [2.579]
 [2.571]
 [2.844]] [[0.078]
 [0.155]
 [0.065]
 [0.107]
 [0.043]
 [0.036]
 [0.244]]
siam score:  -0.68864596
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]] [[-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]] [[0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.078]
 [0.994]
 [0.995]
 [0.995]
 [0.995]
 [0.993]
 [0.995]] [[2.307]
 [2.566]
 [2.628]
 [2.628]
 [2.628]
 [2.536]
 [2.628]] [[1.337]
 [1.256]
 [1.278]
 [1.278]
 [1.278]
 [1.244]
 [1.278]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
siam score:  -0.68476456
Printing some Q and Qe and total Qs values:  [[1.485]
 [1.485]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[0.737]
 [0.735]
 [1.569]
 [1.569]
 [1.569]
 [1.569]
 [1.569]] [[2.139]
 [2.137]
 [1.438]
 [1.438]
 [1.438]
 [1.438]
 [1.438]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.648]
 [1.17 ]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[3.174]
 [1.085]
 [3.174]
 [3.174]
 [3.174]
 [3.174]
 [3.174]] [[0.92 ]
 [1.267]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.515]
 [0.536]
 [0.547]
 [0.555]
 [0.609]
 [0.572]] [[4.339]
 [3.478]
 [4.133]
 [4.054]
 [4.009]
 [4.063]
 [4.289]] [[0.397]
 [0.077]
 [0.338]
 [0.334]
 [0.335]
 [0.46 ]
 [0.462]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3222896556179342
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.021]
 [-0.008]
 [-0.009]
 [-0.01 ]
 [-0.007]
 [-0.007]] [[3.507]
 [3.057]
 [3.631]
 [3.679]
 [3.857]
 [3.773]
 [3.747]] [[-0.773]
 [-1.1  ]
 [-0.692]
 [-0.662]
 [-0.545]
 [-0.596]
 [-0.613]]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.083]] [[-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]]
siam score:  -0.6741681
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[6.287]
 [6.287]
 [6.287]
 [6.287]
 [6.287]
 [6.287]
 [6.287]] [[1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[2.783]
 [2.783]
 [2.783]
 [2.783]
 [2.783]
 [2.783]
 [2.783]] [[1.856]
 [1.856]
 [1.856]
 [1.856]
 [1.856]
 [1.856]
 [1.856]]
first move QE:  0.7651057517071316
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[5.26]
 [5.26]
 [5.26]
 [5.26]
 [5.26]
 [5.26]
 [5.26]] [[0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.875]
 [0.792]
 [0.792]
 [0.791]
 [0.789]
 [0.786]] [[2.292]
 [1.374]
 [1.761]
 [1.941]
 [2.091]
 [1.961]
 [1.494]] [[1.806]
 [1.355]
 [1.425]
 [1.515]
 [1.588]
 [1.52 ]
 [1.281]]
siam score:  -0.6864836
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[1.126]
 [1.092]
 [1.092]
 [1.092]
 [1.092]
 [1.092]
 [1.092]] [[4.657]
 [4.583]
 [4.583]
 [4.583]
 [4.583]
 [4.583]
 [4.583]] [[2.311]
 [2.255]
 [2.255]
 [2.255]
 [2.255]
 [2.255]
 [2.255]]
line 256 mcts: sample exp_bonus 5.376456384953084
line 256 mcts: sample exp_bonus 3.830141107241814
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]] [[2.979]
 [2.699]
 [2.699]
 [2.699]
 [2.699]
 [2.699]
 [2.699]] [[ 0.515]
 [-0.154]
 [-0.154]
 [-0.154]
 [-0.154]
 [-0.154]
 [-0.154]]
start point for exploration sampling:  10935
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.469]
 [0.459]
 [0.475]
 [0.49 ]
 [0.472]
 [0.497]] [[2.097]
 [2.431]
 [2.273]
 [2.132]
 [2.114]
 [2.12 ]
 [2.087]] [[0.481]
 [0.469]
 [0.459]
 [0.475]
 [0.49 ]
 [0.472]
 [0.497]]
Printing some Q and Qe and total Qs values:  [[1.03 ]
 [0.435]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]] [[1.052]
 [1.823]
 [2.274]
 [2.274]
 [2.274]
 [2.229]
 [2.274]] [[0.793]
 [0.116]
 [0.407]
 [0.407]
 [0.407]
 [0.377]
 [0.407]]
line 256 mcts: sample exp_bonus 1.5039254810630138
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.498]
 [0.566]
 [0.573]
 [0.562]
 [0.574]
 [0.541]] [[4.438]
 [4.041]
 [4.246]
 [4.185]
 [4.415]
 [4.658]
 [4.168]] [[1.214]
 [0.893]
 [1.077]
 [1.044]
 [1.183]
 [1.348]
 [1.009]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
siam score:  -0.6826059
2346 2455
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
siam score:  -0.68320376
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
start point for exploration sampling:  10935
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.0697112528186397
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
siam score:  -0.6752077
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
in main func line 156:  2352
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.032]
 [0.069]
 [0.065]
 [0.083]
 [0.068]
 [0.066]] [[3.625]
 [3.568]
 [3.65 ]
 [3.572]
 [3.371]
 [3.701]
 [3.518]] [[-0.257]
 [-0.368]
 [-0.24 ]
 [-0.301]
 [-0.398]
 [-0.209]
 [-0.334]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.502]
 [0.492]
 [0.475]
 [0.475]
 [0.475]
 [0.49 ]] [[2.322]
 [1.871]
 [2.117]
 [1.868]
 [1.868]
 [1.868]
 [2.401]] [[ 0.291]
 [-0.028]
 [ 0.116]
 [-0.083]
 [-0.083]
 [-0.083]
 [ 0.302]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [1.477]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]] [[1.389]
 [0.751]
 [1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]] [[1.061]
 [2.912]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.512]
 [0.454]
 [0.503]
 [0.454]
 [0.454]
 [0.535]] [[2.236]
 [2.919]
 [2.027]
 [3.026]
 [2.027]
 [2.027]
 [3.355]] [[0.745]
 [1.508]
 [0.859]
 [1.566]
 [0.859]
 [0.859]
 [1.819]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.599]
 [0.554]
 [0.549]
 [0.548]
 [0.548]
 [0.544]] [[2.388]
 [2.304]
 [2.22 ]
 [2.237]
 [2.499]
 [2.499]
 [2.387]] [[1.368]
 [1.249]
 [1.075]
 [1.088]
 [1.384]
 [1.384]
 [1.249]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.044]
 [0.161]
 [0.263]
 [0.101]
 [0.314]
 [0.121]] [[0.968]
 [1.314]
 [0.543]
 [0.436]
 [1.027]
 [0.537]
 [1.319]] [[0.815]
 [1.158]
 [0.179]
 [0.204]
 [0.817]
 [0.454]
 [1.308]]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
line 256 mcts: sample exp_bonus 3.8112861268256335
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
siam score:  -0.6831945
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.481]
 [0.499]
 [0.497]
 [0.493]
 [0.493]
 [0.494]] [[0.978]
 [2.075]
 [1.601]
 [1.407]
 [1.479]
 [1.553]
 [1.533]] [[0.756]
 [0.481]
 [0.499]
 [0.497]
 [0.493]
 [0.493]
 [0.494]]
first move QE:  0.7640811621065502
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6805362
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[ 0.754]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]] [[0.788]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[-0.149]
 [ 0.362]
 [ 0.362]
 [ 0.362]
 [ 0.362]
 [ 0.362]
 [ 0.362]] [[0.783]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[-0.195]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]] [[0.617]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.853]] [[1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.345]] [[1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.74 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.378]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[-0.125]
 [ 0.734]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]] [[0.35 ]
 [0.378]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.262]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[2.565]
 [3.009]
 [2.565]
 [2.565]
 [2.565]
 [2.565]
 [2.565]] [[0.26 ]
 [0.789]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]] [[1.148]
 [1.148]
 [1.148]
 [1.148]
 [1.148]
 [1.148]
 [1.148]] [[1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]]
siam score:  -0.6890358
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.494]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[2.904]
 [2.33 ]
 [2.904]
 [2.904]
 [2.904]
 [2.904]
 [2.904]] [[0.36 ]
 [0.183]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]]
siam score:  -0.6873961
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 3.339515436519755
siam score:  -0.68677855
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
line 256 mcts: sample exp_bonus 3.4890291866133176
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.655]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]] [[1.785]
 [1.96 ]
 [1.785]
 [1.785]
 [1.785]
 [1.785]
 [1.785]] [[1.206]
 [1.264]
 [1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.463]
 [0.45 ]
 [0.501]
 [0.501]
 [0.501]
 [0.451]] [[0.337]
 [1.067]
 [0.377]
 [0.337]
 [0.337]
 [0.337]
 [0.124]] [[0.501]
 [0.463]
 [0.45 ]
 [0.501]
 [0.501]
 [0.501]
 [0.451]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.8307319975727094
2381 2488
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[2.649]
 [2.649]
 [2.649]
 [2.649]
 [2.649]
 [2.649]
 [2.649]] [[0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]]
using explorer policy with actor:  1
siam score:  -0.69533247
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.473]
 [0.492]
 [0.505]
 [0.491]
 [0.579]
 [0.469]] [[2.464]
 [2.535]
 [1.83 ]
 [2.639]
 [2.8  ]
 [3.419]
 [1.794]] [[0.532]
 [0.55 ]
 [0.168]
 [0.637]
 [0.715]
 [1.139]
 [0.129]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.996]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[0.543]
 [1.614]
 [1.614]
 [1.614]
 [1.614]
 [1.614]
 [1.614]] [[1.89 ]
 [1.193]
 [1.193]
 [1.193]
 [1.193]
 [1.193]
 [1.193]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.993]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]] [[4.575]
 [2.843]
 [2.843]
 [2.843]
 [2.843]
 [2.843]
 [2.843]] [[2.691]
 [2.15 ]
 [2.15 ]
 [2.15 ]
 [2.15 ]
 [2.15 ]
 [2.15 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.459]
 [0.516]
 [0.382]
 [0.488]
 [0.382]
 [0.488]] [[ 1.485]
 [ 0.715]
 [-0.391]
 [ 1.291]
 [ 0.122]
 [ 1.291]
 [ 0.16 ]] [[1.965]
 [1.34 ]
 [0.265]
 [1.841]
 [0.762]
 [1.841]
 [0.8  ]]
in main func line 156:  2388
Printing some Q and Qe and total Qs values:  [[1.081]
 [1.08 ]
 [1.08 ]
 [1.08 ]
 [1.08 ]
 [1.08 ]
 [1.08 ]] [[5.33 ]
 [5.325]
 [5.325]
 [5.325]
 [5.325]
 [5.325]
 [5.325]] [[1.386]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.384]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.517]
 [0.506]
 [0.506]
 [0.505]
 [0.505]
 [0.514]] [[ 0.718]
 [ 1.64 ]
 [-0.418]
 [-0.562]
 [-0.695]
 [-0.686]
 [-0.783]] [[1.208]
 [1.824]
 [0.453]
 [0.357]
 [0.268]
 [0.274]
 [0.219]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7475853332219442
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
first move QE:  0.7609137779923294
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.579]
 [0.516]
 [0.484]
 [0.484]
 [0.484]
 [0.562]] [[1.276]
 [1.695]
 [1.118]
 [1.276]
 [1.276]
 [1.276]
 [1.641]] [[1.857]
 [2.185]
 [1.828]
 [1.857]
 [1.857]
 [1.857]
 [2.137]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.52 ]
 [0.505]
 [0.51 ]
 [0.51 ]
 [0.491]
 [0.51 ]] [[0.991]
 [1.822]
 [0.641]
 [0.175]
 [0.175]
 [0.721]
 [0.175]] [[0.506]
 [0.52 ]
 [0.505]
 [0.51 ]
 [0.51 ]
 [0.491]
 [0.51 ]]
siam score:  -0.71106476
siam score:  -0.7114841
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
2395 2522
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7689006021351915
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.546]
 [0.545]
 [0.538]
 [0.539]
 [0.54 ]
 [0.547]] [[1.225]
 [2.165]
 [1.478]
 [1.284]
 [1.273]
 [1.189]
 [1.512]] [[0.542]
 [0.546]
 [0.545]
 [0.538]
 [0.539]
 [0.54 ]
 [0.547]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[1.944]
 [1.075]
 [1.075]
 [1.075]
 [1.075]
 [1.075]
 [1.075]] [[1.397]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.643]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.638]] [[3.03 ]
 [3.689]
 [3.03 ]
 [3.03 ]
 [3.03 ]
 [3.03 ]
 [3.862]] [[1.398]
 [1.613]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.673]]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.301]
 [0.292]
 [0.295]
 [0.295]
 [0.29 ]
 [0.307]] [[2.945]
 [2.801]
 [3.137]
 [2.788]
 [2.788]
 [3.262]
 [3.446]] [[0.936]
 [0.845]
 [1.085]
 [0.829]
 [0.829]
 [1.175]
 [1.326]]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.259]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]] [[2.31]
 [2.42]
 [2.31]
 [2.31]
 [2.31]
 [2.31]
 [2.31]] [[0.573]
 [0.719]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.413]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]] [[2.182]
 [2.424]
 [2.182]
 [2.182]
 [2.182]
 [2.182]
 [2.182]] [[0.654]
 [0.829]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
siam score:  -0.70884997
first move QE:  0.7569713616235775
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
siam score:  -0.70704466
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[3.55]
 [3.55]
 [3.55]
 [3.55]
 [3.55]
 [3.55]
 [3.55]] [[0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]]
siam score:  -0.7070091
line 256 mcts: sample exp_bonus 4.549697445239889
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.543]
 [0.614]
 [0.613]
 [0.604]
 [0.775]
 [0.604]] [[1.436]
 [2.272]
 [1.085]
 [1.058]
 [2.913]
 [2.905]
 [2.913]] [[0.592]
 [1.044]
 [0.415]
 [0.394]
 [1.574]
 [1.901]
 [1.574]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.403]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]] [[2.298]
 [3.45 ]
 [2.298]
 [2.298]
 [2.298]
 [2.298]
 [2.298]] [[0.069]
 [0.795]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]]
using explorer policy with actor:  1
siam score:  -0.70488924
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]] [[2.552]
 [2.552]
 [2.552]
 [2.552]
 [2.552]
 [2.552]
 [2.552]] [[-0.544]
 [-0.544]
 [-0.544]
 [-0.544]
 [-0.544]
 [-0.544]
 [-0.544]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.561]
 [0.588]
 [0.597]
 [0.602]
 [0.6  ]
 [0.619]] [[3.776]
 [4.46 ]
 [4.044]
 [4.102]
 [4.124]
 [4.072]
 [3.879]] [[0.763]
 [1.095]
 [0.872]
 [0.928]
 [0.953]
 [0.915]
 [0.824]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[3.169]
 [1.843]
 [1.843]
 [1.843]
 [1.843]
 [1.843]
 [1.843]] [[0.733]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.485]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]] [[0.935]
 [2.772]
 [0.935]
 [0.935]
 [0.935]
 [0.935]
 [0.935]] [[0.842]
 [1.506]
 [0.842]
 [0.842]
 [0.842]
 [0.842]
 [0.842]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.039]
 [0.027]
 [0.058]
 [0.022]
 [0.058]
 [0.025]] [[3.502]
 [3.972]
 [3.226]
 [3.502]
 [3.168]
 [3.502]
 [3.161]] [[-0.541]
 [-0.267]
 [-0.786]
 [-0.541]
 [-0.835]
 [-0.541]
 [-0.834]]
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.219]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]] [[4.142]
 [4.245]
 [4.142]
 [4.142]
 [4.142]
 [4.142]
 [4.142]] [[0.941]
 [1.148]
 [0.941]
 [0.941]
 [0.941]
 [0.941]
 [0.941]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.05 ]
 [-0.049]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]] [[7.925]
 [8.723]
 [8.49 ]
 [8.723]
 [8.723]
 [8.723]
 [8.723]] [[1.169]
 [1.52 ]
 [1.415]
 [1.52 ]
 [1.52 ]
 [1.52 ]
 [1.52 ]]
first move QE:  0.7518536233661761
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.458]
 [0.469]
 [0.44 ]
 [0.44 ]
 [0.462]
 [0.44 ]] [[5.319]
 [5.363]
 [5.281]
 [6.592]
 [6.592]
 [5.345]
 [6.592]] [[1.134]
 [1.154]
 [1.118]
 [1.798]
 [1.798]
 [1.148]
 [1.798]]
using explorer policy with actor:  1
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]] [[3.39]
 [3.39]
 [3.39]
 [3.39]
 [3.39]
 [3.39]
 [3.39]] [[1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.602]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[1.534]
 [1.581]
 [1.534]
 [1.534]
 [1.534]
 [1.534]
 [1.534]] [[1.019]
 [1.106]
 [1.019]
 [1.019]
 [1.019]
 [1.019]
 [1.019]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [0.181]
 [0.211]
 [0.21 ]
 [0.207]
 [0.201]
 [0.2  ]] [[1.221]
 [1.371]
 [1.598]
 [1.936]
 [1.944]
 [1.891]
 [1.415]] [[-0.646]
 [-0.383]
 [ 0.13 ]
 [ 0.805]
 [ 0.815]
 [ 0.697]
 [-0.257]]
start point for exploration sampling:  10935
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.045]
 [-0.032]
 [-0.036]
 [-0.026]
 [-0.017]
 [-0.022]] [[3.517]
 [2.974]
 [3.296]
 [3.673]
 [3.366]
 [3.526]
 [3.515]] [[ 0.445]
 [-0.04 ]
 [ 0.244]
 [ 0.552]
 [ 0.31 ]
 [ 0.455]
 [ 0.438]]
siam score:  -0.69911456
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.632]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]] [[2.289]
 [3.29 ]
 [2.686]
 [2.686]
 [2.686]
 [2.686]
 [2.686]] [[1.344]
 [2.08 ]
 [1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.409]
 [0.458]
 [0.422]
 [0.439]
 [0.442]
 [0.446]] [[3.982]
 [3.62 ]
 [3.988]
 [3.632]
 [4.106]
 [4.149]
 [3.987]] [[0.435]
 [0.409]
 [0.458]
 [0.422]
 [0.439]
 [0.442]
 [0.446]]
line 256 mcts: sample exp_bonus 2.152477991186779
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.416]
 [0.433]
 [0.416]
 [0.416]
 [0.447]
 [0.416]] [[3.704]
 [4.847]
 [4.149]
 [4.847]
 [4.847]
 [4.409]
 [4.847]] [[0.771]
 [1.068]
 [0.869]
 [1.068]
 [1.068]
 [0.983]
 [1.068]]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.565]
 [0.565]
 [0.555]
 [0.565]
 [0.565]
 [0.565]] [[1.027]
 [1.027]
 [1.027]
 [0.838]
 [1.027]
 [1.027]
 [1.027]] [[0.565]
 [0.565]
 [0.565]
 [0.555]
 [0.565]
 [0.565]
 [0.565]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.479]
 [0.479]
 [0.46 ]
 [0.479]
 [0.679]
 [0.479]] [[2.309]
 [2.309]
 [2.309]
 [2.605]
 [2.309]
 [2.825]
 [2.309]] [[0.479]
 [0.479]
 [0.479]
 [0.46 ]
 [0.479]
 [0.679]
 [0.479]]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]] [[3.806]
 [1.85 ]
 [1.85 ]
 [1.85 ]
 [1.85 ]
 [1.85 ]
 [1.85 ]] [[2.509]
 [1.231]
 [1.231]
 [1.231]
 [1.231]
 [1.231]
 [1.231]]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.501]
 [0.484]
 [0.485]
 [0.48 ]
 [0.482]
 [0.493]] [[2.93 ]
 [3.22 ]
 [3.226]
 [3.985]
 [3.187]
 [2.954]
 [3.456]] [[0.497]
 [0.501]
 [0.484]
 [0.485]
 [0.48 ]
 [0.482]
 [0.493]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.467]
 [0.458]
 [0.487]
 [0.477]
 [0.421]
 [0.485]] [[2.586]
 [4.405]
 [1.593]
 [3.456]
 [2.231]
 [1.596]
 [3.213]] [[0.48 ]
 [0.467]
 [0.458]
 [0.487]
 [0.477]
 [0.421]
 [0.485]]
line 256 mcts: sample exp_bonus -0.7609966236270221
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.472]
 [0.462]
 [0.461]
 [0.461]
 [0.461]
 [0.461]] [[3.059]
 [2.992]
 [2.875]
 [2.677]
 [2.677]
 [2.677]
 [2.677]] [[0.456]
 [0.472]
 [0.462]
 [0.461]
 [0.461]
 [0.461]
 [0.461]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.544]
 [0.541]
 [0.527]
 [0.528]
 [0.549]
 [0.543]] [[2.718]
 [2.757]
 [2.542]
 [2.526]
 [2.302]
 [2.935]
 [2.549]] [[0.529]
 [0.544]
 [0.541]
 [0.527]
 [0.528]
 [0.549]
 [0.543]]
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.617]
 [0.614]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[-0.397]
 [ 1.027]
 [ 0.962]
 [ 1.027]
 [ 1.027]
 [ 1.027]
 [ 1.027]] [[0.906]
 [0.617]
 [0.614]
 [0.617]
 [0.617]
 [0.617]
 [0.617]]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.544]
 [0.502]] [[2.576]
 [2.576]
 [2.576]
 [2.576]
 [2.576]
 [3.126]
 [2.576]] [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.544]
 [0.502]]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.504]
 [0.12 ]
 [0.401]
 [0.401]
 [0.337]
 [0.401]] [[2.975]
 [3.108]
 [1.015]
 [1.743]
 [1.743]
 [2.069]
 [1.743]] [[ 0.911]
 [ 1.3  ]
 [-0.167]
 [ 0.637]
 [ 0.637]
 [ 0.619]
 [ 0.637]]
line 256 mcts: sample exp_bonus 0.27527543290124895
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.307]
 [0.258]
 [0.313]
 [0.093]
 [0.111]
 [0.285]] [[2.691]
 [1.221]
 [0.857]
 [0.81 ]
 [2.704]
 [2.282]
 [1.276]] [[ 0.185]
 [ 0.153]
 [-0.066]
 [ 0.026]
 [ 0.219]
 [ 0.114]
 [ 0.127]]
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.521]
 [0.48 ]
 [0.485]
 [0.477]
 [0.59 ]
 [0.523]] [[1.213]
 [1.302]
 [1.192]
 [1.052]
 [0.998]
 [1.204]
 [1.302]] [[0.485]
 [0.521]
 [0.48 ]
 [0.485]
 [0.477]
 [0.59 ]
 [0.523]]
Printing some Q and Qe and total Qs values:  [[1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]] [[0.755]
 [0.746]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[1.679]
 [1.676]
 [1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.408]
 [0.368]
 [0.36 ]
 [0.373]
 [0.509]
 [0.308]] [[2.489]
 [4.127]
 [4.561]
 [4.002]
 [4.004]
 [5.557]
 [4.52 ]] [[-0.184]
 [ 0.428]
 [ 0.613]
 [ 0.271]
 [ 0.296]
 [ 1.441]
 [ 0.483]]
siam score:  -0.71949506
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.1868306595434643
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.948]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]] [[2.198]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]] [[0.948]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.599]
 [0.745]
 [0.653]
 [0.555]
 [0.536]
 [0.522]] [[1.081]
 [1.884]
 [1.712]
 [1.588]
 [1.008]
 [1.049]
 [0.991]] [[0.522]
 [0.599]
 [0.745]
 [0.653]
 [0.555]
 [0.536]
 [0.522]]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.335]
 [0.291]
 [0.27 ]
 [0.267]
 [0.27 ]
 [0.272]] [[3.206]
 [3.219]
 [3.285]
 [2.981]
 [2.939]
 [3.033]
 [2.959]] [[ 0.167]
 [ 0.283]
 [ 0.237]
 [-0.007]
 [-0.04 ]
 [ 0.029]
 [-0.018]]
Printing some Q and Qe and total Qs values:  [[0.965]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]] [[2.125]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]] [[0.965]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]]
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.796]
 [0.837]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[1.225]
 [1.225]
 [0.982]
 [1.225]
 [1.225]
 [1.225]
 [1.225]] [[0.796]
 [0.796]
 [0.837]
 [0.796]
 [0.796]
 [0.796]
 [0.796]]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.441]
 [0.441]
 [0.441]
 [0.298]
 [0.506]
 [0.441]] [[2.001]
 [1.755]
 [1.755]
 [1.755]
 [1.864]
 [1.123]
 [1.755]] [[1.046]
 [0.673]
 [0.673]
 [0.673]
 [0.459]
 [0.381]
 [0.673]]
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.56 ]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[1.555]
 [2.358]
 [1.555]
 [1.555]
 [1.555]
 [1.555]
 [1.555]] [[0.548]
 [0.56 ]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[ 0.045]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[2.842]
 [3.015]
 [3.015]
 [3.015]
 [3.015]
 [3.015]
 [3.015]] [[-0.467]
 [-0.445]
 [-0.445]
 [-0.445]
 [-0.445]
 [-0.445]
 [-0.445]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]] [[1.306]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]] [[0.872]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]]
Printing some Q and Qe and total Qs values:  [[0.912]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]] [[0.301]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[0.912]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]]
line 256 mcts: sample exp_bonus 5.9230223541047895
siam score:  -0.7184337
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.581]
 [0.619]
 [0.582]
 [0.579]
 [0.582]
 [0.58 ]] [[-2.485]
 [ 0.347]
 [-0.731]
 [-1.717]
 [-0.919]
 [-0.763]
 [-1.131]] [[0.625]
 [0.581]
 [0.619]
 [0.582]
 [0.579]
 [0.582]
 [0.58 ]]
Printing some Q and Qe and total Qs values:  [[0.848]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]] [[-0.342]
 [ 0.891]
 [ 0.891]
 [ 0.891]
 [ 0.891]
 [ 0.891]
 [ 0.891]] [[0.848]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]]
Printing some Q and Qe and total Qs values:  [[0.982]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]] [[0.752]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]] [[0.982]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]]
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.623]
 [0.622]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[-0.037]
 [ 1.03 ]
 [ 1.041]
 [ 1.03 ]
 [ 1.03 ]
 [ 1.03 ]
 [ 1.03 ]] [[0.918]
 [0.623]
 [0.622]
 [0.623]
 [0.623]
 [0.623]
 [0.623]]
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[0.595]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]] [[0.774]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]]
line 256 mcts: sample exp_bonus 0.4784034963135203
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.926]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[0.083]
 [1.03 ]
 [1.037]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]] [[0.926]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[1.02 ]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[0.864]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.638]
 [0.636]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[-0.132]
 [ 0.676]
 [ 0.791]
 [ 0.676]
 [ 0.676]
 [ 0.676]
 [ 0.676]] [[0.852]
 [0.638]
 [0.636]
 [0.638]
 [0.638]
 [0.638]
 [0.638]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.851]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]] [[1.435]
 [1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]] [[0.851]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]] [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[-1.536]
 [-1.239]
 [-1.239]
 [-1.239]
 [-1.239]
 [-1.239]
 [-1.239]] [[0.782]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]] [[2.793]
 [2.165]
 [2.165]
 [2.165]
 [2.165]
 [2.165]
 [2.165]] [[0.801]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[4.309]
 [4.309]
 [4.309]
 [4.309]
 [4.309]
 [4.309]
 [4.309]] [[0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]]
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]] [[1.255]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]] [[0.85 ]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]]
siam score:  -0.7206219
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.602]
 [0.601]
 [0.597]
 [0.596]
 [0.603]
 [0.598]] [[3.079]
 [5.085]
 [4.111]
 [3.327]
 [3.183]
 [3.432]
 [3.691]] [[0.594]
 [0.602]
 [0.601]
 [0.597]
 [0.596]
 [0.603]
 [0.598]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 0.06925728808155304
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
rdn probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[1.013]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]] [[1.389]
 [1.648]
 [1.648]
 [1.648]
 [1.648]
 [1.648]
 [1.648]] [[0.746]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]]
using explorer policy with actor:  1
siam score:  -0.72126424
siam score:  -0.72273195
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.201]
 [0.154]
 [0.201]
 [0.201]
 [0.171]
 [0.201]] [[3.169]
 [2.69 ]
 [2.896]
 [2.69 ]
 [2.69 ]
 [2.987]
 [2.69 ]] [[-0.694]
 [-0.874]
 [-0.9  ]
 [-0.874]
 [-0.874]
 [-0.834]
 [-0.874]]
Printing some Q and Qe and total Qs values:  [[0.856]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]] [[3.131]
 [2.871]
 [2.871]
 [2.871]
 [2.871]
 [2.871]
 [2.871]] [[2.477]
 [2.272]
 [2.272]
 [2.272]
 [2.272]
 [2.272]
 [2.272]]
using explorer policy with actor:  1
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.451]
 [0.381]
 [0.379]
 [0.381]
 [0.381]
 [0.381]] [[1.725]
 [2.585]
 [1.725]
 [2.36 ]
 [1.725]
 [1.725]
 [1.725]] [[0.381]
 [0.451]
 [0.381]
 [0.379]
 [0.381]
 [0.381]
 [0.381]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.148]
 [1.276]
 [1.148]
 [1.136]
 [1.148]
 [1.148]
 [1.09 ]] [[1.117]
 [1.223]
 [1.117]
 [1.072]
 [1.117]
 [1.117]
 [1.302]] [[1.335]
 [1.513]
 [1.335]
 [1.295]
 [1.335]
 [1.335]
 [1.414]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.813]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[1.466]
 [2.269]
 [1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.466]] [[1.048]
 [1.619]
 [1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.57 ]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.526]] [[3.284]
 [3.22 ]
 [3.284]
 [3.284]
 [3.284]
 [3.284]
 [3.406]] [[1.67 ]
 [1.752]
 [1.67 ]
 [1.67 ]
 [1.67 ]
 [1.67 ]
 [1.725]]
siam score:  -0.71164
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.363]
 [0.422]] [[4.765]
 [3.69 ]
 [3.69 ]
 [3.69 ]
 [3.69 ]
 [3.766]
 [3.69 ]] [[ 0.272]
 [ 0.012]
 [ 0.012]
 [ 0.012]
 [ 0.012]
 [-0.081]
 [ 0.012]]
siam score:  -0.71348923
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.546]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[1.137]
 [1.525]
 [1.137]
 [1.137]
 [1.137]
 [1.137]
 [1.137]] [[0.496]
 [0.806]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.497]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[0.166]
 [0.768]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]] [[0.183]
 [0.424]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]]
siam score:  -0.71560425
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[0.567]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]] [[0.334]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
using explorer policy with actor:  1
siam score:  -0.71443224
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.985]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]] [[1.835]
 [1.034]
 [1.835]
 [1.835]
 [1.835]
 [1.835]
 [1.835]] [[0.87 ]
 [1.358]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]]
line 256 mcts: sample exp_bonus 3.061257482530207
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.452]
 [0.435]
 [0.452]
 [0.452]
 [0.452]
 [0.452]] [[3.517]
 [4.77 ]
 [4.647]
 [4.789]
 [4.789]
 [4.789]
 [4.789]] [[0.944]
 [1.396]
 [1.296]
 [1.407]
 [1.407]
 [1.407]
 [1.407]]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.025]
 [-0.01 ]
 [-0.016]
 [-0.016]
 [-0.017]
 [ 0.092]] [[2.96 ]
 [3.562]
 [2.848]
 [2.749]
 [2.749]
 [2.379]
 [2.495]] [[ 0.445]
 [ 0.943]
 [ 0.356]
 [ 0.264]
 [ 0.264]
 [-0.051]
 [ 0.186]]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.434]
 [0.34 ]
 [0.511]
 [0.511]
 [0.511]
 [0.336]] [[0.779]
 [1.244]
 [0.597]
 [0.541]
 [0.541]
 [0.541]
 [0.676]] [[ 0.174]
 [ 0.082]
 [-0.321]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [-0.303]]
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
UNIT TEST: sample policy line 217 mcts : [0.061 0.327 0.143 0.163 0.061 0.163 0.082]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
siam score:  -0.70960206
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.489]
 [0.677]
 [0.658]
 [0.677]
 [0.677]
 [0.611]] [[2.582]
 [2.237]
 [2.582]
 [2.234]
 [2.582]
 [2.582]
 [2.305]] [[2.253]
 [1.71 ]
 [2.253]
 [1.927]
 [2.253]
 [2.253]
 [1.928]]
siam score:  -0.7102757
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.324]
 [0.311]
 [0.324]
 [0.323]
 [0.333]
 [0.324]] [[0.844]
 [1.538]
 [0.656]
 [0.479]
 [0.545]
 [0.719]
 [0.801]] [[ 0.078]
 [ 0.795]
 [-0.107]
 [-0.272]
 [-0.206]
 [-0.022]
 [ 0.053]]
using explorer policy with actor:  1
from probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]] [[1.767]
 [1.767]
 [1.767]
 [1.767]
 [1.767]
 [1.767]
 [1.767]] [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
first move QE:  0.7551984348782007
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.098]
 [0.109]
 [0.098]
 [0.096]
 [0.098]
 [0.101]] [[1.378]
 [1.464]
 [1.486]
 [1.543]
 [1.692]
 [1.632]
 [1.905]] [[0.093]
 [0.098]
 [0.109]
 [0.098]
 [0.096]
 [0.098]
 [0.101]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.36597445871556156, 0.3190428213736827, 0.011527829253057797, 0.28039923215158236, 0.011527829253057797, 0.011527829253057797]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  124 total reward:  0.004999999999999227  reward:  1.0 rdn_beta:  0.833
siam score:  -0.6988333
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.246]
 [0.242]
 [0.246]
 [0.246]
 [0.265]
 [0.246]] [[4.252]
 [3.913]
 [3.891]
 [3.913]
 [3.913]
 [3.362]
 [3.913]] [[ 0.039]
 [-0.107]
 [-0.121]
 [-0.107]
 [-0.107]
 [-0.252]
 [-0.107]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.543]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[2.312]
 [3.007]
 [2.312]
 [2.312]
 [2.312]
 [2.312]
 [2.312]] [[0.527]
 [0.543]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
from probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.711]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]] [[5.085]
 [5.293]
 [5.11 ]
 [5.11 ]
 [5.11 ]
 [5.11 ]
 [5.11 ]] [[1.368]
 [1.47 ]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.605]
 [0.617]
 [0.616]
 [0.643]
 [0.651]
 [0.674]] [[-0.565]
 [ 2.838]
 [-0.314]
 [-0.361]
 [-0.128]
 [-0.056]
 [-0.191]] [[0.641]
 [0.605]
 [0.617]
 [0.616]
 [0.643]
 [0.651]
 [0.674]]
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.425]
 [0.424]
 [0.424]
 [0.424]
 [0.43 ]
 [0.427]] [[0.895]
 [1.146]
 [1.078]
 [1.064]
 [1.091]
 [1.148]
 [1.073]] [[-0.3  ]
 [-0.129]
 [-0.177]
 [-0.185]
 [-0.168]
 [-0.118]
 [-0.174]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.473]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[1.314]
 [1.506]
 [1.314]
 [1.314]
 [1.314]
 [1.314]
 [1.314]] [[0.561]
 [0.66 ]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]]
siam score:  -0.6898494
line 256 mcts: sample exp_bonus 5.17336340359577
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.218]
 [0.2  ]
 [0.195]
 [0.185]
 [0.082]
 [0.2  ]] [[3.342]
 [2.886]
 [3.091]
 [3.499]
 [3.421]
 [3.44 ]
 [2.893]] [[0.369]
 [0.145]
 [0.253]
 [0.498]
 [0.441]
 [0.358]
 [0.133]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.472]
 [0.532]
 [0.472]
 [0.472]
 [0.474]
 [0.472]] [[2.916]
 [3.497]
 [3.041]
 [3.497]
 [3.497]
 [2.546]
 [3.497]] [[0.742]
 [1.172]
 [0.894]
 [1.172]
 [1.172]
 [0.486]
 [1.172]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Printing some Q and Qe and total Qs values:  [[0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]] [[4.675]
 [4.675]
 [4.675]
 [4.675]
 [4.675]
 [4.675]
 [4.675]] [[1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]]
first move QE:  0.753555075133293
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.367]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[2.5  ]
 [2.641]
 [2.5  ]
 [2.5  ]
 [2.5  ]
 [2.5  ]
 [2.5  ]] [[-0.091]
 [ 0.064]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]]
from probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.139]
 [0.115]
 [0.071]
 [0.112]
 [0.11 ]
 [0.071]] [[2.409]
 [2.218]
 [2.377]
 [2.585]
 [2.075]
 [1.98 ]
 [2.585]] [[-1.094]
 [-1.038]
 [-1.032]
 [-1.052]
 [-1.139]
 [-1.175]
 [-1.052]]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.601]
 [0.574]
 [0.574]
 [0.574]
 [0.564]
 [0.574]] [[1.382]
 [0.653]
 [0.806]
 [0.806]
 [0.806]
 [0.665]
 [0.806]] [[2.559]
 [2.373]
 [2.37 ]
 [2.37 ]
 [2.37 ]
 [2.303]
 [2.37 ]]
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]] [[-0.59]
 [-0.59]
 [-0.59]
 [-0.59]
 [-0.59]
 [-0.59]
 [-0.59]]
using explorer policy with actor:  1
using another actor
from probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.987]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[-0.511]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]] [[1.284]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.848]
 [1.048]
 [0.85 ]
 [0.952]
 [0.952]
 [0.952]
 [0.952]] [[2.259]
 [2.21 ]
 [2.218]
 [1.929]
 [1.929]
 [1.929]
 [1.929]] [[1.655]
 [2.038]
 [1.645]
 [1.752]
 [1.752]
 [1.752]
 [1.752]]
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.874]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[2.142]
 [1.691]
 [2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]] [[0.684]
 [1.061]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]]
siam score:  -0.6783644
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[6.589]
 [4.819]
 [4.819]
 [4.819]
 [4.819]
 [4.819]
 [4.819]] [[1.506]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
siam score:  -0.6779126
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
siam score:  -0.6813019
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2501 2690
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]] [[4.075]
 [2.577]
 [2.577]
 [2.577]
 [2.577]
 [2.577]
 [2.577]] [[ 0.346]
 [-0.417]
 [-0.417]
 [-0.417]
 [-0.417]
 [-0.417]
 [-0.417]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.472]
 [0.477]
 [0.472]
 [0.472]
 [0.473]
 [0.472]] [[1.568]
 [1.528]
 [1.317]
 [1.111]
 [1.111]
 [1.72 ]
 [1.111]] [[0.617]
 [0.592]
 [0.48 ]
 [0.363]
 [0.363]
 [0.698]
 [0.363]]
siam score:  -0.6825513
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.565]
 [0.545]
 [0.512]
 [0.512]
 [0.701]
 [0.512]] [[3.719]
 [4.264]
 [5.455]
 [3.719]
 [3.719]
 [4.016]
 [3.719]] [[0.771]
 [1.148]
 [1.847]
 [0.771]
 [0.771]
 [1.12 ]
 [0.771]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[0.72 ]
 [0.726]
 [0.734]
 [0.744]
 [0.734]
 [0.734]
 [0.744]] [[1.249]
 [1.261]
 [1.278]
 [1.296]
 [1.278]
 [1.278]
 [1.296]]
from probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.1823112495469386
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.293]
 [0.281]
 [0.298]
 [0.286]
 [0.287]
 [0.289]] [[-0.574]
 [ 0.813]
 [-0.728]
 [ 0.   ]
 [-0.431]
 [-0.317]
 [-0.831]] [[0.28 ]
 [0.293]
 [0.281]
 [0.298]
 [0.286]
 [0.287]
 [0.289]]
from probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]] [[4.297]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]] [[1.738]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.336]
 [0.382]
 [0.424]
 [0.327]
 [0.482]
 [0.397]] [[ 1.159]
 [ 0.938]
 [ 0.061]
 [-0.201]
 [ 0.606]
 [ 0.832]
 [ 0.661]] [[1.362]
 [1.143]
 [0.65 ]
 [0.56 ]
 [0.904]
 [1.364]
 [1.08 ]]
from probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
start point for exploration sampling:  10935
2517 2708
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
using explorer policy with actor:  1
from probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.257]
 [0.265]
 [0.257]
 [0.257]
 [0.26 ]
 [0.257]] [[2.267]
 [2.328]
 [2.209]
 [2.328]
 [2.328]
 [2.198]
 [2.328]] [[-0.482]
 [-0.481]
 [-0.504]
 [-0.481]
 [-0.481]
 [-0.519]
 [-0.481]]
line 256 mcts: sample exp_bonus 0.331962864871136
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 3.612059005215898
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[3.649]
 [1.887]
 [1.887]
 [1.887]
 [1.887]
 [1.887]
 [1.887]] [[0.737]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[2.991]
 [2.809]
 [2.809]
 [2.809]
 [2.809]
 [2.809]
 [2.809]] [[1.378]
 [1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.097]]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]] [[3.623]
 [2.684]
 [2.684]
 [2.684]
 [2.684]
 [2.684]
 [2.684]] [[0.442]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[2.798]
 [2.805]
 [2.805]
 [2.805]
 [2.805]
 [2.805]
 [2.805]] [[0.333]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.1  ]
 [0.24 ]
 [0.15 ]
 [0.154]
 [0.175]
 [0.206]] [[1.594]
 [1.618]
 [1.729]
 [1.806]
 [1.745]
 [1.541]
 [1.5  ]] [[ 0.306]
 [-0.184]
 [ 0.205]
 [ 0.101]
 [ 0.05 ]
 [-0.113]
 [-0.092]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]] [[7.031]
 [2.593]
 [2.593]
 [2.593]
 [2.593]
 [2.593]
 [2.593]] [[1.627]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.618]
 [0.595]
 [0.588]
 [0.588]
 [0.587]
 [0.586]] [[1.792]
 [1.783]
 [1.707]
 [1.678]
 [1.844]
 [1.711]
 [1.857]] [[0.635]
 [0.648]
 [0.589]
 [0.569]
 [0.656]
 [0.585]
 [0.662]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.537]
 [0.536]
 [0.553]
 [0.547]
 [0.518]
 [0.514]] [[5.022]
 [5.355]
 [4.769]
 [4.768]
 [4.815]
 [4.693]
 [4.957]] [[0.393]
 [0.476]
 [0.279]
 [0.312]
 [0.317]
 [0.218]
 [0.297]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.392]
 [0.364]
 [0.38 ]
 [0.355]
 [0.379]
 [0.363]] [[2.668]
 [2.669]
 [2.564]
 [2.426]
 [2.345]
 [2.76 ]
 [2.462]] [[0.833]
 [0.89 ]
 [0.713]
 [0.575]
 [0.432]
 [0.976]
 [0.589]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.29685922607159465, 0.25879075105156313, 0.009350768581935233, 0.22744510461119985, 0.19820338110177196, 0.009350768581935233]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
first move QE:  0.7491196723214234
actor:  1 policy actor:  1  step number:  79 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 5.6128158170704285
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
from probs:  [0.2272418440326967, 0.19810092570065782, 0.0071578907073263755, 0.40861936020038553, 0.1517220886516073, 0.0071578907073263755]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.349]
 [0.315]
 [0.288]
 [0.349]
 [0.322]
 [0.297]] [[5.693]
 [5.693]
 [5.991]
 [6.489]
 [5.693]
 [6.492]
 [6.21 ]] [[0.977]
 [0.977]
 [1.108]
 [1.384]
 [0.977]
 [1.455]
 [1.217]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2272418440326967, 0.19810092570065782, 0.0071578907073263755, 0.40861936020038553, 0.1517220886516073, 0.0071578907073263755]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.257]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]] [[2.347]
 [2.159]
 [2.367]
 [2.367]
 [2.367]
 [2.367]
 [2.367]] [[-0.174]
 [-0.69 ]
 [-0.595]
 [-0.595]
 [-0.595]
 [-0.595]
 [-0.595]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.372]
 [0.535]
 [0.479]
 [0.519]
 [0.511]
 [0.552]] [[5.191]
 [5.341]
 [5.549]
 [5.202]
 [5.134]
 [5.059]
 [5.055]] [[1.429]
 [1.364]
 [1.613]
 [1.376]
 [1.371]
 [1.323]
 [1.355]]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.495]
 [0.488]] [[4.873]
 [4.873]
 [4.873]
 [4.873]
 [4.873]
 [5.25 ]
 [5.183]] [[1.388]
 [1.388]
 [1.388]
 [1.388]
 [1.388]
 [1.667]
 [1.612]]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.526]
 [0.49 ]
 [0.505]
 [0.547]
 [0.504]
 [0.547]] [[5.143]
 [4.894]
 [5.409]
 [5.272]
 [4.921]
 [5.655]
 [4.921]] [[1.356]
 [1.234]
 [1.503]
 [1.436]
 [1.268]
 [1.659]
 [1.268]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.    0.347 0.02  0.    0.082 0.    0.551]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2272418440326967, 0.19810092570065782, 0.0071578907073263755, 0.40861936020038553, 0.1517220886516073, 0.0071578907073263755]
from probs:  [0.2272418440326967, 0.19810092570065782, 0.0071578907073263755, 0.40861936020038553, 0.1517220886516073, 0.0071578907073263755]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2272418440326967, 0.19810092570065782, 0.0071578907073263755, 0.40861936020038553, 0.1517220886516073, 0.0071578907073263755]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2272418440326967, 0.19810092570065782, 0.0071578907073263755, 0.40861936020038553, 0.1517220886516073, 0.0071578907073263755]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.689521892014918
siam score:  -0.70098764
2537 2730
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.226]
 [0.336]
 [0.344]
 [0.334]
 [0.33 ]
 [0.342]] [[-0.026]
 [ 1.391]
 [-1.68 ]
 [-1.698]
 [-1.475]
 [-0.766]
 [-2.438]] [[0.4  ]
 [0.226]
 [0.336]
 [0.344]
 [0.334]
 [0.33 ]
 [0.342]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2272418440326967, 0.19810092570065782, 0.0071578907073263755, 0.40861936020038553, 0.1517220886516073, 0.0071578907073263755]
siam score:  -0.7004944
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2272418440326967, 0.19810092570065782, 0.0071578907073263755, 0.40861936020038553, 0.1517220886516073, 0.0071578907073263755]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.2272418440326967, 0.19810092570065782, 0.0071578907073263755, 0.40861936020038553, 0.1517220886516073, 0.0071578907073263755]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.101]
 [0.227]
 [0.227]
 [0.244]
 [0.194]
 [0.227]] [[4.015]
 [4.37 ]
 [4.015]
 [4.015]
 [4.225]
 [4.249]
 [4.015]] [[-0.071]
 [-0.086]
 [-0.071]
 [-0.071]
 [ 0.103]
 [ 0.02 ]
 [-0.071]]
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.054]
 [0.199]
 [0.054]
 [0.054]
 [0.054]
 [0.054]] [[2.516]
 [2.244]
 [1.79 ]
 [2.244]
 [2.244]
 [2.244]
 [2.244]] [[-0.466]
 [-0.801]
 [-0.814]
 [-0.801]
 [-0.801]
 [-0.801]
 [-0.801]]
siam score:  -0.7021921
UNIT TEST: sample policy line 217 mcts : [0.    0.061 0.673 0.    0.    0.163 0.102]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.356]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]] [[1.515]
 [1.679]
 [1.515]
 [1.515]
 [1.515]
 [1.515]
 [1.515]] [[1.287]
 [1.525]
 [1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]]
start point for exploration sampling:  10935
siam score:  -0.7052598
siam score:  -0.7048985
using explorer policy with actor:  1
2542 2738
actor:  1 policy actor:  1  step number:  117 total reward:  0.06999999999999929  reward:  1.0 rdn_beta:  1.0
in main func line 156:  2544
2544 2740
start point for exploration sampling:  10935
from probs:  [0.1952466325007777, 0.17020869903156982, 0.006150073558692815, 0.35108654567276004, 0.1303599124153756, 0.14694813682082417]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]] [[0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.752]
 [0.617]
 [0.67 ]
 [0.607]
 [0.518]
 [0.67 ]] [[3.131]
 [3.155]
 [2.431]
 [2.219]
 [2.351]
 [1.945]
 [2.219]] [[0.621]
 [0.752]
 [0.617]
 [0.67 ]
 [0.607]
 [0.518]
 [0.67 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.653]
 [0.651]
 [0.649]
 [0.649]
 [0.648]
 [0.649]] [[5.092]
 [4.334]
 [4.182]
 [4.413]
 [4.413]
 [4.285]
 [4.413]] [[0.636]
 [0.653]
 [0.651]
 [0.649]
 [0.649]
 [0.648]
 [0.649]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.7152417
first move QE:  0.7488585496164345
Printing some Q and Qe and total Qs values:  [[0.737]
 [1.   ]
 [0.779]
 [0.725]
 [0.718]
 [0.684]
 [0.779]] [[1.794]
 [1.926]
 [2.052]
 [1.728]
 [2.142]
 [1.943]
 [2.052]] [[1.263]
 [1.876]
 [1.518]
 [1.195]
 [1.455]
 [1.256]
 [1.518]]
using another actor
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]] [[4.684]
 [4.684]
 [4.684]
 [4.684]
 [4.684]
 [4.684]
 [4.684]] [[0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1952466325007777, 0.17020869903156982, 0.006150073558692815, 0.35108654567276004, 0.1303599124153756, 0.14694813682082417]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[3.185]
 [3.185]
 [3.185]
 [3.185]
 [3.185]
 [3.185]
 [3.185]] [[-0.505]
 [-0.505]
 [-0.505]
 [-0.505]
 [-0.505]
 [-0.505]
 [-0.505]]
siam score:  -0.7078801
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.638]
 [0.618]
 [0.568]
 [0.568]
 [0.609]
 [0.568]] [[4.309]
 [3.648]
 [3.774]
 [4.148]
 [4.148]
 [4.004]
 [4.148]] [[1.421]
 [1.037]
 [1.081]
 [1.231]
 [1.231]
 [1.217]
 [1.231]]
start point for exploration sampling:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6942955
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.1952466325007777, 0.17020869903156982, 0.006150073558692815, 0.35108654567276004, 0.1303599124153756, 0.14694813682082417]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 4.873663235294823
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[-0.276]
 [-0.276]
 [-0.276]
 [-0.276]
 [-0.276]
 [-0.276]
 [-0.276]] [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1952466325007777, 0.17020869903156982, 0.006150073558692815, 0.35108654567276004, 0.1303599124153756, 0.14694813682082417]
from probs:  [0.1952466325007777, 0.17020869903156982, 0.006150073558692815, 0.35108654567276004, 0.1303599124153756, 0.14694813682082417]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2558 2761
using explorer policy with actor:  1
from probs:  [0.1952466325007777, 0.17020869903156982, 0.006150073558692815, 0.35108654567276004, 0.1303599124153756, 0.14694813682082417]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1952466325007777, 0.17020869903156982, 0.006150073558692815, 0.35108654567276004, 0.1303599124153756, 0.14694813682082417]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1952466325007777, 0.17020869903156982, 0.006150073558692815, 0.35108654567276004, 0.1303599124153756, 0.14694813682082417]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[2.849]
 [2.849]
 [2.849]
 [2.849]
 [2.849]
 [2.849]
 [2.849]] [[5.179]
 [5.179]
 [5.179]
 [5.179]
 [5.179]
 [5.179]
 [5.179]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1952466325007777, 0.17020869903156982, 0.006150073558692815, 0.35108654567276004, 0.1303599124153756, 0.14694813682082417]
actor:  1 policy actor:  1  step number:  101 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  0.833
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
using another actor
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]] [[3.072]
 [2.968]
 [2.968]
 [2.968]
 [2.968]
 [2.968]
 [2.968]] [[-0.709]
 [-0.749]
 [-0.749]
 [-0.749]
 [-0.749]
 [-0.749]
 [-0.749]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.483]
 [0.481]
 [0.509]
 [0.503]
 [0.469]
 [0.483]] [[4.476]
 [3.836]
 [4.876]
 [4.211]
 [4.492]
 [4.227]
 [3.836]] [[1.601]
 [1.138]
 [1.896]
 [1.431]
 [1.632]
 [1.414]
 [1.138]]
Printing some Q and Qe and total Qs values:  [[1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]] [[0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.363]
 [0.463]
 [0.463]
 [0.463]
 [0.366]
 [0.463]] [[1.919]
 [1.441]
 [1.213]
 [1.213]
 [1.213]
 [1.234]
 [1.213]] [[2.211]
 [1.971]
 [2.062]
 [2.062]
 [2.062]
 [1.925]
 [2.062]]
siam score:  -0.68255395
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.068]
 [0.105]
 [0.106]
 [0.107]
 [0.109]
 [0.104]] [[-2.613]
 [ 0.241]
 [-2.901]
 [-2.56 ]
 [-2.561]
 [-2.691]
 [-2.541]] [[0.114]
 [0.068]
 [0.105]
 [0.106]
 [0.107]
 [0.109]
 [0.104]]
in main func line 156:  2572
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.348]
 [0.329]] [[5.007]
 [5.007]
 [5.007]
 [5.007]
 [5.007]
 [5.514]
 [5.007]] [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.223]
 [0.016]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
siam score:  -0.68779826
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.637]
 [0.635]
 [0.622]
 [0.622]
 [0.649]
 [0.622]] [[4.28 ]
 [4.526]
 [4.416]
 [4.28 ]
 [4.28 ]
 [4.59 ]
 [4.28 ]] [[1.745]
 [1.893]
 [1.83 ]
 [1.745]
 [1.745]
 [1.937]
 [1.745]]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.527]
 [0.537]
 [0.536]
 [0.539]
 [0.521]
 [0.536]] [[3.898]
 [3.505]
 [3.407]
 [3.656]
 [3.709]
 [3.731]
 [3.665]] [[0.939]
 [0.663]
 [0.61 ]
 [0.765]
 [0.801]
 [0.798]
 [0.771]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]] [[3.76 ]
 [4.183]
 [4.183]
 [4.183]
 [4.183]
 [4.183]
 [4.183]] [[0.946]
 [1.089]
 [1.089]
 [1.089]
 [1.089]
 [1.089]
 [1.089]]
siam score:  -0.6812254
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.268]
 [0.251]
 [0.244]
 [0.245]
 [0.245]
 [0.253]] [[2.633]
 [3.348]
 [2.434]
 [1.823]
 [2.111]
 [1.917]
 [2.393]] [[0.283]
 [0.268]
 [0.251]
 [0.244]
 [0.245]
 [0.245]
 [0.253]]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.485]
 [0.472]
 [0.471]
 [0.472]
 [0.472]
 [0.464]] [[1.117]
 [1.832]
 [1.117]
 [1.606]
 [1.117]
 [1.117]
 [1.981]] [[0.301]
 [0.566]
 [0.301]
 [0.461]
 [0.301]
 [0.301]
 [0.574]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]] [[1.778]
 [1.778]
 [1.778]
 [1.778]
 [1.778]
 [1.778]
 [1.778]] [[0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
Printing some Q and Qe and total Qs values:  [[1.447]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]] [[0.735]
 [2.127]
 [2.127]
 [2.127]
 [2.127]
 [2.127]
 [2.127]] [[1.622]
 [1.156]
 [1.156]
 [1.156]
 [1.156]
 [1.156]
 [1.156]]
Printing some Q and Qe and total Qs values:  [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]] [[2.509]
 [2.509]
 [2.509]
 [2.509]
 [2.509]
 [2.509]
 [2.509]] [[2.229]
 [2.229]
 [2.229]
 [2.229]
 [2.229]
 [2.229]
 [2.229]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6851895
siam score:  -0.6880098
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.586]
 [0.586]
 [0.665]
 [0.586]
 [0.586]
 [0.545]] [[2.249]
 [2.249]
 [2.249]
 [3.922]
 [2.249]
 [2.249]
 [2.512]] [[0.871]
 [0.871]
 [0.871]
 [1.715]
 [0.871]
 [0.871]
 [0.966]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.6893403
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.68948007
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[3.163]
 [3.163]
 [3.163]
 [3.163]
 [3.163]
 [3.163]
 [3.163]] [[1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6944646
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Starting evaluation
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
line 256 mcts: sample exp_bonus -0.37453987488506685
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.447]
 [0.447]
 [0.005]
 [0.447]
 [0.447]
 [0.013]] [[1.192]
 [1.022]
 [1.022]
 [1.724]
 [1.022]
 [1.022]
 [1.047]] [[0.006]
 [0.447]
 [0.447]
 [0.005]
 [0.447]
 [0.447]
 [0.013]]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.463]
 [0.396]
 [0.5  ]
 [0.396]
 [0.396]
 [0.396]] [[ 0.457]
 [ 0.367]
 [ 0.457]
 [-0.789]
 [ 0.457]
 [ 0.457]
 [ 0.457]] [[0.396]
 [0.463]
 [0.396]
 [0.5  ]
 [0.396]
 [0.396]
 [0.396]]
using explorer policy with actor:  0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.1311142284236433
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.493]
 [0.487]
 [0.487]
 [0.872]
 [0.487]
 [0.768]] [[1.919]
 [2.922]
 [1.919]
 [1.919]
 [1.355]
 [1.919]
 [1.532]] [[0.487]
 [0.493]
 [0.487]
 [0.487]
 [0.872]
 [0.487]
 [0.768]]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.514]
 [0.506]
 [0.506]
 [0.506]
 [0.444]
 [0.506]] [[3.066]
 [3.692]
 [3.066]
 [3.066]
 [3.066]
 [2.468]
 [3.066]] [[ 0.452]
 [ 0.837]
 [ 0.452]
 [ 0.452]
 [ 0.452]
 [-0.015]
 [ 0.452]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.403]
 [0.415]
 [0.394]
 [0.394]
 [0.391]
 [0.4  ]] [[4.4  ]
 [4.435]
 [4.43 ]
 [4.286]
 [4.617]
 [4.44 ]
 [4.522]] [[0.511]
 [0.546]
 [0.566]
 [0.428]
 [0.649]
 [0.526]
 [0.599]]
from probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
using explorer policy with actor:  1
rdn probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
siam score:  -0.6849014
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.032]
 [0.028]
 [0.034]
 [0.037]
 [0.032]
 [0.037]] [[1.983]
 [1.23 ]
 [1.683]
 [1.116]
 [1.707]
 [1.567]
 [1.655]] [[0.355]
 [0.096]
 [0.239]
 [0.062]
 [0.264]
 [0.208]
 [0.247]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17154308699487977, 0.1495448361453383, 0.005403435593182633, 0.30846355235773, 0.23593691072664277, 0.12910817818222667]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[2.677]
 [2.522]
 [2.522]
 [2.522]
 [2.522]
 [2.522]
 [2.522]] [[1.531]
 [1.34 ]
 [1.34 ]
 [1.34 ]
 [1.34 ]
 [1.34 ]
 [1.34 ]]
2601 2812
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]] [[0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]]
siam score:  -0.683952
actor:  1 policy actor:  1  step number:  49 total reward:  0.6499999999999998  reward:  1.0 rdn_beta:  0.833
2602 2812
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1325494912377278, 0.11555168031274622, 0.004175176344084129, 0.23834645654720965, 0.4096167002070273, 0.09976049535120496]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1325494912377278, 0.11555168031274622, 0.004175176344084129, 0.23834645654720965, 0.4096167002070273, 0.09976049535120496]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1325494912377278, 0.11555168031274622, 0.004175176344084129, 0.23834645654720965, 0.4096167002070273, 0.09976049535120496]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.575]
 [0.575]
 [0.575]
 [0.549]
 [0.575]
 [0.575]] [[6.777]
 [4.776]
 [4.776]
 [4.776]
 [6.172]
 [4.776]
 [4.776]] [[1.878]
 [0.867]
 [0.867]
 [0.867]
 [1.589]
 [0.867]
 [0.867]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1325494912377278, 0.11555168031274622, 0.004175176344084129, 0.23834645654720965, 0.4096167002070273, 0.09976049535120496]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.358]
 [0.492]
 [0.892]
 [0.527]
 [0.39 ]
 [0.384]] [[2.801]
 [2.841]
 [2.132]
 [3.287]
 [2.801]
 [2.123]
 [2.54 ]] [[1.281]
 [1.081]
 [0.95 ]
 [1.952]
 [1.281]
 [0.817]
 [0.987]]
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.288]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[2.171]
 [1.418]
 [2.171]
 [2.171]
 [2.171]
 [2.171]
 [2.171]] [[1.932]
 [0.784]
 [1.932]
 [1.932]
 [1.932]
 [1.932]
 [1.932]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1325494912377278, 0.11555168031274622, 0.004175176344084129, 0.23834645654720965, 0.4096167002070273, 0.09976049535120496]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.1325494912377278, 0.11555168031274622, 0.004175176344084129, 0.23834645654720965, 0.4096167002070273, 0.09976049535120496]
actor:  1 policy actor:  1  step number:  119 total reward:  0.1899999999999994  reward:  1.0 rdn_beta:  0.167
2611 2836
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22426431162977478, 0.10333449731633032, 0.0037337384238399197, 0.21314628405033823, 0.36630826738049693, 0.08921290119921992]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[-0.794]
 [-0.794]
 [-0.794]
 [-0.794]
 [-0.794]
 [-0.794]
 [-0.794]] [[1.118]
 [1.118]
 [1.118]
 [1.118]
 [1.118]
 [1.118]
 [1.118]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.448]
 [0.439]
 [0.424]
 [0.448]
 [0.428]
 [0.447]] [[3.319]
 [3.429]
 [3.221]
 [3.235]
 [3.452]
 [3.59 ]
 [3.449]] [[0.454]
 [0.448]
 [0.439]
 [0.424]
 [0.448]
 [0.428]
 [0.447]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22426431162977478, 0.10333449731633032, 0.0037337384238399197, 0.21314628405033823, 0.36630826738049693, 0.08921290119921992]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.455]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[3.658]
 [3.5  ]
 [3.498]
 [3.498]
 [3.498]
 [3.498]
 [3.498]] [[0.459]
 [0.455]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[3.453]
 [3.453]
 [3.453]
 [3.453]
 [3.453]
 [3.453]
 [3.453]] [[0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22426431162977478, 0.10333449731633032, 0.0037337384238399197, 0.21314628405033823, 0.36630826738049693, 0.08921290119921992]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22426431162977478, 0.10333449731633032, 0.0037337384238399197, 0.21314628405033823, 0.36630826738049693, 0.08921290119921992]
2612 2842
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6849594
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.49 ]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.529]] [[3.707]
 [3.834]
 [4.093]
 [4.093]
 [4.093]
 [4.093]
 [3.922]] [[1.653]
 [1.594]
 [1.866]
 [1.866]
 [1.866]
 [1.866]
 [1.716]]
line 256 mcts: sample exp_bonus 1.3064456208442055
siam score:  -0.6883642
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.78 ]
 [0.718]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[2.382]
 [1.925]
 [0.818]
 [2.382]
 [2.382]
 [2.382]
 [2.382]] [[0.439]
 [0.78 ]
 [0.718]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22426431162977478, 0.10333449731633032, 0.0037337384238399197, 0.21314628405033823, 0.36630826738049693, 0.08921290119921992]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.552]
 [0.586]
 [0.567]
 [0.598]
 [0.598]
 [0.567]] [[4.342]
 [4.741]
 [4.702]
 [4.613]
 [4.342]
 [4.342]
 [4.681]] [[1.166]
 [1.39 ]
 [1.399]
 [1.319]
 [1.166]
 [1.166]
 [1.365]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.262]
 [0.279]
 [0.298]
 [0.294]
 [0.294]
 [0.275]] [[3.569]
 [3.324]
 [3.421]
 [3.538]
 [3.665]
 [3.542]
 [3.687]] [[0.792]
 [0.576]
 [0.67 ]
 [0.779]
 [0.876]
 [0.779]
 [0.876]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22426431162977478, 0.10333449731633032, 0.0037337384238399197, 0.21314628405033823, 0.36630826738049693, 0.08921290119921992]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22426431162977478, 0.10333449731633032, 0.0037337384238399197, 0.21314628405033823, 0.36630826738049693, 0.08921290119921992]
actor:  1 policy actor:  1  step number:  76 total reward:  0.24499999999999944  reward:  1.0 rdn_beta:  0.833
using explorer policy with actor:  1
2625 2861
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.265]
 [0.243]
 [0.242]
 [0.253]
 [0.245]
 [0.255]] [[0.704]
 [1.948]
 [0.816]
 [0.559]
 [0.697]
 [0.288]
 [0.23 ]] [[0.231]
 [0.265]
 [0.243]
 [0.242]
 [0.253]
 [0.245]
 [0.255]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.20100246432341715, 0.09261611202986568, 0.003346455880014461, 0.19103765572038053, 0.43203803158761206, 0.07995928045871017]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.6  ]
 [0.579]
 [0.583]
 [0.583]
 [0.59 ]
 [0.578]] [[3.456]
 [3.424]
 [3.452]
 [3.504]
 [3.503]
 [3.41 ]
 [3.5  ]] [[1.457]
 [1.478]
 [1.464]
 [1.524]
 [1.524]
 [1.445]
 [1.509]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.148162631957912
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.245]
 [0.253]
 [0.245]
 [0.245]
 [0.245]
 [0.245]] [[4.194]
 [4.114]
 [3.987]
 [4.114]
 [4.114]
 [4.114]
 [4.114]] [[1.563]
 [1.47 ]
 [1.361]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]]
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.256]
 [0.23 ]
 [0.232]
 [0.257]
 [0.242]
 [0.242]] [[3.776]
 [3.538]
 [3.748]
 [3.653]
 [3.727]
 [3.82 ]
 [3.756]] [[1.151]
 [0.955]
 [1.119]
 [1.034]
 [1.129]
 [1.198]
 [1.14 ]]
Printing some Q and Qe and total Qs values:  [[1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]] [[0.707]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]] [[1.518]
 [1.554]
 [1.554]
 [1.554]
 [1.554]
 [1.554]
 [1.554]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.49]
 [0.51]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]] [[0.953]
 [1.957]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]] [[0.547]
 [1.516]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]] [[1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.1320913211091854
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.20100246432341715, 0.09261611202986568, 0.003346455880014461, 0.19103765572038053, 0.43203803158761206, 0.07995928045871017]
line 256 mcts: sample exp_bonus 5.5107581293157235
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.146]
 [0.19 ]
 [0.147]
 [0.148]
 [0.16 ]
 [0.149]] [[5.63 ]
 [6.209]
 [5.449]
 [5.807]
 [5.747]
 [5.592]
 [5.761]] [[1.261]
 [1.57 ]
 [1.122]
 [1.316]
 [1.279]
 [1.19 ]
 [1.289]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 4.927387217386451
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.20100246432341715, 0.09261611202986568, 0.003346455880014461, 0.19103765572038053, 0.43203803158761206, 0.07995928045871017]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
rdn beta is 0 so we're just using the maxi policy
2638 2882
from probs:  [0.20100246432341715, 0.09261611202986568, 0.003346455880014461, 0.19103765572038053, 0.43203803158761206, 0.07995928045871017]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.20100246432341715, 0.09261611202986568, 0.003346455880014461, 0.19103765572038053, 0.43203803158761206, 0.07995928045871017]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.20100246432341715, 0.09261611202986568, 0.003346455880014461, 0.19103765572038053, 0.43203803158761206, 0.07995928045871017]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.52 ]
 [0.427]
 [0.608]
 [0.427]
 [0.427]
 [0.457]] [[2.521]
 [2.229]
 [2.521]
 [2.089]
 [2.521]
 [2.521]
 [2.148]] [[1.024]
 [1.015]
 [1.024]
 [1.098]
 [1.024]
 [1.024]
 [0.836]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.20100246432341715, 0.09261611202986568, 0.003346455880014461, 0.19103765572038053, 0.43203803158761206, 0.07995928045871017]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.597]
 [0.512]
 [0.537]
 [0.562]
 [0.548]
 [0.553]] [[2.899]
 [3.539]
 [2.532]
 [2.499]
 [2.608]
 [2.586]
 [3.225]] [[0.556]
 [0.597]
 [0.512]
 [0.537]
 [0.562]
 [0.548]
 [0.553]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.20100246432341715, 0.09261611202986568, 0.003346455880014461, 0.19103765572038053, 0.43203803158761206, 0.07995928045871017]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.20100246432341715, 0.09261611202986568, 0.003346455880014461, 0.19103765572038053, 0.43203803158761206, 0.07995928045871017]
first move QE:  0.7318393445075414
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.013]
 [ 0.013]
 [ 0.013]
 [ 0.013]
 [ 0.013]
 [ 0.013]] [[2.746]
 [2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]] [[-0.662]
 [-0.797]
 [-0.797]
 [-0.797]
 [-0.797]
 [-0.797]
 [-0.797]]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[2.42 ]
 [2.246]
 [2.246]
 [2.246]
 [2.246]
 [2.246]
 [2.246]] [[-0.892]
 [-1.007]
 [-1.007]
 [-1.007]
 [-1.007]
 [-1.007]
 [-1.007]]
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.006]
 [-0.024]
 [-0.025]
 [-0.027]
 [-0.022]
 [-0.027]] [[2.744]
 [3.012]
 [2.191]
 [2.142]
 [2.256]
 [2.202]
 [2.244]] [[-0.503]
 [-0.274]
 [-0.856]
 [-0.891]
 [-0.818]
 [-0.845]
 [-0.827]]
siam score:  -0.66327685
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[5.356]
 [5.356]
 [5.356]
 [5.356]
 [5.356]
 [5.356]
 [5.356]] [[1.769]
 [1.769]
 [1.769]
 [1.769]
 [1.769]
 [1.769]
 [1.769]]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.543]
 [0.532]
 [0.534]
 [0.557]
 [0.55 ]
 [0.546]] [[3.042]
 [3.484]
 [3.341]
 [3.336]
 [3.238]
 [3.369]
 [3.416]] [[1.172]
 [1.511]
 [1.388]
 [1.385]
 [1.329]
 [1.427]
 [1.46 ]]
start point for exploration sampling:  10935
actor:  1 policy actor:  1  step number:  105 total reward:  0.04999999999999927  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
siam score:  -0.6629388
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.181]
 [0.216]
 [0.231]
 [0.189]
 [0.178]
 [0.272]] [[1.457]
 [1.072]
 [1.385]
 [1.742]
 [1.822]
 [2.083]
 [1.604]] [[ 0.083]
 [-0.423]
 [ 0.064]
 [ 0.571]
 [ 0.593]
 [ 0.918]
 [ 0.467]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18719229689143604, 0.1549592452851211, 0.0031165327486623183, 0.17791213499508932, 0.4023542285890169, 0.07446556149067424]
line 256 mcts: sample exp_bonus 1.0618067191812481
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18719229689143604, 0.1549592452851211, 0.0031165327486623183, 0.17791213499508932, 0.4023542285890169, 0.07446556149067424]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.66605943
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18719229689143604, 0.1549592452851211, 0.0031165327486623183, 0.17791213499508932, 0.4023542285890169, 0.07446556149067424]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18719229689143604, 0.1549592452851211, 0.0031165327486623183, 0.17791213499508932, 0.4023542285890169, 0.07446556149067424]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  110 total reward:  0.26499999999999946  reward:  1.0 rdn_beta:  0.333
siam score:  -0.67424583
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.525]
 [0.523]
 [0.592]
 [0.457]
 [0.568]
 [0.539]] [[1.951]
 [1.259]
 [3.113]
 [1.359]
 [1.028]
 [1.048]
 [1.248]] [[1.189]
 [0.904]
 [1.519]
 [1.07 ]
 [0.69 ]
 [0.919]
 [0.928]]
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[2.23]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]] [[0.57 ]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17021701318676702, 0.23159063873189384, 0.0028339141342109075, 0.16177841039104454, 0.3658672721624232, 0.06771275139366056]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[ 0.454]
 [-1.244]
 [-1.244]
 [-1.244]
 [-1.244]
 [-1.244]
 [-1.244]] [[0.504]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.17021701318676702, 0.23159063873189384, 0.0028339141342109075, 0.16177841039104454, 0.3658672721624232, 0.06771275139366056]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  72 total reward:  0.27499999999999947  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.55 ]
 [0.474]
 [0.632]
 [0.469]
 [0.472]
 [0.479]] [[0.89 ]
 [1.554]
 [0.669]
 [0.56 ]
 [0.781]
 [0.969]
 [0.878]] [[0.47 ]
 [0.55 ]
 [0.474]
 [0.632]
 [0.469]
 [0.472]
 [0.479]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.66988665
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.571]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]] [[4.313]
 [4.304]
 [4.313]
 [4.313]
 [4.313]
 [4.313]
 [4.313]] [[1.178]
 [1.181]
 [1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]]
siam score:  -0.6707425
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.554]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]] [[0.788]
 [1.684]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]] [[0.686]
 [1.491]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.517]
 [0.517]
 [0.517]
 [0.463]
 [0.517]
 [0.517]] [[1.95 ]
 [1.398]
 [1.398]
 [1.398]
 [2.077]
 [1.398]
 [1.398]] [[1.616]
 [1.279]
 [1.279]
 [1.279]
 [1.711]
 [1.279]
 [1.279]]
using explorer policy with actor:  1
in main func line 156:  2655
2655 2893
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1950563047381102
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]] [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.365]
 [0.329]
 [0.367]
 [0.39 ]
 [0.295]
 [0.316]] [[1.748]
 [1.744]
 [1.627]
 [1.646]
 [2.228]
 [2.309]
 [2.25 ]] [[0.395]
 [0.365]
 [0.329]
 [0.367]
 [0.39 ]
 [0.295]
 [0.316]]
siam score:  -0.6722542
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.176]
 [0.168]
 [0.176]
 [0.176]
 [0.161]
 [0.153]] [[5.207]
 [4.802]
 [5.096]
 [4.802]
 [4.802]
 [5.145]
 [5.224]] [[0.861]
 [0.589]
 [0.785]
 [0.589]
 [0.589]
 [0.81 ]
 [0.853]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  124 total reward:  0.12499999999999933  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]] [[1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]]
from probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
from probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
siam score:  -0.6759253
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.724452882451963
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.577]
 [0.597]
 [0.589]
 [0.58 ]
 [0.567]
 [0.587]] [[3.855]
 [3.855]
 [3.913]
 [3.94 ]
 [4.007]
 [4.031]
 [3.969]] [[1.491]
 [1.501]
 [1.594]
 [1.605]
 [1.652]
 [1.651]
 [1.629]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]] [[4.828]
 [4.091]
 [4.091]
 [4.091]
 [4.091]
 [4.091]
 [4.091]] [[1.181]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.191]
 [0.291]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[4.502]
 [5.483]
 [3.383]
 [5.483]
 [5.483]
 [5.483]
 [5.483]] [[0.766]
 [1.391]
 [0.106]
 [1.391]
 [1.391]
 [1.391]
 [1.391]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.621]
 [0.621]
 [0.621]
 [0.682]
 [0.621]
 [0.621]] [[2.57 ]
 [1.993]
 [1.993]
 [1.993]
 [3.011]
 [1.993]
 [1.993]] [[0.651]
 [0.231]
 [0.231]
 [0.231]
 [0.933]
 [0.231]
 [0.231]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.878 0.02  0.02  0.02  0.02  0.02  0.02 ]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.669]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.682]] [[1.364]
 [1.137]
 [1.364]
 [1.364]
 [1.364]
 [1.364]
 [1.695]] [[1.814]
 [1.699]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [2.065]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.735]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]] [[2.21 ]
 [0.717]
 [2.21 ]
 [2.21 ]
 [2.21 ]
 [2.21 ]
 [2.21 ]] [[-0.833]
 [-0.194]
 [-0.833]
 [-0.833]
 [-0.833]
 [-0.833]
 [-0.833]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
using another actor
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.409]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[0.952]
 [2.   ]
 [0.952]
 [0.952]
 [0.952]
 [0.952]
 [0.952]] [[0.601]
 [1.062]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.252]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]] [[2.45 ]
 [2.421]
 [2.45 ]
 [2.45 ]
 [2.45 ]
 [2.45 ]
 [2.45 ]] [[0.724]
 [0.635]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[1.038]
 [1.038]
 [1.038]
 [1.038]
 [1.038]
 [1.038]
 [1.038]] [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[1.942]
 [1.778]
 [1.778]
 [1.778]
 [1.778]
 [1.778]
 [1.778]] [[0.538]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[-0.724]
 [-0.724]
 [-0.724]
 [-0.724]
 [-0.724]
 [-0.724]
 [-0.724]] [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]]
siam score:  -0.67064774
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.717]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[5.796]
 [2.778]
 [3.133]
 [3.133]
 [3.133]
 [3.133]
 [3.133]] [[2.125]
 [1.359]
 [1.437]
 [1.437]
 [1.437]
 [1.437]
 [1.437]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[-1.645]
 [-1.645]
 [-1.645]
 [-1.645]
 [-1.645]
 [-1.645]
 [-1.645]] [[0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6671963
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[-1.079]
 [-1.079]
 [-1.079]
 [-1.079]
 [-1.079]
 [-1.079]
 [-1.079]] [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]] [[0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[1.961]
 [1.961]
 [1.961]
 [1.961]
 [1.961]
 [1.961]
 [1.961]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.7123528296750141
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.538]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[1.998]
 [1.783]
 [1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]] [[0.731]
 [0.642]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6675513
line 256 mcts: sample exp_bonus 0.5209773968776095
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6685308
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.566]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.553]] [[3.258]
 [3.279]
 [3.258]
 [3.258]
 [3.258]
 [3.258]
 [3.403]] [[1.071]
 [1.068]
 [1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.124]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.3757],
        [-0.0000],
        [-0.3799],
        [-0.3264],
        [-0.4210],
        [-0.0000],
        [-0.5370],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.039254489999999254 -0.039254489999999254
-0.0339629152995 -0.4096525992148971
-0.9219315600000001 -0.9219315600000001
-0.024259925299500003 -0.4041646041571423
-0.024259925299500003 -0.350696675877068
-0.0337698257985 -0.4547670570589247
-0.9703485 -0.9703485
-0.0727797758985 -0.6097337549472459
-0.9372103732035 -0.9372103732035
0.99 0.99
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.419]
 [0.414]] [[3.629]
 [3.289]
 [2.928]
 [2.928]
 [2.928]
 [4.046]
 [2.928]] [[1.343]
 [1.037]
 [0.732]
 [0.732]
 [0.732]
 [1.683]
 [0.732]]
siam score:  -0.6645143
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]] [[2.924]
 [2.924]
 [2.924]
 [2.924]
 [2.924]
 [2.924]
 [2.924]] [[0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.7203311913472163
Printing some Q and Qe and total Qs values:  [[0.566]
 [1.062]
 [0.566]
 [0.566]
 [0.566]
 [1.248]
 [0.566]] [[2.567]
 [0.891]
 [2.567]
 [2.567]
 [2.567]
 [2.114]
 [2.567]] [[1.664]
 [1.608]
 [1.664]
 [1.664]
 [1.664]
 [2.143]
 [1.664]]
Printing some Q and Qe and total Qs values:  [[0.88 ]
 [1.211]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.901]] [[2.519]
 [1.742]
 [2.519]
 [2.519]
 [2.519]
 [2.519]
 [2.541]] [[2.062]
 [2.189]
 [2.062]
 [2.062]
 [2.062]
 [2.062]
 [2.113]]
using explorer policy with actor:  1
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.    0.082 0.184 0.633 0.102]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.912]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[4.084]
 [4.624]
 [4.084]
 [4.084]
 [4.084]
 [4.084]
 [4.084]] [[2.176]
 [2.529]
 [2.176]
 [2.176]
 [2.176]
 [2.176]
 [2.176]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]] [[-0.22 ]
 [ 0.594]
 [ 0.594]
 [ 0.594]
 [ 0.594]
 [ 0.594]
 [ 0.594]] [[-0.473]
 [-0.694]
 [-0.694]
 [-0.694]
 [-0.694]
 [-0.694]
 [-0.694]]
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6721165
from probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[2.542]
 [2.679]
 [2.679]
 [2.679]
 [2.679]
 [2.679]
 [2.679]] [[0.79 ]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
from probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]] [[2.481]
 [2.385]
 [2.385]
 [2.385]
 [2.385]
 [2.385]
 [2.385]] [[0.64 ]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]]
line 256 mcts: sample exp_bonus 0.6926887664758778
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.114]
 [0.3  ]
 [0.304]
 [0.31 ]
 [0.313]
 [0.302]] [[-0.023]
 [ 1.281]
 [-0.618]
 [-0.593]
 [-0.429]
 [-0.362]
 [-0.53 ]] [[0.454]
 [0.114]
 [0.3  ]
 [0.304]
 [0.31 ]
 [0.313]
 [0.302]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.781]
 [0.758]
 [0.758]
 [0.758]
 [0.761]
 [0.754]] [[1.817]
 [1.651]
 [1.158]
 [1.158]
 [1.158]
 [1.386]
 [1.51 ]] [[0.759]
 [0.781]
 [0.758]
 [0.758]
 [0.758]
 [0.761]
 [0.754]]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.542]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.569]] [[3.8  ]
 [3.41 ]
 [3.8  ]
 [3.8  ]
 [3.8  ]
 [3.8  ]
 [3.707]] [[1.844]
 [1.595]
 [1.844]
 [1.844]
 [1.844]
 [1.844]
 [1.833]]
first move QE:  0.7202430835221109
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
siam score:  -0.66804177
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.222]
 [0.175]
 [0.168]
 [0.188]
 [0.359]
 [0.198]] [[1.965]
 [1.949]
 [1.781]
 [1.661]
 [1.636]
 [1.372]
 [1.719]] [[-0.559]
 [-0.427]
 [-0.633]
 [-0.728]
 [-0.703]
 [-0.537]
 [-0.628]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]] [[4.264]
 [4.264]
 [4.264]
 [4.264]
 [4.264]
 [4.264]
 [4.264]] [[0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]]
first move QE:  0.7196501747475881
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886143, 0.19898771874672952, 0.06405894713815885, 0.13900353227771808, 0.31436112552015355, 0.05818027016837858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.22540840614886146, 0.19898771874672955, 0.06405894713815886, 0.1390035322777181, 0.31436112552015355, 0.05818027016837859]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.229]
 [0.228]
 [0.208]
 [0.208]
 [0.219]
 [0.217]] [[1.76 ]
 [3.112]
 [1.788]
 [1.76 ]
 [1.76 ]
 [1.614]
 [1.416]] [[-0.371]
 [ 0.264]
 [-0.332]
 [-0.371]
 [-0.371]
 [-0.423]
 [-0.514]]
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.17 ]
 [0.085]
 [0.085]
 [0.085]
 [0.09 ]
 [0.085]] [[2.505]
 [3.166]
 [2.759]
 [2.759]
 [2.759]
 [2.823]
 [2.759]] [[-0.309]
 [ 0.296]
 [-0.145]
 [-0.145]
 [-0.145]
 [-0.093]
 [-0.145]]
siam score:  -0.66530275
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]] [[2.814]
 [2.814]
 [2.814]
 [2.814]
 [2.814]
 [2.814]
 [2.814]] [[-0.418]
 [-0.418]
 [-0.418]
 [-0.418]
 [-0.418]
 [-0.418]
 [-0.418]]
siam score:  -0.6670581
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.752]
 [0.781]
 [0.715]
 [0.715]
 [0.774]
 [0.715]] [[3.306]
 [2.965]
 [2.943]
 [3.5  ]
 [3.5  ]
 [2.999]
 [3.5  ]] [[1.895]
 [1.569]
 [1.587]
 [1.973]
 [1.973]
 [1.626]
 [1.973]]
siam score:  -0.6646304
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]] [[3.743]
 [3.711]
 [3.711]
 [3.711]
 [3.711]
 [3.711]
 [3.711]] [[-1.102]
 [-1.151]
 [-1.151]
 [-1.151]
 [-1.151]
 [-1.151]
 [-1.151]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.486]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[0.552]
 [0.747]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[1.958]
 [2.186]
 [1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.958]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.492]
 [0.491]
 [0.491]
 [0.49 ]
 [0.488]
 [0.493]] [[4.248]
 [3.508]
 [3.997]
 [3.662]
 [3.831]
 [4.191]
 [3.921]] [[1.524]
 [0.927]
 [1.331]
 [1.054]
 [1.192]
 [1.489]
 [1.271]]
siam score:  -0.66183525
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
first move QE:  0.7194676308154644
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.65888876
siam score:  -0.65765595
first move QE:  0.7188073786298164
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.176]
 [0.179]
 [0.286]
 [0.198]
 [0.164]
 [0.158]] [[1.99 ]
 [0.798]
 [1.178]
 [1.808]
 [1.792]
 [1.922]
 [1.464]] [[-0.145]
 [-1.018]
 [-0.759]
 [-0.125]
 [-0.313]
 [-0.294]
 [-0.61 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.983]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.704]] [[3.285]
 [1.113]
 [3.285]
 [3.285]
 [3.285]
 [3.285]
 [1.086]] [[0.712]
 [0.973]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.405]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
UNIT TEST: sample policy line 217 mcts : [0.02  0.633 0.02  0.061 0.02  0.163 0.082]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.629]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[1.822]
 [2.692]
 [1.822]
 [1.822]
 [1.822]
 [1.822]
 [1.822]] [[1.188]
 [1.872]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]]
Printing some Q and Qe and total Qs values:  [[1.033]
 [1.035]
 [1.088]
 [1.088]
 [1.088]
 [1.088]
 [1.088]] [[0.899]
 [0.729]
 [2.013]
 [2.013]
 [2.013]
 [2.013]
 [2.013]] [[1.705]
 [1.628]
 [2.274]
 [2.274]
 [2.274]
 [2.274]
 [2.274]]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.535]
 [0.569]
 [0.381]
 [0.537]
 [0.552]
 [0.503]] [[1.912]
 [2.189]
 [1.5  ]
 [2.132]
 [2.408]
 [0.862]
 [1.846]] [[1.638]
 [1.914]
 [1.424]
 [1.626]
 [2.091]
 [0.893]
 [1.593]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
using another actor
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.301]
 [0.286]
 [0.279]
 [0.266]
 [0.274]
 [0.267]] [[-0.419]
 [ 0.357]
 [-0.32 ]
 [-0.585]
 [-0.425]
 [-0.036]
 [-0.173]] [[0.268]
 [0.301]
 [0.286]
 [0.279]
 [0.266]
 [0.274]
 [0.267]]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.299]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[-0.764]
 [ 0.169]
 [-0.764]
 [-0.764]
 [-0.764]
 [-0.764]
 [-0.764]] [[0.266]
 [0.299]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]]
siam score:  -0.66573787
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.186]
 [0.239]
 [0.189]
 [0.239]
 [0.196]
 [0.204]] [[-0.734]
 [-0.032]
 [ 0.   ]
 [-0.619]
 [ 0.   ]
 [-0.245]
 [-0.4  ]] [[0.194]
 [0.186]
 [0.239]
 [0.189]
 [0.239]
 [0.196]
 [0.204]]
siam score:  -0.6693947
using explorer policy with actor:  0
using another actor
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.514]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[2.082]
 [3.198]
 [2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.082]] [[0.501]
 [0.514]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.255]
 [0.321]
 [0.262]
 [0.437]
 [0.326]
 [0.324]] [[-1.364]
 [ 0.535]
 [-1.028]
 [-0.291]
 [ 0.93 ]
 [-1.102]
 [-1.328]] [[0.323]
 [0.255]
 [0.321]
 [0.262]
 [0.437]
 [0.326]
 [0.324]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2408740143603687
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.919]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]] [[0.83 ]
 [0.903]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]] [[0.673]
 [0.703]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]]
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.116]
 [0.102]
 [0.105]
 [0.117]
 [0.112]
 [0.109]] [[2.081]
 [1.889]
 [1.974]
 [1.843]
 [2.108]
 [1.986]
 [2.165]] [[0.121]
 [0.116]
 [0.102]
 [0.105]
 [0.117]
 [0.112]
 [0.109]]
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.259]
 [0.259]
 [0.259]
 [0.433]
 [0.259]
 [0.259]] [[0.779]
 [0.913]
 [0.913]
 [0.913]
 [0.907]
 [0.913]
 [0.913]] [[0.261]
 [0.259]
 [0.259]
 [0.259]
 [0.433]
 [0.259]
 [0.259]]
Printing some Q and Qe and total Qs values:  [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]] [[0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]] [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.94 ]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[1.023]
 [0.562]
 [1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.023]] [[0.755]
 [0.94 ]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.948]
 [0.735]
 [0.738]
 [0.738]
 [0.736]
 [0.738]] [[0.967]
 [0.631]
 [0.733]
 [0.685]
 [0.685]
 [0.611]
 [0.685]] [[0.736]
 [0.948]
 [0.735]
 [0.738]
 [0.738]
 [0.736]
 [0.738]]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.493]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[0.685]
 [1.065]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]] [[0.487]
 [0.493]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.512]
 [0.503]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[1.8  ]
 [1.605]
 [1.573]
 [1.8  ]
 [1.8  ]
 [1.8  ]
 [1.8  ]] [[0.498]
 [0.512]
 [0.503]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
line 256 mcts: sample exp_bonus -1.1181426123281255
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]]
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.872]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]] [[-0.098]
 [-0.407]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]] [[0.033]
 [0.937]
 [0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.524]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[3.147]
 [3.41 ]
 [3.147]
 [3.147]
 [3.147]
 [3.147]
 [3.147]] [[0.509]
 [0.524]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]]
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.726]
 [0.77 ]
 [0.779]
 [0.774]
 [0.779]
 [0.784]] [[ 0.147]
 [ 0.908]
 [ 0.279]
 [-0.197]
 [-0.124]
 [ 0.239]
 [ 0.134]] [[0.79 ]
 [0.726]
 [0.77 ]
 [0.779]
 [0.774]
 [0.779]
 [0.784]]
using explorer policy with actor:  0
rdn probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
siam score:  -0.6696916
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.006]
 [-0.013]
 [-0.013]
 [-0.008]
 [-0.01 ]
 [ 0.072]] [[1.542]
 [1.804]
 [1.54 ]
 [1.531]
 [1.625]
 [1.522]
 [1.46 ]] [[-1.408]
 [-1.309]
 [-1.412]
 [-1.414]
 [-1.373]
 [-1.412]
 [-1.269]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
siam score:  -0.66946197
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6687162
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.564]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.576]] [[1.962]
 [1.479]
 [1.039]
 [1.039]
 [1.039]
 [1.039]
 [0.721]] [[1.804]
 [1.835]
 [1.55 ]
 [1.55 ]
 [1.55 ]
 [1.55 ]
 [1.502]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.107]
 [ 0.271]
 [ 0.425]
 [ 0.313]
 [ 0.407]
 [ 0.179]] [[ 1.622]
 [ 1.27 ]
 [ 0.433]
 [-0.949]
 [ 0.038]
 [ 0.669]
 [ 0.959]] [[0.927]
 [0.913]
 [0.726]
 [0.225]
 [0.579]
 [1.076]
 [0.862]]
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.689]
 [0.635]
 [0.645]
 [0.626]
 [0.659]
 [0.749]] [[1.709]
 [1.821]
 [1.756]
 [2.137]
 [1.796]
 [1.479]
 [1.361]] [[0.656]
 [0.689]
 [0.635]
 [0.645]
 [0.626]
 [0.659]
 [0.749]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 3.6044023600726502
line 256 mcts: sample exp_bonus 1.3565541328281923
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.364]
 [0.352]
 [0.344]
 [0.343]
 [0.352]
 [0.404]] [[2.767]
 [2.989]
 [2.437]
 [2.479]
 [2.469]
 [2.437]
 [2.099]] [[ 0.66 ]
 [ 0.392]
 [ 0.002]
 [ 0.014]
 [ 0.004]
 [ 0.002]
 [-0.118]]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[4.059]
 [3.586]
 [3.586]
 [3.586]
 [3.586]
 [3.586]
 [3.586]] [[1.094]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]]
using another actor
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.271]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]] [[0.766]
 [0.851]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]] [[0.24 ]
 [0.271]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.396]
 [0.382]
 [0.381]
 [0.386]
 [0.383]
 [0.387]] [[3.258]
 [3.641]
 [3.149]
 [2.865]
 [3.192]
 [3.4  ]
 [3.566]] [[0.382]
 [0.396]
 [0.382]
 [0.381]
 [0.386]
 [0.383]
 [0.387]]
first move QE:  0.7192806516000169
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.503]
 [0.493]
 [0.51 ]
 [0.51 ]
 [0.492]
 [0.49 ]] [[0.742]
 [0.872]
 [0.981]
 [0.742]
 [0.742]
 [1.921]
 [1.621]] [[0.51 ]
 [0.503]
 [0.493]
 [0.51 ]
 [0.51 ]
 [0.492]
 [0.49 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.2271333611099524
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
line 256 mcts: sample exp_bonus 2.597553664441884
Printing some Q and Qe and total Qs values:  [[ 0.063]
 [ 0.072]
 [-0.006]
 [ 0.109]
 [ 0.101]
 [ 0.019]
 [-0.009]] [[2.749]
 [3.495]
 [3.982]
 [3.705]
 [3.189]
 [4.062]
 [3.276]] [[-0.534]
 [-0.267]
 [-0.26 ]
 [-0.121]
 [-0.31 ]
 [-0.183]
 [-0.501]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.727]
 [0.727]] [[2.219]
 [2.219]
 [2.219]
 [2.219]
 [2.219]
 [0.719]
 [0.719]] [[ 1.081]
 [ 1.081]
 [ 1.081]
 [ 1.081]
 [ 1.081]
 [-0.405]
 [-0.405]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
siam score:  -0.6752182
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
using another actor
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.397]
 [0.371]] [[4.117]
 [4.638]
 [4.638]
 [4.638]
 [4.638]
 [5.455]
 [4.638]] [[0.34 ]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.876]
 [0.552]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5917207799155344
Printing some Q and Qe and total Qs values:  [[0.856]
 [0.762]
 [0.807]
 [0.808]
 [0.804]
 [0.817]
 [0.828]] [[1.613]
 [2.527]
 [1.989]
 [1.76 ]
 [1.924]
 [1.804]
 [1.725]] [[0.74 ]
 [0.857]
 [0.769]
 [0.693]
 [0.739]
 [0.727]
 [0.721]]
using explorer policy with actor:  1
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
siam score:  -0.67156947
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6774929
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.7242059993440316
Printing some Q and Qe and total Qs values:  [[1.243]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[0.984]
 [1.363]
 [1.363]
 [1.363]
 [1.363]
 [1.363]
 [1.363]] [[2.365]
 [1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.195]]
siam score:  -0.67652315
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.361]
 [0.301]
 [0.306]
 [0.314]
 [0.3  ]
 [0.319]] [[2.086]
 [1.922]
 [1.784]
 [1.593]
 [1.738]
 [2.115]
 [1.889]] [[0.306]
 [0.361]
 [0.301]
 [0.306]
 [0.314]
 [0.3  ]
 [0.319]]
siam score:  -0.6764583
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
in main func line 156:  2785
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.695]
 [0.64 ]
 [0.681]
 [0.64 ]
 [0.64 ]
 [0.599]] [[2.908]
 [2.502]
 [2.908]
 [2.302]
 [2.908]
 [2.908]
 [2.558]] [[1.384]
 [1.359]
 [1.384]
 [1.263]
 [1.384]
 [1.384]
 [1.185]]
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[3.144]
 [3.144]
 [3.144]
 [3.144]
 [3.144]
 [3.144]
 [3.144]] [[0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
siam score:  -0.6734076
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]] [[4.788]
 [4.123]
 [4.123]
 [4.123]
 [4.123]
 [4.123]
 [4.123]] [[1.43 ]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.604]
 [0.599]
 [0.596]
 [0.587]
 [0.586]
 [0.595]] [[ 1.066]
 [ 1.155]
 [ 0.083]
 [-0.023]
 [ 0.839]
 [ 0.786]
 [ 0.082]] [[0.596]
 [0.604]
 [0.599]
 [0.596]
 [0.587]
 [0.586]
 [0.595]]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[1.512]
 [1.512]
 [1.512]
 [1.512]
 [1.512]
 [1.512]
 [1.512]] [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.323]
 [0.343]
 [0.348]
 [0.365]
 [0.319]
 [0.37 ]] [[3.383]
 [2.858]
 [2.933]
 [3.143]
 [3.324]
 [2.997]
 [2.88 ]] [[1.429]
 [0.875]
 [0.965]
 [1.137]
 [1.305]
 [0.978]
 [0.966]]
line 256 mcts: sample exp_bonus 9.917996220351267
2796 3076
using explorer policy with actor:  0
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
line 256 mcts: sample exp_bonus 2.1723861605628962
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.422]
 [0.578]
 [0.582]
 [0.573]
 [0.571]
 [0.569]] [[2.847]
 [2.686]
 [2.663]
 [2.438]
 [2.435]
 [2.701]
 [2.764]] [[1.035]
 [0.576]
 [0.865]
 [0.65 ]
 [0.629]
 [0.89 ]
 [0.948]]
using explorer policy with actor:  1
first move QE:  0.7238179199034821
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[2.691]
 [2.753]
 [2.753]
 [2.753]
 [2.753]
 [2.753]
 [2.753]] [[1.215]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]]
siam score:  -0.6756376
using another actor
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.675]
 [0.641]] [[3.702]
 [3.702]
 [3.702]
 [3.702]
 [3.702]
 [5.133]
 [3.702]] [[1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.102]
 [2.101]
 [1.102]]
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.957]
 [0.606]
 [0.47 ]
 [0.603]
 [0.47 ]
 [0.47 ]] [[2.072]
 [1.618]
 [1.559]
 [2.217]
 [1.603]
 [2.217]
 [2.217]] [[2.148]
 [1.793]
 [1.336]
 [1.807]
 [1.373]
 [1.807]
 [1.807]]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.541]
 [0.445]
 [0.5  ]
 [0.445]
 [0.445]
 [0.513]] [[2.459]
 [3.008]
 [2.459]
 [2.962]
 [2.459]
 [2.459]
 [2.795]] [[1.361]
 [1.834]
 [1.361]
 [1.766]
 [1.361]
 [1.361]
 [1.658]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.67464507
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.427]
 [0.422]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[0.887]
 [0.898]
 [0.93 ]
 [0.898]
 [0.898]
 [0.898]
 [0.898]] [[-0.155]
 [-0.181]
 [-0.169]
 [-0.181]
 [-0.181]
 [-0.181]
 [-0.181]]
2808 3103
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
siam score:  -0.67159003
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.47 ]
 [0.469]
 [0.447]
 [0.441]
 [0.442]
 [0.448]] [[ 1.448]
 [ 0.059]
 [-0.607]
 [-0.913]
 [-0.616]
 [-0.377]
 [-0.537]] [[1.427]
 [0.64 ]
 [0.257]
 [0.056]
 [0.219]
 [0.358]
 [0.272]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.67263305
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.271]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[3.714]
 [3.445]
 [3.579]
 [3.579]
 [3.579]
 [3.579]
 [3.579]] [[1.253]
 [0.957]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.108]
 [0.248]
 [0.361]
 [0.311]
 [0.311]
 [0.311]] [[2.775]
 [3.033]
 [2.828]
 [1.967]
 [2.347]
 [2.347]
 [2.347]] [[1.088]
 [0.901]
 [0.962]
 [0.455]
 [0.678]
 [0.678]
 [0.678]]
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.296]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[4.571]
 [4.056]
 [4.17 ]
 [4.17 ]
 [4.17 ]
 [4.17 ]
 [4.17 ]] [[0.908]
 [0.568]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
first move QE:  0.7216964542003544
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]] [[-1.157]
 [-1.157]
 [-1.157]
 [-1.157]
 [-1.157]
 [-1.157]
 [-1.157]] [[0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]] [[1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]]
siam score:  -0.6636745
start point for exploration sampling:  10935
siam score:  -0.6675055
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
2824 3130
line 256 mcts: sample exp_bonus 1.3910326788760177
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.581]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[0.433]
 [0.343]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[1.096]
 [1.079]
 [1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.096]]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.408]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[-0.123]
 [ 0.345]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]] [[0.41 ]
 [0.408]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
siam score:  -0.6637529
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.282]
 [0.398]
 [0.408]
 [0.355]
 [0.242]
 [0.29 ]] [[2.386]
 [1.999]
 [2.362]
 [2.35 ]
 [2.473]
 [2.494]
 [2.191]] [[0.339]
 [0.025]
 [0.62 ]
 [0.627]
 [0.645]
 [0.441]
 [0.233]]
siam score:  -0.6560781
2833 3146
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
siam score:  -0.65561324
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.535]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.526]] [[1.05 ]
 [1.19 ]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.463]] [[1.788]
 [1.898]
 [1.788]
 [1.788]
 [1.788]
 [1.788]
 [2.039]]
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[3.251]
 [3.251]
 [3.251]
 [3.251]
 [3.251]
 [3.251]
 [3.251]] [[1.33]
 [1.33]
 [1.33]
 [1.33]
 [1.33]
 [1.33]
 [1.33]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[ 0.248]
 [ 0.271]
 [ 0.06 ]
 [ 0.208]
 [ 0.328]
 [-0.   ]
 [ 0.328]] [[2.48 ]
 [2.033]
 [2.754]
 [3.002]
 [2.098]
 [1.939]
 [2.098]] [[0.835]
 [0.473]
 [0.877]
 [1.244]
 [0.588]
 [0.112]
 [0.588]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.384]
 [0.252]
 [0.252]
 [0.436]
 [0.398]
 [0.252]] [[2.387]
 [2.325]
 [2.387]
 [2.387]
 [3.073]
 [2.273]
 [2.387]] [[0.356]
 [0.45 ]
 [0.356]
 [0.356]
 [1.252]
 [0.416]
 [0.356]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 3.2278647795481663
using explorer policy with actor:  0
using another actor
from probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[1.211]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[0.445]
 [1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.479]] [[2.582]
 [1.499]
 [1.499]
 [1.499]
 [1.499]
 [1.499]
 [1.499]]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[ 0.018]
 [-0.007]
 [-0.01 ]
 [-0.014]
 [-0.004]
 [-0.004]
 [ 0.011]] [[0.916]
 [0.974]
 [1.336]
 [1.3  ]
 [1.315]
 [1.341]
 [1.316]] [[-1.143]
 [-1.172]
 [-1.058]
 [-1.078]
 [-1.053]
 [-1.045]
 [-1.023]]
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.472]
 [0.532]
 [0.543]
 [0.281]
 [0.558]
 [0.508]] [[2.41 ]
 [1.896]
 [1.42 ]
 [1.381]
 [2.461]
 [1.957]
 [2.813]] [[1.107]
 [0.576]
 [0.221]
 [0.205]
 [0.759]
 [0.811]
 [1.567]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[3.967]
 [3.967]
 [3.967]
 [3.967]
 [3.967]
 [3.967]
 [3.967]] [[1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
2842 3173
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]] [[3.916]
 [4.257]
 [4.257]
 [4.257]
 [4.257]
 [4.257]
 [4.257]] [[2.176]
 [2.178]
 [2.178]
 [2.178]
 [2.178]
 [2.178]
 [2.178]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.544]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]] [[0.722]
 [1.108]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]] [[0.862]
 [1.117]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.021]
 [0.072]
 [0.102]
 [0.055]
 [0.053]
 [0.054]] [[2.633]
 [2.952]
 [2.382]
 [2.252]
 [2.492]
 [2.594]
 [2.258]] [[-0.525]
 [-0.527]
 [-0.615]
 [-0.598]
 [-0.613]
 [-0.582]
 [-0.692]]
siam score:  -0.6561791
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.465]
 [0.465]
 [0.465]
 [0.517]
 [0.465]
 [0.465]] [[1.532]
 [0.615]
 [0.615]
 [0.615]
 [3.064]
 [0.615]
 [0.615]] [[1.134]
 [0.773]
 [0.773]
 [0.773]
 [1.702]
 [0.773]
 [0.773]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.475]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.487]] [[2.458]
 [3.444]
 [2.458]
 [2.458]
 [2.458]
 [2.458]
 [3.482]] [[0.921]
 [1.279]
 [0.921]
 [0.921]
 [0.921]
 [0.921]
 [1.316]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]] [[3.686]
 [3.686]
 [3.686]
 [3.686]
 [3.686]
 [3.686]
 [3.686]] [[1.823]
 [1.823]
 [1.823]
 [1.823]
 [1.823]
 [1.823]
 [1.823]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
line 256 mcts: sample exp_bonus 2.6583325870297703
line 256 mcts: sample exp_bonus 1.9169768658948079
Printing some Q and Qe and total Qs values:  [[1.171]
 [1.23 ]
 [1.171]
 [1.169]
 [1.171]
 [1.171]
 [1.179]] [[0.866]
 [0.812]
 [0.881]
 [0.973]
 [0.866]
 [0.866]
 [0.864]] [[2.142]
 [2.199]
 [2.155]
 [2.236]
 [2.142]
 [2.142]
 [2.153]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.417]
 [0.417]
 [0.415]
 [0.414]
 [0.413]
 [0.413]] [[3.841]
 [3.659]
 [3.823]
 [3.862]
 [3.758]
 [3.795]
 [3.856]] [[1.331]
 [1.151]
 [1.311]
 [1.345]
 [1.243]
 [1.278]
 [1.337]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.312]
 [0.322]
 [0.403]
 [0.345]
 [1.182]
 [0.298]] [[2.068]
 [1.454]
 [1.1  ]
 [0.683]
 [1.118]
 [2.047]
 [1.427]] [[1.251]
 [0.635]
 [0.419]
 [0.304]
 [0.478]
 [2.77 ]
 [0.59 ]]
Printing some Q and Qe and total Qs values:  [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[0.712]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[1.919]
 [1.923]
 [1.923]
 [1.923]
 [1.923]
 [1.923]
 [1.923]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.33980131431330385, 0.06288860379159929]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.65184236
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.414]
 [0.583]] [[1.52 ]
 [1.52 ]
 [1.52 ]
 [1.52 ]
 [1.52 ]
 [2.164]
 [1.52 ]] [[0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [1.234]
 [0.499]]
Printing some Q and Qe and total Qs values:  [[ 0.066]
 [ 0.055]
 [ 0.183]
 [ 0.19 ]
 [ 0.077]
 [-0.005]
 [ 0.183]] [[0.827]
 [1.737]
 [0.849]
 [0.599]
 [0.745]
 [0.867]
 [0.849]] [[-0.724]
 [-0.441]
 [-0.482]
 [-0.551]
 [-0.729]
 [-0.852]
 [-0.482]]
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.073]] [[3.382]
 [3.382]
 [3.382]
 [3.382]
 [3.382]
 [3.382]
 [3.693]] [[0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.395]]
using another actor
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16272331316048913, 0.21509112569330682, 0.069243022320447, 0.15025262072085388, 0.3398013143133039, 0.06288860379159929]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[1.699]
 [1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]] [[-1.078]
 [-1.223]
 [-1.223]
 [-1.223]
 [-1.223]
 [-1.223]
 [-1.223]]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.305]
 [0.305]
 [0.282]
 [0.282]
 [0.28 ]
 [0.282]] [[3.419]
 [3.395]
 [3.509]
 [3.419]
 [3.419]
 [3.952]
 [3.419]] [[0.968]
 [0.976]
 [1.072]
 [0.968]
 [0.968]
 [1.414]
 [0.968]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.469]
 [0.681]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[2.735]
 [3.981]
 [1.905]
 [2.735]
 [2.735]
 [2.735]
 [2.735]] [[0.57 ]
 [1.219]
 [0.302]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6540407
Printing some Q and Qe and total Qs values:  [[-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]] [[4.634]
 [4.634]
 [4.634]
 [4.634]
 [4.634]
 [4.634]
 [4.634]] [[1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]]
actor:  1 policy actor:  1  step number:  114 total reward:  0.004999999999999227  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.15450996268889963, 0.20423454488545742, 0.06574802704912398, 0.1931430517401071, 0.32265007008802615, 0.05971434354838569]
actor:  1 policy actor:  1  step number:  74 total reward:  0.49499999999999966  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13898543298973334, 0.28418980619356093, 0.0591419339479246, 0.17373682711390492, 0.29023150944410786, 0.05371449031076833]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[3.598]
 [3.598]
 [3.598]
 [3.598]
 [3.598]
 [3.598]
 [3.598]] [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]]
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[3.863]
 [3.863]
 [3.863]
 [3.863]
 [3.863]
 [3.863]
 [3.863]] [[0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]]
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.097]
 [0.092]
 [0.114]
 [0.09 ]
 [0.067]
 [0.082]] [[3.988]
 [3.166]
 [3.71 ]
 [3.599]
 [3.641]
 [3.778]
 [3.832]] [[-0.431]
 [-0.759]
 [-0.588]
 [-0.58 ]
 [-0.615]
 [-0.615]
 [-0.566]]
from probs:  [0.13898543298973334, 0.28418980619356093, 0.0591419339479246, 0.17373682711390492, 0.29023150944410786, 0.05371449031076833]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 3.8576089260726096
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13898543298973334, 0.28418980619356093, 0.0591419339479246, 0.17373682711390492, 0.29023150944410786, 0.05371449031076833]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.565]
 [0.544]
 [0.609]
 [0.544]
 [0.544]
 [0.592]] [[2.076]
 [2.262]
 [2.076]
 [2.2  ]
 [2.076]
 [2.076]
 [2.203]] [[1.198]
 [1.426]
 [1.198]
 [1.451]
 [1.198]
 [1.198]
 [1.421]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13898543298973334, 0.28418980619356093, 0.0591419339479246, 0.17373682711390492, 0.29023150944410786, 0.05371449031076833]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13898543298973334, 0.28418980619356093, 0.0591419339479246, 0.17373682711390492, 0.29023150944410786, 0.05371449031076833]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13898543298973334, 0.28418980619356093, 0.0591419339479246, 0.17373682711390492, 0.29023150944410786, 0.05371449031076833]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.13898543298973334, 0.28418980619356093, 0.0591419339479246, 0.17373682711390492, 0.29023150944410786, 0.05371449031076833]
actor:  1 policy actor:  1  step number:  117 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.491]
 [0.453]
 [0.437]
 [0.437]
 [0.44 ]
 [0.441]] [[1.831]
 [1.909]
 [1.708]
 [1.836]
 [1.836]
 [1.762]
 [1.633]] [[0.137]
 [0.247]
 [0.104]
 [0.115]
 [0.115]
 [0.096]
 [0.056]]
siam score:  -0.63654333
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.057]
 [0.044]
 [0.054]
 [0.06 ]
 [0.064]
 [0.039]] [[0.836]
 [0.699]
 [0.721]
 [0.73 ]
 [0.802]
 [1.305]
 [0.799]] [[-0.577]
 [-0.605]
 [-0.624]
 [-0.601]
 [-0.565]
 [-0.388]
 [-0.607]]
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.094]
 [0.085]
 [0.09 ]
 [0.089]
 [0.082]
 [0.082]] [[5.25 ]
 [5.242]
 [5.084]
 [5.199]
 [5.161]
 [5.195]
 [5.017]] [[1.033]
 [1.042]
 [0.967]
 [1.02 ]
 [1.004]
 [1.012]
 [0.937]]
from probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.099]
 [0.169]
 [0.344]
 [0.296]
 [0.331]
 [0.108]] [[2.643]
 [1.421]
 [0.889]
 [1.238]
 [1.653]
 [0.517]
 [0.902]] [[2.424]
 [0.44 ]
 [0.241]
 [0.778]
 [0.944]
 [0.312]
 [0.137]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.166]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]] [[4.866]
 [2.566]
 [2.566]
 [2.566]
 [2.566]
 [2.566]
 [2.566]] [[2.32 ]
 [1.088]
 [1.088]
 [1.088]
 [1.088]
 [1.088]
 [1.088]]
siam score:  -0.63883257
line 256 mcts: sample exp_bonus 1.9377934496948008
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.244]
 [0.241]
 [0.256]
 [0.242]
 [0.242]
 [0.244]] [[3.936]
 [3.835]
 [3.862]
 [3.665]
 [3.797]
 [3.988]
 [3.734]] [[0.454]
 [0.356]
 [0.375]
 [0.219]
 [0.316]
 [0.496]
 [0.259]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.26 ]
 [0.342]] [[4.172]
 [4.172]
 [4.172]
 [4.172]
 [4.172]
 [5.01 ]
 [4.172]] [[1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.488]
 [1.037]]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.246]
 [0.264]
 [0.254]
 [0.248]
 [0.246]
 [0.248]] [[5.546]
 [5.502]
 [5.204]
 [5.348]
 [5.479]
 [5.502]
 [5.346]] [[0.873]
 [0.827]
 [0.662]
 [0.74 ]
 [0.814]
 [0.827]
 [0.731]]
line 256 mcts: sample exp_bonus 3.1444446975799782
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
from probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
siam score:  -0.65864944
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
siam score:  -0.6562633
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.418]
 [0.399]
 [0.399]
 [0.365]
 [0.399]
 [0.399]] [[-0.453]
 [ 0.401]
 [-0.453]
 [-0.453]
 [-0.19 ]
 [-0.453]
 [-0.453]] [[0.399]
 [0.418]
 [0.399]
 [0.399]
 [0.365]
 [0.399]
 [0.399]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
first move QE:  0.71342928413831
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.731]
 [0.667]
 [0.774]
 [0.983]
 [0.764]
 [0.812]] [[2.261]
 [1.604]
 [2.943]
 [2.565]
 [1.807]
 [2.668]
 [2.508]] [[0.844]
 [0.429]
 [1.191]
 [1.154]
 [1.068]
 [1.202]
 [1.193]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
from probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
from probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.348]
 [1.33 ]
 [1.276]
 [1.276]
 [1.276]
 [1.276]
 [1.276]] [[0.962]
 [0.971]
 [1.125]
 [1.125]
 [1.125]
 [1.125]
 [1.125]] [[1.86 ]
 [1.826]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]]
from probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.345]
 [0.517]
 [0.52 ]
 [0.521]
 [0.525]
 [0.52 ]] [[-2.417]
 [ 0.347]
 [-2.276]
 [-2.183]
 [-2.222]
 [-2.123]
 [-2.102]] [[0.27 ]
 [1.472]
 [0.335]
 [0.38 ]
 [0.363]
 [0.411]
 [0.419]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.005]
 [0.012]
 [0.161]
 [0.161]
 [0.161]
 [0.161]] [[1.954]
 [1.81 ]
 [2.101]
 [1.515]
 [1.515]
 [1.515]
 [1.515]] [[0.674]
 [0.433]
 [0.932]
 [0.253]
 [0.253]
 [0.253]
 [0.253]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7115739195070299
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]] [[-0.943]
 [-2.55 ]
 [-2.55 ]
 [-2.55 ]
 [-2.55 ]
 [-2.55 ]
 [-2.55 ]] [[0.232]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]]
from probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.506]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]] [[-3.089]
 [-3.666]
 [-3.089]
 [-3.089]
 [-3.089]
 [-3.089]
 [-3.089]] [[0.515]
 [0.506]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.2  ]
 [0.199]
 [0.201]
 [0.203]
 [0.2  ]
 [0.241]] [[5.385]
 [5.437]
 [5.535]
 [5.521]
 [5.542]
 [5.464]
 [4.699]] [[0.56 ]
 [0.553]
 [0.617]
 [0.61 ]
 [0.629]
 [0.571]
 [0.143]]
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.337]
 [0.312]
 [0.311]
 [0.311]
 [0.26 ]
 [0.311]] [[2.484]
 [2.26 ]
 [1.903]
 [1.654]
 [1.654]
 [2.161]
 [1.654]] [[ 0.066]
 [ 0.112]
 [-0.057]
 [-0.143]
 [-0.143]
 [-0.075]
 [-0.143]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.036]
 [0.158]
 [0.158]] [[1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.837]
 [1.12 ]
 [1.12 ]] [[1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.998]
 [1.839]
 [1.839]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.66295415
rdn probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1985517611957235
siam score:  -0.664449
Printing some Q and Qe and total Qs values:  [[1.018]
 [1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]] [[1.67 ]
 [1.648]
 [1.648]
 [1.648]
 [1.648]
 [1.648]
 [1.648]] [[1.382]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]]
from probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
line 256 mcts: sample exp_bonus 2.228683478565802
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.064]
 [0.334]
 [0.334]
 [0.068]
 [0.396]
 [0.334]] [[1.233]
 [1.241]
 [0.36 ]
 [0.36 ]
 [1.043]
 [1.146]
 [0.36 ]] [[1.733]
 [1.741]
 [1.546]
 [1.546]
 [1.674]
 [1.848]
 [1.546]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
line 256 mcts: sample exp_bonus 3.855210338941171
using another actor
using explorer policy with actor:  0
from probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.696]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]] [[3.268]
 [3.428]
 [3.268]
 [3.268]
 [3.268]
 [3.268]
 [3.268]] [[0.681]
 [0.696]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2913 3285
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.6402004
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.484]
 [0.46 ]
 [0.46 ]
 [0.484]
 [0.465]
 [0.47 ]] [[1.548]
 [1.774]
 [1.925]
 [1.819]
 [1.774]
 [1.971]
 [1.917]] [[0.16 ]
 [0.254]
 [0.257]
 [0.222]
 [0.254]
 [0.283]
 [0.274]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.601]
 [0.562]
 [0.617]
 [0.556]
 [0.565]
 [0.559]] [[ 0.667]
 [ 1.62 ]
 [-0.167]
 [ 0.667]
 [-0.372]
 [-0.153]
 [-0.06 ]] [[0.617]
 [0.601]
 [0.562]
 [0.617]
 [0.556]
 [0.565]
 [0.559]]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.671]
 [0.667]
 [0.663]
 [0.667]
 [0.663]
 [0.668]] [[2.292]
 [2.846]
 [2.436]
 [2.284]
 [2.463]
 [2.454]
 [2.539]] [[0.671]
 [0.671]
 [0.667]
 [0.663]
 [0.667]
 [0.663]
 [0.668]]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]] [[1.569]
 [2.25 ]
 [2.25 ]
 [2.25 ]
 [2.25 ]
 [2.25 ]
 [2.25 ]] [[0.037]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]]
using another actor
from probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.351]
 [0.488]
 [0.736]
 [0.488]
 [0.488]
 [0.488]] [[2.414]
 [2.204]
 [2.414]
 [1.624]
 [2.414]
 [2.414]
 [2.414]] [[0.488]
 [0.351]
 [0.488]
 [0.736]
 [0.488]
 [0.488]
 [0.488]]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.666]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[2.338]
 [4.137]
 [2.338]
 [2.338]
 [2.338]
 [2.338]
 [2.338]] [[0.671]
 [0.666]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.612]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[1.095]
 [0.87 ]
 [1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.095]] [[0.54 ]
 [0.612]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.6465463
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.522]
 [0.55 ]
 [0.521]
 [0.516]
 [0.517]
 [0.569]] [[3.802]
 [3.68 ]
 [3.972]
 [3.776]
 [3.394]
 [3.849]
 [3.533]] [[1.214]
 [1.129]
 [1.385]
 [1.202]
 [0.903]
 [1.253]
 [1.071]]
2925 3299
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.457]
 [0.458]
 [0.457]
 [0.457]
 [0.453]
 [0.463]] [[1.817]
 [1.317]
 [1.237]
 [1.317]
 [1.317]
 [1.257]
 [1.478]] [[ 0.161]
 [-0.077]
 [-0.102]
 [-0.077]
 [-0.077]
 [-0.104]
 [-0.01 ]]
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.775]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]] [[0.871]
 [2.573]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]] [[1.344]
 [1.957]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]]
2927 3301
line 256 mcts: sample exp_bonus -0.5676311148658807
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.966]
 [0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.845]] [[1.899]
 [1.748]
 [2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.006]] [[1.261]
 [1.471]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.4  ]]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.43 ]
 [0.506]
 [0.506]
 [0.506]
 [0.457]
 [0.506]] [[1.274]
 [1.51 ]
 [1.167]
 [1.167]
 [1.167]
 [1.357]
 [1.167]] [[0.302]
 [0.329]
 [0.368]
 [0.368]
 [0.368]
 [0.333]
 [0.368]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.772]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[1.427]
 [1.187]
 [1.427]
 [1.427]
 [1.427]
 [1.427]
 [1.427]] [[0.427]
 [0.772]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]]
Printing some Q and Qe and total Qs values:  [[0.014]
 [1.5  ]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[0.721]
 [0.698]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[1.639]
 [2.126]
 [1.639]
 [1.639]
 [1.639]
 [1.639]
 [1.639]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
2933 3318
from probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2935 3320
siam score:  -0.65427554
line 256 mcts: sample exp_bonus 1.3325322239559882
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
start point for exploration sampling:  10935
using explorer policy with actor:  0
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.353]
 [0.342]
 [0.339]
 [0.339]
 [0.341]
 [0.343]] [[-0.832]
 [-1.021]
 [-0.903]
 [-1.006]
 [-0.749]
 [-0.564]
 [-0.972]] [[0.343]
 [0.353]
 [0.342]
 [0.339]
 [0.339]
 [0.341]
 [0.343]]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[3.512]
 [3.512]
 [3.512]
 [3.512]
 [3.512]
 [3.512]
 [3.512]] [[0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.735561166789582
siam score:  -0.6477858
siam score:  -0.647632
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.235]
 [0.23 ]
 [0.235]
 [0.228]
 [0.233]
 [0.235]] [[3.076]
 [3.066]
 [3.709]
 [3.066]
 [2.817]
 [3.431]
 [3.066]] [[0.308]
 [0.297]
 [0.929]
 [0.297]
 [0.035]
 [0.658]
 [0.297]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.225]
 [0.295]
 [0.225]
 [0.2  ]
 [0.46 ]
 [0.225]] [[-0.753]
 [-0.696]
 [-0.943]
 [-0.696]
 [ 0.476]
 [-1.004]
 [-0.696]] [[0.231]
 [0.225]
 [0.295]
 [0.225]
 [0.2  ]
 [0.46 ]
 [0.225]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.807]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[0.701]
 [1.21 ]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[1.163]
 [1.933]
 [1.163]
 [1.163]
 [1.163]
 [1.163]
 [1.163]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[-1.66]
 [-1.66]
 [-1.66]
 [-1.66]
 [-1.66]
 [-1.66]
 [-1.66]] [[0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[-0.58]
 [-0.58]
 [-0.58]
 [-0.58]
 [-0.58]
 [-0.58]
 [-0.58]] [[0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.217]
 [0.2  ]
 [0.223]
 [0.196]
 [0.197]
 [0.199]] [[2.978]
 [4.2  ]
 [3.275]
 [3.746]
 [3.067]
 [3.109]
 [2.942]] [[-0.263]
 [ 0.188]
 [-0.155]
 [ 0.048]
 [-0.233]
 [-0.217]
 [-0.27 ]]
2958 3347
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.516]
 [0.484]
 [0.532]
 [0.491]
 [0.484]
 [0.513]] [[ 1.791]
 [ 1.588]
 [ 1.46 ]
 [-0.142]
 [-0.145]
 [ 0.974]
 [ 0.544]] [[0.494]
 [0.516]
 [0.484]
 [0.532]
 [0.491]
 [0.484]
 [0.513]]
using explorer policy with actor:  0
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.474]
 [0.453]
 [0.453]
 [0.446]
 [0.38 ]
 [0.453]] [[2.617]
 [2.451]
 [2.645]
 [2.645]
 [3.404]
 [2.981]
 [2.645]] [[0.506]
 [0.542]
 [0.564]
 [0.564]
 [0.804]
 [0.53 ]
 [0.564]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6515272
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[1.192]
 [1.137]
 [1.137]
 [1.137]
 [1.137]
 [1.137]
 [1.137]] [[2.287]
 [2.072]
 [2.072]
 [2.072]
 [2.072]
 [2.072]
 [2.072]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
from probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
from probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [1.053]
 [0.931]
 [0.931]
 [0.931]
 [0.94 ]
 [0.931]] [[0.854]
 [0.713]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[1.22 ]
 [1.379]
 [1.139]
 [1.139]
 [1.139]
 [1.156]
 [1.139]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.103]
 [0.163]
 [0.169]
 [0.176]
 [0.178]
 [0.186]] [[ 0.302]
 [ 0.503]
 [-0.683]
 [-0.981]
 [-0.849]
 [-0.673]
 [-0.735]] [[0.302]
 [0.103]
 [0.163]
 [0.169]
 [0.176]
 [0.178]
 [0.186]]
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.374]
 [0.137]
 [0.374]
 [0.171]
 [0.374]
 [0.374]] [[-0.286]
 [ 0.   ]
 [-0.704]
 [ 0.   ]
 [-0.645]
 [ 0.   ]
 [ 0.   ]] [[0.18 ]
 [0.374]
 [0.137]
 [0.374]
 [0.171]
 [0.374]
 [0.374]]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[0.144]
 [0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.077]] [[0.699]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]]
siam score:  -0.6622033
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.498]
 [0.406]
 [0.406]
 [0.406]
 [0.415]
 [0.406]] [[0.89 ]
 [1.246]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.839]
 [0.89 ]] [[0.754]
 [1.177]
 [0.754]
 [0.754]
 [0.754]
 [0.738]
 [0.754]]
2970 3364
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[3.284]
 [3.284]
 [3.284]
 [3.284]
 [3.284]
 [3.284]
 [3.284]] [[0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.475]
 [0.46 ]
 [0.46 ]
 [0.452]
 [0.46 ]
 [0.46 ]] [[2.804]
 [1.854]
 [2.804]
 [2.804]
 [2.809]
 [2.804]
 [2.804]] [[0.46 ]
 [0.475]
 [0.46 ]
 [0.46 ]
 [0.452]
 [0.46 ]
 [0.46 ]]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.265]
 [0.282]
 [0.265]
 [0.265]
 [0.274]
 [0.293]] [[1.13 ]
 [1.064]
 [1.097]
 [1.064]
 [1.064]
 [0.973]
 [0.952]] [[0.312]
 [0.232]
 [0.287]
 [0.232]
 [0.232]
 [0.189]
 [0.213]]
2973 3370
using another actor
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.22 ]
 [0.275]
 [0.594]
 [0.376]
 [0.253]
 [0.272]] [[0.949]
 [1.313]
 [0.737]
 [0.81 ]
 [0.789]
 [0.812]
 [1.123]] [[-0.432]
 [-0.164]
 [-0.437]
 [ 0.25 ]
 [-0.201]
 [-0.431]
 [-0.186]]
Printing some Q and Qe and total Qs values:  [[0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]] [[-1.451]
 [-1.451]
 [-1.451]
 [-1.451]
 [-1.451]
 [-1.451]
 [-1.451]] [[0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.374]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[1.75 ]
 [1.022]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]] [[1.886]
 [1.355]
 [1.487]
 [1.487]
 [1.487]
 [1.487]
 [1.487]]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]] [[2.892]
 [1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]] [[2.023]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496488, 0.270031255531645, 0.056195438156041576, 0.16508112724387738, 0.2757719565657405, 0.05103839385773081]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.61 ]
 [0.545]
 [0.534]
 [0.533]
 [0.535]
 [0.537]] [[3.564]
 [4.895]
 [3.685]
 [3.197]
 [3.237]
 [3.301]
 [3.405]] [[0.904]
 [1.907]
 [0.972]
 [0.625]
 [0.648]
 [0.696]
 [0.77 ]]
Printing some Q and Qe and total Qs values:  [[1.491]
 [1.491]
 [1.484]
 [1.484]
 [1.482]
 [1.484]
 [1.483]] [[0.662]
 [0.654]
 [0.679]
 [0.679]
 [0.67 ]
 [0.673]
 [0.681]] [[1.74 ]
 [1.737]
 [1.745]
 [1.745]
 [1.74 ]
 [1.742]
 [1.745]]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [1.113]
 [0.572]] [[1.281]
 [1.281]
 [1.281]
 [1.281]
 [1.281]
 [1.853]
 [1.281]] [[2.012]
 [2.012]
 [2.012]
 [2.012]
 [2.012]
 [2.908]
 [2.012]]
using explorer policy with actor:  1
from probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
siam score:  -0.6581862
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[-1.219]
 [-1.219]
 [-1.219]
 [-1.219]
 [-1.219]
 [-1.219]
 [-1.219]] [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.583]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]] [[-0.068]
 [ 0.494]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]] [[0.553]
 [0.583]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.528]
 [0.37 ]
 [0.508]
 [0.498]
 [0.489]
 [0.506]] [[3.019]
 [3.515]
 [2.396]
 [3.11 ]
 [3.114]
 [3.263]
 [3.241]] [[1.147]
 [1.336]
 [0.646]
 [1.16 ]
 [1.142]
 [1.173]
 [1.201]]
from probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
line 256 mcts: sample exp_bonus 0.682881555868281
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.423]
 [0.441]
 [0.492]
 [0.441]
 [0.441]
 [0.419]] [[1.573]
 [1.53 ]
 [1.573]
 [2.039]
 [1.573]
 [1.573]
 [1.54 ]] [[1.005]
 [0.955]
 [1.005]
 [1.263]
 [1.005]
 [1.005]
 [0.951]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]] [[0.683]
 [0.679]
 [0.691]
 [0.683]
 [0.683]
 [0.688]
 [0.688]] [[2.735]
 [2.732]
 [2.744]
 [2.736]
 [2.736]
 [2.741]
 [2.741]]
from probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
first move QE:  0.6996574753083366
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.261]
 [0.256]
 [0.258]
 [0.256]
 [0.257]
 [0.257]] [[-0.297]
 [-0.661]
 [-0.567]
 [-0.5  ]
 [-0.616]
 [-0.522]
 [-0.581]] [[0.246]
 [0.261]
 [0.256]
 [0.258]
 [0.256]
 [0.257]
 [0.257]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
from probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[2.717]
 [2.717]
 [2.717]
 [2.717]
 [2.717]
 [2.717]
 [2.717]] [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]] [[1.302]
 [1.904]
 [1.904]
 [1.904]
 [1.904]
 [1.904]
 [1.904]] [[0.604]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.284]
 [0.394]
 [0.392]
 [0.392]
 [0.389]
 [0.387]] [[-3.341]
 [-1.917]
 [-3.439]
 [-3.456]
 [-3.403]
 [-3.375]
 [-3.478]] [[0.077]
 [0.589]
 [0.024]
 [0.017]
 [0.038]
 [0.048]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[0.256]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]] [[0.494]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[-0.204]
 [ 0.719]
 [ 0.719]
 [ 0.719]
 [ 0.719]
 [ 0.719]
 [ 0.719]] [[0.621]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.23020856067472795
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.267]
 [0.304]
 [0.3  ]
 [0.276]
 [0.642]
 [0.335]] [[1.33 ]
 [1.13 ]
 [2.104]
 [1.047]
 [2.02 ]
 [1.984]
 [2.179]] [[0.128]
 [0.267]
 [0.304]
 [0.3  ]
 [0.276]
 [0.642]
 [0.335]]
siam score:  -0.6643145
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]] [[0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[2.771]
 [3.771]
 [3.771]
 [3.771]
 [3.771]
 [3.771]
 [3.771]] [[0.413]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.65908355
Printing some Q and Qe and total Qs values:  [[0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]] [[-4.142]
 [-4.142]
 [-4.142]
 [-4.142]
 [-4.142]
 [-4.142]
 [-4.142]] [[0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.655]
 [0.47 ]] [[2.427]
 [2.427]
 [2.427]
 [2.427]
 [2.427]
 [3.492]
 [2.427]] [[1.124]
 [1.124]
 [1.124]
 [1.124]
 [1.124]
 [1.872]
 [1.124]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
Printing some Q and Qe and total Qs values:  [[1.081]
 [0.514]
 [0.502]
 [0.522]
 [0.502]
 [0.502]
 [0.502]] [[3.314]
 [1.855]
 [2.184]
 [2.277]
 [2.184]
 [2.184]
 [2.184]] [[2.339]
 [0.407]
 [0.635]
 [0.733]
 [0.635]
 [0.635]
 [0.635]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
siam score:  -0.6605432
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
2997 3429
siam score:  -0.6641929
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
from probs:  [0.18188182864496485, 0.2700312555316449, 0.05619543815604157, 0.16508112724387736, 0.2757719565657405, 0.051038393857730804]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  0.35499999999999954  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
first move QE:  0.6940720383649949
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.207]
 [0.246]
 [0.246]
 [0.246]
 [0.244]
 [0.246]] [[-1.475]
 [-0.245]
 [-1.689]
 [-1.689]
 [-1.689]
 [-1.416]
 [-1.689]] [[0.217]
 [0.207]
 [0.246]
 [0.246]
 [0.246]
 [0.244]
 [0.246]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.596]
 [0.589]
 [0.59 ]
 [0.583]
 [0.587]
 [0.59 ]] [[-3.356]
 [-2.932]
 [-3.604]
 [-3.353]
 [-3.541]
 [-3.229]
 [-3.512]] [[0.584]
 [0.596]
 [0.589]
 [0.59 ]
 [0.583]
 [0.587]
 [0.59 ]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.16865496610547281, 0.25039396507281936, 0.12483106435718112, 0.15307605013317058, 0.2557171891988368, 0.04732676513251923]
from probs:  [0.16865496610547281, 0.25039396507281936, 0.12483106435718112, 0.15307605013317058, 0.2557171891988368, 0.04732676513251923]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
actor:  0 policy actor:  1  step number:  86 total reward:  0.2049999999999994  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.313]
 [0.5  ]
 [0.504]
 [0.506]
 [0.507]
 [0.503]] [[-0.3  ]
 [ 0.388]
 [-0.165]
 [-0.044]
 [-0.075]
 [-0.288]
 [-0.597]] [[0.832]
 [1.035]
 [0.928]
 [1.006]
 [0.989]
 [0.864]
 [0.674]]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.16863449490137356, 0.2504205991690917, 0.12484343234071817, 0.15302927365292038, 0.25576555976586124, 0.04730664017003494]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.973]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]] [[0.626]
 [0.392]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[0.74 ]
 [1.368]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]]
from probs:  [0.16863449490137356, 0.2504205991690917, 0.12484343234071817, 0.15302927365292038, 0.25576555976586124, 0.04730664017003494]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.919]
 [0.639]] [[1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.952]
 [1.365]] [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [1.773]
 [0.822]]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.28 ]
 [0.301]
 [0.269]
 [0.269]
 [0.28 ]
 [0.294]] [[1.999]
 [2.92 ]
 [3.079]
 [3.09 ]
 [3.09 ]
 [3.156]
 [3.238]] [[0.172]
 [0.47 ]
 [0.565]
 [0.506]
 [0.506]
 [0.548]
 [0.604]]
start point for exploration sampling:  10935
siam score:  -0.66862726
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.16863449490137356, 0.2504205991690917, 0.12484343234071817, 0.15302927365292038, 0.25576555976586124, 0.04730664017003494]
first move QE:  0.6887988600708495
actor:  1 policy actor:  1  step number:  92 total reward:  0.3949999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.315]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]] [[-2.06 ]
 [-1.814]
 [-2.06 ]
 [-2.06 ]
 [-2.06 ]
 [-2.06 ]
 [-2.06 ]] [[0.292]
 [0.315]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
from probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
siam score:  -0.6607413
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
from probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
siam score:  -0.66189295
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[0.637]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[0.759]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]] [[2.263]
 [2.199]
 [2.199]
 [2.199]
 [2.199]
 [2.199]
 [2.199]] [[0.514]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.563]
 [0.534]
 [0.526]
 [0.213]
 [0.55 ]
 [0.52 ]] [[-0.466]
 [-0.86 ]
 [-1.451]
 [-1.267]
 [-0.546]
 [-0.69 ]
 [-0.864]] [[1.036]
 [1.022]
 [0.765]
 [0.811]
 [0.426]
 [1.051]
 [0.934]]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.486]
 [0.535]
 [0.744]
 [0.535]
 [0.535]
 [0.433]] [[2.199]
 [1.238]
 [2.199]
 [1.772]
 [2.199]
 [2.199]
 [1.005]] [[1.439]
 [1.02 ]
 [1.439]
 [1.715]
 [1.439]
 [1.439]
 [0.838]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
siam score:  -0.6733525
3027 3490
from probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.607]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[-3.337]
 [-2.987]
 [-3.337]
 [-3.337]
 [-3.337]
 [-3.337]
 [-3.337]] [[0.582]
 [0.607]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.605]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[0.212]
 [0.554]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]] [[0.447]
 [0.666]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
siam score:  -0.6685823
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.349]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]] [[0.912]
 [1.324]
 [0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.912]] [[0.404]
 [0.668]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
from probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[1.277]
 [1.182]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]] [[0.919]
 [0.793]
 [0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]] [[2.392]
 [2.074]
 [2.286]
 [2.286]
 [2.286]
 [2.286]
 [2.286]]
using explorer policy with actor:  0
siam score:  -0.6756674
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
from probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
3037 3510
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]] [[2.126]
 [2.126]
 [2.126]
 [2.126]
 [2.126]
 [2.126]
 [2.126]] [[1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]]
using another actor
from probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Printing some Q and Qe and total Qs values:  [[1.009]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]] [[0.291]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[1.617]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[2.79 ]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]] [[1.729]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]]
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]] [[0.862]
 [0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]] [[1.022]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
from probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
siam score:  -0.68823874
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
first move QE:  0.6755505305229779
start point for exploration sampling:  10935
from probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.70534426
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
siam score:  -0.70280504
using explorer policy with actor:  1
siam score:  -0.7046846
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
line 256 mcts: sample exp_bonus -0.7798455840702911
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.14123602272619
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
line 256 mcts: sample exp_bonus -0.03495557802829273
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
using explorer policy with actor:  1
siam score:  -0.7269611
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.47 ]
 [0.383]
 [0.397]
 [0.427]
 [0.389]
 [0.432]] [[-2.957]
 [-2.319]
 [-3.241]
 [-3.119]
 [ 0.   ]
 [-2.529]
 [-2.631]] [[0.38 ]
 [0.47 ]
 [0.383]
 [0.397]
 [0.427]
 [0.389]
 [0.432]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[-0.259]
 [-0.259]
 [-0.259]
 [-0.259]
 [-0.259]
 [-0.259]
 [-0.259]] [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]]
using explorer policy with actor:  0
siam score:  -0.71967924
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.586]
 [0.569]
 [0.578]
 [0.573]
 [0.551]
 [0.578]] [[3.81 ]
 [2.032]
 [1.843]
 [1.693]
 [2.095]
 [2.233]
 [1.518]] [[0.56 ]
 [0.586]
 [0.569]
 [0.578]
 [0.573]
 [0.551]
 [0.578]]
Printing some Q and Qe and total Qs values:  [[0.64]
 [0.64]
 [0.64]
 [0.64]
 [0.64]
 [0.64]
 [0.64]] [[1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.133]] [[0.64]
 [0.64]
 [0.64]
 [0.64]
 [0.64]
 [0.64]
 [0.64]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.642]
 [0.629]
 [0.642]
 [0.642]
 [0.632]
 [0.642]] [[0.019]
 [0.016]
 [0.06 ]
 [0.016]
 [0.016]
 [0.343]
 [0.016]] [[0.637]
 [0.642]
 [0.629]
 [0.642]
 [0.642]
 [0.632]
 [0.642]]
line 256 mcts: sample exp_bonus 1.1082906305721612
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.615]
 [0.714]
 [0.69 ]
 [0.692]
 [0.74 ]
 [0.747]] [[-0.214]
 [ 1.07 ]
 [ 0.026]
 [ 0.153]
 [ 0.111]
 [-0.185]
 [-0.506]] [[0.697]
 [0.615]
 [0.714]
 [0.69 ]
 [0.692]
 [0.74 ]
 [0.747]]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.518]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[ 1.264]
 [ 0.538]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]] [[1.767]
 [1.378]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]]
first move QE:  0.672222389158368
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.739]
 [0.658]
 [0.628]
 [0.654]
 [0.66 ]
 [0.683]] [[ 2.056]
 [ 1.162]
 [ 1.068]
 [-0.125]
 [ 0.883]
 [ 1.137]
 [ 0.278]] [[0.652]
 [0.739]
 [0.658]
 [0.628]
 [0.654]
 [0.66 ]
 [0.683]]
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]] [[1.03]
 [1.03]
 [1.03]
 [1.03]
 [1.03]
 [1.03]
 [1.03]] [[0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]]
first move QE:  0.6718515601217413
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
using explorer policy with actor:  0
using explorer policy with actor:  0
siam score:  -0.7199936
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.936]
 [0.945]
 [0.934]
 [0.934]
 [0.934]
 [0.93 ]
 [0.94 ]] [[0.422]
 [1.253]
 [0.237]
 [0.124]
 [0.25 ]
 [0.222]
 [0.138]] [[0.936]
 [0.945]
 [0.934]
 [0.934]
 [0.934]
 [0.93 ]
 [0.94 ]]
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.937]
 [0.922]
 [0.929]
 [0.937]
 [0.931]
 [0.928]] [[ 1.284]
 [ 0.189]
 [ 0.443]
 [-0.031]
 [ 0.189]
 [ 0.654]
 [ 0.033]] [[0.918]
 [0.937]
 [0.922]
 [0.929]
 [0.937]
 [0.931]
 [0.928]]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.593]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.59 ]] [[3.882]
 [3.049]
 [3.53 ]
 [3.533]
 [3.525]
 [3.489]
 [3.317]] [[ 0.069]
 [-0.198]
 [-0.046]
 [-0.046]
 [-0.048]
 [-0.061]
 [-0.114]]
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]] [[2.213]
 [2.161]
 [2.161]
 [2.161]
 [2.161]
 [2.161]
 [2.161]] [[1.284]
 [1.077]
 [1.077]
 [1.077]
 [1.077]
 [1.077]
 [1.077]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[1.482]
 [1.482]
 [1.464]
 [1.482]
 [1.482]
 [1.482]
 [1.482]] [[0.672]
 [0.651]
 [0.687]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[1.805]
 [1.773]
 [1.794]
 [1.805]
 [1.805]
 [1.805]
 [1.804]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]] [[6.459]
 [2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]] [[1.607]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.589]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]] [[0.137]
 [0.224]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]] [[0.564]
 [0.589]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]]
first move QE:  0.6716655949550883
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.523]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[1.103]
 [3.415]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[-0.549]
 [ 1.172]
 [-0.396]
 [-0.396]
 [-0.396]
 [-0.396]
 [-0.396]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]] [[-0.741]
 [-0.741]
 [-0.741]
 [-0.741]
 [-0.741]
 [-0.741]
 [-0.741]] [[0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]]
rdn probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.387]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]] [[-1.211]
 [-1.5  ]
 [-1.211]
 [-1.211]
 [-1.211]
 [-1.211]
 [-1.211]] [[0.358]
 [0.387]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.362]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[-1.476]
 [-0.967]
 [-1.476]
 [-1.476]
 [-1.476]
 [-1.476]
 [-1.476]] [[0.349]
 [0.362]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.554]
 [0.556]
 [0.554]
 [0.556]
 [0.556]
 [0.556]] [[ 0.   ]
 [-1.077]
 [ 0.   ]
 [-1.546]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[1.013]
 [0.649]
 [1.013]
 [0.493]
 [1.013]
 [1.013]
 [1.013]]
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.417]
 [0.392]
 [0.392]
 [0.389]
 [0.391]
 [0.392]] [[-1.428]
 [-0.834]
 [-1.428]
 [-1.428]
 [-1.509]
 [-1.574]
 [-1.428]] [[0.392]
 [0.417]
 [0.392]
 [0.392]
 [0.389]
 [0.391]
 [0.392]]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]] [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]]
using another actor
rdn beta is 0 so we're just using the maxi policy
UNIT TEST: sample policy line 217 mcts : [0.347 0.204 0.102 0.061 0.061 0.122 0.102]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7350137303468356
3068 3555
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Printing some Q and Qe and total Qs values:  [[0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]] [[1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]] [[0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[1.914]
 [1.914]
 [1.914]
 [1.914]
 [1.914]
 [1.914]
 [1.914]] [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]] [[0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]] [[2.042]
 [2.042]
 [2.042]
 [2.042]
 [2.042]
 [2.042]
 [2.042]] [[0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.266]
 [0.24 ]
 [0.24 ]
 [0.222]
 [0.384]
 [0.276]] [[ 0.397]
 [-0.033]
 [ 0.   ]
 [ 0.   ]
 [-1.045]
 [-1.827]
 [-0.998]] [[0.163]
 [0.266]
 [0.24 ]
 [0.24 ]
 [0.222]
 [0.384]
 [0.276]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
3072 3572
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
3072 3575
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.361]
 [0.339]
 [0.338]
 [0.337]
 [0.338]
 [0.34 ]] [[2.488]
 [2.582]
 [1.961]
 [2.093]
 [2.028]
 [1.919]
 [1.838]] [[ 0.124]
 [ 0.23 ]
 [-0.227]
 [-0.141]
 [-0.186]
 [-0.256]
 [-0.308]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.848]
 [1.272]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.925]] [[0.944]
 [0.501]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.813]] [[1.13 ]
 [1.464]
 [1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.149]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.238]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[2.669]
 [1.51 ]
 [2.669]
 [2.669]
 [2.669]
 [2.669]
 [2.669]] [[ 0.199]
 [-0.209]
 [ 0.199]
 [ 0.199]
 [ 0.199]
 [ 0.199]
 [ 0.199]]
Printing some Q and Qe and total Qs values:  [[1.093]
 [1.08 ]
 [1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.093]] [[0.881]
 [1.035]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]] [[2.201]
 [2.226]
 [2.201]
 [2.201]
 [2.201]
 [2.201]
 [2.201]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
3075 3582
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.531]
 [0.484]
 [0.504]
 [0.491]
 [0.489]
 [0.499]] [[-1.947]
 [-2.082]
 [-0.628]
 [-1.935]
 [-1.871]
 [-1.749]
 [-1.96 ]] [[-0.144]
 [-0.121]
 [ 0.27 ]
 [-0.127]
 [-0.132]
 [-0.093]
 [-0.145]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
using another actor
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.647]
 [0.524]] [[1.145]
 [1.333]
 [1.333]
 [1.333]
 [1.333]
 [0.917]
 [1.333]] [[1.682]
 [1.9  ]
 [1.9  ]
 [1.9  ]
 [1.9  ]
 [1.794]
 [1.9  ]]
line 256 mcts: sample exp_bonus 1.3868619892171132
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.257]
 [0.56 ]
 [0.315]] [[1.294]
 [1.294]
 [1.294]
 [1.294]
 [0.999]
 [1.505]
 [1.294]] [[1.672]
 [1.672]
 [1.672]
 [1.672]
 [1.492]
 [1.946]
 [1.672]]
from probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
siam score:  -0.701615
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.452]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[0.641]
 [0.36 ]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]] [[-0.338]
 [-0.298]
 [-0.338]
 [-0.338]
 [-0.338]
 [-0.338]
 [-0.338]]
line 256 mcts: sample exp_bonus 1.6677282238156987
3085 3616
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
using another actor
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
using another actor
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[ 0.27 ]
 [-1.161]
 [-1.161]
 [-1.161]
 [-1.161]
 [-1.161]
 [-1.161]] [[1.507]
 [1.085]
 [1.085]
 [1.085]
 [1.085]
 [1.085]
 [1.085]]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.543]
 [0.605]
 [0.601]
 [0.441]
 [0.441]
 [0.58 ]] [[1.071]
 [0.971]
 [0.646]
 [0.889]
 [0.979]
 [0.979]
 [0.923]] [[0.68 ]
 [0.451]
 [0.25 ]
 [0.485]
 [0.255]
 [0.255]
 [0.476]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.744]
 [0.247]
 [0.563]
 [0.355]
 [0.349]
 [0.472]] [[1.332]
 [2.195]
 [0.355]
 [0.378]
 [0.771]
 [0.127]
 [0.351]] [[0.506]
 [1.576]
 [0.047]
 [0.493]
 [0.387]
 [0.082]
 [0.354]]
first move QE:  0.6530055838321226
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.264]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.447]] [[0.816]
 [0.303]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.801]] [[0.11 ]
 [0.055]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.587]]
UNIT TEST: sample policy line 217 mcts : [0.388 0.265 0.061 0.041 0.061 0.122 0.061]
line 256 mcts: sample exp_bonus 1.9782658571621061
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.657]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[-0.485]
 [-0.402]
 [-0.485]
 [-0.485]
 [-0.485]
 [-0.485]
 [-0.485]] [[0.825]
 [0.882]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
from probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.607]
 [0.604]
 [0.608]
 [0.614]
 [0.727]
 [0.64 ]] [[2.652]
 [2.284]
 [2.061]
 [2.342]
 [2.77 ]
 [1.975]
 [2.185]] [[0.418]
 [0.294]
 [0.214]
 [0.315]
 [0.471]
 [0.431]
 [0.328]]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.101]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.268]
 [0.163]] [[ 0.   ]
 [ 1.318]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.113]
 [-0.225]] [[0.28 ]
 [0.101]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.268]
 [0.163]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
siam score:  -0.69999087
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211265, 0.30408488515048776, 0.11590557512558547, 0.14207352073904306, 0.23745465617345687, 0.043919838099314164]
using another actor
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Printing some Q and Qe and total Qs values:  [[1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]] [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[2.399]
 [2.399]
 [2.399]
 [2.399]
 [2.399]
 [2.399]
 [2.399]]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.522]
 [0.557]
 [0.226]
 [0.226]
 [0.563]
 [0.226]] [[ 0.627]
 [-1.003]
 [-1.384]
 [ 0.   ]
 [ 0.   ]
 [-1.816]
 [ 0.   ]] [[1.426]
 [0.641]
 [0.416]
 [1.101]
 [1.101]
 [0.139]
 [1.101]]
3100 3654
using another actor
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
siam score:  -0.70186234
Printing some Q and Qe and total Qs values:  [[1.084]
 [1.084]
 [1.084]
 [1.084]
 [1.084]
 [1.084]
 [1.084]] [[0.647]
 [0.639]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]] [[0.949]
 [0.946]
 [0.949]
 [0.949]
 [0.949]
 [0.949]
 [0.949]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.6875847
siam score:  -0.68739206
Printing some Q and Qe and total Qs values:  [[1.264]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]] [[ 1.078]
 [-0.385]
 [-0.385]
 [-0.385]
 [-0.385]
 [-0.385]
 [-0.385]] [[2.189]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [0.97 ]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.58 ]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[-1.864]
 [-1.317]
 [-1.864]
 [-1.864]
 [-1.864]
 [-1.864]
 [-1.864]] [[0.006]
 [0.472]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[0.848]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]] [[1.503]
 [1.356]
 [1.356]
 [1.356]
 [1.356]
 [1.356]
 [1.356]]
in main func line 156:  3109
from probs:  [0.15656152471211268, 0.3040848851504878, 0.11590557512558548, 0.14207352073904306, 0.23745465617345687, 0.04391983809931417]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[1.98]
 [1.98]
 [1.98]
 [1.98]
 [1.98]
 [1.98]
 [1.98]] [[0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]]
first move QE:  0.6410568201774725
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
first move QE:  0.6408902520950097
actor:  1 policy actor:  1  step number:  73 total reward:  0.4099999999999996  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.544]
 [0.541]
 [0.541]
 [0.541]
 [0.524]
 [0.541]] [[-0.583]
 [-0.684]
 [-0.672]
 [-0.672]
 [-0.672]
 [-0.702]
 [-0.672]] [[1.077]
 [1.005]
 [1.007]
 [1.007]
 [1.007]
 [0.954]
 [1.007]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6402938601537725
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.48210462780969404
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.326]
 [0.28 ]
 [0.278]
 [0.278]
 [0.301]
 [0.281]] [[-0.904]
 [-0.746]
 [-1.301]
 [-1.331]
 [-1.259]
 [-0.787]
 [-1.271]] [[-0.118]
 [ 0.02 ]
 [-0.257]
 [-0.271]
 [-0.248]
 [-0.044]
 [-0.246]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[1.335]
 [0.696]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[ 0.157]
 [-0.073]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[2.391]
 [1.458]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.221270234884792, 0.04092635220948595]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
siam score:  -0.69122297
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
siam score:  -0.6916957
3120 3707
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.221270234884792, 0.04092635220948595]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.108]
 [1.108]
 [1.108]
 [1.108]
 [1.108]
 [1.108]
 [1.108]] [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[1.831]
 [1.831]
 [1.831]
 [1.831]
 [1.831]
 [1.831]
 [1.831]]
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[-1.31]
 [-1.31]
 [-1.31]
 [-1.31]
 [-1.31]
 [-1.31]
 [-1.31]] [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.221270234884792, 0.04092635220948595]
Printing some Q and Qe and total Qs values:  [[1.481]
 [1.481]
 [1.481]
 [1.481]
 [1.481]
 [1.481]
 [1.481]] [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[2.651]
 [2.651]
 [2.651]
 [2.651]
 [2.651]
 [2.651]
 [2.651]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.221270234884792, 0.04092635220948595]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.221270234884792, 0.04092635220948595]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.536]
 [0.486]
 [0.486]] [[-0.576]
 [-0.576]
 [-0.576]
 [-0.576]
 [ 0.379]
 [-0.576]
 [-0.576]] [[1.505]
 [1.505]
 [1.505]
 [1.505]
 [1.917]
 [1.505]
 [1.505]]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.844]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.61 ]] [[ 0.645]
 [-0.126]
 [ 0.645]
 [ 0.645]
 [ 0.645]
 [ 0.645]
 [ 0.717]] [[1.039]
 [1.275]
 [1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.088]]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.454]
 [0.417]
 [0.417]
 [0.417]
 [0.412]
 [0.417]] [[-0.473]
 [-0.521]
 [-1.066]
 [-1.066]
 [-1.066]
 [-0.401]
 [-1.066]] [[0.419]
 [0.454]
 [0.417]
 [0.417]
 [0.417]
 [0.412]
 [0.417]]
line 256 mcts: sample exp_bonus -1.2744421736626899
line 256 mcts: sample exp_bonus 3.037456317871614
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.534]
 [0.362]
 [0.438]
 [0.433]
 [0.421]
 [0.455]] [[ 1.626]
 [ 1.218]
 [ 0.376]
 [-0.288]
 [ 0.357]
 [ 0.141]
 [ 0.676]] [[ 1.16 ]
 [ 1.346]
 [ 0.303]
 [-0.151]
 [ 0.409]
 [ 0.199]
 [ 0.729]]
siam score:  -0.69191796
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.818]
 [0.814]
 [0.63 ]
 [0.63 ]
 [0.856]
 [0.63 ]] [[0.432]
 [0.494]
 [0.665]
 [0.596]
 [0.596]
 [0.654]
 [0.596]] [[0.88 ]
 [1.222]
 [1.271]
 [0.88 ]
 [0.88 ]
 [1.352]
 [0.88 ]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.221270234884792, 0.04092635220948595]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.213]
 [0.543]
 [0.545]
 [0.544]
 [0.543]
 [0.545]] [[-2.672]
 [ 0.581]
 [-2.886]
 [-2.588]
 [-2.439]
 [-1.697]
 [-2.165]] [[0.31 ]
 [1.552]
 [0.226]
 [0.353]
 [0.415]
 [0.729]
 [0.531]]
siam score:  -0.6932071
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.221270234884792, 0.04092635220948595]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.753]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]] [[0.85 ]
 [0.356]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]] [[1.577]
 [1.438]
 [1.577]
 [1.577]
 [1.577]
 [1.577]
 [1.577]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.221270234884792, 0.04092635220948595]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.221270234884792, 0.04092635220948595]
first move QE:  0.6322678408036185
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.619]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]] [[-0.13 ]
 [ 0.473]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]] [[0.931]
 [1.38 ]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.624]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[-1.615]
 [-1.242]
 [-1.02 ]
 [-1.02 ]
 [-1.02 ]
 [-1.02 ]
 [-1.02 ]] [[0.885]
 [1.077]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.6025201570589873
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
from probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.221270234884792, 0.04092635220948595]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.221270234884792, 0.04092635220948595]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
from probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.221270234884792, 0.04092635220948595]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.21404855754231664, 0.2833590844098372, 0.10800568936311308, 0.13239008159045512, 0.22127023488479203, 0.04092635220948595]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
line 256 mcts: sample exp_bonus 1.0094144146617634
siam score:  -0.6924317
actor:  1 policy actor:  1  step number:  122 total reward:  0.034999999999999254  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.4860954192656743
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6259446928252235
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.255]
 [0.412]
 [0.39 ]
 [0.358]
 [0.329]
 [0.398]] [[ 0.536]
 [ 0.735]
 [ 0.536]
 [-0.069]
 [ 0.576]
 [ 0.637]
 [ 0.502]] [[0.412]
 [0.255]
 [0.412]
 [0.39 ]
 [0.358]
 [0.329]
 [0.398]]
Printing some Q and Qe and total Qs values:  [[0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]] [[0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]]
using another actor
Printing some Q and Qe and total Qs values:  [[1.481]
 [1.481]
 [1.481]
 [1.481]
 [1.481]
 [1.481]
 [1.481]] [[0.603]
 [0.611]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[2.846]
 [2.856]
 [2.876]
 [2.876]
 [2.877]
 [2.877]
 [2.877]]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[-1.159]
 [-2.753]
 [-2.753]
 [-2.753]
 [-2.753]
 [-2.753]
 [-2.753]] [[2.116]
 [1.325]
 [1.325]
 [1.325]
 [1.325]
 [1.325]
 [1.325]]
from probs:  [0.24185527195262405, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
3140 3770
3141 3773
from probs:  [0.24185527195262402, 0.27333392927924555, 0.10418448210199126, 0.1277061622149381, 0.21344176368852683, 0.039478390762674176]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]] [[2.247]
 [2.247]
 [2.247]
 [2.247]
 [2.247]
 [2.247]
 [2.247]] [[2.157]
 [2.157]
 [2.157]
 [2.157]
 [2.157]
 [2.157]
 [2.157]]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.392]
 [0.383]
 [0.379]
 [0.381]
 [0.377]
 [0.394]] [[-2.681]
 [-1.611]
 [-2.716]
 [-3.505]
 [-3.505]
 [-2.501]
 [-2.872]] [[0.378]
 [0.392]
 [0.383]
 [0.379]
 [0.381]
 [0.377]
 [0.394]]
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.668]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[0.734]
 [1.247]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]] [[0.311]
 [0.683]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.568]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]] [[1.73 ]
 [0.262]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[2.072]
 [1.54 ]
 [1.573]
 [1.573]
 [1.573]
 [1.573]
 [1.573]]
siam score:  -0.6919553
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]] [[0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]] [[1.895]
 [1.895]
 [1.895]
 [1.895]
 [1.895]
 [1.895]
 [1.895]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
start point for exploration sampling:  10935
from probs:  [0.24185527195262405, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.553]
 [0.484]
 [0.516]
 [0.502]
 [0.474]
 [0.558]] [[ 1.102]
 [ 1.13 ]
 [ 0.357]
 [-0.234]
 [ 0.01 ]
 [ 0.207]
 [ 1.142]] [[0.392]
 [0.553]
 [0.484]
 [0.516]
 [0.502]
 [0.474]
 [0.558]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.017]
 [1.429]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[0.754]
 [0.603]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[0.072]
 [2.795]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.911]
 [0.512]
 [0.509]
 [0.507]
 [0.512]
 [0.514]] [[-1.634]
 [ 0.539]
 [-1.634]
 [-1.628]
 [-1.713]
 [-1.693]
 [-1.615]] [[0.482]
 [1.891]
 [0.481]
 [0.478]
 [0.447]
 [0.464]
 [0.492]]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.672]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[-1.418]
 [-0.947]
 [-1.418]
 [-1.418]
 [-1.418]
 [-1.418]
 [-1.418]] [[0.525]
 [0.892]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.59 ]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[1.099]
 [1.542]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]] [[0.592]
 [0.59 ]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262405, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262405, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
first move QE:  0.6182220335836786
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[1.13]
 [1.13]
 [1.13]
 [1.13]
 [1.13]
 [1.13]
 [1.13]] [[1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.839]]
line 256 mcts: sample exp_bonus 1.1917904553135665
Printing some Q and Qe and total Qs values:  [[1.459]
 [1.459]
 [1.459]
 [1.459]
 [1.459]
 [1.459]
 [1.459]] [[0.594]
 [0.618]
 [0.618]
 [0.618]
 [0.61 ]
 [0.618]
 [0.618]] [[2.892]
 [2.924]
 [2.924]
 [2.924]
 [2.914]
 [2.924]
 [2.924]]
siam score:  -0.7002536
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.6167163341863138
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262405, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262405, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.499]
 [0.443]
 [0.443]
 [0.481]
 [0.443]
 [0.443]] [[2.733]
 [2.592]
 [2.966]
 [2.966]
 [3.26 ]
 [2.966]
 [2.966]] [[0.236]
 [0.219]
 [0.357]
 [0.357]
 [0.628]
 [0.357]
 [0.357]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262405, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.48 ]
 [0.449]
 [0.449]
 [0.449]
 [0.45 ]
 [0.449]] [[-3.36 ]
 [-2.286]
 [-3.36 ]
 [-3.36 ]
 [-3.204]
 [-3.152]
 [-3.36 ]] [[0.449]
 [0.48 ]
 [0.449]
 [0.449]
 [0.449]
 [0.45 ]
 [0.449]]
using explorer policy with actor:  1
first move QE:  0.6162423002379419
using another actor
first move QE:  0.6143308331299414
Printing some Q and Qe and total Qs values:  [[ 0.115]
 [ 0.045]
 [ 0.005]
 [ 0.095]
 [ 0.057]
 [-0.001]
 [ 0.043]] [[1.467]
 [1.038]
 [1.699]
 [1.609]
 [1.343]
 [1.871]
 [1.233]] [[-0.345]
 [-0.769]
 [-0.41 ]
 [-0.291]
 [-0.542]
 [-0.307]
 [-0.644]]
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]] [[1.266]
 [1.119]
 [1.119]
 [1.119]
 [1.119]
 [1.119]
 [1.119]] [[1.004]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.608]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[0.147]
 [0.86 ]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[0.482]
 [0.608]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]]
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.51 ]
 [0.492]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]] [[2.226]
 [2.226]
 [2.319]
 [2.226]
 [2.226]
 [2.226]
 [2.226]] [[0.51 ]
 [0.51 ]
 [0.492]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]]
from probs:  [0.24185527195262405, 0.27333392927924555, 0.10418448210199126, 0.1277061622149381, 0.21344176368852683, 0.039478390762674176]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262407, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.715]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[1.114]
 [1.614]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]] [[1.552]
 [1.966]
 [1.552]
 [1.552]
 [1.552]
 [1.552]
 [1.552]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262407, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
3167 3821
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262407, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262407, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.504]
 [0.539]
 [0.552]
 [0.115]
 [0.552]
 [0.566]] [[ 0.099]
 [-1.104]
 [-1.414]
 [-1.4  ]
 [ 0.565]
 [-1.411]
 [-0.888]] [[1.007]
 [0.407]
 [0.262]
 [0.286]
 [0.95 ]
 [0.28 ]
 [0.615]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262407, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]] [[1.966]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [0.97 ]] [[2.248]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262405, 0.27333392927924555, 0.10418448210199126, 0.1277061622149381, 0.21344176368852683, 0.039478390762674176]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
siam score:  -0.68179893
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.641]
 [0.614]
 [0.641]
 [0.641]
 [0.641]
 [0.641]] [[2.005]
 [0.203]
 [0.082]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[0.693]
 [0.641]
 [0.614]
 [0.641]
 [0.641]
 [0.641]
 [0.641]]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.399]
 [0.398]
 [0.502]
 [0.417]
 [0.404]
 [0.387]] [[-2.849]
 [-1.594]
 [-2.943]
 [-2.571]
 [-2.769]
 [-2.598]
 [-2.693]] [[0.373]
 [0.399]
 [0.398]
 [0.502]
 [0.417]
 [0.404]
 [0.387]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
from probs:  [0.24185527195262407, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.789]
 [0.792]
 [0.791]
 [0.793]
 [0.792]
 [0.792]] [[0.33 ]
 [0.527]
 [0.313]
 [0.333]
 [0.322]
 [0.326]
 [0.358]] [[0.837]
 [0.789]
 [0.792]
 [0.791]
 [0.793]
 [0.792]
 [0.792]]
first move QE:  0.6058039669365737
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262407, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[-3.738]
 [-3.188]
 [-3.188]
 [-3.188]
 [-3.188]
 [-3.188]
 [-3.188]] [[0.237]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.67844796
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
siam score:  -0.6781601
using another actor
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.642]
 [0.625]
 [0.606]
 [0.612]
 [0.603]
 [0.605]] [[-0.051]
 [ 2.612]
 [ 1.128]
 [-0.029]
 [ 1.676]
 [ 0.232]
 [ 1.419]] [[0.606]
 [0.642]
 [0.625]
 [0.606]
 [0.612]
 [0.603]
 [0.605]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262407, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.367]
 [0.41 ]
 [0.324]
 [0.317]
 [0.41 ]
 [0.41 ]] [[2.303]
 [2.591]
 [2.303]
 [1.042]
 [1.386]
 [2.303]
 [2.303]] [[ 0.741]
 [ 0.847]
 [ 0.741]
 [-0.271]
 [-0.057]
 [ 0.741]
 [ 0.741]]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[3.771]
 [3.771]
 [3.771]
 [3.771]
 [3.771]
 [3.771]
 [3.771]] [[1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]]
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.351]
 [0.343]
 [0.369]
 [0.369]
 [0.369]
 [0.369]] [[2.481]
 [3.258]
 [2.95 ]
 [2.481]
 [2.481]
 [2.481]
 [2.481]] [[0.44 ]
 [0.664]
 [0.544]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262407, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262407, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262407, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.24185527195262407, 0.2733339292792456, 0.10418448210199127, 0.12770616221493813, 0.21344176368852683, 0.039478390762674176]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
actor:  1 policy actor:  1  step number:  111 total reward:  0.04999999999999927  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958933, 0.29886583058322114, 0.10052389022683263, 0.12321912028335703, 0.20594234371528286, 0.038091290941717044]
siam score:  -0.6955728
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.683]
 [0.706]
 [0.715]
 [0.713]
 [0.71 ]
 [0.728]] [[2.782]
 [4.419]
 [2.717]
 [1.787]
 [1.852]
 [2.39 ]
 [2.107]] [[0.765]
 [0.683]
 [0.706]
 [0.715]
 [0.713]
 [0.71 ]
 [0.728]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958936, 0.2988658305832212, 0.10052389022683263, 0.12321912028335705, 0.20594234371528286, 0.03809129094171705]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.591]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[1.878]
 [2.158]
 [1.878]
 [1.878]
 [1.878]
 [1.878]
 [1.878]] [[1.034]
 [1.267]
 [1.034]
 [1.034]
 [1.034]
 [1.034]
 [1.034]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958936, 0.2988658305832212, 0.10052389022683263, 0.12321912028335705, 0.20594234371528286, 0.03809129094171705]
using explorer policy with actor:  1
start point for exploration sampling:  10935
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.578]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[ 0.686]
 [ 0.478]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]] [[2.036]
 [2.168]
 [1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.755]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.481]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.479]] [[0.309]
 [0.906]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [1.419]] [[0.473]
 [0.481]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.479]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958933, 0.29886583058322114, 0.10052389022683263, 0.12321912028335703, 0.20594234371528286, 0.038091290941717044]
Printing some Q and Qe and total Qs values:  [[0.794]
 [1.026]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]] [[0.411]
 [3.849]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[0.683]
 [1.753]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
3189 3887
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958933, 0.29886583058322114, 0.10052389022683263, 0.12321912028335703, 0.20594234371528286, 0.038091290941717044]
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.471]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[0.752]
 [1.007]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[1.765]
 [2.121]
 [1.765]
 [1.765]
 [1.765]
 [1.765]
 [1.765]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958933, 0.29886583058322114, 0.10052389022683263, 0.12321912028335703, 0.20594234371528286, 0.038091290941717044]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.466]
 [0.487]
 [0.512]
 [0.513]
 [0.512]
 [0.487]] [[-1.775]
 [-0.595]
 [ 0.   ]
 [-2.89 ]
 [-2.765]
 [-2.492]
 [ 0.   ]] [[0.994]
 [1.583]
 [1.957]
 [0.355]
 [0.43 ]
 [0.582]
 [1.957]]
siam score:  -0.6904622
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
using explorer policy with actor:  1
siam score:  -0.6895285
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958936, 0.2988658305832212, 0.10052389022683263, 0.12321912028335705, 0.20594234371528286, 0.03809129094171705]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958936, 0.2988658305832212, 0.10052389022683263, 0.12321912028335705, 0.20594234371528286, 0.03809129094171705]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.301]
 [0.114]
 [0.303]
 [0.318]
 [0.62 ]
 [0.228]] [[1.167]
 [1.49 ]
 [1.576]
 [1.136]
 [1.657]
 [1.919]
 [1.248]] [[0.329]
 [0.301]
 [0.114]
 [0.303]
 [0.318]
 [0.62 ]
 [0.228]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958936, 0.2988658305832212, 0.10052389022683263, 0.12321912028335705, 0.20594234371528286, 0.03809129094171705]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958936, 0.2988658305832212, 0.10052389022683263, 0.12321912028335705, 0.20594234371528286, 0.03809129094171705]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.8  ]
 [0.839]
 [0.84 ]
 [0.712]
 [0.84 ]
 [0.731]] [[2.972]
 [3.352]
 [2.669]
 [2.813]
 [2.851]
 [2.813]
 [2.872]] [[1.174]
 [1.359]
 [0.981]
 [1.079]
 [0.849]
 [1.079]
 [0.9  ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.151]
 [0.164]
 [0.14 ]
 [0.101]
 [0.113]
 [0.179]] [[2.09 ]
 [2.935]
 [2.685]
 [3.336]
 [3.55 ]
 [2.749]
 [3.07 ]] [[-0.553]
 [-0.154]
 [-0.212]
 [-0.041]
 [-0.049]
 [-0.293]
 [-0.052]]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.423]
 [0.65 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]] [[3.022]
 [3.116]
 [3.367]
 [3.022]
 [3.022]
 [3.022]
 [3.022]] [[0.311]
 [0.349]
 [0.886]
 [0.311]
 [0.311]
 [0.311]
 [0.311]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.498]
 [0.513]
 [0.498]
 [0.515]
 [0.513]
 [0.498]] [[5.211]
 [4.653]
 [5.053]
 [4.653]
 [5.03 ]
 [4.917]
 [4.653]] [[ 0.17 ]
 [-0.048]
 [ 0.117]
 [-0.048]
 [ 0.113]
 [ 0.07 ]
 [-0.048]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958933, 0.29886583058322114, 0.10052389022683263, 0.12321912028335703, 0.20594234371528286, 0.038091290941717044]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
main train batch thing paused
add a thread
Adding thread: now have 4 threads
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958933, 0.29886583058322114, 0.10052389022683263, 0.12321912028335703, 0.20594234371528286, 0.038091290941717044]
Printing some Q and Qe and total Qs values:  [[1.272]
 [0.533]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]] [[0.79 ]
 [0.588]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]] [[1.492]
 [0.484]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]]
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -3.0939878187503713
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.438]
 [0.426]
 [0.427]
 [0.427]
 [0.429]
 [0.426]] [[-0.359]
 [-0.304]
 [-0.561]
 [-0.951]
 [-1.032]
 [-0.894]
 [-0.744]] [[0.425]
 [0.438]
 [0.426]
 [0.427]
 [0.427]
 [0.429]
 [0.426]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958933, 0.29886583058322114, 0.10052389022683263, 0.12321912028335703, 0.20594234371528286, 0.038091290941717044]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.501]
 [0.546]
 [0.517]
 [0.501]
 [0.482]
 [0.475]] [[-1.428]
 [-1.094]
 [-1.538]
 [-2.229]
 [-2.361]
 [-2.152]
 [-2.333]] [[0.484]
 [0.501]
 [0.546]
 [0.517]
 [0.501]
 [0.482]
 [0.475]]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.609]
 [0.563]
 [0.563]
 [0.563]
 [0.554]
 [0.563]] [[0.429]
 [0.777]
 [0.011]
 [0.011]
 [0.011]
 [0.234]
 [0.011]] [[0.566]
 [0.609]
 [0.563]
 [0.563]
 [0.563]
 [0.554]
 [0.563]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.025]
 [0.343]
 [0.265]
 [0.111]
 [0.43 ]
 [0.265]] [[ 0.841]
 [ 0.835]
 [-2.57 ]
 [-1.915]
 [ 0.499]
 [-1.764]
 [-1.988]] [[0.1  ]
 [0.025]
 [0.343]
 [0.265]
 [0.111]
 [0.43 ]
 [0.265]]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.748]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]] [[0.694]
 [0.483]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]] [[0.642]
 [0.748]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]]
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.658]
 [0.684]
 [0.691]
 [0.69 ]
 [0.677]
 [0.685]] [[-0.86 ]
 [-0.235]
 [-1.271]
 [-1.409]
 [-1.435]
 [-1.606]
 [-1.234]] [[0.698]
 [0.658]
 [0.684]
 [0.691]
 [0.69 ]
 [0.677]
 [0.685]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.575]
 [0.626]
 [0.628]
 [0.621]
 [0.586]
 [0.568]] [[1.645]
 [0.996]
 [1.245]
 [1.337]
 [1.64 ]
 [1.617]
 [1.595]] [[0.634]
 [0.575]
 [0.626]
 [0.628]
 [0.621]
 [0.586]
 [0.568]]
Printing some Q and Qe and total Qs values:  [[0.885]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.935]
 [0.899]] [[ 0.606]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [ 0.479]
 [-0.059]] [[0.885]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.935]
 [0.899]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.953]
 [0.909]
 [0.959]
 [0.931]
 [0.909]
 [0.909]] [[0.181]
 [1.1  ]
 [0.181]
 [0.309]
 [0.491]
 [0.181]
 [0.181]] [[0.909]
 [0.953]
 [0.909]
 [0.959]
 [0.931]
 [0.909]
 [0.909]]
siam score:  -0.7015585
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958933, 0.29886583058322114, 0.10052389022683263, 0.12321912028335703, 0.20594234371528286, 0.038091290941717044]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.628]] [[1.275]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [1.099]] [[0.632]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.628]]
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99749 -1.0 -0.99749
probs:  [0.23335752424958933, 0.29886583058322114, 0.10052389022683263, 0.12321912028335703, 0.20594234371528286, 0.038091290941717044]
Printing some Q and Qe and total Qs values:  [[0.903]
 [0.914]
 [0.97 ]
 [0.961]
 [0.926]
 [0.97 ]
 [0.97 ]] [[2.899]
 [3.021]
 [4.422]
 [3.255]
 [3.541]
 [4.422]
 [4.422]] [[0.903]
 [0.914]
 [0.97 ]
 [0.961]
 [0.926]
 [0.97 ]
 [0.97 ]]
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.861]
 [0.862]
 [0.861]
 [0.861]
 [0.864]
 [0.861]] [[-0.918]
 [-1.402]
 [-0.781]
 [-1.402]
 [-0.75 ]
 [-0.753]
 [-1.402]] [[0.867]
 [0.861]
 [0.862]
 [0.861]
 [0.861]
 [0.864]
 [0.861]]
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[3.041]
 [3.041]
 [3.041]
 [3.041]
 [3.041]
 [3.041]
 [3.041]] [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.953]
 [0.908]
 [0.905]
 [0.902]
 [0.904]
 [0.914]] [[-1.378]
 [ 0.855]
 [-1.516]
 [-1.55 ]
 [-1.643]
 [-1.572]
 [-1.618]] [[0.909]
 [0.953]
 [0.908]
 [0.905]
 [0.902]
 [0.904]
 [0.914]]
using explorer policy with actor:  0
siam score:  -0.7074841
actor:  0 policy actor:  1  step number:  98 total reward:  0.1949999999999994  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.864]
 [0.87 ]
 [0.864]
 [0.864]
 [0.868]
 [0.864]] [[-0.739]
 [-1.169]
 [-0.806]
 [-1.169]
 [-1.169]
 [-0.718]
 [-1.169]] [[0.877]
 [0.864]
 [0.87 ]
 [0.864]
 [0.864]
 [0.868]
 [0.864]]
maxi score, test score, baseline:  -0.9951000000000001 -1.0 -0.9951000000000001
using explorer policy with actor:  0
main train batch thing paused
add a thread
Adding thread: now have 5 threads
actor:  0 policy actor:  1  step number:  111 total reward:  0.26999999999999946  reward:  1.0 rdn_beta:  1
main train batch thing paused
add a thread
Adding thread: now have 6 threads
actor:  0 policy actor:  1  step number:  120 total reward:  0.2149999999999994  reward:  1.0 rdn_beta:  1
actor:  1 policy actor:  1  step number:  76 total reward:  0.38499999999999956  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  0
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.694]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]] [[0.242]
 [1.224]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[0.552]
 [1.039]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]]
from probs:  [0.2775832230308057, 0.2816728919461136, 0.09474653044283009, 0.11599281443293372, 0.194160325560637, 0.03584421458667987]
maxi score, test score, baseline:  -0.9901300000000001 -1.0 -0.9901300000000001
siam score:  -0.7097185
maxi score, test score, baseline:  -0.9901300000000001 -1.0 -0.9901300000000001
using explorer policy with actor:  0
using explorer policy with actor:  0
rdn probs:  [0.2775832230308057, 0.2816728919461136, 0.09474653044283009, 0.11599281443293372, 0.194160325560637, 0.03584421458667987]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.2777992082768562, 0.28282844472850754, 0.09528974796686578, 0.11269558646651452, 0.19689366728889932, 0.03449334527235662]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.2777992082768562, 0.28282844472850754, 0.09528974796686578, 0.11269558646651452, 0.19689366728889932, 0.03449334527235662]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[0.978]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[-0.117]
 [-2.063]
 [-2.063]
 [-2.063]
 [-2.063]
 [-2.063]
 [-2.063]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.2777992082768562, 0.28282844472850754, 0.09528974796686578, 0.11269558646651452, 0.19689366728889932, 0.03449334527235662]
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.091]
 [0.091]
 [0.091]
 [0.091]
 [0.091]
 [0.091]] [[2.484]
 [2.484]
 [2.484]
 [2.484]
 [2.484]
 [2.484]
 [2.484]] [[-0.378]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.378]]
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.345]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]] [[0.902]
 [1.026]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]] [[0.3  ]
 [0.455]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]]
start point for exploration sampling:  10935
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.2777992082768562, 0.2828284447285076, 0.09528974796686578, 0.11269558646651452, 0.19689366728889932, 0.03449334527235662]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.175]
 [0.31 ]
 [0.295]
 [0.302]
 [0.315]
 [0.326]] [[1.697]
 [2.043]
 [1.859]
 [1.829]
 [1.902]
 [1.892]
 [1.861]] [[-0.636]
 [-0.807]
 [-0.599]
 [-0.639]
 [-0.6  ]
 [-0.578]
 [-0.567]]
start point for exploration sampling:  10935
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
actor:  1 policy actor:  1  step number:  90 total reward:  0.0849999999999993  reward:  1.0 rdn_beta:  0.167
from probs:  [0.2777992082768562, 0.28282844472850754, 0.09528974796686578, 0.11269558646651452, 0.19689366728889932, 0.03449334527235662]
actor:  1 policy actor:  1  step number:  89 total reward:  0.26999999999999946  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]] [[-3.404]
 [-2.016]
 [-2.016]
 [-2.016]
 [-2.016]
 [-2.016]
 [-2.016]] [[0.491]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]]
3229 3946
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3326892161500389, 0.26133240687334686, 0.08804736457975376, 0.10413029313076412, 0.18192899946863678, 0.03187171979745978]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]] [[2.805]
 [2.805]
 [2.805]
 [2.805]
 [2.805]
 [2.805]
 [2.805]] [[0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]]
from probs:  [0.3326892161500389, 0.26133240687334686, 0.08804736457975376, 0.10413029313076412, 0.18192899946863678, 0.03187171979745978]
actor:  1 policy actor:  1  step number:  99 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
Printing some Q and Qe and total Qs values:  [[-0.057]
 [ 1.194]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]] [[1.704]
 [0.556]
 [1.704]
 [1.704]
 [1.704]
 [1.704]
 [1.704]] [[0.64 ]
 [1.609]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.294]
 [0.268]
 [0.265]
 [0.275]
 [0.265]
 [0.264]] [[-0.758]
 [-0.648]
 [-0.604]
 [-0.874]
 [-0.798]
 [-1.05 ]
 [-1.121]] [[ 0.01 ]
 [ 0.081]
 [ 0.044]
 [-0.052]
 [-0.007]
 [-0.111]
 [-0.137]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.024]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]] [[2.623]
 [2.624]
 [2.623]
 [2.623]
 [2.623]
 [2.623]
 [2.623]] [[-1.006]
 [-0.959]
 [-1.006]
 [-1.006]
 [-1.006]
 [-1.006]
 [-1.006]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
from probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
siam score:  -0.678232
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.216]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[-1.548]
 [-1.046]
 [-1.548]
 [-1.548]
 [-1.548]
 [-1.548]
 [-1.548]] [[0.191]
 [0.216]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.231]
 [0.187]
 [0.219]
 [0.185]
 [0.19 ]
 [0.187]] [[-1.133]
 [-0.737]
 [-1.345]
 [ 0.   ]
 [-1.341]
 [-1.138]
 [-1.255]] [[0.19 ]
 [0.231]
 [0.187]
 [0.219]
 [0.185]
 [0.19 ]
 [0.187]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
from probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
using another actor
start point for exploration sampling:  10935
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.049]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[1.098]
 [1.73 ]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]] [[-0.827]
 [-0.552]
 [-0.827]
 [-0.827]
 [-0.827]
 [-0.827]
 [-0.827]]
Printing some Q and Qe and total Qs values:  [[ 0.061]
 [-0.006]
 [ 0.061]
 [ 0.061]
 [ 0.061]
 [ 0.061]
 [ 0.061]] [[1.014]
 [1.119]
 [1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]] [[-1.19 ]
 [-1.289]
 [-1.19 ]
 [-1.19 ]
 [-1.19 ]
 [-1.19 ]
 [-1.19 ]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
siam score:  -0.6818862
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
line 256 mcts: sample exp_bonus -0.1872177057109881
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.124]
 [0.335]
 [0.306]
 [0.306]
 [0.046]
 [0.149]] [[1.171]
 [1.352]
 [0.754]
 [1.839]
 [1.839]
 [1.46 ]
 [1.326]] [[1.169]
 [1.27 ]
 [1.124]
 [1.666]
 [1.666]
 [1.266]
 [1.276]]
line 256 mcts: sample exp_bonus 2.6618624004670335
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.226]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[0.045]
 [0.411]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]] [[0.246]
 [0.226]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.401]
 [0.386]
 [0.35 ]
 [0.39 ]
 [0.457]
 [0.373]] [[2.271]
 [2.091]
 [2.084]
 [2.198]
 [2.705]
 [1.966]
 [2.035]] [[-0.35 ]
 [-0.383]
 [-0.414]
 [-0.447]
 [-0.198]
 [-0.311]
 [-0.456]]
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.194]
 [0.272]
 [0.232]
 [0.232]
 [0.232]
 [0.275]] [[-0.579]
 [ 0.625]
 [-1.59 ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-1.819]] [[0.232]
 [0.194]
 [0.272]
 [0.232]
 [0.232]
 [0.232]
 [0.275]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.527]
 [0.525]
 [0.509]
 [0.515]
 [0.514]
 [0.517]] [[2.436]
 [2.738]
 [2.594]
 [2.459]
 [2.663]
 [2.486]
 [2.359]] [[0.508]
 [0.527]
 [0.525]
 [0.509]
 [0.515]
 [0.514]
 [0.517]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
3266 4019
Printing some Q and Qe and total Qs values:  [[0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]] [[3.169]
 [3.169]
 [3.169]
 [3.169]
 [3.169]
 [3.169]
 [3.169]] [[0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.755 0.143 0.02  0.02  0.02  0.02 ]
siam score:  -0.67503273
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.249]
 [0.111]
 [0.112]
 [0.127]
 [0.133]
 [0.107]] [[-1.03 ]
 [-0.619]
 [-0.97 ]
 [-0.594]
 [-0.671]
 [-0.637]
 [-1.012]] [[-0.327]
 [ 0.055]
 [-0.338]
 [-0.211]
 [-0.208]
 [-0.184]
 [-0.362]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.304]
 [0.262]
 [0.264]
 [0.251]
 [0.234]
 [0.29 ]] [[0.744]
 [1.299]
 [0.544]
 [0.427]
 [0.426]
 [0.412]
 [0.061]] [[-0.714]
 [-0.362]
 [-0.698]
 [-0.734]
 [-0.758]
 [-0.798]
 [-0.803]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
3269 4033
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.169]
 [0.132]
 [0.122]
 [0.125]
 [0.129]
 [0.133]] [[3.704]
 [3.426]
 [3.682]
 [3.432]
 [3.39 ]
 [3.823]
 [3.161]] [[-0.72 ]
 [-0.757]
 [-0.745]
 [-0.847]
 [-0.855]
 [-0.704]
 [-0.917]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.035]
 [-0.026]
 [-0.021]
 [-0.024]
 [-0.02 ]
 [-0.02 ]] [[1.062]
 [0.967]
 [0.843]
 [0.881]
 [0.749]
 [0.87 ]
 [0.906]] [[-1.333]
 [-1.381]
 [-1.404]
 [-1.382]
 [-1.432]
 [-1.383]
 [-1.372]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.127]
 [0.284]
 [0.295]
 [0.3  ]
 [0.31 ]
 [0.306]] [[ 0.515]
 [ 0.944]
 [-0.783]
 [-0.911]
 [-0.974]
 [-1.006]
 [-0.919]] [[1.082]
 [1.191]
 [0.382]
 [0.321]
 [0.291]
 [0.284]
 [0.329]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
using another actor
from probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
line 256 mcts: sample exp_bonus -2.5511922819911566
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[-1.627]
 [-1.627]
 [-1.627]
 [-1.627]
 [-1.627]
 [-1.627]
 [-1.627]] [[0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]]
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.121]
 [0.13 ]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[1.325]
 [1.416]
 [1.653]
 [0.908]
 [0.908]
 [0.908]
 [0.908]] [[-1.117]
 [-1.024]
 [-0.847]
 [-1.265]
 [-1.265]
 [-1.265]
 [-1.265]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.588]
 [0.581]
 [0.563]
 [0.601]
 [0.609]
 [0.599]] [[ 0.296]
 [ 0.293]
 [ 0.117]
 [-0.328]
 [ 0.717]
 [ 0.147]
 [ 0.244]] [[0.567]
 [0.588]
 [0.581]
 [0.563]
 [0.601]
 [0.609]
 [0.599]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.041]
 [ 0.243]
 [ 0.243]
 [-0.034]
 [ 0.243]
 [ 0.243]] [[0.269]
 [0.598]
 [0.261]
 [0.261]
 [0.411]
 [0.261]
 [0.261]] [[1.573]
 [1.682]
 [1.665]
 [1.665]
 [1.623]
 [1.665]
 [1.665]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.7937744250951613
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.3533977458515802, 0.253222527577692, 0.08531500732033287, 0.10089883738283259, 0.1762832311396252, 0.030882650727937142]
actor:  1 policy actor:  1  step number:  105 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
from probs:  [0.34230608722903766, 0.2766607098760932, 0.08263733054484412, 0.09773205017833292, 0.17075044706380188, 0.029913375107890168]
line 256 mcts: sample exp_bonus 1.4823092573556282
using another actor
from probs:  [0.34230608722903766, 0.2766607098760932, 0.08263733054484412, 0.09773205017833292, 0.17075044706380188, 0.029913375107890168]
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.724]
 [0.701]
 [0.701]
 [0.692]
 [0.625]
 [0.64 ]] [[ 0.667]
 [ 2.099]
 [ 1.243]
 [ 1.243]
 [ 1.079]
 [-0.043]
 [-0.09 ]] [[0.667]
 [0.724]
 [0.701]
 [0.701]
 [0.692]
 [0.625]
 [0.64 ]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.34230608722903766, 0.2766607098760932, 0.08263733054484412, 0.09773205017833292, 0.17075044706380188, 0.029913375107890168]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
siam score:  -0.67772937
from probs:  [0.34230608722903766, 0.2766607098760932, 0.08263733054484412, 0.09773205017833292, 0.17075044706380188, 0.029913375107890168]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.387]
 [0.291]
 [0.327]
 [0.311]
 [0.268]
 [0.221]] [[3.189]
 [3.041]
 [3.636]
 [3.399]
 [3.86 ]
 [3.783]
 [2.746]] [[0.946]
 [1.063]
 [1.221]
 [1.155]
 [1.368]
 [1.262]
 [0.656]]
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.731]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]] [[1.348]
 [1.795]
 [1.348]
 [1.348]
 [1.348]
 [1.348]
 [1.348]] [[0.709]
 [0.731]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.47 ]
 [0.453]
 [0.414]
 [0.758]
 [0.453]
 [0.456]] [[ 1.127]
 [ 2.274]
 [ 0.563]
 [-0.31 ]
 [ 0.64 ]
 [ 0.563]
 [ 1.029]] [[0.384]
 [0.47 ]
 [0.453]
 [0.414]
 [0.758]
 [0.453]
 [0.456]]
line 256 mcts: sample exp_bonus 2.56976200028409
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[-0.659]
 [-0.659]
 [-0.659]
 [-0.659]
 [-0.659]
 [-0.659]
 [-0.659]] [[0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.57 ]
 [0.532]
 [0.53 ]
 [0.527]
 [0.525]
 [0.535]] [[0.648]
 [0.667]
 [0.425]
 [0.397]
 [0.371]
 [0.273]
 [0.486]] [[0.53 ]
 [0.57 ]
 [0.532]
 [0.53 ]
 [0.527]
 [0.525]
 [0.535]]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.34 ]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[3.304]
 [3.07 ]
 [3.185]
 [3.185]
 [3.185]
 [3.185]
 [3.185]] [[-0.075]
 [-0.04 ]
 [-0.199]
 [-0.199]
 [-0.199]
 [-0.199]
 [-0.199]]
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.34230608722903766, 0.2766607098760932, 0.08263733054484412, 0.09773205017833292, 0.17075044706380188, 0.029913375107890168]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8207341496221918
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
probs:  [0.34230608722903766, 0.2766607098760932, 0.08263733054484412, 0.09773205017833292, 0.17075044706380188, 0.029913375107890168]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]] [[4.03]
 [4.03]
 [4.03]
 [4.03]
 [4.03]
 [4.03]
 [4.03]] [[0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901300000000001 -0.8160000000000001 -0.8160000000000001
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  116 total reward:  0.06499999999999928  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9248248724349566
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.34230608722903766, 0.2766607098760932, 0.08263733054484412, 0.09773205017833292, 0.17075044706380188, 0.029913375107890168]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.508]
 [0.454]
 [0.461]
 [0.441]
 [0.445]
 [0.435]] [[-0.248]
 [ 0.928]
 [-0.435]
 [-0.47 ]
 [-0.411]
 [-0.5  ]
 [-0.299]] [[0.432]
 [0.508]
 [0.454]
 [0.461]
 [0.441]
 [0.445]
 [0.435]]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]]
siam score:  -0.6759198
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
using another actor
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[3.575]
 [3.575]
 [3.575]
 [3.575]
 [3.575]
 [3.575]
 [3.575]] [[1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.34230608722903766, 0.2766607098760932, 0.08263733054484412, 0.09773205017833292, 0.17075044706380188, 0.029913375107890168]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.6608993001624137
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.34230608722903766, 0.2766607098760932, 0.08263733054484412, 0.09773205017833292, 0.17075044706380188, 0.029913375107890168]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5663608464759835
from probs:  [0.34230608722903766, 0.2766607098760932, 0.08263733054484412, 0.09773205017833292, 0.17075044706380188, 0.029913375107890168]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.34230608722903766, 0.2766607098760932, 0.08263733054484412, 0.09773205017833292, 0.17075044706380188, 0.029913375107890168]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.385]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[0.015]
 [0.025]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[0.31 ]
 [0.407]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
actor:  1 policy actor:  1  step number:  86 total reward:  0.24499999999999944  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.202]
 [0.217]
 [0.221]
 [0.202]
 [0.202]
 [0.238]
 [0.171]] [[2.647]
 [2.6  ]
 [2.81 ]
 [2.647]
 [2.647]
 [2.778]
 [2.304]] [[0.309]
 [0.324]
 [0.401]
 [0.309]
 [0.309]
 [0.424]
 [0.132]]
siam score:  -0.69073045
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.49 ]
 [0.499]
 [0.502]
 [0.49 ]
 [0.492]
 [0.493]] [[ 0.577]
 [ 0.967]
 [ 0.086]
 [-0.001]
 [-0.06 ]
 [ 0.109]
 [ 0.09 ]] [[0.483]
 [0.49 ]
 [0.499]
 [0.502]
 [0.49 ]
 [0.492]
 [0.493]]
Printing some Q and Qe and total Qs values:  [[0.816]
 [1.132]
 [0.816]
 [0.866]
 [0.816]
 [0.816]
 [0.816]] [[0.807]
 [0.846]
 [0.807]
 [0.663]
 [0.807]
 [0.807]
 [0.807]] [[1.718]
 [2.115]
 [1.718]
 [1.72 ]
 [1.718]
 [1.718]
 [1.718]]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[1.492]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]] [[1.805]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
using explorer policy with actor:  0
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
siam score:  -0.69530916
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.3678016807576928, 0.2659359200196202, 0.0794338832436705, 0.09394345402792786, 0.16413128287717685, 0.028753779073911745]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
from probs:  [0.3678016807576928, 0.2659359200196202, 0.0794338832436705, 0.09394345402792786, 0.16413128287717685, 0.028753779073911745]
in main func line 156:  3322
from probs:  [0.3678016807576928, 0.2659359200196202, 0.0794338832436705, 0.09394345402792786, 0.16413128287717685, 0.028753779073911745]
using explorer policy with actor:  1
from probs:  [0.3678016807576928, 0.2659359200196202, 0.0794338832436705, 0.09394345402792786, 0.16413128287717685, 0.028753779073911745]
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.842]
 [0.907]
 [0.899]
 [0.9  ]
 [0.907]
 [0.907]] [[4.151]
 [2.531]
 [4.588]
 [3.85 ]
 [3.941]
 [4.588]
 [4.588]] [[1.431]
 [0.263]
 [1.761]
 [1.255]
 [1.316]
 [1.761]
 [1.761]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.597691873693263
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.3678016807576928, 0.2659359200196202, 0.0794338832436705, 0.09394345402792786, 0.16413128287717685, 0.028753779073911745]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.095669473087374
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]] [[-6.193]
 [-3.992]
 [-3.992]
 [-3.992]
 [-3.992]
 [-3.992]
 [-3.992]] [[0.391]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.262]
 [0.277]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]] [[2.857]
 [2.995]
 [3.197]
 [2.857]
 [2.857]
 [2.857]
 [2.857]] [[0.472]
 [0.572]
 [0.805]
 [0.472]
 [0.472]
 [0.472]
 [0.472]]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.479]
 [0.428]
 [0.418]
 [0.406]
 [0.409]
 [0.419]] [[-2.708]
 [-1.891]
 [-3.04 ]
 [-3.175]
 [-3.158]
 [-3.048]
 [-2.971]] [[0.41 ]
 [0.479]
 [0.428]
 [0.418]
 [0.406]
 [0.409]
 [0.419]]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.23 ]
 [0.244]] [[-5.362]
 [-3.772]
 [-3.772]
 [-3.772]
 [-3.772]
 [-4.078]
 [-3.772]] [[0.512]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.23 ]
 [0.244]]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.342]
 [0.221]
 [0.098]
 [0.104]
 [0.247]
 [0.297]] [[2.053]
 [1.237]
 [1.433]
 [1.899]
 [1.81 ]
 [1.621]
 [1.541]] [[-0.226]
 [-0.135]
 [-0.246]
 [-0.183]
 [-0.229]
 [-0.07 ]
 [-0.022]]
actor:  1 policy actor:  1  step number:  108 total reward:  0.12499999999999933  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
3332 4135
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.392]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[-0.66 ]
 [ 0.261]
 [-0.66 ]
 [-0.66 ]
 [-0.66 ]
 [-0.66 ]
 [-0.66 ]] [[0.432]
 [0.958]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]]
from probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.317]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[1.69 ]
 [1.696]
 [1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]] [[1.359]
 [1.467]
 [1.359]
 [1.359]
 [1.359]
 [1.359]
 [1.359]]
from probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.303]
 [0.284]
 [0.297]
 [0.297]
 [0.297]
 [0.288]] [[-0.679]
 [-0.612]
 [-0.87 ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.79 ]] [[0.224]
 [0.292]
 [0.168]
 [0.484]
 [0.484]
 [0.484]
 [0.203]]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.69367903
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.45 ]
 [0.471]
 [0.471]
 [0.473]
 [0.472]
 [0.479]] [[ 0.343]
 [ 0.441]
 [ 0.47 ]
 [ 0.184]
 [ 0.154]
 [ 0.072]
 [-0.137]] [[0.473]
 [0.45 ]
 [0.471]
 [0.471]
 [0.473]
 [0.472]
 [0.479]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.232]
 [0.142]
 [0.145]
 [0.146]
 [0.148]
 [0.15 ]] [[1.442]
 [1.321]
 [1.543]
 [1.535]
 [1.563]
 [1.633]
 [1.644]] [[-0.884]
 [-0.743]
 [-0.847]
 [-0.844]
 [-0.833]
 [-0.806]
 [-0.799]]
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.409]
 [0.355]
 [0.352]
 [0.352]
 [0.354]
 [0.357]] [[-0.884]
 [-1.021]
 [-0.799]
 [-1.126]
 [-1.126]
 [-0.923]
 [-0.877]] [[0.355]
 [0.409]
 [0.355]
 [0.352]
 [0.352]
 [0.354]
 [0.357]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 1.5167124813097845
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
3343 4151
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.014]
 [-0.015]
 [-0.014]
 [-0.014]
 [-0.016]
 [-0.014]] [[-0.416]
 [-0.416]
 [-0.676]
 [-0.416]
 [-0.416]
 [-0.304]
 [-0.416]] [[-0.645]
 [-0.645]
 [-0.993]
 [-0.645]
 [-0.645]
 [-0.5  ]
 [-0.645]]
using explorer policy with actor:  1
3347 4156
using explorer policy with actor:  1
using explorer policy with actor:  1
3348 4160
from probs:  [0.35691842704243004, 0.2876568801012318, 0.07708343421051625, 0.09116366671709275, 0.1592746373325343, 0.027902954596195028]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.502]
 [0.507]
 [0.497]
 [0.5  ]
 [0.502]
 [0.502]] [[-0.852]
 [ 0.45 ]
 [-0.232]
 [-1.32 ]
 [-0.358]
 [-0.105]
 [-0.241]] [[0.508]
 [0.502]
 [0.507]
 [0.497]
 [0.5  ]
 [0.502]
 [0.502]]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.543]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[-3.483]
 [-1.295]
 [-3.658]
 [-3.658]
 [-3.658]
 [-3.658]
 [-3.658]] [[0.497]
 [0.543]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
siam score:  -0.67482054
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.35691842704243004, 0.2876568801012318, 0.07708343421051625, 0.09116366671709275, 0.1592746373325343, 0.027902954596195028]
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[3.131]
 [3.131]
 [3.131]
 [3.131]
 [3.131]
 [3.131]
 [3.131]] [[-0.453]
 [-0.453]
 [-0.453]
 [-0.453]
 [-0.453]
 [-0.453]
 [-0.453]]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
rdn beta is 0 so we're just using the maxi policy
3354 4171
using explorer policy with actor:  1
siam score:  -0.6745464
using another actor
from probs:  [0.35691842704243004, 0.2876568801012318, 0.07708343421051625, 0.09116366671709275, 0.1592746373325343, 0.027902954596195028]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.373]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]] [[-1.123]
 [-0.673]
 [-1.137]
 [-1.137]
 [-1.137]
 [-1.137]
 [-1.137]] [[0.373]
 [0.373]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]]
Printing some Q and Qe and total Qs values:  [[1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]] [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]] [[1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
start point for exploration sampling:  10935
from probs:  [0.35691842704243004, 0.2876568801012318, 0.07708343421051625, 0.09116366671709275, 0.1592746373325343, 0.027902954596195028]
line 256 mcts: sample exp_bonus -2.8997001590963283
first move QE:  0.5352249180513007
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using another actor
from probs:  [0.35691842704243004, 0.2876568801012318, 0.07708343421051625, 0.09116366671709275, 0.1592746373325343, 0.027902954596195028]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
line 256 mcts: sample exp_bonus -4.279832211072109
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.35691842704243004, 0.2876568801012318, 0.07708343421051625, 0.09116366671709275, 0.1592746373325343, 0.027902954596195028]
line 256 mcts: sample exp_bonus -0.6448410134520045
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.35691842704243004, 0.2876568801012318, 0.07708343421051625, 0.09116366671709275, 0.1592746373325343, 0.027902954596195028]
siam score:  -0.6716542
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
first move QE:  0.5339564765074664
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.24 ]
 [0.366]
 [0.171]
 [0.16 ]
 [0.141]
 [0.164]] [[1.744]
 [2.147]
 [1.521]
 [1.742]
 [1.831]
 [1.932]
 [2.072]] [[-0.915]
 [-0.653]
 [-0.611]
 [-0.927]
 [-0.919]
 [-0.924]
 [-0.831]]
start point for exploration sampling:  10935
from probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.2465610946674546
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[-0.466]
 [-0.466]
 [-0.466]
 [-0.466]
 [-0.466]
 [-0.466]
 [-0.466]] [[0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]]
Starting evaluation
siam score:  -0.66331667
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.405]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[-3.823]
 [-0.262]
 [-3.823]
 [-3.823]
 [-3.823]
 [-3.823]
 [-3.823]] [[0.476]
 [0.405]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.4954868269661334
line 256 mcts: sample exp_bonus -3.958451911352673
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
using another actor
Printing some Q and Qe and total Qs values:  [[0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]] [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.535]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]] [[1.702]
 [1.934]
 [1.702]
 [1.702]
 [1.702]
 [1.702]
 [1.702]] [[0.564]
 [0.535]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]]
3375 4233
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.781]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[-0.939]
 [-1.134]
 [-0.939]
 [-0.939]
 [-0.939]
 [-0.939]
 [-0.939]] [[0.739]
 [0.781]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
Printing some Q and Qe and total Qs values:  [[0.629]
 [1.   ]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]] [[1.618]
 [0.512]
 [1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]] [[0.629]
 [1.   ]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]]
using another actor
from probs:  [0.35691842704243004, 0.2876568801012318, 0.07708343421051625, 0.09116366671709275, 0.1592746373325343, 0.027902954596195028]
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.008]
 [ 0.56 ]
 [ 0.201]
 [ 0.143]
 [ 0.197]
 [ 0.101]] [[ 1.025]
 [ 0.51 ]
 [ 1.349]
 [-0.468]
 [-0.073]
 [-0.139]
 [-0.158]] [[0.614]
 [0.379]
 [1.803]
 [0.183]
 [0.307]
 [0.36 ]
 [0.188]]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.052]
 [-0.04 ]] [[0.624]
 [1.035]
 [1.035]
 [1.035]
 [1.035]
 [0.746]
 [1.035]] [[0.434]
 [1.143]
 [1.143]
 [1.143]
 [1.143]
 [0.635]
 [1.143]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.869]
 [0.832]
 [0.832]
 [0.832]
 [0.861]
 [0.862]] [[-1.633]
 [ 1.316]
 [ 1.103]
 [ 1.103]
 [ 1.103]
 [-0.798]
 [-1.825]] [[0.863]
 [0.869]
 [0.832]
 [0.832]
 [0.832]
 [0.861]
 [0.862]]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.1636321130578217
from probs:  [0.35691842704243004, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.5194580942442114
Printing some Q and Qe and total Qs values:  [[1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.201]] [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[2.404]
 [2.404]
 [2.404]
 [2.404]
 [2.404]
 [2.404]
 [2.404]]
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.295]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[1.961]
 [0.616]
 [1.335]
 [1.335]
 [1.335]
 [1.335]
 [1.335]] [[1.974]
 [1.304]
 [1.496]
 [1.496]
 [1.496]
 [1.496]
 [1.496]]
using another actor
using explorer policy with actor:  0
3378 4245
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.301]
 [0.29 ]
 [0.292]
 [0.29 ]
 [0.298]
 [0.301]] [[-2.182]
 [-3.39 ]
 [-3.029]
 [-3.197]
 [-3.311]
 [-3.293]
 [-3.306]] [[0.293]
 [0.301]
 [0.29 ]
 [0.292]
 [0.29 ]
 [0.298]
 [0.301]]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.32 ]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[-3.486]
 [-2.756]
 [-3.486]
 [-3.486]
 [-3.486]
 [-3.486]
 [-3.486]] [[0.309]
 [0.32 ]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.196]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]] [[ 0.566]
 [-0.09 ]
 [-0.607]
 [-0.607]
 [-0.607]
 [-0.607]
 [-0.607]] [[1.329]
 [1.15 ]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]]
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.878]
 [0.874]
 [0.869]
 [0.873]
 [0.872]
 [0.869]] [[-1.009]
 [ 0.244]
 [-1.761]
 [-2.045]
 [ 0.   ]
 [-1.029]
 [-1.901]] [[0.875]
 [0.878]
 [0.874]
 [0.869]
 [0.873]
 [0.872]
 [0.869]]
from probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
UNIT TEST: sample policy line 217 mcts : [0.143 0.265 0.163 0.061 0.184 0.102 0.082]
from probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
from probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.033]
 [-0.011]
 [ 0.028]
 [-0.032]
 [ 0.023]
 [-0.009]] [[0.902]
 [0.741]
 [0.814]
 [0.56 ]
 [0.877]
 [0.594]
 [1.048]] [[0.305]
 [0.204]
 [0.294]
 [0.205]
 [0.294]
 [0.217]
 [0.455]]
from probs:  [0.35691842704243, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.015]
 [-0.015]
 [-0.049]
 [-0.049]
 [-0.048]
 [-0.048]] [[0.62 ]
 [0.774]
 [0.774]
 [0.564]
 [0.66 ]
 [0.51 ]
 [0.459]] [[1.703]
 [1.766]
 [1.766]
 [1.685]
 [1.716]
 [1.667]
 [1.65 ]]
siam score:  -0.6486802
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
Printing some Q and Qe and total Qs values:  [[-0.027]
 [ 0.205]
 [ 0.205]
 [ 0.205]
 [ 0.205]
 [ 0.205]
 [ 0.205]] [[ 0.243]
 [-1.345]
 [-1.345]
 [-1.345]
 [-1.345]
 [-1.345]
 [-1.345]] [[1.485]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]]
line 256 mcts: sample exp_bonus 0.46377907562564413
3385 4259
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
rdn beta is 0 so we're just using the maxi policy
3387 4268
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
probs:  [0.35691842704243004, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2191656005358016
maxi score, test score, baseline:  -0.988 -0.8160000000000001 -0.8160000000000001
rdn probs:  [0.35691842704243004, 0.28765688010123175, 0.07708343421051625, 0.09116366671709274, 0.1592746373325343, 0.027902954596195025]
Printing some Q and Qe and total Qs values:  [[ 0.013]
 [-0.001]
 [ 0.022]
 [ 0.326]
 [ 0.326]
 [ 0.052]
 [ 0.146]] [[0.528]
 [0.983]
 [0.526]
 [0.154]
 [0.154]
 [0.491]
 [0.464]] [[-1.052]
 [-0.928]
 [-1.035]
 [-0.551]
 [-0.551]
 [-0.987]
 [-0.809]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.723]
 [0.647]
 [0.647]
 [0.674]
 [0.647]
 [0.647]] [[2.496]
 [1.453]
 [1.17 ]
 [1.17 ]
 [2.318]
 [2.585]
 [1.17 ]] [[0.649]
 [0.723]
 [0.647]
 [0.647]
 [0.674]
 [0.647]
 [0.647]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.3575984344020317, 0.2876027532198305, 0.07627842766419246, 0.09334943744987038, 0.15632709520813984, 0.02884385205593511]
from probs:  [0.3575984344020317, 0.2876027532198305, 0.07627842766419246, 0.09334943744987038, 0.15632709520813984, 0.02884385205593511]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.3575984344020317, 0.2876027532198305, 0.07627842766419246, 0.09334943744987038, 0.15632709520813984, 0.02884385205593511]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.3575984344020317, 0.2876027532198305, 0.07627842766419246, 0.09334943744987038, 0.15632709520813984, 0.02884385205593511]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.125]
 [0.163]
 [0.166]
 [0.162]
 [0.165]
 [0.16 ]] [[-1.833]
 [-0.588]
 [-3.054]
 [-3.297]
 [-3.175]
 [-3.145]
 [-2.952]] [[0.142]
 [0.125]
 [0.163]
 [0.166]
 [0.162]
 [0.165]
 [0.16 ]]
first move QE:  0.516082100224619
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.663]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[2.287]
 [2.461]
 [2.287]
 [2.287]
 [2.287]
 [2.287]
 [2.287]] [[1.838]
 [1.974]
 [1.838]
 [1.838]
 [1.838]
 [1.838]
 [1.838]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.65 ]
 [0.561]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[0.114]
 [0.962]
 [0.07 ]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[0.72 ]
 [1.12 ]
 [0.644]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Printing some Q and Qe and total Qs values:  [[0.618]
 [1.144]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[1.675]
 [0.577]
 [1.675]
 [1.675]
 [1.675]
 [1.675]
 [1.675]] [[1.392]
 [2.077]
 [1.392]
 [1.392]
 [1.392]
 [1.392]
 [1.392]]
from probs:  [0.3575984344020317, 0.2876027532198305, 0.07627842766419246, 0.09334943744987038, 0.15632709520813984, 0.02884385205593511]
line 256 mcts: sample exp_bonus 2.917775587819537
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.553]
 [0.634]
 [0.553]
 [0.519]
 [0.553]
 [0.553]] [[0.528]
 [0.223]
 [0.443]
 [0.223]
 [0.878]
 [0.223]
 [0.223]] [[1.37 ]
 [1.378]
 [1.614]
 [1.378]
 [1.529]
 [1.378]
 [1.378]]
using another actor
maxi score, test score, baseline:  -0.988 -1.0 -0.988
from probs:  [0.3575984344020317, 0.2876027532198305, 0.07627842766419246, 0.09334943744987038, 0.15632709520813984, 0.02884385205593511]
3401 4293
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Printing some Q and Qe and total Qs values:  [[1.445]
 [1.448]
 [1.443]
 [1.419]
 [1.433]
 [1.439]
 [1.413]] [[0.493]
 [0.474]
 [0.491]
 [0.512]
 [0.507]
 [0.503]
 [0.516]] [[2.795]
 [2.79 ]
 [2.79 ]
 [2.76 ]
 [2.782]
 [2.791]
 [2.752]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -1.0 -0.988
siam score:  -0.65155655
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.3575984344020317, 0.2876027532198305, 0.07627842766419246, 0.09334943744987038, 0.15632709520813984, 0.02884385205593511]
Printing some Q and Qe and total Qs values:  [[1.042]
 [1.038]
 [1.024]
 [1.042]
 [1.042]
 [1.287]
 [1.042]] [[1.113]
 [0.952]
 [1.077]
 [1.113]
 [1.113]
 [1.142]
 [1.113]] [[2.178]
 [2.073]
 [2.123]
 [2.178]
 [2.178]
 [2.637]
 [2.178]]
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.59 ]
 [0.482]
 [0.552]
 [0.603]
 [0.547]
 [0.549]] [[1.884]
 [2.072]
 [1.922]
 [1.186]
 [1.481]
 [1.205]
 [1.22 ]] [[0.566]
 [0.701]
 [0.435]
 [0.328]
 [0.529]
 [0.325]
 [0.333]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.5899999999999997  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.559]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[0.351]
 [0.316]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]] [[1.384]
 [1.149]
 [0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]]
from probs:  [0.3369053403047286, 0.32882693611350683, 0.07186443551713821, 0.08794759978156345, 0.14728093901758096, 0.02717474926548183]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.3369053403047286, 0.32882693611350683, 0.07186443551713821, 0.08794759978156345, 0.14728093901758096, 0.02717474926548183]
Printing some Q and Qe and total Qs values:  [[1.466]
 [1.466]
 [1.467]
 [1.467]
 [1.466]
 [1.466]
 [1.467]] [[0.5  ]
 [0.502]
 [0.508]
 [0.508]
 [0.502]
 [0.502]
 [0.508]] [[2.429]
 [2.431]
 [2.437]
 [2.437]
 [2.431]
 [2.431]
 [2.437]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.351]
 [0.461]
 [0.481]
 [0.398]
 [0.488]
 [0.473]] [[ 1.014]
 [-0.113]
 [-0.346]
 [-1.25 ]
 [-1.132]
 [-0.809]
 [-0.846]] [[1.957]
 [0.898]
 [0.961]
 [0.431]
 [0.35 ]
 [0.72 ]
 [0.67 ]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.988 -1.0 -0.988
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -1.0 -0.988
using another actor
maxi score, test score, baseline:  -0.988 -1.0 -0.988
using another actor
from probs:  [0.3460788037005014, 0.3377804358094541, 0.0738212040506321, 0.06311368263182947, 0.15129119395076424, 0.027914679856818624]
from probs:  [0.3460788037005014, 0.3377804358094541, 0.0738212040506321, 0.06311368263182947, 0.15129119395076424, 0.027914679856818624]
from probs:  [0.3460788037005014, 0.3377804358094541, 0.0738212040506321, 0.06311368263182947, 0.15129119395076424, 0.027914679856818624]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]] [[-3.549]
 [-3.549]
 [-3.549]
 [-3.549]
 [-3.549]
 [-3.549]
 [-3.549]] [[0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
line 256 mcts: sample exp_bonus 2.2227082446100446
line 256 mcts: sample exp_bonus 0.3068862566238427
from probs:  [0.3460788037005014, 0.3377804358094541, 0.0738212040506321, 0.06311368263182947, 0.15129119395076424, 0.027914679856818624]
Printing some Q and Qe and total Qs values:  [[0.934]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]] [[2.337]
 [2.4  ]
 [2.4  ]
 [2.4  ]
 [2.4  ]
 [2.4  ]
 [2.4  ]] [[0.934]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]]
siam score:  -0.6587143
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.808]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]] [[-1.234]
 [ 0.985]
 [-1.234]
 [-1.234]
 [-1.234]
 [-1.234]
 [-1.234]] [[0.761]
 [0.808]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.7176311860784428
3417 4347
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.3460788037005014, 0.3377804358094541, 0.0738212040506321, 0.06311368263182947, 0.15129119395076424, 0.027914679856818624]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -1.0 -0.988
line 256 mcts: sample exp_bonus 0.4915723836758371
Printing some Q and Qe and total Qs values:  [[0.005]
 [1.46 ]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[1.298]
 [0.486]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]] [[0.821]
 [2.107]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.597]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[0.653]
 [1.201]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[0.525]
 [0.597]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.3460788037005014, 0.3377804358094541, 0.0738212040506321, 0.06311368263182947, 0.15129119395076424, 0.027914679856818624]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.571]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[-1.274]
 [-0.47 ]
 [-1.274]
 [-1.274]
 [-1.274]
 [-1.274]
 [-1.274]] [[-0.095]
 [ 0.481]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]]
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[-1.669]
 [-1.669]
 [-1.669]
 [-1.669]
 [-1.669]
 [-1.669]
 [-1.669]] [[0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[-1.42]
 [-1.42]
 [-1.42]
 [-1.42]
 [-1.42]
 [-1.42]
 [-1.42]] [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.988 -1.0 -0.988
3425 4362
line 256 mcts: sample exp_bonus 0.3539192580576853
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[-4.303]
 [-4.303]
 [-4.303]
 [-4.303]
 [-4.303]
 [-4.303]
 [-4.303]] [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.3460788037005014, 0.3377804358094541, 0.0738212040506321, 0.06311368263182947, 0.15129119395076424, 0.027914679856818624]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]] [[0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]] [[2.532]
 [2.532]
 [2.532]
 [2.532]
 [2.532]
 [2.532]
 [2.532]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.5099999999999997  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.37932709607242615, 0.3206061604520221, 0.07006780228079744, 0.05990469937102253, 0.14359886974069422, 0.02649537208303742]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
siam score:  -0.63833654
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.536]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[2.629]
 [2.637]
 [2.629]
 [2.629]
 [2.629]
 [2.629]
 [2.629]] [[0.546]
 [0.536]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]]
siam score:  -0.6399801
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.63864714
using explorer policy with actor:  1
3439 4390
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.649]
 [0.651]
 [0.685]
 [0.685]
 [0.658]
 [0.665]] [[0.24 ]
 [0.476]
 [0.251]
 [1.377]
 [1.377]
 [0.275]
 [0.147]] [[1.199]
 [1.259]
 [1.188]
 [1.631]
 [1.631]
 [1.211]
 [1.18 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using another actor
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.37932709607242615, 0.3206061604520221, 0.07006780228079744, 0.05990469937102253, 0.14359886974069422, 0.02649537208303742]
using another actor
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.445]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.403]] [[0.693]
 [1.968]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.541]] [[0.375]
 [0.445]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.403]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[ 0.545]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]] [[1.579]
 [1.442]
 [1.442]
 [1.442]
 [1.442]
 [1.442]
 [1.442]]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.4049999999999996  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5851837968259871
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.698]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[0.37 ]
 [1.272]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]] [[1.285]
 [1.826]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]]
from probs:  [0.4054666837402414, 0.30710385870025236, 0.06711690262202359, 0.05738181794505056, 0.13755121529850178, 0.025379521693930117]
from probs:  [0.4054666837402414, 0.30710385870025236, 0.06711690262202359, 0.05738181794505056, 0.13755121529850178, 0.025379521693930117]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.7129820161596722
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[ 0.093]
 [-1.379]
 [-1.379]
 [-1.379]
 [-1.379]
 [-1.379]
 [-1.379]] [[1.423]
 [0.996]
 [0.996]
 [0.996]
 [0.996]
 [0.996]
 [0.996]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.4054666837402414, 0.30710385870025236, 0.06711690262202359, 0.05738181794505056, 0.13755121529850178, 0.025379521693930117]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.4054666837402414, 0.30710385870025236, 0.06711690262202359, 0.05738181794505056, 0.13755121529850178, 0.025379521693930117]
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.315]
 [0.288]
 [0.292]
 [0.291]
 [0.379]
 [0.227]] [[1.763]
 [1.974]
 [1.235]
 [1.199]
 [1.176]
 [1.418]
 [1.36 ]] [[-0.912]
 [-0.669]
 [-0.97 ]
 [-0.974]
 [-0.984]
 [-0.726]
 [-1.049]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -1.0 -0.988
first move QE:  0.4878743501702389
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.398]
 [0.398]
 [0.398]
 [0.452]
 [0.398]
 [0.398]] [[0.653]
 [0.487]
 [0.487]
 [0.487]
 [0.865]
 [0.487]
 [0.487]] [[1.084]
 [1.046]
 [1.046]
 [1.046]
 [1.412]
 [1.046]
 [1.046]]
siam score:  -0.6349817
maxi score, test score, baseline:  -0.988 -1.0 -0.988
actor:  1 policy actor:  1  step number:  91 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.988 -1.0 -0.988
line 256 mcts: sample exp_bonus 1.8827164781620462
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.598]
 [0.659]] [[0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [1.811]
 [0.74 ]] [[1.152]
 [1.152]
 [1.152]
 [1.152]
 [1.152]
 [1.388]
 [1.152]]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.445]
 [0.666]
 [0.666]
 [0.666]
 [0.474]
 [0.666]] [[5.16 ]
 [2.341]
 [5.16 ]
 [5.16 ]
 [5.16 ]
 [1.874]
 [5.16 ]] [[ 2.433]
 [ 0.113]
 [ 2.433]
 [ 2.433]
 [ 2.433]
 [-0.14 ]
 [ 2.433]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
line 256 mcts: sample exp_bonus 4.049012406508826
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.467]
 [1.467]
 [1.467]
 [1.467]
 [1.467]
 [1.467]
 [1.467]] [[0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[2.21]
 [2.21]
 [2.21]
 [2.21]
 [2.21]
 [2.21]
 [2.21]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  80 total reward:  0.3149999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]] [[2.985]
 [2.276]
 [2.276]
 [2.276]
 [2.276]
 [2.276]
 [2.276]] [[1.031]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]]
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.411]
 [0.303]
 [0.299]
 [0.299]
 [0.3  ]
 [0.305]] [[-0.373]
 [ 0.273]
 [-1.036]
 [-1.335]
 [-1.341]
 [-1.327]
 [-1.237]] [[ 0.428]
 [ 0.905]
 [ 0.093]
 [-0.062]
 [-0.065]
 [-0.057]
 [-0.004]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.394]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.414]] [[1.104]
 [1.231]
 [1.051]
 [1.051]
 [1.051]
 [1.051]
 [1.139]] [[0.974]
 [1.222]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.205]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.4052568028565836, 0.32399315747647994, 0.06322213527287017, 0.05405197371451371, 0.12956917260505388, 0.02390675807449851]
line 256 mcts: sample exp_bonus -3.0107460591461765
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.988 -1.0 -0.988
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.25924863856470937
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[-1.232]
 [-1.232]
 [-1.232]
 [-1.232]
 [-1.232]
 [-1.232]
 [-1.232]] [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.501]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[0.848]
 [2.349]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]] [[-0.028]
 [ 0.458]
 [ 0.019]
 [ 0.019]
 [ 0.019]
 [ 0.019]
 [ 0.019]]
rdn beta is 0 so we're just using the maxi policy
3468 4467
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.4052568028565836, 0.32399315747647994, 0.06322213527287017, 0.05405197371451371, 0.12956917260505388, 0.02390675807449851]
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.8999024402529119
actor:  1 policy actor:  1  step number:  102 total reward:  0.024999999999999245  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.053]
 [ 0.209]
 [ 1.144]
 [ 0.258]
 [-0.023]
 [ 0.42 ]
 [ 0.209]] [[2.125]
 [2.592]
 [1.632]
 [2.247]
 [2.678]
 [1.384]
 [2.592]] [[0.862]
 [1.283]
 [1.903]
 [1.217]
 [1.078]
 [1.09 ]
 [1.283]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.6558304
using explorer policy with actor:  1
siam score:  -0.6563933
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.689]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[0.904]
 [0.829]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[2.087]
 [2.241]
 [1.904]
 [1.904]
 [1.904]
 [1.904]
 [1.904]]
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.908]
 [0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.839]] [[1.203]
 [1.352]
 [1.203]
 [1.203]
 [1.203]
 [1.203]
 [1.203]] [[0.537]
 [0.775]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]]
Printing some Q and Qe and total Qs values:  [[0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]] [[-0.051]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]] [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
siam score:  -0.6569901
line 256 mcts: sample exp_bonus 0.7374144267831126
from probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.657]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]] [[0.102]
 [1.222]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]] [[0.304]
 [0.852]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]]
using explorer policy with actor:  0
3476 4495
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
from probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
line 256 mcts: sample exp_bonus -1.0718400363217075
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.3032552155735582
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
line 256 mcts: sample exp_bonus -0.20327796926065309
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.369]
 [0.342]
 [0.346]
 [0.346]
 [0.344]
 [0.34 ]] [[-1.638]
 [-1.539]
 [-1.776]
 [-1.729]
 [-1.723]
 [-1.808]
 [-1.779]] [[0.332]
 [0.369]
 [0.342]
 [0.346]
 [0.346]
 [0.344]
 [0.34 ]]
from probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
Printing some Q and Qe and total Qs values:  [[1.274]
 [1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]] [[0.565]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[2.395]
 [2.248]
 [2.248]
 [2.248]
 [2.248]
 [2.248]
 [2.248]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[ 2.254]
 [-0.476]
 [-0.476]
 [-0.476]
 [-0.476]
 [-0.476]
 [-0.476]] [[1.602]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]]
Printing some Q and Qe and total Qs values:  [[1.013]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]] [[1.89]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]] [[2.356]
 [1.916]
 [1.916]
 [1.916]
 [1.916]
 [1.916]
 [1.916]]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.42 ]
 [0.399]
 [0.395]
 [0.398]
 [0.402]
 [0.399]] [[-1.37 ]
 [-0.913]
 [-1.816]
 [-1.929]
 [-1.839]
 [-1.886]
 [-1.925]] [[0.396]
 [0.42 ]
 [0.399]
 [0.395]
 [0.398]
 [0.402]
 [0.399]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
siam score:  -0.6439894
siam score:  -0.6447454
3489 4540
maxi score, test score, baseline:  -0.988 -1.0 -0.988
from probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
from probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.382]
 [0.363]
 [0.342]
 [0.355]
 [0.356]
 [0.354]] [[-1.778]
 [-1.036]
 [-1.815]
 [-1.705]
 [-1.346]
 [-1.098]
 [-1.421]] [[0.355]
 [0.382]
 [0.363]
 [0.342]
 [0.355]
 [0.356]
 [0.354]]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.004]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[1.906]
 [1.579]
 [1.763]
 [1.09 ]
 [1.151]
 [1.786]
 [1.298]] [[-0.134]
 [-0.354]
 [-0.23 ]
 [-0.678]
 [-0.637]
 [-0.214]
 [-0.539]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[-1.186]
 [-1.186]
 [-1.186]
 [-1.186]
 [-1.186]
 [-1.186]
 [-1.186]] [[1.841]
 [1.841]
 [1.841]
 [1.841]
 [1.841]
 [1.841]
 [1.841]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
3498 4561
from probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
siam score:  -0.638942
using explorer policy with actor:  1
using explorer policy with actor:  0
siam score:  -0.63599825
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.477]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[-0.196]
 [ 0.507]
 [-0.196]
 [-0.196]
 [-0.196]
 [-0.196]
 [-0.196]] [[0.448]
 [0.477]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]]
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]] [[1.019]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]] [[0.818]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
Printing some Q and Qe and total Qs values:  [[1.353]
 [1.055]
 [1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]] [[0.797]
 [0.79 ]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]] [[2.273]
 [2.046]
 [2.103]
 [2.103]
 [2.103]
 [2.103]
 [2.103]]
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.239]
 [0.191]
 [0.185]
 [0.203]
 [0.199]
 [0.205]] [[2.511]
 [1.502]
 [2.511]
 [2.574]
 [2.862]
 [2.787]
 [2.533]] [[-0.526]
 [-0.808]
 [-0.569]
 [-0.559]
 [-0.427]
 [-0.46 ]
 [-0.533]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
first move QE:  0.46740824011294124
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
line 256 mcts: sample exp_bonus 0.4796265947751787
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.271]
 [0.222]
 [0.208]
 [0.231]
 [0.208]
 [0.208]] [[-1.186]
 [-1.611]
 [-1.587]
 [-1.396]
 [-1.393]
 [-1.396]
 [-1.396]] [[0.21 ]
 [0.271]
 [0.222]
 [0.208]
 [0.231]
 [0.208]
 [0.208]]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.221]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]] [[-3.583]
 [-0.752]
 [-3.583]
 [-3.583]
 [-3.583]
 [-3.583]
 [-3.583]] [[0.203]
 [1.137]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.366]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]] [[-3.485]
 [ 0.129]
 [-3.485]
 [-3.485]
 [-3.485]
 [-3.485]
 [-3.485]] [[0.175]
 [1.363]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]]
3510 4575
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
from probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[1.691]
 [1.691]
 [1.691]
 [1.691]
 [1.691]
 [1.691]
 [1.691]] [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.048]
 [0.484]
 [0.485]
 [0.486]
 [0.485]
 [0.485]] [[-3.278]
 [-0.447]
 [-3.974]
 [-4.164]
 [-4.133]
 [-3.947]
 [-3.865]] [[0.489]
 [1.345]
 [0.236]
 [0.168]
 [0.18 ]
 [0.246]
 [0.276]]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.426]
 [0.131]
 [0.475]
 [0.37 ]
 [0.472]
 [0.461]] [[-1.507]
 [-2.534]
 [ 0.   ]
 [-2.88 ]
 [-2.459]
 [-3.129]
 [-2.593]] [[1.306]
 [0.53 ]
 [2.177]
 [0.312]
 [0.544]
 [0.126]
 [0.513]]
Printing some Q and Qe and total Qs values:  [[1.079]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]] [[1.887]
 [2.077]
 [2.077]
 [2.077]
 [2.077]
 [2.077]
 [2.077]] [[1.803]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]]
from probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
from probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
siam score:  -0.6463444
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.763]
 [0.765]
 [0.765]
 [0.763]
 [0.779]
 [0.762]] [[1.24 ]
 [1.786]
 [1.829]
 [1.424]
 [2.264]
 [2.315]
 [2.023]] [[0.762]
 [0.763]
 [0.765]
 [0.765]
 [0.763]
 [0.779]
 [0.762]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
rdn beta is 0 so we're just using the maxi policy
using another actor
maxi score, test score, baseline:  -0.988 -1.0 -0.988
siam score:  -0.6551099
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.288]
 [0.432]] [[1.035]
 [1.035]
 [1.035]
 [1.035]
 [1.035]
 [2.531]
 [1.035]] [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [1.254]
 [0.566]]
Printing some Q and Qe and total Qs values:  [[1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.096]] [[0.456]
 [0.446]
 [0.461]
 [0.456]
 [0.454]
 [0.461]
 [0.461]] [[1.387]
 [1.381]
 [1.391]
 [1.387]
 [1.386]
 [1.391]
 [1.391]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40759235250004766, 0.32009723881723473, 0.06358649297014944, 0.05436348253323707, 0.1303158972318796, 0.02404453594745142]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.503]
 [0.511]
 [0.482]
 [0.511]
 [0.511]
 [0.486]] [[-0.288]
 [-0.091]
 [ 0.   ]
 [-0.825]
 [ 0.   ]
 [ 0.   ]
 [-0.629]] [[0.479]
 [0.503]
 [0.511]
 [0.482]
 [0.511]
 [0.511]
 [0.486]]
siam score:  -0.64581215
UNIT TEST: sample policy line 217 mcts : [0.286 0.041 0.163 0.082 0.041 0.082 0.306]
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.721]
 [0.295]] [[ 0.732]
 [-0.205]
 [-0.205]
 [-0.205]
 [-0.205]
 [ 0.457]
 [-0.205]] [[ 0.182]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]
 [ 0.943]
 [-0.132]]
Printing some Q and Qe and total Qs values:  [[0.575]
 [1.144]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[0.687]
 [0.588]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]] [[1.729]
 [2.356]
 [1.729]
 [1.729]
 [1.729]
 [1.729]
 [1.729]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
maxi score, test score, baseline:  -0.988 -1.0 -0.988
actor:  1 policy actor:  1  step number:  62 total reward:  0.4049999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.55 ]
 [0.521]
 [0.521]
 [0.519]
 [0.521]
 [0.521]] [[0.511]
 [0.473]
 [0.63 ]
 [0.63 ]
 [0.476]
 [0.63 ]
 [0.63 ]] [[0.625]
 [0.644]
 [0.743]
 [0.743]
 [0.585]
 [0.743]
 [0.743]]
Printing some Q and Qe and total Qs values:  [[0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]] [[0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[1.438]
 [1.438]
 [1.438]
 [1.438]
 [1.438]
 [1.438]
 [1.438]]
using explorer policy with actor:  1
from probs:  [0.3919535492972217, 0.3461842480900449, 0.06114675963408807, 0.05227762444600158, 0.12531584102741777, 0.02312197750522593]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.378]
 [0.388]
 [0.378]
 [0.378]
 [0.377]
 [0.378]] [[1.116]
 [2.387]
 [3.057]
 [2.387]
 [2.387]
 [2.249]
 [2.387]] [[-0.665]
 [ 0.581]
 [ 1.269]
 [ 0.581]
 [ 0.581]
 [ 0.442]
 [ 0.581]]
maxi score, test score, baseline:  -0.9879999999999999 -1.0 -0.9879999999999999
probs:  [0.39195354929722165, 0.34618424809004483, 0.06114675963408809, 0.05227762444600159, 0.12531584102741777, 0.023121977505225925]
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]] [[-0.37]
 [-0.37]
 [-0.37]
 [-0.37]
 [-0.37]
 [-0.37]
 [-0.37]] [[0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]]
siam score:  -0.6453304
maxi score, test score, baseline:  -0.9879999999999999 -1.0 -0.9879999999999999
probs:  [0.39195354929722165, 0.34618424809004483, 0.06114675963408809, 0.05227762444600159, 0.12531584102741777, 0.023121977505225925]
from probs:  [0.39195354929722165, 0.34618424809004483, 0.06114675963408809, 0.05227762444600159, 0.12531584102741777, 0.023121977505225925]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.366]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]] [[ 0.426]
 [ 0.163]
 [-0.451]
 [-0.451]
 [-0.451]
 [-0.451]
 [-0.451]] [[1.769]
 [1.491]
 [1.38 ]
 [1.38 ]
 [1.38 ]
 [1.38 ]
 [1.38 ]]
using explorer policy with actor:  0
Starting evaluation
using another actor
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.541]
 [0.587]
 [0.6  ]
 [0.641]
 [0.608]
 [0.623]] [[0.967]
 [1.264]
 [0.568]
 [0.345]
 [0.486]
 [0.327]
 [0.325]] [[0.547]
 [0.541]
 [0.587]
 [0.6  ]
 [0.641]
 [0.608]
 [0.623]]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.506]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]] [[-1.661]
 [-2.005]
 [-1.661]
 [-1.661]
 [-1.661]
 [-1.661]
 [-1.661]] [[0.483]
 [0.506]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]] [[-1.763]
 [-1.763]
 [-1.763]
 [-1.763]
 [-1.763]
 [-1.763]
 [-1.763]] [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
UNIT TEST: sample policy line 217 mcts : [0.02  0.878 0.02  0.02  0.02  0.02  0.02 ]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.658]
 [0.707]] [[ 0.387]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.511]
 [ 0.   ]] [[0.692]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.658]
 [0.707]]
first move QE:  0.4545344054310215
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.754]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]] [[-1.684]
 [-0.693]
 [-1.684]
 [-1.684]
 [-1.684]
 [-1.684]
 [-1.684]] [[0.707]
 [0.754]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]]
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.19 ]
 [0.236]
 [0.261]
 [0.232]
 [0.398]
 [0.208]] [[ 1.421]
 [ 0.427]
 [ 0.378]
 [-0.122]
 [ 0.209]
 [ 0.453]
 [ 0.619]] [[ 1.308]
 [-0.013]
 [ 0.037]
 [-0.195]
 [-0.062]
 [ 0.341]
 [ 0.121]]
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[-0.344]
 [-0.344]
 [-0.344]
 [-0.344]
 [-0.344]
 [-0.344]
 [-0.344]] [[0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.3841084445896523
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Printing some Q and Qe and total Qs values:  [[0.948]
 [0.866]
 [0.894]
 [0.866]
 [0.903]
 [0.927]
 [0.925]] [[-0.953]
 [-0.974]
 [-1.057]
 [-0.974]
 [-0.974]
 [-1.082]
 [-1.226]] [[0.948]
 [0.866]
 [0.894]
 [0.866]
 [0.903]
 [0.927]
 [0.925]]
Printing some Q and Qe and total Qs values:  [[0.842]
 [0.846]
 [0.846]
 [0.846]
 [0.843]
 [0.844]
 [0.846]] [[-1.045]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.928]
 [-0.834]
 [ 0.   ]] [[0.842]
 [0.846]
 [0.846]
 [0.846]
 [0.843]
 [0.844]
 [0.846]]
from probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
using explorer policy with actor:  0
using another actor
3542 4659
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]] [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[2.343]
 [2.343]
 [2.343]
 [2.343]
 [2.343]
 [2.343]
 [2.343]]
Printing some Q and Qe and total Qs values:  [[1.168]
 [1.387]
 [1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]] [[1.244]
 [0.948]
 [1.196]
 [1.196]
 [1.196]
 [1.196]
 [1.196]] [[2.642]
 [2.846]
 [2.658]
 [2.658]
 [2.658]
 [2.658]
 [2.658]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.80813888091756
Printing some Q and Qe and total Qs values:  [[0. ]
 [0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]] [[0.499]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[1.401]
 [1.435]
 [1.435]
 [1.435]
 [1.435]
 [1.435]
 [1.435]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
Printing some Q and Qe and total Qs values:  [[0.882]
 [0.745]
 [0.847]
 [0.839]
 [0.841]
 [0.851]
 [0.829]] [[-0.459]
 [ 0.446]
 [-1.246]
 [-1.2  ]
 [-1.206]
 [-1.169]
 [-1.233]] [[0.882]
 [0.745]
 [0.847]
 [0.839]
 [0.841]
 [0.851]
 [0.829]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
rdn probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.23660977547226547
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.465]
 [0.415]
 [0.488]
 [0.241]
 [0.435]
 [0.415]] [[1.912]
 [1.075]
 [1.106]
 [1.253]
 [1.685]
 [0.854]
 [1.106]] [[1.589]
 [1.141]
 [1.102]
 [1.224]
 [1.12 ]
 [1.039]
 [1.102]]
using another actor
from probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[-1.422]
 [-4.143]
 [-4.143]
 [-4.143]
 [-4.143]
 [-4.143]
 [-4.143]] [[0.691]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]]
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.088]
 [0.355]
 [0.37 ]
 [0.258]
 [0.836]
 [0.303]] [[ 1.312]
 [ 1.108]
 [ 0.795]
 [-0.103]
 [ 0.754]
 [ 1.283]
 [ 1.223]] [[1.226]
 [0.856]
 [1.065]
 [0.252]
 [0.844]
 [2.422]
 [1.368]]
line 256 mcts: sample exp_bonus 1.3784954199593875
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.426]
 [0.398]
 [0.373]
 [0.409]
 [0.381]
 [0.405]] [[ 0.025]
 [-0.325]
 [-0.42 ]
 [-0.839]
 [-0.354]
 [-0.448]
 [-0.332]] [[0.397]
 [0.426]
 [0.398]
 [0.373]
 [0.409]
 [0.381]
 [0.405]]
Printing some Q and Qe and total Qs values:  [[1.46]
 [1.46]
 [1.46]
 [1.46]
 [1.46]
 [1.46]
 [1.46]] [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[2.493]
 [2.493]
 [2.493]
 [2.493]
 [2.493]
 [2.493]
 [2.493]]
3551 4686
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.309]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[-0.837]
 [-1.076]
 [-0.837]
 [-0.837]
 [-0.837]
 [-0.837]
 [-0.837]] [[0.298]
 [0.309]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]]
3557 4700
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.594]
 [0.548]
 [0.467]
 [0.467]
 [0.591]
 [0.561]] [[1.2  ]
 [1.058]
 [0.881]
 [1.707]
 [1.707]
 [1.104]
 [1.052]] [[ 0.001]
 [ 0.014]
 [-0.208]
 [ 0.427]
 [ 0.427]
 [ 0.051]
 [-0.037]]
from probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
siam score:  -0.65366524
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[-0.837]
 [-0.837]
 [-0.837]
 [-0.837]
 [-0.837]
 [-0.837]
 [-0.837]] [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
line 256 mcts: sample exp_bonus 1.1001603873809167
from probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.675]
 [0.698]
 [0.673]
 [0.651]
 [0.67 ]
 [0.694]] [[1.338]
 [1.83 ]
 [2.153]
 [1.892]
 [1.361]
 [2.025]
 [2.083]] [[0.655]
 [0.675]
 [0.698]
 [0.673]
 [0.651]
 [0.67 ]
 [0.694]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.682]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[1.886]
 [2.018]
 [1.687]
 [1.687]
 [1.687]
 [1.687]
 [1.687]] [[0.675]
 [0.682]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
from probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
from probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
first move QE:  0.4415390223194659
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[0.989]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]] [[1.286]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]]
line 256 mcts: sample exp_bonus 0.18789531508748564
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.988 -1.0 -0.988
rdn beta is 0 so we're just using the maxi policy
using another actor
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.451]
 [0.416]
 [0.416]
 [0.416]
 [0.408]
 [0.417]] [[-0.33 ]
 [ 0.283]
 [-1.053]
 [-1.053]
 [-1.053]
 [-0.65 ]
 [-0.737]] [[0.406]
 [0.451]
 [0.416]
 [0.416]
 [0.416]
 [0.408]
 [0.417]]
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.704]
 [0.729]
 [0.562]
 [0.562]
 [0.569]
 [0.583]] [[1.737]
 [2.339]
 [2.047]
 [2.286]
 [2.286]
 [1.892]
 [1.924]] [[0.713]
 [0.704]
 [0.729]
 [0.562]
 [0.562]
 [0.569]
 [0.583]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
line 256 mcts: sample exp_bonus 1.7902633929434977
from probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.852]
 [0.565]
 [0.56 ]
 [0.559]
 [0.558]
 [0.557]] [[-1.31 ]
 [-0.355]
 [-1.792]
 [-1.885]
 [-1.884]
 [-1.692]
 [-1.725]] [[0.626]
 [1.478]
 [0.424]
 [0.385]
 [0.382]
 [0.445]
 [0.432]]
from probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[ 0.772]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]] [[1.525]
 [1.066]
 [1.066]
 [1.066]
 [1.066]
 [1.066]
 [1.066]]
Printing some Q and Qe and total Qs values:  [[1.188]
 [1.141]
 [1.14 ]
 [1.281]
 [1.058]
 [1.172]
 [1.328]] [[0.545]
 [0.629]
 [0.573]
 [0.556]
 [0.643]
 [0.486]
 [0.654]] [[2.336]
 [2.299]
 [2.259]
 [2.528]
 [2.142]
 [2.265]
 [2.689]]
Printing some Q and Qe and total Qs values:  [[ 0.159]
 [ 0.004]
 [ 0.072]
 [ 0.159]
 [-0.011]
 [ 0.263]
 [ 0.159]] [[ 0.005]
 [ 1.143]
 [-0.659]
 [ 0.005]
 [ 0.763]
 [-0.173]
 [ 0.005]] [[ 0.159]
 [ 0.004]
 [ 0.072]
 [ 0.159]
 [-0.011]
 [ 0.263]
 [ 0.159]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.366]
 [0.366]
 [0.229]
 [0.366]
 [0.323]
 [0.366]] [[ 0.568]
 [ 0.305]
 [ 0.305]
 [-0.203]
 [ 0.305]
 [ 1.193]
 [ 0.305]] [[0.277]
 [0.72 ]
 [0.72 ]
 [0.107]
 [0.72 ]
 [1.226]
 [0.72 ]]
from probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
3577 4746
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.308]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]] [[-0.508]
 [ 0.002]
 [-0.508]
 [-0.508]
 [-0.508]
 [-0.508]
 [-0.508]] [[-0.406]
 [-0.142]
 [-0.406]
 [-0.406]
 [-0.406]
 [-0.406]
 [-0.406]]
first move QE:  0.4369495674945525
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]] [[-4.524]
 [-4.524]
 [-4.524]
 [-4.524]
 [-4.524]
 [-4.524]
 [-4.524]] [[0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.525]
 [0.572]
 [0.513]
 [0.513]
 [0.578]
 [0.495]] [[ 0.041]
 [-0.69 ]
 [-1.39 ]
 [-0.865]
 [-0.865]
 [-1.407]
 [-0.639]] [[2.451]
 [2.165]
 [2.038]
 [2.09 ]
 [2.09 ]
 [2.044]
 [2.125]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]] [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]] [[0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]]
3583 4757
maxi score, test score, baseline:  -0.988 -1.0 -0.988
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
start point for exploration sampling:  10935
using explorer policy with actor:  1
using another actor
from probs:  [0.39195354929722165, 0.34618424809004494, 0.06114675963408808, 0.052277624446001585, 0.12531584102741777, 0.023121977505225932]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.588]
 [0.55 ]
 [0.512]
 [0.551]
 [0.544]
 [0.558]] [[3.823]
 [3.688]
 [3.753]
 [4.231]
 [3.692]
 [3.591]
 [3.427]] [[ 0.018]
 [ 0.085]
 [ 0.031]
 [ 0.115]
 [ 0.012]
 [-0.036]
 [-0.062]]
first move QE:  0.4348216373701887
actor:  1 policy actor:  1  step number:  111 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40900620579653285, 0.3364755143883207, 0.05943189938470929, 0.05081149900238244, 0.12180135954579258, 0.022473521882262096]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
line 256 mcts: sample exp_bonus -0.38582923682499404
3590 4776
using another actor
from probs:  [0.40900620579653285, 0.3364755143883207, 0.05943189938470929, 0.05081149900238244, 0.12180135954579258, 0.022473521882262096]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]] [[0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]] [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.804]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[0.526]
 [0.936]
 [1.827]
 [1.827]
 [1.827]
 [1.827]
 [1.827]] [[0.658]
 [0.804]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]]
using explorer policy with actor:  1
3592 4780
in main func line 156:  3593
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.263]
 [0.283]
 [0.165]
 [0.087]
 [0.21 ]
 [0.263]] [[ 0.791]
 [ 0.337]
 [-0.023]
 [ 0.288]
 [ 0.697]
 [ 0.934]
 [ 0.337]] [[1.171]
 [1.115]
 [0.918]
 [0.964]
 [1.119]
 [1.417]
 [1.115]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
maxi score, test score, baseline:  -0.988 -1.0 -0.988
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40900620579653285, 0.3364755143883207, 0.05943189938470929, 0.05081149900238244, 0.12180135954579258, 0.022473521882262096]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40900620579653285, 0.3364755143883207, 0.05943189938470929, 0.05081149900238244, 0.12180135954579258, 0.022473521882262096]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.749]
 [0.669]
 [0.668]
 [0.668]
 [0.797]
 [0.668]] [[1.163]
 [1.134]
 [1.352]
 [1.163]
 [1.163]
 [1.662]
 [1.163]] [[1.968]
 [2.106]
 [2.092]
 [1.968]
 [1.968]
 [2.539]
 [1.968]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40900620579653285, 0.3364755143883207, 0.05943189938470929, 0.05081149900238244, 0.12180135954579255, 0.022473521882262096]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.40900620579653285, 0.3364755143883207, 0.05943189938470929, 0.05081149900238244, 0.12180135954579255, 0.022473521882262096]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  58 total reward:  0.5049999999999997  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.988 -1.0 -0.988
siam score:  -0.6331018
line 256 mcts: sample exp_bonus 1.6168190952074317
3602 4798
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.715]
 [0.707]
 [0.713]
 [0.73 ]
 [0.703]
 [0.714]] [[2.125]
 [3.075]
 [3.699]
 [3.831]
 [2.974]
 [3.035]
 [2.77 ]] [[0.723]
 [0.715]
 [0.707]
 [0.713]
 [0.73 ]
 [0.703]
 [0.714]]
using explorer policy with actor:  0
3603 4800
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.266]
 [0.257]
 [0.266]
 [0.266]
 [0.259]
 [0.266]] [[ 0.   ]
 [ 0.   ]
 [-4.509]
 [ 0.   ]
 [ 0.   ]
 [-4.307]
 [ 0.   ]] [[0.266]
 [0.266]
 [0.257]
 [0.266]
 [0.266]
 [0.259]
 [0.266]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.988 -1.0 -0.988
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.517]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[-1.307]
 [-0.443]
 [-1.307]
 [-1.307]
 [-1.307]
 [-1.307]
 [-1.307]] [[0.439]
 [0.517]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.529]
 [0.492]
 [0.479]
 [0.521]
 [0.506]
 [0.521]] [[-0.371]
 [-0.054]
 [-0.547]
 [-0.857]
 [ 0.   ]
 [-0.434]
 [ 0.   ]] [[0.487]
 [0.529]
 [0.492]
 [0.479]
 [0.521]
 [0.506]
 [0.521]]
maxi score, test score, baseline:  -0.988 -1.0 -0.988
probs:  [0.39220449148451264, 0.3637326774540165, 0.05699047482847807, 0.04872419500086754, 0.11679783730847446, 0.02155032392365082]
using another actor
Printing some Q and Qe and total Qs values:  [[-0.  ]
 [ 1.46]
 [-0.  ]
 [-0.  ]
 [-0.  ]
 [-0.  ]
 [-0.  ]] [[1.145]
 [0.407]
 [1.145]
 [1.145]
 [1.145]
 [1.145]
 [1.145]] [[1.284]
 [2.728]
 [1.284]
 [1.284]
 [1.284]
 [1.284]
 [1.284]]
from probs:  [0.3922271072531728, 0.36369310440407, 0.05699092887509746, 0.048738881760083626, 0.11678801520182426, 0.021561962505752055]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
from probs:  [0.3922271072531728, 0.36369310440407, 0.05699092887509746, 0.048738881760083626, 0.11678801520182426, 0.021561962505752055]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.749]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.714]] [[1.991]
 [2.292]
 [1.885]
 [1.885]
 [1.885]
 [1.885]
 [1.874]] [[0.725]
 [0.749]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.714]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.359]
 [0.435]
 [0.428]
 [0.42 ]
 [0.428]
 [0.423]] [[-0.141]
 [ 1.417]
 [-1.363]
 [-1.994]
 [-1.999]
 [-1.457]
 [-0.86 ]] [[ 0.905]
 [ 1.676]
 [ 0.188]
 [-0.191]
 [-0.207]
 [ 0.121]
 [ 0.46 ]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.297]
 [0.336]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-1.026]
 [ 0.   ]] [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.297]
 [0.336]]
Printing some Q and Qe and total Qs values:  [[0.957]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[2.782]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[2.016]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]]
in main func line 156:  3619
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.331]
 [0.325]
 [0.328]
 [0.327]
 [0.326]
 [0.324]] [[-2.71 ]
 [-2.363]
 [-2.797]
 [-2.834]
 [-2.888]
 [-2.778]
 [-2.79 ]] [[0.331]
 [0.331]
 [0.325]
 [0.328]
 [0.327]
 [0.326]
 [0.324]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.3922271072531728, 0.36369310440407, 0.05699092887509746, 0.048738881760083626, 0.11678801520182426, 0.021561962505752055]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.13 ]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[-0.717]
 [ 0.449]
 [-0.599]
 [-0.599]
 [-0.599]
 [-0.599]
 [-0.599]] [[0.753]
 [1.268]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.46164346323915945
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.3922271072531728, 0.36369310440407, 0.05699092887509746, 0.048738881760083626, 0.11678801520182426, 0.021561962505752055]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.3922271072531728, 0.36369310440407, 0.05699092887509746, 0.048738881760083626, 0.11678801520182426, 0.021561962505752055]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.3922271072531728, 0.36369310440407, 0.05699092887509746, 0.048738881760083626, 0.11678801520182426, 0.021561962505752055]
from probs:  [0.3922271072531728, 0.36369310440407, 0.05699092887509746, 0.048738881760083626, 0.11678801520182426, 0.021561962505752055]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.415]
 [0.391]
 [0.405]
 [0.387]
 [0.432]
 [0.402]] [[2.061]
 [2.031]
 [2.757]
 [2.288]
 [1.892]
 [2.296]
 [2.331]] [[0.717]
 [0.704]
 [1.258]
 [0.898]
 [0.543]
 [0.948]
 [0.927]]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.803]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[1.123]
 [0.706]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[0.399]
 [0.45 ]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.008362888605010341
using another actor
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.492]
 [0.499]
 [0.487]
 [0.48 ]
 [0.494]
 [0.481]] [[1.199]
 [1.199]
 [1.428]
 [1.07 ]
 [1.619]
 [2.038]
 [1.336]] [[0.48 ]
 [0.492]
 [0.499]
 [0.487]
 [0.48 ]
 [0.494]
 [0.481]]
line 256 mcts: sample exp_bonus 3.8462541066495985
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.3922271072531728, 0.36369310440407, 0.05699092887509746, 0.048738881760083626, 0.11678801520182426, 0.021561962505752055]
actor:  1 policy actor:  1  step number:  71 total reward:  0.34999999999999953  reward:  1.0 rdn_beta:  0.167
from probs:  [0.3922271072531728, 0.36369310440407, 0.05699092887509746, 0.048738881760083626, 0.11678801520182426, 0.021561962505752055]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[-2.604]
 [-2.604]
 [-2.604]
 [-2.604]
 [-2.604]
 [-2.604]
 [-2.604]] [[0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]]
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[ 1.181]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [ 1.181]] [[0.402]
 [1.69 ]
 [1.69 ]
 [1.69 ]
 [1.69 ]
 [1.69 ]
 [0.402]] [[2.141]
 [1.704]
 [1.704]
 [1.704]
 [1.704]
 [1.704]
 [2.141]]
using another actor
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.205]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[-0.789]
 [-0.353]
 [-0.789]
 [-0.789]
 [-0.789]
 [-0.789]
 [-0.789]] [[0.385]
 [0.769]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]]
line 256 mcts: sample exp_bonus 0.06338262677803536
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.528]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[ 0.077]
 [ 0.271]
 [-0.723]
 [-0.723]
 [-0.723]
 [-0.723]
 [-0.723]] [[2.111]
 [2.354]
 [2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]]
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.761]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]] [[-0.002]
 [ 0.32 ]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[1.574]
 [1.676]
 [1.574]
 [1.574]
 [1.574]
 [1.574]
 [1.574]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.396]
 [0.357]
 [0.33 ]
 [0.33 ]
 [0.331]
 [0.37 ]] [[-2.623]
 [-2.417]
 [ 0.   ]
 [-2.615]
 [-2.609]
 [-2.608]
 [-2.542]] [[0.306]
 [0.396]
 [0.357]
 [0.33 ]
 [0.33 ]
 [0.331]
 [0.37 ]]
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[-2.451]
 [-2.451]
 [-2.451]
 [-2.451]
 [-2.451]
 [-2.451]
 [-2.451]] [[0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]]
using another actor
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.295]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]] [[-2.315]
 [-0.023]
 [-2.315]
 [-2.315]
 [-2.315]
 [-2.315]
 [-2.315]] [[0.3  ]
 [0.295]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.41291621905804377
from probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.444]
 [0.438]
 [0.438]
 [0.444]
 [0.436]
 [0.436]] [[-0.623]
 [-0.058]
 [-0.483]
 [-0.416]
 [ 0.   ]
 [-0.507]
 [ 0.21 ]] [[0.432]
 [0.444]
 [0.438]
 [0.438]
 [0.444]
 [0.436]
 [0.436]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[2.345]
 [2.345]
 [2.345]
 [2.345]
 [2.345]
 [2.345]
 [2.345]] [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.593]
 [0.605]
 [0.602]
 [0.604]
 [0.583]
 [0.593]] [[2.565]
 [2.672]
 [2.231]
 [2.464]
 [2.477]
 [2.44 ]
 [2.311]] [[0.597]
 [0.593]
 [0.605]
 [0.602]
 [0.604]
 [0.583]
 [0.593]]
3647 4881
using explorer policy with actor:  0
from probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  0
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
rdn beta is 0 so we're just using the maxi policy
using another actor
from probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.6355206
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.491]
 [0.482]
 [0.478]
 [0.477]
 [0.479]
 [0.476]] [[2.44 ]
 [3.074]
 [2.591]
 [2.532]
 [2.469]
 [2.458]
 [2.427]] [[0.465]
 [0.491]
 [0.482]
 [0.478]
 [0.477]
 [0.479]
 [0.476]]
using explorer policy with actor:  0
siam score:  -0.63363844
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.581]
 [0.555]
 [0.561]
 [0.552]
 [0.553]
 [0.569]] [[0.479]
 [0.658]
 [0.618]
 [0.732]
 [1.312]
 [0.39 ]
 [0.401]] [[1.325]
 [1.543]
 [1.456]
 [1.577]
 [2.114]
 [1.233]
 [1.274]]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.606]
 [0.655]
 [0.658]
 [0.662]
 [0.647]
 [0.646]] [[0.836]
 [1.578]
 [0.61 ]
 [0.499]
 [0.543]
 [0.604]
 [0.378]] [[0.64 ]
 [0.606]
 [0.655]
 [0.658]
 [0.662]
 [0.647]
 [0.646]]
using another actor
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 2.5598385187556465
line 256 mcts: sample exp_bonus 1.7268368575569744
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.776]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[2.103]
 [2.59 ]
 [2.103]
 [2.103]
 [2.103]
 [2.103]
 [2.103]] [[0.739]
 [0.776]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.49 ]
 [0.471]
 [0.471]
 [0.471]
 [0.49 ]
 [0.471]] [[3.53 ]
 [3.322]
 [3.53 ]
 [3.53 ]
 [3.53 ]
 [3.235]
 [3.53 ]] [[0.702]
 [0.672]
 [0.702]
 [0.702]
 [0.702]
 [0.642]
 [0.702]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[1.502]
 [1.502]
 [1.502]
 [1.502]
 [1.502]
 [1.502]
 [1.502]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
siam score:  -0.6314239
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.076]
 [0.195]
 [0.214]
 [0.226]
 [0.194]
 [0.196]] [[0.988]
 [1.015]
 [0.854]
 [0.605]
 [0.846]
 [0.963]
 [1.199]] [[-0.236]
 [-0.458]
 [-0.274]
 [-0.318]
 [-0.214]
 [-0.239]
 [-0.156]]
siam score:  -0.63311285
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.379]
 [0.452]
 [0.498]
 [0.449]
 [0.452]
 [0.434]] [[ 0.272]
 [ 0.511]
 [ 0.   ]
 [-0.912]
 [-0.725]
 [ 0.   ]
 [ 0.107]] [[0.979]
 [1.167]
 [0.974]
 [0.459]
 [0.485]
 [0.974]
 [1.009]]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.2  ]
 [0.269]
 [0.238]
 [0.268]
 [0.238]
 [0.237]] [[-0.587]
 [ 0.403]
 [-0.501]
 [-0.35 ]
 [-0.519]
 [-0.363]
 [-0.435]] [[0.242]
 [0.2  ]
 [0.269]
 [0.238]
 [0.268]
 [0.238]
 [0.237]]
using explorer policy with actor:  1
3678 4917
line 256 mcts: sample exp_bonus 1.257605829315797
line 256 mcts: sample exp_bonus -0.14793664466237288
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
using another actor
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.163 0.122 0.122 0.041 0.265 0.184 0.102]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41163458284253596, 0.35207961336164867, 0.05517108782236508, 0.04718254604768174, 0.11305872654612939, 0.020873443379639166]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
using explorer policy with actor:  0
using explorer policy with actor:  0
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.396]
 [0.377]
 [0.383]
 [0.383]
 [0.383]
 [0.399]] [[2.28 ]
 [1.866]
 [2.242]
 [2.445]
 [2.445]
 [2.445]
 [1.824]] [[-0.31 ]
 [-0.437]
 [-0.349]
 [-0.27 ]
 [-0.27 ]
 [-0.27 ]
 [-0.446]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.38999999999999957  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4309076719103478, 0.34054653961291786, 0.0533638482080695, 0.04563698713836241, 0.10935526124535734, 0.0201896918849451]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.584]
 [0.601]
 [0.601]
 [0.569]
 [0.575]
 [0.57 ]] [[ 0.377]
 [ 0.313]
 [ 1.283]
 [ 1.283]
 [-0.298]
 [-0.321]
 [-0.241]] [[0.835]
 [0.894]
 [1.574]
 [1.574]
 [0.458]
 [0.456]
 [0.498]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.337]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.272]] [[1.61 ]
 [1.827]
 [1.61 ]
 [1.61 ]
 [1.61 ]
 [1.61 ]
 [2.194]] [[-0.239]
 [-0.069]
 [-0.239]
 [-0.239]
 [-0.239]
 [-0.239]
 [-0.076]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.611]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.589]] [[1.826]
 [2.386]
 [1.826]
 [1.826]
 [1.826]
 [1.826]
 [2.458]] [[0.586]
 [0.611]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.589]]
siam score:  -0.6338754
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.356]
 [0.402]
 [0.402]
 [0.362]
 [0.373]
 [0.402]] [[3.507]
 [3.001]
 [3.634]
 [3.634]
 [3.249]
 [3.58 ]
 [3.634]] [[-0.275]
 [-0.463]
 [-0.159]
 [-0.159]
 [-0.368]
 [-0.236]
 [-0.159]]
using another actor
from probs:  [0.4309076719103478, 0.34054653961291786, 0.0533638482080695, 0.04563698713836241, 0.10935526124535734, 0.0201896918849451]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]] [[0.396]
 [0.387]
 [0.391]
 [0.401]
 [0.401]
 [0.396]
 [0.401]] [[0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4309076719103478, 0.34054653961291786, 0.0533638482080695, 0.04563698713836241, 0.10935526124535734, 0.0201896918849451]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.643]
 [0.636]
 [0.632]
 [0.627]
 [0.629]
 [0.642]] [[1.178]
 [1.739]
 [0.893]
 [1.122]
 [1.27 ]
 [1.469]
 [1.023]] [[0.625]
 [0.643]
 [0.636]
 [0.632]
 [0.627]
 [0.629]
 [0.642]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4309076719103478, 0.34054653961291786, 0.0533638482080695, 0.04563698713836241, 0.10935526124535734, 0.0201896918849451]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.314]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[0.   ]
 [0.185]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[-0.73 ]
 [-0.379]
 [-0.73 ]
 [-0.73 ]
 [-0.73 ]
 [-0.73 ]
 [-0.73 ]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
actor:  1 policy actor:  1  step number:  77 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.667
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.412]
 [0.392]
 [0.371]
 [0.366]
 [0.378]
 [0.381]] [[-2.328]
 [-2.306]
 [-2.08 ]
 [-1.46 ]
 [-1.371]
 [-1.959]
 [-2.071]] [[0.394]
 [0.412]
 [0.392]
 [0.371]
 [0.366]
 [0.378]
 [0.381]]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.486]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[-1.258]
 [-0.941]
 [-1.258]
 [-1.258]
 [-1.258]
 [-1.258]
 [-1.258]] [[0.473]
 [0.486]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]]
line 256 mcts: sample exp_bonus 0.49022725604383677
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.465]
 [0.467]
 [0.469]
 [0.492]
 [0.488]
 [0.472]] [[-0.94 ]
 [-0.803]
 [-0.624]
 [ 0.   ]
 [-2.422]
 [-2.54 ]
 [-0.993]] [[0.476]
 [0.465]
 [0.467]
 [0.469]
 [0.492]
 [0.488]
 [0.472]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.546]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[0.043]
 [0.455]
 [0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[0.506]
 [0.546]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]]
siam score:  -0.63169897
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.543]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[0.47 ]
 [0.624]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]] [[0.578]
 [0.543]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.355]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[1.069]
 [0.668]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]] [[0.354]
 [0.355]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.488]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]] [[0.529]
 [0.599]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[-0.251]
 [-0.023]
 [-0.251]
 [-0.251]
 [-0.251]
 [-0.251]
 [-0.251]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.539]
 [0.53 ]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[-3.497]
 [ 0.388]
 [-1.617]
 [-1.4  ]
 [-1.4  ]
 [-1.4  ]
 [-1.4  ]] [[0.651]
 [0.539]
 [0.53 ]
 [0.521]
 [0.521]
 [0.521]
 [0.521]]
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]] [[-1.526]
 [-1.783]
 [-1.783]
 [-1.783]
 [-1.783]
 [-1.783]
 [-1.783]] [[0.328]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.474]
 [0.498]
 [0.498]
 [0.472]
 [0.498]
 [0.474]] [[-1.568]
 [ 0.577]
 [ 0.   ]
 [ 0.   ]
 [-1.43 ]
 [ 0.   ]
 [-1.562]] [[0.465]
 [0.474]
 [0.498]
 [0.498]
 [0.472]
 [0.498]
 [0.474]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[-0.247]
 [-0.247]
 [-0.247]
 [-0.247]
 [-0.247]
 [-0.247]
 [-0.247]] [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4196289897807218, 0.33163299172083643, 0.051967089875863126, 0.07061671657928568, 0.10649297006051808, 0.019661241982774958]
using explorer policy with actor:  0
using explorer policy with actor:  0
start point for exploration sampling:  10935
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  94 total reward:  0.14499999999999935  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[0.981]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[2.011]
 [1.84 ]
 [1.84 ]
 [1.84 ]
 [1.84 ]
 [1.84 ]
 [1.84 ]]
3706 4951
from probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.878]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[-1.01 ]
 [ 1.001]
 [-1.01 ]
 [-1.01 ]
 [-1.01 ]
 [-1.01 ]
 [-1.01 ]] [[0.648]
 [1.93 ]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]]
from probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.558]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[0.716]
 [0.786]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]] [[0.529]
 [0.558]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.39589753331342586
3710 4959
from probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
using another actor
from probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
in main func line 156:  3711
from probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
line 256 mcts: sample exp_bonus -2.0004569998822843
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.369]
 [0.401]
 [0.401]
 [0.402]
 [0.406]
 [0.402]] [[-1.369]
 [-0.56 ]
 [-1.356]
 [-1.505]
 [-1.51 ]
 [-1.405]
 [-1.428]] [[-0.019]
 [ 0.189]
 [-0.012]
 [-0.061]
 [-0.062]
 [-0.018]
 [-0.034]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
siam score:  -0.6329702
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
line 256 mcts: sample exp_bonus -0.4108418450763974
rdn probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
line 256 mcts: sample exp_bonus 0.37406686304387243
from probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.3068213969614097
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]] [[ 1.408]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]] [[1.837]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.233]
 [0.492]
 [0.21 ]] [[ 1.763]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.209]
 [-1.804]
 [ 0.   ]] [[2.001]
 [0.992]
 [0.992]
 [0.992]
 [0.88 ]
 [0.095]
 [0.992]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.104]
 [0.123]
 [0.164]
 [0.167]
 [0.162]
 [0.15 ]] [[-1.136]
 [-0.064]
 [-0.525]
 [-2.506]
 [-2.615]
 [-2.322]
 [-1.866]] [[0.148]
 [0.104]
 [0.123]
 [0.164]
 [0.167]
 [0.162]
 [0.15 ]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
using another actor
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
3728 4995
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.768]] [[0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [1.158]] [[0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.768]]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.821]] [[-2.908]
 [-2.908]
 [-2.908]
 [-2.908]
 [-2.908]
 [-2.908]
 [-1.743]] [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.821]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
line 256 mcts: sample exp_bonus 0.26703978402067235
line 256 mcts: sample exp_bonus -0.18469055451914945
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]] [[ 1.134]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]] [[0.788]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.917]] [[0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [6.324]] [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [2.199]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.526]
 [0.516]] [[-0.663]
 [-0.663]
 [-0.663]
 [-0.663]
 [-0.663]
 [-0.474]
 [-0.663]] [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.74 ]
 [0.657]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.756]
 [0.759]] [[1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.503]
 [1.458]] [[1.563]
 [1.563]
 [1.563]
 [1.563]
 [1.563]
 [1.578]
 [1.569]]
siam score:  -0.6422673
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.443]
 [0.604]
 [0.443]
 [0.337]
 [0.586]
 [0.443]] [[0.928]
 [0.724]
 [0.157]
 [0.724]
 [0.922]
 [0.709]
 [0.724]] [[0.366]
 [0.525]
 [0.471]
 [0.525]
 [0.446]
 [0.802]
 [0.525]]
line 256 mcts: sample exp_bonus -5.328390321861541
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.474]
 [1.037]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[0.939]
 [0.495]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]] [[1.429]
 [2.124]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.485]
 [0.417]
 [0.417]
 [0.43 ]
 [0.417]
 [0.417]] [[-0.856]
 [-1.001]
 [-0.856]
 [-0.856]
 [-0.905]
 [-0.856]
 [-0.856]] [[0.14 ]
 [0.227]
 [0.14 ]
 [0.14 ]
 [0.149]
 [0.14 ]
 [0.14 ]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  0.333
from probs:  [0.4107642234067476, 0.3246271626072579, 0.07199451998139716, 0.06912492094595818, 0.10424327968390605, 0.01924589337473301]
siam score:  -0.635944
Printing some Q and Qe and total Qs values:  [[1.159]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]] [[0.426]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[2.062]
 [1.867]
 [1.867]
 [1.867]
 [1.867]
 [1.867]
 [1.867]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.435]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[-0.337]
 [ 0.701]
 [-0.337]
 [-0.337]
 [-0.337]
 [-0.337]
 [-0.337]] [[0.355]
 [0.628]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.3974386010338771, 0.346536966039866, 0.06965894221799934, 0.06688243598599554, 0.10086151832100923, 0.01862153640125271]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.913]
 [1.353]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]] [[-0.147]
 [ 1.61 ]
 [-0.147]
 [-0.147]
 [-0.147]
 [-0.147]
 [-0.147]] [[1.469]
 [2.715]
 [1.469]
 [1.469]
 [1.469]
 [1.469]
 [1.469]]
siam score:  -0.63975173
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.6475279
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
using explorer policy with actor:  1
3753 5039
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.164]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]] [[0.107]
 [0.307]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[0.121]
 [0.309]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.3974386010338771, 0.346536966039866, 0.06965894221799934, 0.06688243598599554, 0.10086151832100923, 0.01862153640125271]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.3974386010338771, 0.346536966039866, 0.06965894221799934, 0.06688243598599554, 0.10086151832100923, 0.01862153640125271]
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[-2.338]
 [-2.338]
 [-2.338]
 [-2.338]
 [-2.338]
 [-2.338]
 [-2.338]] [[0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.456]
 [0.485]
 [0.509]
 [0.507]
 [0.516]
 [0.538]] [[-0.688]
 [ 0.906]
 [-0.829]
 [-0.85 ]
 [-0.843]
 [-0.806]
 [-0.745]] [[0.465]
 [0.456]
 [0.485]
 [0.509]
 [0.507]
 [0.516]
 [0.538]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [0.626]
 [0.483]
 [0.483]
 [0.483]
 [0.572]
 [0.569]] [[-0.525]
 [ 0.105]
 [ 0.77 ]
 [ 0.77 ]
 [ 0.77 ]
 [-0.303]
 [-0.35 ]] [[0.733]
 [0.476]
 [0.412]
 [0.412]
 [0.412]
 [0.232]
 [0.209]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.841]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[1.283]
 [0.362]
 [1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]] [[-0.523]
 [ 0.822]
 [-0.523]
 [-0.523]
 [-0.523]
 [-0.523]
 [-0.523]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
first move QE:  0.37758021673048786
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.705]
 [0.377]] [[ 0.119]
 [ 0.119]
 [ 0.119]
 [ 0.119]
 [ 0.119]
 [-0.193]
 [ 0.119]] [[1.648]
 [1.648]
 [1.648]
 [1.648]
 [1.648]
 [2.016]
 [1.648]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5840411672185697
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.429]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[-2.052]
 [-1.828]
 [-2.052]
 [-2.052]
 [-2.052]
 [-2.052]
 [-2.052]] [[0.41 ]
 [0.429]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.39743860103387707, 0.34653696603986606, 0.06965894221799936, 0.06688243598599555, 0.10086151832100926, 0.018621536401252713]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.39743860103387707, 0.34653696603986606, 0.06965894221799936, 0.06688243598599555, 0.10086151832100926, 0.018621536401252713]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.39743860103387707, 0.34653696603986606, 0.06965894221799936, 0.06688243598599555, 0.10086151832100926, 0.018621536401252713]
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.774]
 [0.743]
 [0.743]
 [0.747]
 [0.734]
 [0.744]] [[1.061]
 [0.109]
 [1.061]
 [1.061]
 [0.628]
 [0.603]
 [0.57 ]] [[1.577]
 [1.158]
 [1.577]
 [1.577]
 [1.377]
 [1.352]
 [1.346]]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[-1.035]
 [-1.035]
 [-1.035]
 [-1.035]
 [-1.035]
 [-1.035]
 [-1.035]] [[2.119]
 [2.119]
 [2.119]
 [2.119]
 [2.119]
 [2.119]
 [2.119]]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]] [[1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.701]] [[1.714]
 [1.714]
 [1.714]
 [1.714]
 [1.714]
 [1.714]
 [1.714]]
from probs:  [0.3974386010338771, 0.346536966039866, 0.06965894221799934, 0.06688243598599554, 0.10086151832100923, 0.01862153640125271]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.491]
 [0.421]
 [0.504]
 [0.469]
 [0.487]
 [0.473]] [[2.594]
 [2.135]
 [1.986]
 [2.083]
 [2.415]
 [2.258]
 [1.988]] [[0.444]
 [0.491]
 [0.421]
 [0.504]
 [0.469]
 [0.487]
 [0.473]]
from probs:  [0.3974386010338771, 0.346536966039866, 0.06965894221799934, 0.06688243598599554, 0.10086151832100923, 0.01862153640125271]
siam score:  -0.6517843
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.3974386010338771, 0.346536966039866, 0.06965894221799934, 0.06688243598599554, 0.10086151832100923, 0.01862153640125271]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.451]
 [0.495]
 [0.495]
 [0.538]
 [0.495]
 [0.495]] [[1.298]
 [0.059]
 [1.298]
 [1.298]
 [0.269]
 [1.298]
 [1.298]] [[1.93 ]
 [0.37 ]
 [1.93 ]
 [1.93 ]
 [0.776]
 [1.93 ]
 [1.93 ]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.3974386010338771, 0.346536966039866, 0.06965894221799934, 0.06688243598599554, 0.10086151832100923, 0.01862153640125271]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.495]
 [0.475]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[-1.834]
 [-2.265]
 [-2.015]
 [-1.834]
 [-1.834]
 [-1.834]
 [-1.834]] [[0.603]
 [0.503]
 [0.548]
 [0.603]
 [0.603]
 [0.603]
 [0.603]]
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[-1.485]
 [-1.485]
 [-1.485]
 [-1.485]
 [-1.485]
 [-1.485]
 [-1.485]] [[0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]]
3775 5100
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.383]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[-2.123]
 [-2.314]
 [-2.123]
 [-2.123]
 [-2.123]
 [-2.123]
 [-2.123]] [[0.187]
 [0.205]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]]
from probs:  [0.3974386010338771, 0.346536966039866, 0.06965894221799934, 0.06688243598599554, 0.10086151832100923, 0.01862153640125271]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.059]
 [0.103]
 [0.1  ]
 [0.101]
 [0.101]
 [0.102]] [[-2.106]
 [ 1.383]
 [-1.489]
 [-1.732]
 [-1.715]
 [-1.661]
 [-1.618]] [[-0.836]
 [ 0.269]
 [-0.603]
 [-0.69 ]
 [-0.682]
 [-0.665]
 [-0.648]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.3974386010338771, 0.346536966039866, 0.06965894221799934, 0.06688243598599554, 0.10086151832100923, 0.01862153640125271]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.447]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[-0.982]
 [ 1.027]
 [-0.982]
 [-0.982]
 [-0.982]
 [-0.982]
 [-0.982]] [[0.776]
 [1.684]
 [0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.776]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
line 256 mcts: sample exp_bonus -0.7328729800644709
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.352]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]] [[1.447]
 [2.124]
 [1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]] [[-0.359]
 [ 0.24 ]
 [-0.359]
 [-0.359]
 [-0.359]
 [-0.359]
 [-0.359]]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]] [[-0.557]
 [-0.557]
 [-0.557]
 [-0.557]
 [-0.557]
 [-0.557]
 [-0.557]] [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]]
UNIT TEST: sample policy line 217 mcts : [0.061 0.041 0.041 0.02  0.041 0.776 0.02 ]
from probs:  [0.3974386010338771, 0.346536966039866, 0.06965894221799934, 0.06688243598599554, 0.10086151832100923, 0.01862153640125271]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.013]] [[0.432]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.265]] [[1.392]
 [1.475]
 [1.475]
 [1.475]
 [1.475]
 [1.475]
 [1.315]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]] [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.53 ]
 [0.515]
 [0.517]
 [0.518]
 [0.519]
 [0.517]] [[-2.408]
 [-2.485]
 [-2.379]
 [-2.376]
 [-2.387]
 [-2.276]
 [-2.285]] [[0.509]
 [0.53 ]
 [0.515]
 [0.517]
 [0.518]
 [0.519]
 [0.517]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.38499999999999956  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
first move QE:  0.36926967307245423
UNIT TEST: sample policy line 217 mcts : [0.306 0.02  0.02  0.184 0.02  0.02  0.429]
line 256 mcts: sample exp_bonus -2.8347364617822133
3788 5120
siam score:  -0.6686925
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.342]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[-1.502]
 [-0.573]
 [-1.502]
 [-1.502]
 [-1.502]
 [-1.502]
 [-1.502]] [[0.428]
 [1.308]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41498567363090705, 0.3364455308581744, 0.0676304177917271, 0.06493476565443078, 0.09792434978140134, 0.018079262283359227]
Printing some Q and Qe and total Qs values:  [[ 0.043]
 [ 0.043]
 [-0.004]
 [ 0.043]
 [ 0.043]
 [ 0.146]
 [ 0.043]] [[ 0.354]
 [ 0.354]
 [-0.213]
 [ 0.354]
 [ 0.354]
 [-0.344]
 [ 0.354]] [[ 0.043]
 [ 0.043]
 [-0.004]
 [ 0.043]
 [ 0.043]
 [ 0.146]
 [ 0.043]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41498567363090705, 0.3364455308581744, 0.0676304177917271, 0.06493476565443078, 0.09792434978140134, 0.018079262283359227]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41498567363090705, 0.3364455308581744, 0.0676304177917271, 0.06493476565443078, 0.09792434978140134, 0.018079262283359227]
Printing some Q and Qe and total Qs values:  [[1.463]
 [1.463]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]] [[0.361]
 [0.355]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]] [[2.834]
 [2.828]
 [2.496]
 [2.496]
 [2.496]
 [2.496]
 [2.496]]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.028]
 [-0.04 ]
 [-0.027]
 [-0.025]
 [-0.032]
 [-0.024]] [[2.739]
 [2.932]
 [3.161]
 [3.131]
 [3.077]
 [2.869]
 [3.127]] [[0.17 ]
 [0.304]
 [0.444]
 [0.441]
 [0.407]
 [0.255]
 [0.443]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41498567363090705, 0.3364455308581744, 0.0676304177917271, 0.06493476565443078, 0.09792434978140134, 0.018079262283359227]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.356]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[0.8  ]
 [0.585]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[1.688]
 [1.579]
 [1.558]
 [1.558]
 [1.558]
 [1.558]
 [1.558]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.062]
 [0.065]
 [0.059]
 [0.057]
 [0.054]
 [0.054]] [[1.606]
 [1.546]
 [1.4  ]
 [1.483]
 [1.525]
 [2.645]
 [2.645]] [[-1.028]
 [-1.046]
 [-1.159]
 [-1.101]
 [-1.07 ]
 [-0.17 ]
 [-0.17 ]]
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.41498567363090705, 0.3364455308581744, 0.0676304177917271, 0.06493476565443078, 0.09792434978140134, 0.018079262283359227]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[-2.198]
 [-2.198]
 [-2.198]
 [-2.198]
 [-2.198]
 [-2.198]
 [-2.198]] [[0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]]
actor:  1 policy actor:  1  step number:  81 total reward:  0.0899999999999993  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0985911536776838
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[-2.823]
 [-2.823]
 [-2.823]
 [-2.823]
 [-2.823]
 [-2.823]
 [-2.823]] [[0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]]
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.524]
 [0.815]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[1.256]
 [1.551]
 [0.802]
 [1.256]
 [1.256]
 [1.256]
 [1.256]] [[0.505]
 [0.524]
 [0.815]
 [0.505]
 [0.505]
 [0.505]
 [0.505]]
Printing some Q and Qe and total Qs values:  [[1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]] [[0.357]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[2.694]
 [2.702]
 [2.702]
 [2.702]
 [2.702]
 [2.702]
 [2.702]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[-2.417]
 [-2.417]
 [-2.417]
 [-2.417]
 [-2.417]
 [-2.417]
 [-2.417]] [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4076605511813471, 0.3305067602746499, 0.08408814583367184, 0.06378856919542907, 0.09619583745309314, 0.017760136061808974]
line 256 mcts: sample exp_bonus -0.6310563633517325
siam score:  -0.6677123
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4147123380829982, 0.33622392676588375, 0.08554271798605613, 0.06489199555204822, 0.0978598506733552, 0.0007691709396585474]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4147123380829982, 0.33622392676588375, 0.08554271798605613, 0.06489199555204822, 0.0978598506733552, 0.0007691709396585474]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4147123380829982, 0.33622392676588375, 0.08554271798605613, 0.06489199555204822, 0.0978598506733552, 0.0007691709396585474]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.987918627520742
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4147123380829982, 0.33622392676588375, 0.08554271798605613, 0.06489199555204822, 0.0978598506733552, 0.0007691709396585474]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
in main func line 156:  3807
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.367]
 [0.347]
 [0.335]
 [0.346]
 [0.342]
 [0.335]] [[-2.685]
 [-2.457]
 [-2.639]
 [-2.33 ]
 [-2.678]
 [-2.645]
 [-2.33 ]] [[0.342]
 [0.367]
 [0.347]
 [0.335]
 [0.346]
 [0.342]
 [0.335]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
from probs:  [0.4147123380829982, 0.33622392676588375, 0.08554271798605613, 0.06489199555204822, 0.0978598506733552, 0.0007691709396585474]
Printing some Q and Qe and total Qs values:  [[1.068]
 [1.134]
 [1.123]
 [1.123]
 [1.123]
 [1.28 ]
 [1.123]] [[0.807]
 [0.537]
 [0.448]
 [0.448]
 [0.448]
 [0.819]
 [0.448]] [[2.471]
 [2.493]
 [2.465]
 [2.465]
 [2.465]
 [2.705]
 [2.465]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.675]
 [0.688]
 [0.682]
 [0.682]
 [0.871]
 [0.682]] [[-0.619]
 [-0.736]
 [-0.982]
 [-0.619]
 [-0.619]
 [-0.666]
 [-0.619]] [[0.692]
 [0.64 ]
 [0.584]
 [0.692]
 [0.692]
 [1.055]
 [0.692]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.554]
 [0.537]
 [0.52 ]
 [0.535]
 [0.537]
 [0.537]] [[1.034]
 [2.196]
 [1.542]
 [1.723]
 [1.192]
 [1.542]
 [1.542]] [[0.535]
 [0.554]
 [0.537]
 [0.52 ]
 [0.535]
 [0.537]
 [0.537]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.541]
 [0.541]
 [0.566]
 [0.541]
 [0.541]
 [0.541]] [[2.014]
 [2.014]
 [2.014]
 [2.386]
 [2.014]
 [2.014]
 [2.014]] [[0.541]
 [0.541]
 [0.541]
 [0.566]
 [0.541]
 [0.541]
 [0.541]]
from probs:  [0.4323960074076893, 0.3079199784460126, 0.089190328628636, 0.06765904269722404, 0.10203267381016029, 0.0008019690102777087]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4323960074076893, 0.3079199784460126, 0.089190328628636, 0.06765904269722404, 0.10203267381016029, 0.0008019690102777087]
siam score:  -0.6818459
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.391]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]] [[-2.223]
 [-1.888]
 [-2.223]
 [-2.223]
 [-2.223]
 [-2.223]
 [-2.223]] [[0.344]
 [0.391]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
from probs:  [0.4323960074076893, 0.3079199784460126, 0.089190328628636, 0.06765904269722404, 0.10203267381016029, 0.0008019690102777087]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.003]
 [-0.01 ]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[0.43 ]
 [0.34 ]
 [0.43 ]
 [0.441]
 [0.45 ]
 [0.518]
 [0.722]] [[-0.009]
 [-0.003]
 [-0.01 ]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
siam score:  -0.68327147
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.452325236768421
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.46 ]
 [0.425]
 [0.426]
 [0.427]
 [0.429]
 [0.435]] [[-1.209]
 [-1.064]
 [-1.476]
 [-1.425]
 [-1.418]
 [-1.355]
 [-1.183]] [[0.562]
 [0.729]
 [0.385]
 [0.42 ]
 [0.427]
 [0.472]
 [0.599]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.6812515
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4323960074076893, 0.3079199784460126, 0.089190328628636, 0.06765904269722404, 0.10203267381016029, 0.0008019690102777087]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.416]
 [0.416]
 [0.416]
 [0.017]
 [0.416]
 [0.416]] [[ 0.414]
 [-0.779]
 [-0.779]
 [-0.779]
 [ 1.143]
 [-0.779]
 [-0.779]] [[1.488]
 [1.115]
 [1.115]
 [1.115]
 [1.754]
 [1.115]
 [1.115]]
from probs:  [0.4323960074076893, 0.3079199784460126, 0.089190328628636, 0.06765904269722404, 0.10203267381016029, 0.0008019690102777087]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4323960074076893, 0.3079199784460126, 0.089190328628636, 0.06765904269722404, 0.10203267381016029, 0.0008019690102777087]
line 256 mcts: sample exp_bonus 1.983273741552103
3823 5233
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.454]
 [0.385]
 [0.381]
 [0.412]
 [0.411]
 [0.411]] [[-1.239]
 [ 0.418]
 [-1.607]
 [-1.761]
 [-0.995]
 [-0.91 ]
 [-0.91 ]] [[0.384]
 [0.454]
 [0.385]
 [0.381]
 [0.412]
 [0.411]
 [0.411]]
siam score:  -0.6815164
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.3498351617747431
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.005]
 [0.168]
 [0.168]] [[0.749]
 [0.749]
 [0.749]
 [0.749]
 [1.333]
 [0.749]
 [0.749]] [[1.752]
 [1.752]
 [1.752]
 [1.752]
 [2.001]
 [1.752]
 [1.752]]
Printing some Q and Qe and total Qs values:  [[1.467]
 [1.47 ]
 [0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[0.349]
 [0.334]
 [1.73 ]
 [1.73 ]
 [1.73 ]
 [1.73 ]
 [1.73 ]] [[2.408]
 [2.396]
 [2.002]
 [2.002]
 [2.002]
 [2.002]
 [2.002]]
using another actor
from probs:  [0.4323960074076893, 0.3079199784460126, 0.089190328628636, 0.06765904269722404, 0.10203267381016029, 0.0008019690102777087]
Printing some Q and Qe and total Qs values:  [[1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]] [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4323960074076893, 0.3079199784460126, 0.089190328628636, 0.06765904269722404, 0.10203267381016029, 0.0008019690102777087]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
probs:  [0.4323960074076893, 0.3079199784460126, 0.089190328628636, 0.06765904269722404, 0.10203267381016029, 0.0008019690102777087]
from probs:  [0.4323960074076893, 0.3079199784460126, 0.089190328628636, 0.06765904269722404, 0.10203267381016029, 0.0008019690102777087]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.577]] [[1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]
 [2.52 ]] [[0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [1.645]]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.4  ]
 [0.414]
 [0.213]
 [0.414]
 [0.414]] [[0.859]
 [0.859]
 [0.486]
 [0.859]
 [1.569]
 [0.859]
 [0.859]] [[1.522]
 [1.522]
 [1.192]
 [1.522]
 [1.776]
 [1.522]
 [1.522]]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.031]
 [0.382]
 [0.42 ]
 [0.182]
 [0.342]
 [0.163]] [[ 0.49 ]
 [ 0.952]
 [ 0.121]
 [-0.395]
 [ 0.403]
 [ 0.711]
 [ 0.581]] [[0.656]
 [0.968]
 [0.861]
 [0.499]
 [0.765]
 [1.283]
 [0.88 ]]
maxi score, test score, baseline:  -0.99041 -1.0 -0.99041
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[1.264]
 [1.264]
 [1.264]
 [1.264]
 [1.264]
 [1.264]
 [1.264]] [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]]
from probs:  [0.4323960074076893, 0.3079199784460126, 0.089190328628636, 0.06765904269722404, 0.10203267381016029, 0.0008019690102777087]
from probs:  [0.43937882511068194, 0.31289261704526455, 0.09063067450374616, 0.06875167711803232, 0.0875312861132214, 0.0008149201090535553]
using explorer policy with actor:  0
from probs:  [0.43940059887805166, 0.312847901658538, 0.09065795314676281, 0.06876992774449787, 0.0875117203916353, 0.0008118981805142893]
maxi score, test score, baseline:  -0.9928 -1.0 -0.9928
from probs:  [0.43942362403520213, 0.31280056241235804, 0.0906868351375184, 0.06878925420598296, 0.08749101937208985, 0.0008087048368484687]
maxi score, test score, baseline:  -0.9953400000000001 -1.0 -0.9953400000000001
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.306]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[ 0.   ]
 [-1.154]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.307]
 [0.306]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]]
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.151]
 [0.201]
 [0.2  ]
 [0.196]
 [0.199]
 [0.197]] [[1.83 ]
 [0.816]
 [1.235]
 [0.577]
 [0.496]
 [0.515]
 [0.456]] [[0.193]
 [0.151]
 [0.201]
 [0.2  ]
 [0.196]
 [0.199]
 [0.197]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.2121],
        [-0.6196],
        [-0.6493],
        [-0.0000],
        [-0.0000],
        [-0.1759],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.0727797758985 -0.28484695452712294
-0.0727797758985 -0.6924128642630718
-0.024259925299500003 -0.6735207268600462
-0.965595015 -0.965595015
-0.97044651 -0.97044651
-0.024259925299500003 -0.20013083853442065
-0.9801 -0.9801
-0.965595015 -0.965595015
-0.9608890648499999 -0.9608890648499999
-0.4704974999999996 -0.4704974999999996
line 256 mcts: sample exp_bonus 2.8492112999005426
line 256 mcts: sample exp_bonus 0.33105654744179025
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
probs:  [0.43944554173649353, 0.31275544836339136, 0.09071436206765443, 0.06880767691499315, 0.08747130366413045, 0.0008056672533370536]
from probs:  [0.43944554173649353, 0.31275544836339136, 0.09071436206765443, 0.06880767691499315, 0.08747130366413045, 0.0008056672533370536]
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.703]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[-1.522]
 [-0.012]
 [-1.522]
 [-1.522]
 [-1.522]
 [-1.522]
 [-1.522]] [[0.361]
 [1.434]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.482]
 [0.491]
 [0.491]
 [0.464]
 [0.491]
 [0.491]] [[-0.33 ]
 [-0.783]
 [ 0.   ]
 [ 0.   ]
 [-1.125]
 [ 0.   ]
 [ 0.   ]] [[0.465]
 [0.482]
 [0.491]
 [0.491]
 [0.464]
 [0.491]
 [0.491]]
3839 5306
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.436]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[0.916]
 [1.362]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]] [[0.412]
 [0.571]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]]
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
probs:  [0.43944554173649353, 0.31275544836339136, 0.09071436206765443, 0.06880767691499315, 0.08747130366413045, 0.0008056672533370536]
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.413]
 [0.422]
 [0.377]
 [0.422]
 [0.377]
 [0.422]] [[ 0.382]
 [ 0.883]
 [ 0.468]
 [-0.265]
 [ 0.468]
 [-0.2  ]
 [ 0.468]] [[0.377]
 [0.413]
 [0.422]
 [0.377]
 [0.422]
 [0.377]
 [0.422]]
siam score:  -0.69867027
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
probs:  [0.43944554173649353, 0.31275544836339136, 0.09071436206765443, 0.06880767691499315, 0.08747130366413045, 0.0008056672533370536]
siam score:  -0.70076597
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
probs:  [0.43944554173649353, 0.31275544836339136, 0.09071436206765443, 0.06880767691499315, 0.08747130366413045, 0.0008056672533370536]
3843 5314
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.172]
 [0.414]
 [0.42 ]
 [0.421]
 [0.425]
 [0.426]] [[ 0.257]
 [ 0.155]
 [-0.711]
 [-1.013]
 [-1.011]
 [-0.908]
 [-0.978]] [[1.313]
 [1.12 ]
 [0.752]
 [0.539]
 [0.542]
 [0.621]
 [0.57 ]]
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.266]
 [0.268]
 [0.265]
 [0.265]
 [0.558]
 [0.265]] [[2.413]
 [2.325]
 [2.326]
 [2.413]
 [2.413]
 [4.509]
 [2.413]] [[0.527]
 [0.489]
 [0.491]
 [0.527]
 [0.527]
 [1.716]
 [0.527]]
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[-0.13]
 [-0.13]
 [-0.13]
 [-0.13]
 [-0.13]
 [-0.13]
 [-0.13]] [[0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]]
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
probs:  [0.42640622481440077, 0.3200305977270317, 0.09282451086577455, 0.07040824416183467, 0.08950601417842814, 0.0008244082525301693]
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.462]
 [0.452]
 [0.451]
 [0.451]
 [0.465]
 [0.454]] [[2.825]
 [3.12 ]
 [2.865]
 [2.825]
 [2.825]
 [2.736]
 [2.678]] [[0.82 ]
 [0.94 ]
 [0.835]
 [0.82 ]
 [0.82 ]
 [0.819]
 [0.777]]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.471]
 [0.474]
 [0.395]
 [0.474]
 [0.399]
 [0.39 ]] [[0.384]
 [1.704]
 [0.83 ]
 [0.026]
 [0.83 ]
 [0.213]
 [0.203]] [[0.25 ]
 [0.842]
 [0.555]
 [0.128]
 [0.555]
 [0.199]
 [0.177]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  0.3299999999999995  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.809]
 [1.17 ]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]] [[1.571]
 [0.471]
 [1.571]
 [1.571]
 [1.571]
 [1.571]
 [1.571]] [[1.512]
 [1.867]
 [1.512]
 [1.512]
 [1.512]
 [1.512]
 [1.512]]
3847 5328
actor:  1 policy actor:  1  step number:  63 total reward:  0.38999999999999957  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
probs:  [0.44313733166816366, 0.30953648215587337, 0.11789572283440018, 0.0420620636025428, 0.08657102463751144, 0.0007973751015086303]
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.321]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[0.478]
 [1.132]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[0.292]
 [0.734]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.485]
 [0.368]
 [0.406]
 [0.406]
 [0.406]
 [0.399]] [[2.178]
 [2.396]
 [1.895]
 [2.178]
 [2.178]
 [2.178]
 [2.338]] [[0.299]
 [0.602]
 [0.034]
 [0.299]
 [0.299]
 [0.299]
 [0.39 ]]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]] [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.153]
 [0.193]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[-0.366]
 [ 0.468]
 [-0.088]
 [-0.366]
 [-0.366]
 [-0.366]
 [-0.366]] [[0.191]
 [0.153]
 [0.193]
 [0.191]
 [0.191]
 [0.191]
 [0.191]]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [ 0.179]
 [ 0.112]
 [ 0.108]
 [-0.009]
 [ 0.134]
 [ 0.088]] [[1.655]
 [1.386]
 [1.3  ]
 [1.612]
 [1.882]
 [0.998]
 [1.446]] [[-0.25 ]
 [-0.044]
 [-0.234]
 [-0.035]
 [-0.09 ]
 [-0.392]
 [-0.185]]
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.427]
 [0.458]
 [0.47 ]
 [0.471]
 [0.471]
 [0.506]] [[ 0.057]
 [ 0.803]
 [ 0.02 ]
 [-0.303]
 [-0.295]
 [ 0.131]
 [-0.084]] [[0.474]
 [0.427]
 [0.458]
 [0.47 ]
 [0.471]
 [0.471]
 [0.506]]
siam score:  -0.6997583
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
siam score:  -0.6993364
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.483]
 [0.455]
 [0.455]
 [0.455]
 [0.465]
 [0.48 ]] [[1.185]
 [2.137]
 [1.185]
 [1.185]
 [1.185]
 [1.397]
 [1.279]] [[0.455]
 [0.483]
 [0.455]
 [0.455]
 [0.455]
 [0.465]
 [0.48 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
probs:  [0.42836980765564076, 0.31774512621291173, 0.12102221706168648, 0.04317751372982025, 0.0888668145293943, 0.0008185208105463557]
using another actor
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.261]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[-0.977]
 [-1.297]
 [-0.977]
 [-0.977]
 [-0.977]
 [-0.977]
 [-0.977]] [[0.242]
 [0.261]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]]
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.4099999999999996  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.485]
 [0.464]
 [0.464]
 [0.464]
 [0.415]
 [0.464]] [[2.254]
 [3.068]
 [2.254]
 [2.254]
 [2.254]
 [3.62 ]
 [2.254]] [[0.058]
 [0.371]
 [0.058]
 [0.058]
 [0.058]
 [0.416]
 [0.058]]
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
probs:  [0.41490885379217135, 0.30776040645009267, 0.14864292975360147, 0.041820717546080674, 0.0860742925799125, 0.0007927998781413545]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.354]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[-1.128]
 [-0.464]
 [-1.128]
 [-1.128]
 [-1.128]
 [-1.128]
 [-1.128]] [[0.345]
 [0.354]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]]
from probs:  [0.41490885379217135, 0.30776040645009267, 0.14864292975360147, 0.041820717546080674, 0.0860742925799125, 0.0007927998781413545]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.434]
 [0.545]
 [0.513]
 [0.424]
 [0.515]
 [0.49 ]] [[1.061]
 [1.019]
 [1.052]
 [0.937]
 [0.716]
 [0.479]
 [1.014]] [[0.721]
 [0.741]
 [0.946]
 [0.828]
 [0.552]
 [0.572]
 [0.833]]
from probs:  [0.41490885379217135, 0.30776040645009267, 0.14864292975360147, 0.041820717546080674, 0.0860742925799125, 0.0007927998781413545]
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
probs:  [0.41490885379217135, 0.30776040645009267, 0.14864292975360147, 0.041820717546080674, 0.0860742925799125, 0.0007927998781413545]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.902]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[-1.111]
 [-0.304]
 [-1.111]
 [-1.111]
 [-1.111]
 [-1.111]
 [-1.111]] [[0.399]
 [1.211]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.4263576699199705
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -0.018911061596977446
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
line 256 mcts: sample exp_bonus -3.6358531495366586
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]] [[-2.837]
 [-2.674]
 [-2.674]
 [-2.674]
 [-2.674]
 [-2.674]
 [-2.674]] [[0.881]
 [0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.9817932389837687
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.191]
 [0.419]
 [0.419]
 [0.354]
 [0.419]
 [0.434]] [[ 1.041]
 [ 0.747]
 [ 0.563]
 [ 0.563]
 [ 0.358]
 [ 0.563]
 [-0.228]] [[1.786]
 [1.653]
 [1.727]
 [1.727]
 [1.584]
 [1.727]
 [1.364]]
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
probs:  [0.41490885379217135, 0.30776040645009267, 0.14864292975360147, 0.041820717546080674, 0.0860742925799125, 0.0007927998781413545]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5049999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.99777 -1.0 -0.99777
probs:  [0.43513545535073167, 0.2971211288655444, 0.14350434351091165, 0.040374975295184946, 0.08309870419213262, 0.0007653927854947255]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[-0.599]
 [-0.751]
 [-0.751]
 [-0.751]
 [-0.751]
 [-0.751]
 [-0.751]] [[0.543]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.22499999999999942  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  67 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[-0.94 ]
 [-2.357]
 [-2.357]
 [-2.357]
 [-2.357]
 [-2.357]
 [-2.357]] [[0.731]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]] [[3.043]
 [3.043]
 [3.043]
 [3.043]
 [3.043]
 [3.043]
 [3.043]] [[1.204]
 [1.204]
 [1.204]
 [1.204]
 [1.204]
 [1.204]
 [1.204]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.349]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[-0.393]
 [ 0.945]
 [-0.393]
 [-0.393]
 [-0.393]
 [-0.393]
 [-0.393]] [[0.605]
 [1.208]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]]
using explorer policy with actor:  1
from probs:  [0.4152527904482804, 0.2835447590983187, 0.18264023997481024, 0.038530119643071147, 0.07930167117870383, 0.0007304196568157967]
using another actor
rdn probs:  [0.4152527904482804, 0.2835447590983187, 0.18264023997481024, 0.038530119643071147, 0.07930167117870383, 0.0007304196568157967]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.252]
 [0.321]
 [0.321]] [[-0.619]
 [-0.619]
 [-0.619]
 [-0.619]
 [ 0.254]
 [-0.619]
 [-0.619]] [[0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.252]
 [0.321]
 [0.321]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5349999999999997  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.40148322361986977, 0.2741155392705751, 0.2097715263231568, 0.03726672859573075, 0.07665908519579574, 0.0007038969948717344]
using explorer policy with actor:  0
from probs:  [0.40148322361986977, 0.2741155392705751, 0.2097715263231568, 0.03726672859573075, 0.07665908519579574, 0.0007038969948717344]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.62 ]
 [1.053]
 [0.62 ]
 [0.62 ]
 [1.141]
 [0.62 ]] [[2.165]
 [2.165]
 [1.451]
 [2.165]
 [2.165]
 [3.03 ]
 [2.165]] [[1.244]
 [1.244]
 [1.762]
 [1.244]
 [1.244]
 [2.342]
 [1.244]]
using explorer policy with actor:  0
first move QE:  0.33480241287147805
from probs:  [0.40148322361986977, 0.2741155392705751, 0.2097715263231568, 0.03726672859573075, 0.07665908519579574, 0.0007038969948717344]
Printing some Q and Qe and total Qs values:  [[1.156]
 [1.159]
 [1.164]
 [1.164]
 [1.164]
 [1.164]
 [1.164]] [[0.551]
 [0.458]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[1.608]
 [1.546]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.386]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[1.606]
 [1.933]
 [1.606]
 [1.606]
 [1.606]
 [1.606]
 [1.606]] [[0.791]
 [0.983]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]]
line 256 mcts: sample exp_bonus 2.0550960688663507
from probs:  [0.40148322361986977, 0.2741155392705751, 0.2097715263231568, 0.03726672859573075, 0.07665908519579574, 0.0007038969948717344]
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.4014832236198699, 0.2741155392705752, 0.20977152632315688, 0.03726672859573076, 0.07665908519579577, 0.0007038969948717347]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.344]
 [0.344]
 [0.344]
 [0.193]
 [0.344]
 [0.344]] [[1.63 ]
 [1.095]
 [1.095]
 [1.095]
 [1.751]
 [1.095]
 [1.095]] [[1.172]
 [1.208]
 [1.208]
 [1.208]
 [1.408]
 [1.208]
 [1.208]]
from probs:  [0.4014832236198699, 0.2741155392705752, 0.20977152632315688, 0.03726672859573076, 0.07665908519579577, 0.0007038969948717347]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.485]
 [0.465]
 [0.468]
 [0.465]
 [0.466]
 [0.469]] [[-0.13 ]
 [ 1.979]
 [ 0.099]
 [-0.167]
 [-0.182]
 [-0.011]
 [ 0.113]] [[0.465]
 [0.485]
 [0.465]
 [0.468]
 [0.465]
 [0.466]
 [0.469]]
line 256 mcts: sample exp_bonus 0.9306983637111934
3891 5423
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.383]
 [0.347]
 [0.296]
 [0.296]
 [0.296]
 [0.296]] [[-0.323]
 [-0.271]
 [-0.052]
 [-0.355]
 [-0.355]
 [-0.22 ]
 [-0.355]] [[0.173]
 [0.319]
 [0.371]
 [0.157]
 [0.157]
 [0.219]
 [0.157]]
using another actor
siam score:  -0.66239077
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.782]
 [0.759]] [[-0.419]
 [-0.419]
 [-0.419]
 [-0.419]
 [-0.419]
 [-0.435]
 [-0.419]] [[0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.782]
 [0.759]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4079347064690045, 0.2785203352775929, 0.2131423705445134, 0.03786557219923422, 0.06182180750818869, 0.0007152080014663016]
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.563]
 [0.568]
 [0.544]
 [0.164]
 [0.55 ]
 [0.554]] [[ 1.776]
 [ 1.206]
 [ 0.418]
 [ 0.731]
 [ 1.769]
 [-0.062]
 [ 1.121]] [[2.092]
 [1.628]
 [0.889]
 [1.142]
 [1.406]
 [0.398]
 [1.532]]
using another actor
line 256 mcts: sample exp_bonus 2.68783724718025
from probs:  [0.4079347064690045, 0.2785203352775929, 0.2131423705445134, 0.03786557219923422, 0.06182180750818869, 0.0007152080014663016]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.521]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[ 0.081]
 [-1.109]
 [ 0.081]
 [ 0.081]
 [ 0.081]
 [ 0.081]
 [ 0.081]] [[ 1.313]
 [-0.22 ]
 [ 1.313]
 [ 1.313]
 [ 1.313]
 [ 1.313]
 [ 1.313]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.467]
 [0.726]
 [0.784]
 [0.706]
 [0.708]
 [0.768]] [[0.986]
 [1.237]
 [0.699]
 [1.134]
 [0.841]
 [0.569]
 [1.203]] [[1.575]
 [1.066]
 [1.406]
 [1.667]
 [1.413]
 [1.326]
 [1.658]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.395]
 [0.354]
 [0.348]
 [0.36 ]
 [0.377]
 [0.366]] [[-1.023]
 [-1.009]
 [-1.46 ]
 [-1.673]
 [-1.415]
 [-1.374]
 [-1.49 ]] [[0.362]
 [0.395]
 [0.354]
 [0.348]
 [0.36 ]
 [0.377]
 [0.366]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[3.476]
 [3.248]
 [3.697]
 [3.697]
 [3.697]
 [3.697]
 [3.697]] [[0.466]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]] [[1.796]
 [1.796]
 [1.796]
 [1.796]
 [1.796]
 [1.796]
 [1.796]]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.528]
 [0.519]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[0.784]
 [1.422]
 [1.623]
 [0.748]
 [0.748]
 [0.748]
 [0.748]] [[-0.115]
 [ 0.375]
 [ 0.51 ]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]]
line 256 mcts: sample exp_bonus 3.994620333441173
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4079347064690046, 0.278520335277593, 0.2131423705445135, 0.03786557219923424, 0.06182180750818872, 0.000715208001466302]
siam score:  -0.6562644
actor:  1 policy actor:  1  step number:  71 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.5
3912 5459
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  70 total reward:  0.5549999999999997  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.709]
 [1.382]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]] [[1.536]
 [0.502]
 [1.536]
 [1.536]
 [1.536]
 [1.536]
 [1.536]] [[1.757]
 [1.883]
 [1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]]
3914 5470
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.158]
 [0.136]
 [0.136]
 [0.185]
 [0.136]
 [0.136]] [[0.595]
 [0.551]
 [0.581]
 [0.581]
 [1.143]
 [0.581]
 [0.581]] [[0.859]
 [0.505]
 [0.511]
 [0.511]
 [1.361]
 [0.511]
 [0.511]]
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.084]
 [0.237]
 [0.239]
 [0.239]
 [0.247]
 [0.247]] [[0.411]
 [0.306]
 [0.337]
 [0.342]
 [0.354]
 [0.378]
 [0.349]] [[0.468]
 [0.048]
 [0.342]
 [0.353]
 [0.369]
 [0.414]
 [0.375]]
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]] [[2.53 ]
 [2.521]
 [2.521]
 [2.521]
 [2.521]
 [2.521]
 [2.521]] [[2.235]
 [2.218]
 [2.218]
 [2.218]
 [2.218]
 [2.218]
 [2.218]]
line 256 mcts: sample exp_bonus 0.7647925564046526
deleting a thread, now have 5 threads
Frames:  262884 train batches done:  30803 episodes:  9391
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.5  ]
 [0.552]
 [0.545]
 [0.544]
 [0.543]
 [0.546]] [[-2.637]
 [-1.033]
 [-2.354]
 [-2.989]
 [-3.025]
 [-2.972]
 [-2.898]] [[0.673]
 [1.141]
 [0.803]
 [0.577]
 [0.563]
 [0.578]
 [0.61 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4002981047495305, 0.273306390894925, 0.2660818921697696, 0.037156722745682005, 0.022455070244646734, 0.0007018191954462036]
Printing some Q and Qe and total Qs values:  [[1.156]
 [1.101]
 [1.067]
 [1.055]
 [1.055]
 [1.041]
 [1.041]] [[0.764]
 [1.141]
 [1.249]
 [1.817]
 [1.817]
 [1.765]
 [2.041]] [[2.495]
 [2.51 ]
 [2.477]
 [2.644]
 [2.644]
 [2.599]
 [2.69 ]]
using explorer policy with actor:  1
siam score:  -0.6432862
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4002981047495305, 0.273306390894925, 0.2660818921697696, 0.037156722745682005, 0.022455070244646734, 0.0007018191954462036]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4002981047495305, 0.273306390894925, 0.2660818921697696, 0.037156722745682005, 0.022455070244646734, 0.0007018191954462036]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4002981047495304, 0.273306390894925, 0.2660818921697696, 0.037156722745682005, 0.022455070244646734, 0.0007018191954462036]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.4002981047495304, 0.273306390894925, 0.2660818921697696, 0.037156722745682005, 0.022455070244646734, 0.0007018191954462036]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6499999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4223690502351338, 0.26324784263606127, 0.2562892285425646, 0.03578923665194353, 0.021628651927195864, 0.000675990007100928]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4223690502351338, 0.26324784263606127, 0.2562892285425646, 0.03578923665194353, 0.021628651927195864, 0.000675990007100928]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  65 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.5
in main func line 156:  3925
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.0815502236104746
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.41844111518578475, 0.26079969822969423, 0.26320556771126036, 0.03545640498031334, 0.021427510437478026, 0.0006697034554693081]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.491]
 [0.462]
 [0.377]
 [0.375]
 [0.376]
 [0.462]] [[ 0.52 ]
 [ 1.167]
 [ 0.614]
 [-0.228]
 [-0.238]
 [ 0.116]
 [ 0.614]] [[-0.299]
 [ 0.138]
 [-0.106]
 [-0.557]
 [-0.564]
 [-0.444]
 [-0.106]]
deleting a thread, now have 4 threads
Frames:  263630 train batches done:  30889 episodes:  9428
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.6316172094818262
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]] [[2.192]
 [1.826]
 [1.826]
 [1.826]
 [1.826]
 [1.826]
 [1.826]] [[1.319]
 [0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.928]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.3949999999999996  reward:  1.0 rdn_beta:  0.167
deleting a thread, now have 3 threads
Frames:  263877 train batches done:  30910 episodes:  9433
siam score:  -0.65054667
siam score:  -0.6458672
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.501]
 [0.638]
 [0.456]
 [0.287]
 [0.565]
 [0.501]] [[1.108]
 [1.253]
 [0.599]
 [1.192]
 [1.452]
 [1.032]
 [1.253]] [[1.929]
 [1.835]
 [1.673]
 [1.703]
 [1.539]
 [1.815]
 [1.835]]
siam score:  -0.64292806
from probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
siam score:  -0.6417035
using another actor
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.327]
 [0.173]
 [0.327]
 [0.327]
 [0.582]
 [0.327]] [[0.774]
 [0.774]
 [1.158]
 [0.774]
 [0.774]
 [3.047]
 [0.774]] [[0.507]
 [0.507]
 [0.468]
 [0.507]
 [0.507]
 [1.987]
 [0.507]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
3938 5518
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.626]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[1.133]
 [1.525]
 [1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.133]] [[1.207]
 [1.555]
 [1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]]
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.085]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[0.898]
 [0.927]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]] [[-0.22 ]
 [-0.434]
 [-0.22 ]
 [-0.22 ]
 [-0.22 ]
 [-0.22 ]
 [-0.22 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.283]
 [0.283]
 [0.267]
 [0.283]
 [0.283]
 [0.283]] [[3.174]
 [3.174]
 [3.174]
 [4.108]
 [3.174]
 [3.174]
 [3.174]] [[0.291]
 [0.291]
 [0.291]
 [0.881]
 [0.291]
 [0.291]
 [0.291]]
siam score:  -0.6474242
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[0.257]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]] [[-0.125]
 [-0.218]
 [-0.218]
 [-0.218]
 [-0.218]
 [-0.218]
 [-0.218]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.017]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]] [[ 1.233]
 [-0.319]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]] [[ 0.186]
 [-0.822]
 [-0.703]
 [-0.703]
 [-0.703]
 [-0.703]
 [-0.703]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.257]
 [0.351]
 [0.354]
 [0.357]
 [0.358]
 [0.366]] [[ 0.547]
 [-0.399]
 [-0.729]
 [-1.262]
 [-1.323]
 [-1.34 ]
 [-1.365]] [[1.452]
 [0.801]
 [0.604]
 [0.162]
 [0.113]
 [0.1  ]
 [0.086]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
in main func line 156:  3942
from probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.373]
 [0.392]
 [0.294]
 [0.292]
 [0.283]
 [0.313]] [[3.283]
 [3.054]
 [3.107]
 [3.011]
 [2.987]
 [2.882]
 [2.935]] [[0.485]
 [0.522]
 [0.589]
 [0.351]
 [0.334]
 [0.254]
 [0.341]]
using explorer policy with actor:  1
from probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
3945 5530
start point for exploration sampling:  10935
using another actor
from probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.348]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[4.515]
 [3.163]
 [2.648]
 [2.648]
 [2.648]
 [2.648]
 [2.648]] [[1.28 ]
 [0.455]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]]
from probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.44215752316636975, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
using another actor
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
from probs:  [0.4421575231663698, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4421575231663698, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
line 256 mcts: sample exp_bonus 0.24462718053225624
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.852]
 [0.678]
 [0.788]
 [0.76 ]
 [0.634]
 [0.764]] [[1.455]
 [2.059]
 [2.607]
 [2.582]
 [1.629]
 [2.337]
 [2.635]] [[0.767]
 [0.852]
 [0.678]
 [0.788]
 [0.76 ]
 [0.634]
 [0.764]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4421575231663698, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
Printing some Q and Qe and total Qs values:  [[0.013]
 [1.482]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[0.758]
 [0.315]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]] [[0.614]
 [2.665]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
3952 5544
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4421575231663698, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.218]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.155]] [[2.1  ]
 [2.538]
 [1.84 ]
 [1.785]
 [1.852]
 [1.946]
 [1.868]] [[-0.708]
 [-0.278]
 [-0.872]
 [-0.91 ]
 [-0.864]
 [-0.801]
 [-0.849]]
3954 5549
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4421575231663698, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4421575231663698, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.554]
 [0.473]
 [0.467]
 [0.478]
 [0.553]
 [0.497]] [[1.177]
 [1.841]
 [0.928]
 [1.004]
 [0.9  ]
 [1.134]
 [0.926]] [[0.48 ]
 [0.554]
 [0.473]
 [0.467]
 [0.478]
 [0.553]
 [0.497]]
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.687]
 [0.662]
 [0.639]
 [0.639]
 [0.681]
 [0.693]] [[0.933]
 [1.222]
 [0.871]
 [0.774]
 [0.774]
 [1.198]
 [0.976]] [[0.691]
 [0.687]
 [0.662]
 [0.639]
 [0.639]
 [0.681]
 [0.693]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4421575231663698, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.359]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]] [[-1.491]
 [ 0.192]
 [-1.491]
 [-1.491]
 [-1.491]
 [-1.491]
 [-1.491]] [[0.708]
 [1.57 ]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]]
siam score:  -0.65091765
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6508476
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4421575231663698, 0.25942380528278086, 0.26181698219271693, 0.03526934871503978, 0.0006661703215463838, 0.0006661703215463838]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[ 0.208]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[ 0.742]
 [-2.133]
 [-2.133]
 [-2.133]
 [-2.133]
 [-2.133]
 [-2.133]] [[ 1.112]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.2949999999999995  reward:  1.0 rdn_beta:  0.5
siam score:  -0.65155256
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43249547265321725, 0.25375486202245473, 0.27794779809499814, 0.03449864096724356, 0.0006516131310432102, 0.0006516131310432102]
using explorer policy with actor:  1
siam score:  -0.64936614
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.389]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]] [[-0.077]
 [ 1.218]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]] [[-0.603]
 [-0.056]
 [-0.603]
 [-0.603]
 [-0.603]
 [-0.603]
 [-0.603]]
in main func line 156:  3966
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43249547265321725, 0.25375486202245473, 0.2779477980949982, 0.03449864096724356, 0.0006516131310432102, 0.0006516131310432102]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43249547265321725, 0.25375486202245473, 0.2779477980949982, 0.03449864096724356, 0.0006516131310432102, 0.0006516131310432102]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.461]
 [0.361]
 [0.362]
 [0.362]
 [0.363]
 [0.362]] [[0.661]
 [0.635]
 [0.912]
 [0.999]
 [0.999]
 [0.929]
 [0.999]] [[-0.189]
 [-0.004]
 [-0.112]
 [-0.08 ]
 [-0.08 ]
 [-0.101]
 [-0.08 ]]
from probs:  [0.43249547265321725, 0.25375486202245473, 0.2779477980949982, 0.03449864096724356, 0.0006516131310432102, 0.0006516131310432102]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.352]
 [0.404]
 [0.404]
 [0.419]
 [0.402]
 [0.407]] [[ 1.951]
 [ 1.112]
 [ 1.594]
 [ 1.594]
 [-0.094]
 [-0.156]
 [-0.151]] [[1.584]
 [1.022]
 [1.396]
 [1.396]
 [0.337]
 [0.275]
 [0.285]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43249547265321725, 0.25375486202245473, 0.2779477980949982, 0.03449864096724356, 0.0006516131310432102, 0.0006516131310432102]
actor:  1 policy actor:  1  step number:  59 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.9219714671305115
line 256 mcts: sample exp_bonus -1.598118851753574
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4220229544027517, 0.2718245675360133, 0.27121752327762333, 0.03366328506182949, 0.0006358348608910863, 0.0006358348608910863]
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.541]
 [0.528]
 [0.489]
 [0.487]
 [0.528]
 [0.493]] [[ 0.   ]
 [ 0.106]
 [ 0.   ]
 [-0.656]
 [-0.66 ]
 [ 0.   ]
 [-0.375]] [[ 0.279]
 [ 0.358]
 [ 0.279]
 [-0.145]
 [-0.15 ]
 [ 0.279]
 [ 0.016]]
from probs:  [0.4220229544027517, 0.2718245675360133, 0.27121752327762333, 0.03366328506182949, 0.0006358348608910863, 0.0006358348608910863]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.6459935
3978 5566
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.523]
 [0.547]
 [0.523]
 [0.507]
 [0.523]
 [0.523]] [[1.377]
 [2.281]
 [2.169]
 [2.281]
 [1.574]
 [2.281]
 [2.281]] [[0.654]
 [0.933]
 [0.944]
 [0.933]
 [0.665]
 [0.933]
 [0.933]]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4220229544027517, 0.2718245675360133, 0.27121752327762333, 0.03366328506182949, 0.0006358348608910863, 0.0006358348608910863]
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.723]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]] [[-0.07 ]
 [ 0.764]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]] [[1.41 ]
 [1.752]
 [1.41 ]
 [1.41 ]
 [1.41 ]
 [1.41 ]
 [1.41 ]]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.669]
 [0.459]
 [0.459]
 [0.458]
 [0.459]
 [0.461]] [[-1.836]
 [-0.752]
 [-2.049]
 [-1.989]
 [-1.958]
 [-1.899]
 [-1.904]] [[0.093]
 [0.719]
 [0.015]
 [0.037]
 [0.048]
 [0.07 ]
 [0.071]]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.4449999999999996  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.4369708747303255, 0.2647945098381219, 0.26420316525029647, 0.03279266899342032, 0.0006193905939180399, 0.0006193905939180399]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.673]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.656]] [[0.889]
 [1.573]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [1.916]] [[0.644]
 [0.673]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.656]]
from probs:  [0.4369708747303255, 0.2647945098381219, 0.26420316525029647, 0.03279266899342032, 0.0006193905939180399, 0.0006193905939180399]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4369708747303255, 0.2647945098381219, 0.26420316525029647, 0.03279266899342032, 0.0006193905939180399, 0.0006193905939180399]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.531]
 [0.564]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[0.886]
 [0.886]
 [2.815]
 [0.886]
 [0.886]
 [0.886]
 [0.886]] [[1.108]
 [1.108]
 [1.821]
 [1.108]
 [1.108]
 [1.108]
 [1.108]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4369708747303255, 0.2647945098381219, 0.26420316525029647, 0.03279266899342032, 0.0006193905939180399, 0.0006193905939180399]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4369708747303255, 0.2647945098381219, 0.26420316525029647, 0.03279266899342032, 0.0006193905939180399, 0.0006193905939180399]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4369708747303255, 0.2647945098381219, 0.2642031652502964, 0.03279266899342032, 0.0006193905939180399, 0.0006193905939180399]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.8202],
        [-0.0000],
        [-0.3377],
        [-0.0000],
        [-0.5074]], dtype=torch.float64)
-0.9652499999999999 -0.9652499999999999
-0.9703485 -0.9703485
-0.29143421999999963 -0.29143421999999963
-0.9507464999999999 -0.9507464999999999
-0.95128310475 -0.95128310475
-0.024259925299500003 -0.8444672231828838
-0.9703485 -0.9703485
-0.0439609252995 -0.3816848723539667
-0.955892025 -0.955892025
-0.0727797758985 -0.5801628323677837
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.439]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[3.474]
 [3.523]
 [3.474]
 [3.474]
 [3.474]
 [3.474]
 [3.474]] [[0.865]
 [0.927]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.427]
 [0.416]
 [0.282]
 [0.273]
 [0.273]
 [0.285]] [[2.69 ]
 [3.858]
 [3.133]
 [2.152]
 [2.19 ]
 [2.254]
 [2.303]] [[0.32 ]
 [1.119]
 [0.723]
 [0.06 ]
 [0.072]
 [0.105]
 [0.144]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4369708747303255, 0.2647945098381219, 0.2642031652502964, 0.03279266899342032, 0.0006193905939180399, 0.0006193905939180399]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.713]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]] [[-2.191]
 [-0.688]
 [-2.191]
 [-2.191]
 [-2.191]
 [-2.191]
 [-2.191]] [[0.321]
 [1.065]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  72 total reward:  0.3149999999999995  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[ 0.354]
 [-0.312]
 [-0.312]
 [-0.312]
 [-0.312]
 [-0.312]
 [-0.312]] [[1.716]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]]
3991 5597
using another actor
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4278080577390659, 0.25924204908101917, 0.2796320465086192, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.654]
 [0.488]] [[-0.943]
 [-0.943]
 [-0.943]
 [-0.943]
 [-0.943]
 [-1.198]
 [-0.943]] [[0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.85 ]
 [0.604]]
Printing some Q and Qe and total Qs values:  [[1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]] [[0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.319]] [[2.894]
 [2.894]
 [2.894]
 [2.894]
 [2.894]
 [2.894]
 [2.894]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4278080577390659, 0.25924204908101917, 0.2796320465086192, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4278080577390659, 0.25924204908101917, 0.27963204650861917, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.044]
 [0.224]
 [0.251]
 [0.274]
 [0.625]
 [0.222]] [[-1.736]
 [-1.303]
 [-1.608]
 [-1.579]
 [-1.531]
 [-2.728]
 [-1.535]] [[-0.179]
 [-0.311]
 [-0.055]
 [ 0.01 ]
 [ 0.072]
 [ 0.373]
 [-0.034]]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.431]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[2.679]
 [2.983]
 [2.679]
 [2.679]
 [2.679]
 [2.679]
 [2.679]] [[0.341]
 [0.668]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.474]
 [0.182]] [[-1.301]
 [-1.301]
 [-1.301]
 [-1.301]
 [-1.301]
 [-1.612]
 [-1.301]] [[-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [ 0.467]
 [-0.014]]
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.789]
 [0.719]] [[2.971]
 [2.971]
 [2.971]
 [2.971]
 [2.971]
 [3.542]
 [2.971]] [[0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [1.407]
 [0.887]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4278080577390659, 0.25924204908101917, 0.27963204650861917, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
first move QE:  0.31851642120234896
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4278080577390659, 0.25924204908101917, 0.27963204650861917, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
from probs:  [0.4278080577390659, 0.25924204908101917, 0.27963204650861917, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.717]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[1.389]
 [1.276]
 [1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]] [[1.631]
 [1.891]
 [1.631]
 [1.631]
 [1.631]
 [1.631]
 [1.631]]
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.852]
 [0.821]
 [0.788]
 [0.753]
 [0.82 ]
 [0.796]] [[ 1.124]
 [ 0.511]
 [ 0.352]
 [-0.058]
 [ 0.346]
 [ 0.34 ]
 [ 0.355]] [[1.355]
 [1.432]
 [1.318]
 [1.114]
 [1.18 ]
 [1.312]
 [1.268]]
3996 5608
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4278080577390657, 0.25924204908101917, 0.27963204650861917, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
siam score:  -0.64338654
3998 5611
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.4278080577390659, 0.25924204908101917, 0.27963204650861917, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[1.791]
 [1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]] [[ 0.042]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]]
line 256 mcts: sample exp_bonus 0.662775104761764
from probs:  [0.4278080577390657, 0.25924204908101917, 0.2796320465086192, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4278080577390657, 0.25924204908101917, 0.27963204650861917, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
from probs:  [0.4278080577390657, 0.25924204908101917, 0.27963204650861917, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4278080577390657, 0.25924204908101917, 0.27963204650861917, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.425]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[0.4  ]
 [1.039]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]] [[0.41 ]
 [0.425]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 2.842220604443635
from probs:  [0.4278080577390657, 0.25924204908101917, 0.27963204650861917, 0.03210504141451795, 0.0006064026283890304, 0.0006064026283890304]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.662]
 [0.684]
 [0.635]
 [0.625]
 [0.611]
 [0.621]] [[4.393]
 [3.145]
 [4.344]
 [4.577]
 [4.554]
 [4.249]
 [4.82 ]] [[0.748]
 [0.181]
 [0.869]
 [0.916]
 [0.886]
 [0.698]
 [1.024]]
4009 5625
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.438]
 [0.527]
 [0.524]
 [0.246]
 [0.505]
 [0.488]] [[-0.71 ]
 [-0.894]
 [-0.628]
 [-1.003]
 [-0.446]
 [-0.739]
 [ 0.186]] [[0.806]
 [0.683]
 [0.95 ]
 [0.819]
 [0.448]
 [0.869]
 [1.143]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  94 total reward:  0.3249999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.173]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]] [[-1.072]
 [-0.658]
 [-1.072]
 [-1.072]
 [-1.072]
 [-1.072]
 [-1.072]] [[-0.331]
 [ 0.057]
 [-0.331]
 [-0.331]
 [-0.331]
 [-0.331]
 [-0.331]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43030978244512397, 0.25491026422213414, 0.2812672714341461, 0.03229278443114132, 0.0006099487337271975, 0.0006099487337271975]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.944]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[2.522]
 [3.72 ]
 [2.522]
 [2.522]
 [2.522]
 [2.522]
 [2.522]] [[0.729]
 [1.658]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43030978244512397, 0.25491026422213425, 0.2812672714341462, 0.032292784431141336, 0.0006099487337271977, 0.0006099487337271977]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43030978244512397, 0.25491026422213425, 0.2812672714341462, 0.032292784431141336, 0.0006099487337271977, 0.0006099487337271977]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 1.452]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[1.648]
 [0.311]
 [1.648]
 [1.648]
 [1.648]
 [1.648]
 [1.648]] [[0.424]
 [1.109]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.42 ]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[2.306]
 [3.163]
 [2.067]
 [2.067]
 [2.067]
 [2.067]
 [2.067]] [[0.228]
 [0.9  ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]]
from probs:  [0.43030978244512397, 0.25491026422213425, 0.2812672714341462, 0.032292784431141336, 0.0006099487337271977, 0.0006099487337271977]
using another actor
from probs:  [0.43030978244512397, 0.25491026422213425, 0.2812672714341462, 0.032292784431141336, 0.0006099487337271977, 0.0006099487337271977]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43030978244512397, 0.25491026422213425, 0.28126727143414626, 0.032292784431141336, 0.0006099487337271977, 0.0006099487337271977]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43030978244512397, 0.25491026422213425, 0.28126727143414626, 0.032292784431141336, 0.0006099487337271977, 0.0006099487337271977]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.51 ]
 [0.453]
 [0.451]
 [0.453]
 [0.448]
 [0.453]] [[-1.536]
 [-0.968]
 [-1.684]
 [-1.639]
 [-1.57 ]
 [-1.585]
 [-1.559]] [[0.454]
 [0.51 ]
 [0.453]
 [0.451]
 [0.453]
 [0.448]
 [0.453]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.63831025
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43030978244512397, 0.25491026422213425, 0.28126727143414626, 0.032292784431141336, 0.0006099487337271977, 0.0006099487337271977]
line 256 mcts: sample exp_bonus 0.6789303815805714
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43030978244512397, 0.25491026422213425, 0.28126727143414626, 0.032292784431141336, 0.0006099487337271977, 0.0006099487337271977]
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.671]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]] [[3.141]
 [3.281]
 [3.141]
 [3.141]
 [3.141]
 [3.141]
 [3.141]] [[0.45]
 [0.71]
 [0.45]
 [0.45]
 [0.45]
 [0.45]
 [0.45]]
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.443]
 [0.554]
 [0.383]
 [0.383]
 [0.383]
 [0.383]] [[1.658]
 [1.752]
 [2.099]
 [1.658]
 [1.658]
 [1.658]
 [1.658]] [[-0.253]
 [-0.071]
 [ 0.382]
 [-0.253]
 [-0.253]
 [-0.253]
 [-0.253]]
siam score:  -0.6377277
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.519]
 [0.442]
 [0.426]
 [0.422]
 [0.432]
 [0.412]] [[0.988]
 [1.135]
 [0.64 ]
 [0.716]
 [0.74 ]
 [0.94 ]
 [1.122]] [[ 0.064]
 [ 0.261]
 [-0.057]
 [-0.065]
 [-0.065]
 [ 0.022]
 [ 0.042]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43030978244512397, 0.25491026422213414, 0.28126727143414615, 0.03229278443114132, 0.0006099487337271975, 0.0006099487337271975]
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.143]
 [0.143]
 [0.008]
 [0.008]
 [0.143]
 [0.143]] [[0.241]
 [0.241]
 [0.241]
 [0.411]
 [0.751]
 [0.241]
 [0.241]] [[0.801]
 [0.801]
 [0.801]
 [0.734]
 [0.963]
 [0.801]
 [0.801]]
4030 5644
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.43030978244512397, 0.25491026422213425, 0.2812672714341462, 0.032292784431141336, 0.0006099487337271977, 0.0006099487337271977]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]] [[0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.698]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]] [[1.68 ]
 [1.646]
 [1.68 ]
 [1.68 ]
 [1.68 ]
 [1.68 ]
 [1.68 ]] [[0.449]
 [0.093]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
actor:  1 policy actor:  1  step number:  75 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4439847268510166, 0.24879135330474025, 0.2745156822694814, 0.03151762274115708, 0.0005953074168024243, 0.0005953074168024243]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.566]
 [0.54 ]
 [0.571]
 [0.485]
 [0.571]
 [0.552]] [[0.643]
 [2.945]
 [1.076]
 [1.379]
 [0.51 ]
 [1.379]
 [1.737]] [[0.444]
 [0.566]
 [0.54 ]
 [0.571]
 [0.485]
 [0.571]
 [0.552]]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.638]
 [0.482]
 [0.519]
 [0.503]
 [0.393]
 [0.306]] [[1.626]
 [3.561]
 [2.567]
 [2.196]
 [2.584]
 [2.82 ]
 [2.652]] [[0.696]
 [1.56 ]
 [0.916]
 [0.866]
 [0.963]
 [0.822]
 [0.592]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  47 total reward:  0.5999999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.496]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[0.306]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[2.502]
 [1.544]
 [1.544]
 [1.544]
 [1.544]
 [1.544]
 [1.544]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
4036 5666
siam score:  -0.64025396
first move QE:  0.30958498094712034
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.005]
 [-0.002]
 [-0.005]
 [-0.005]
 [-0.008]
 [-0.005]] [[2.09 ]
 [2.09 ]
 [2.992]
 [2.09 ]
 [2.09 ]
 [2.192]
 [2.09 ]] [[-0.63 ]
 [-0.63 ]
 [-0.322]
 [-0.63 ]
 [-0.63 ]
 [-0.601]
 [-0.63 ]]
Printing some Q and Qe and total Qs values:  [[ 0.044]
 [ 0.148]
 [ 0.101]
 [-0.011]
 [-0.011]
 [ 0.021]
 [-0.01 ]] [[-0.272]
 [-0.346]
 [-0.113]
 [-0.063]
 [-0.102]
 [ 0.044]
 [-0.025]] [[-1.035]
 [-0.851]
 [-0.868]
 [-1.075]
 [-1.088]
 [-0.975]
 [-1.06 ]]
Printing some Q and Qe and total Qs values:  [[-0.078]
 [-0.072]
 [-0.078]
 [-0.078]
 [-0.078]
 [-0.078]
 [-0.078]] [[-0.791]
 [-0.464]
 [-0.791]
 [-0.791]
 [-0.791]
 [-0.791]
 [-0.791]] [[-0.143]
 [ 0.132]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.084]
 [-0.077]
 [-0.07 ]
 [-0.084]
 [-0.093]
 [-0.07 ]
 [-0.074]] [[0.772]
 [2.536]
 [1.761]
 [0.149]
 [0.359]
 [1.761]
 [2.852]] [[-0.275]
 [ 0.821]
 [ 0.352]
 [-0.658]
 [-0.539]
 [ 0.352]
 [ 1.019]]
Printing some Q and Qe and total Qs values:  [[-0.066]
 [-0.064]
 [-0.068]
 [-0.075]
 [-0.069]
 [-0.071]
 [-0.071]] [[2.151]
 [2.405]
 [2.386]
 [2.379]
 [2.456]
 [2.396]
 [2.456]] [[0.411]
 [0.607]
 [0.586]
 [0.57 ]
 [0.637]
 [0.589]
 [0.635]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[ 0.013]
 [-0.028]
 [ 0.013]
 [ 0.013]
 [ 0.013]
 [ 0.013]
 [ 0.013]] [[0.298]
 [0.25 ]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[ 0.013]
 [-0.028]
 [ 0.013]
 [ 0.013]
 [ 0.013]
 [ 0.013]
 [ 0.013]]
using explorer policy with actor:  0
siam score:  -0.64596695
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.287]
 [0.447]
 [0.453]
 [0.449]
 [0.406]
 [0.388]] [[-1.906]
 [ 0.164]
 [-1.518]
 [-0.879]
 [-0.86 ]
 [-0.929]
 [-0.946]] [[0.449]
 [0.287]
 [0.447]
 [0.453]
 [0.449]
 [0.406]
 [0.388]]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.819]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[0.84 ]
 [1.258]
 [0.84 ]
 [0.84 ]
 [0.84 ]
 [0.84 ]
 [0.84 ]] [[1.323]
 [1.95 ]
 [1.323]
 [1.323]
 [1.323]
 [1.323]
 [1.323]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.368]
 [0.35 ]
 [0.342]
 [0.371]
 [0.351]
 [0.345]] [[-2.669]
 [-2.442]
 [-2.26 ]
 [-2.453]
 [ 0.   ]
 [-2.132]
 [-2.15 ]] [[0.346]
 [0.368]
 [0.35 ]
 [0.342]
 [0.371]
 [0.351]
 [0.345]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.365]
 [0.347]
 [0.373]
 [0.346]
 [0.351]
 [0.346]] [[-2.348]
 [-2.233]
 [-2.359]
 [ 0.   ]
 [-2.184]
 [-2.132]
 [-2.112]] [[0.34 ]
 [0.365]
 [0.347]
 [0.373]
 [0.346]
 [0.351]
 [0.346]]
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.377]
 [0.348]
 [0.348]
 [0.354]
 [0.35 ]
 [0.346]] [[-2.376]
 [-2.577]
 [-2.45 ]
 [-2.328]
 [-2.169]
 [-2.098]
 [-2.133]] [[0.358]
 [0.377]
 [0.348]
 [0.348]
 [0.354]
 [0.35 ]
 [0.346]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64613146
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[1.19 ]
 [1.189]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]] [[0.305]
 [0.309]
 [1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.243]] [[1.423]
 [1.425]
 [1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.321]]
siam score:  -0.64619124
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.235]
 [0.245]
 [0.253]
 [0.309]
 [0.235]
 [0.235]] [[2.263]
 [2.263]
 [2.864]
 [3.044]
 [3.007]
 [2.263]
 [2.263]] [[-0.299]
 [-0.299]
 [-0.079]
 [-0.003]
 [ 0.097]
 [-0.299]
 [-0.299]]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.434]
 [0.413]
 [0.409]
 [0.409]
 [0.409]
 [0.405]] [[ 0.   ]
 [-3.139]
 [-2.913]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-2.751]] [[0.409]
 [0.434]
 [0.413]
 [0.409]
 [0.409]
 [0.409]
 [0.405]]
start point for exploration sampling:  10935
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
siam score:  -0.6436731
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.549]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[ 0.867]
 [-0.174]
 [ 0.867]
 [ 0.867]
 [ 0.867]
 [ 0.867]
 [ 0.867]] [[0.592]
 [0.549]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4604286650273113, 0.24143344457433807, 0.26639698638888604, 0.030585501152330694, 0.0005777014285669792, 0.0005777014285669792]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.598]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[3.797]
 [3.564]
 [3.797]
 [3.797]
 [3.797]
 [3.797]
 [3.797]] [[1.758]
 [1.63 ]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.85 ]
 [0.598]
 [0.614]
 [0.596]
 [0.598]
 [0.598]] [[2.988]
 [2.015]
 [3.553]
 [2.721]
 [3.182]
 [3.553]
 [3.553]] [[1.385]
 [1.54 ]
 [1.551]
 [1.316]
 [1.429]
 [1.551]
 [1.551]]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.606]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]] [[-0.467]
 [-0.379]
 [-1.257]
 [-1.257]
 [-1.257]
 [-1.257]
 [-1.257]] [[0.666]
 [0.606]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4604286650273113, 0.24143344457433807, 0.26639698638888604, 0.030585501152330694, 0.0005777014285669792, 0.0005777014285669792]
siam score:  -0.6488031
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.421]] [[0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.29 ]] [[0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.014]]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.28 ]
 [0.274]
 [0.275]
 [0.267]
 [0.277]
 [0.27 ]] [[-0.174]
 [-0.884]
 [ 0.01 ]
 [-0.03 ]
 [ 0.101]
 [ 0.042]
 [ 0.13 ]] [[ 0.263]
 [-0.015]
 [ 0.271]
 [ 0.26 ]
 [ 0.287]
 [ 0.288]
 [ 0.304]]
siam score:  -0.6566083
actor:  1 policy actor:  1  step number:  73 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.248]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[-2.994]
 [-0.362]
 [-2.994]
 [-2.994]
 [-2.994]
 [-2.994]
 [-2.994]] [[0.253]
 [0.248]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]]
rdn probs:  [0.47045151535141516, 0.2369486783880026, 0.26144850794255925, 0.030017357738720645, 0.0005669702896512307, 0.0005669702896512307]
using another actor
line 256 mcts: sample exp_bonus 0.7376701069025036
siam score:  -0.6634124
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.47045151535141516, 0.2369486783880026, 0.26144850794255925, 0.030017357738720645, 0.0005669702896512307, 0.0005669702896512307]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.766]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[-1.084]
 [ 1.853]
 [-1.084]
 [-1.084]
 [-1.084]
 [-1.084]
 [-1.084]] [[0.504]
 [0.766]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]]
line 256 mcts: sample exp_bonus 1.9409160970398447
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.47045151535141516, 0.2369486783880026, 0.26144850794255925, 0.030017357738720645, 0.0005669702896512307, 0.0005669702896512307]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.595]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[-0.744]
 [-1.2  ]
 [-0.744]
 [-0.744]
 [-0.744]
 [-0.744]
 [-0.744]] [[1.508]
 [1.383]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
actor:  1 policy actor:  1  step number:  86 total reward:  0.36499999999999955  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
first move QE:  0.30812398999222274
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.616]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]] [[2.514]
 [2.743]
 [3.916]
 [3.916]
 [3.916]
 [3.916]
 [3.916]] [[0.512]
 [0.613]
 [1.437]
 [1.437]
 [1.437]
 [1.437]
 [1.437]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.48140667935861536, 0.23204674455515914, 0.25603972788373375, 0.029396366296721314, 0.0005552409528852858, 0.0005552409528852858]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.191]
 [0.209]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[0.44 ]
 [0.44 ]
 [0.878]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]] [[-0.427]
 [-0.427]
 [-0.246]
 [-0.427]
 [-0.427]
 [-0.427]
 [-0.427]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.48140667935861536, 0.23204674455515914, 0.25603972788373375, 0.029396366296721314, 0.0005552409528852858, 0.0005552409528852858]
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.51 ]
 [0.491]
 [0.483]
 [0.492]
 [0.51 ]
 [0.491]] [[0.197]
 [0.197]
 [0.101]
 [0.151]
 [0.156]
 [0.197]
 [0.317]] [[0.206]
 [0.206]
 [0.071]
 [0.105]
 [0.128]
 [0.206]
 [0.288]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.407]
 [0.384]
 [0.338]
 [0.339]
 [0.366]
 [0.366]] [[-0.238]
 [-0.664]
 [ 0.012]
 [-0.02 ]
 [-0.027]
 [ 0.071]
 [ 0.194]] [[ 0.129]
 [-0.134]
 [ 0.496]
 [ 0.373]
 [ 0.367]
 [ 0.518]
 [ 0.642]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  55 total reward:  0.49999999999999967  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.487]
 [0.438]
 [0.509]
 [0.421]
 [0.407]
 [0.479]] [[ 1.526]
 [ 2.267]
 [ 0.376]
 [ 1.526]
 [-0.124]
 [ 0.289]
 [ 1.584]] [[ 0.409]
 [ 0.611]
 [-0.118]
 [ 0.409]
 [-0.318]
 [-0.209]
 [ 0.368]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.505162003955455, 0.2214173254725334, 0.24431125664534184, 0.028049800123284004, 0.0005298069016928494, 0.0005298069016928494]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.505162003955455, 0.2214173254725334, 0.24431125664534184, 0.028049800123284004, 0.0005298069016928494, 0.0005298069016928494]
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]] [[-0.48]
 [-0.48]
 [-0.48]
 [-0.48]
 [-0.48]
 [-0.48]
 [-0.48]] [[0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]]
line 256 mcts: sample exp_bonus -0.49631409468659987
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.083]
 [0.089]
 [0.094]] [[-0.387]
 [-0.387]
 [-0.387]
 [-0.387]
 [-0.21 ]
 [-0.387]
 [-0.255]] [[-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.063]
 [-0.168]
 [-0.071]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
4063 5703
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.759]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]] [[1.592]
 [1.53 ]
 [1.642]
 [1.642]
 [1.642]
 [1.642]
 [1.642]] [[0.831]
 [0.759]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.628]
 [0.617]
 [0.707]
 [0.623]
 [0.629]
 [0.707]] [[1.882]
 [2.297]
 [1.83 ]
 [1.709]
 [1.813]
 [1.947]
 [1.709]] [[0.628]
 [0.628]
 [0.617]
 [0.707]
 [0.623]
 [0.629]
 [0.707]]
line 256 mcts: sample exp_bonus 2.252299619971827
siam score:  -0.6437915
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.505162003955455, 0.2214173254725334, 0.24431125664534184, 0.028049800123284004, 0.0005298069016928494, 0.0005298069016928494]
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.382]
 [0.351]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[-1.785]
 [-1.321]
 [-1.96 ]
 [-1.547]
 [-1.987]
 [-1.547]
 [-1.547]] [[0.35 ]
 [0.382]
 [0.351]
 [0.349]
 [0.349]
 [0.349]
 [0.349]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.6599999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
siam score:  -0.6418465
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.356]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[-1.183]
 [-1.218]
 [-1.393]
 [-1.393]
 [-1.393]
 [-1.393]
 [-1.393]] [[0.346]
 [0.356]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.116]
 [0.492]
 [0.153]
 [0.485]
 [0.562]
 [0.153]] [[ 0.075]
 [-0.07 ]
 [-0.276]
 [ 0.016]
 [ 0.316]
 [-0.838]
 [ 0.016]] [[0.776]
 [0.612]
 [0.805]
 [0.751]
 [1.473]
 [0.243]
 [0.751]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[2.33 ]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[2.018]
 [1.229]
 [1.229]
 [1.229]
 [1.229]
 [1.229]
 [1.229]]
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.37 ]
 [0.425]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[2.491]
 [2.103]
 [3.691]
 [3.124]
 [3.124]
 [3.124]
 [3.124]] [[ 0.049]
 [-0.229]
 [ 0.84 ]
 [ 0.334]
 [ 0.334]
 [ 0.334]
 [ 0.334]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
line 256 mcts: sample exp_bonus 1.859274175106743
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.64693403
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.428]
 [0.449]
 [0.452]
 [0.446]
 [0.46 ]
 [0.449]] [[-0.889]
 [ 0.621]
 [-0.775]
 [-0.724]
 [-0.605]
 [-0.261]
 [ 0.515]] [[0.45 ]
 [0.428]
 [0.449]
 [0.452]
 [0.446]
 [0.46 ]
 [0.449]]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.42 ]
 [0.407]
 [0.409]
 [0.406]
 [0.41 ]
 [0.407]] [[-1.038]
 [-1.138]
 [-1.207]
 [-1.181]
 [-1.086]
 [-0.993]
 [-1.21 ]] [[0.41 ]
 [0.42 ]
 [0.407]
 [0.409]
 [0.406]
 [0.41 ]
 [0.407]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]] [[-0.946]
 [-0.814]
 [-0.814]
 [-0.814]
 [-0.814]
 [-0.814]
 [-0.814]] [[0.388]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
UNIT TEST: sample policy line 217 mcts : [0.041 0.224 0.082 0.082 0.143 0.143 0.286]
start point for exploration sampling:  10935
siam score:  -0.6557808
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
siam score:  -0.6655625
from probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[-2.627]
 [-2.627]
 [-2.627]
 [-2.627]
 [-2.627]
 [-2.627]
 [-2.627]] [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
siam score:  -0.6650428
from probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
from probs:  [0.49083206361606996, 0.24350339526380246, 0.2373808745014848, 0.0272541109005178, 0.000514777859062552, 0.000514777859062552]
actor:  1 policy actor:  1  step number:  72 total reward:  0.2849999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.492]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[-1.954]
 [-1.483]
 [-1.954]
 [-1.954]
 [-1.954]
 [-1.954]
 [-1.954]] [[0.466]
 [0.492]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]]
line 256 mcts: sample exp_bonus -1.1357959783241869
siam score:  -0.6691005
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.613]
 [0.56 ]
 [0.613]
 [0.613]
 [0.558]
 [0.613]] [[2.036]
 [2.286]
 [2.439]
 [2.286]
 [2.286]
 [2.703]
 [2.286]] [[0.572]
 [0.613]
 [0.56 ]
 [0.613]
 [0.613]
 [0.558]
 [0.613]]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.469]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[1.854]
 [2.551]
 [1.854]
 [1.854]
 [1.854]
 [1.854]
 [1.854]] [[-0.213]
 [ 0.41 ]
 [-0.213]
 [-0.213]
 [-0.213]
 [-0.213]
 [-0.213]]
siam score:  -0.6736981
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.357]
 [0.45 ]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[-2.213]
 [-0.509]
 [-1.917]
 [-2.213]
 [-2.213]
 [-2.213]
 [-2.213]] [[-0.348]
 [ 0.055]
 [-0.229]
 [-0.348]
 [-0.348]
 [-0.348]
 [-0.348]]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.306]
 [0.316]
 [0.322]
 [0.331]
 [0.343]
 [0.36 ]] [[-1.32 ]
 [-1.406]
 [-1.335]
 [-1.288]
 [-1.301]
 [-1.294]
 [-1.258]] [[0.322]
 [0.306]
 [0.316]
 [0.322]
 [0.331]
 [0.343]
 [0.36 ]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.48262851992764216, 0.25614713516412607, 0.23341339861895288, 0.026798598096781045, 0.0005061740962489673, 0.0005061740962489673]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.463]
 [0.366]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[1.412]
 [2.048]
 [1.779]
 [1.412]
 [1.412]
 [1.412]
 [1.412]] [[-0.371]
 [ 0.074]
 [-0.209]
 [-0.371]
 [-0.371]
 [-0.371]
 [-0.371]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4826285199276422, 0.25614713516412607, 0.23341339861895288, 0.026798598096781045, 0.0005061740962489673, 0.0005061740962489673]
siam score:  -0.6710875
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
from probs:  [0.4826285199276422, 0.25614713516412607, 0.23341339861895288, 0.026798598096781045, 0.0005061740962489673, 0.0005061740962489673]
Printing some Q and Qe and total Qs values:  [[0.883]
 [0.857]
 [0.857]
 [0.873]
 [0.9  ]
 [0.835]
 [0.857]] [[2.93 ]
 [2.74 ]
 [2.74 ]
 [2.961]
 [2.671]
 [2.14 ]
 [2.74 ]] [[0.596]
 [0.48 ]
 [0.48 ]
 [0.587]
 [0.543]
 [0.235]
 [0.48 ]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4764475419382557, 0.2592072957363176, 0.2362019618367975, 0.02711875788788404, 0.0005122213003725399, 0.0005122213003725399]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4764475419382557, 0.2592072957363176, 0.2362019618367975, 0.02711875788788404, 0.0005122213003725399, 0.0005122213003725399]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.015]
 [0.015]
 [0.018]
 [0.019]
 [0.019]
 [0.021]] [[1.246]
 [0.53 ]
 [0.451]
 [0.649]
 [0.586]
 [0.425]
 [0.469]] [[ 0.324]
 [-0.621]
 [-0.727]
 [-0.456]
 [-0.538]
 [-0.754]
 [-0.689]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.68246305
Printing some Q and Qe and total Qs values:  [[0.056]
 [1.476]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]] [[0.947]
 [0.296]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]] [[1.043]
 [3.016]
 [1.043]
 [1.043]
 [1.043]
 [1.043]
 [1.043]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.597]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]] [[1.285]
 [1.69 ]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]] [[0.563]
 [0.597]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4764475419382557, 0.2592072957363176, 0.2362019618367975, 0.02711875788788404, 0.0005122213003725399, 0.0005122213003725399]
using another actor
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4117],
        [-0.3622],
        [-0.5483],
        [-0.0000],
        [-0.5223],
        [-0.4960],
        [-0.0000],
        [-0.0000],
        [-0.4725],
        [ 0.0869]], dtype=torch.float64)
-0.0727797758985 -0.48450389073399897
-0.0727797758985 -0.43497606442764597
-0.0727797758985 -0.6210549008077515
-0.6469154999999998 -0.6469154999999998
-0.024259925299500003 -0.5466078559259842
-0.0727797758985 -0.5688161870662519
-0.3882671099999996 -0.3882671099999996
0.92158507485 0.92158507485
-0.0727797758985 -0.545272249658586
-0.024259925299500003 0.06264916563495962
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4764475419382557, 0.2592072957363176, 0.2362019618367975, 0.02711875788788404, 0.0005122213003725399, 0.0005122213003725399]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using another actor
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]] [[0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.426]
 [0.196]] [[1.302]
 [1.302]
 [1.302]
 [1.302]
 [1.302]
 [1.312]
 [1.302]] [[1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.453]
 [1.269]]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
siam score:  -0.68665284
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
probs:  [0.4764475419382557, 0.2592072957363176, 0.2362019618367975, 0.02711875788788404, 0.0005122213003725399, 0.0005122213003725399]
maxi score, test score, baseline:  -0.9999 -1.0 -0.9999
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  97 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  50 total reward:  0.5949999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[-0.742]
 [-0.742]
 [-0.742]
 [-0.742]
 [-0.742]
 [-0.742]
 [-0.742]] [[-0.484]
 [-0.484]
 [-0.484]
 [-0.484]
 [-0.484]
 [-0.484]
 [-0.484]]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.4643625454377327, 0.27803342098931416, 0.2301884488454288, 0.026413336473717866, 0.0005011241269032732, 0.0005011241269032732]
siam score:  -0.704238
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.4643625454377327, 0.27803342098931416, 0.2301884488454288, 0.026413336473717866, 0.0005011241269032732, 0.0005011241269032732]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.625]
 [0.62 ]
 [0.602]
 [0.62 ]
 [0.607]
 [0.699]] [[ 0.   ]
 [-0.215]
 [ 0.   ]
 [-0.274]
 [ 0.   ]
 [-0.071]
 [ 0.025]] [[-0.088]
 [-0.15 ]
 [-0.088]
 [-0.216]
 [-0.088]
 [-0.139]
 [ 0.077]]
start point for exploration sampling:  10935
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
from probs:  [0.4643625454377327, 0.2780334209893142, 0.2301884488454288, 0.026413336473717866, 0.0005011241269032732, 0.0005011241269032732]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
siam score:  -0.7070476
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[-1.494]
 [-1.494]
 [-1.494]
 [-1.494]
 [-1.494]
 [-1.494]
 [-1.494]] [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]]
siam score:  -0.70743924
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.473]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[-0.316]
 [ 0.589]
 [-0.316]
 [-0.316]
 [-0.316]
 [-0.316]
 [-0.316]] [[0.463]
 [0.473]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.545]
 [0.548]
 [0.548]
 [0.54 ]
 [0.544]
 [0.545]] [[-0.65 ]
 [-0.154]
 [-0.267]
 [-0.267]
 [-0.589]
 [-0.598]
 [-0.458]] [[0.543]
 [0.545]
 [0.548]
 [0.548]
 [0.54 ]
 [0.544]
 [0.545]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[1.65]
 [1.65]
 [1.65]
 [1.65]
 [1.65]
 [1.65]
 [1.65]] [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
siam score:  -0.71409386
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.571]
 [0.552]
 [0.472]
 [0.552]
 [0.552]
 [0.552]] [[ 1.428]
 [ 1.595]
 [ 1.428]
 [-0.756]
 [ 1.428]
 [ 1.428]
 [ 1.428]] [[ 0.394]
 [ 0.488]
 [ 0.394]
 [-0.495]
 [ 0.394]
 [ 0.394]
 [ 0.394]]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.4643625454377327, 0.27803342098931416, 0.2301884488454288, 0.026413336473717866, 0.0005011241269032732, 0.0005011241269032732]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.4643625454377327, 0.27803342098931416, 0.2301884488454288, 0.026413336473717866, 0.0005011241269032732, 0.0005011241269032732]
line 256 mcts: sample exp_bonus 3.9558249103530945
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.618]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[1.541]
 [1.942]
 [1.541]
 [1.541]
 [1.541]
 [1.541]
 [1.541]] [[0.174]
 [0.531]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]]
siam score:  -0.71928084
actor:  1 policy actor:  1  step number:  72 total reward:  0.35499999999999954  reward:  1.0 rdn_beta:  0.167
using another actor
Printing some Q and Qe and total Qs values:  [[0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]] [[2.683]
 [2.683]
 [2.683]
 [2.683]
 [2.683]
 [2.683]
 [2.683]] [[1.607]
 [1.607]
 [1.607]
 [1.607]
 [1.607]
 [1.607]
 [1.607]]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.47400315606664856, 0.2730292676562096, 0.226045427875384, 0.02593793900069808, 0.000492104700529929, 0.000492104700529929]
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.858]
 [0.84 ]
 [0.843]
 [0.839]
 [0.846]
 [0.843]] [[2.292]
 [2.104]
 [2.067]
 [2.292]
 [2.131]
 [2.724]
 [2.292]] [[0.843]
 [0.858]
 [0.84 ]
 [0.843]
 [0.839]
 [0.846]
 [0.843]]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
from probs:  [0.47400315606664856, 0.2730292676562096, 0.226045427875384, 0.02593793900069808, 0.000492104700529929, 0.000492104700529929]
using explorer policy with actor:  1
4130 5779
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
actor:  1 policy actor:  1  step number:  71 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.629]
 [0.693]
 [0.743]
 [0.743]
 [0.743]
 [0.743]] [[0.608]
 [0.47 ]
 [1.346]
 [0.237]
 [0.237]
 [0.237]
 [0.237]] [[0.867]
 [0.716]
 [1.247]
 [0.714]
 [0.714]
 [0.714]
 [0.714]]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.592]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[0.078]
 [0.393]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]] [[0.359]
 [0.501]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.466512215463641, 0.26871443135033546, 0.2224731038262213, 0.025528027045615354, 0.016287894623229483, 0.00048432769095741856]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
siam score:  -0.7336598
from probs:  [0.466512215463641, 0.26871443135033546, 0.2224731038262213, 0.025528027045615354, 0.016287894623229483, 0.00048432769095741856]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.466512215463641, 0.26871443135033546, 0.2224731038262213, 0.025528027045615354, 0.016287894623229483, 0.00048432769095741856]
from probs:  [0.466512215463641, 0.26871443135033546, 0.2224731038262213, 0.025528027045615354, 0.016287894623229483, 0.00048432769095741856]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
first move QE:  0.2899199502788601
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
actor:  1 policy actor:  1  step number:  71 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
from probs:  [0.4753313065948579, 0.26427231078629804, 0.2187953990431713, 0.025106023012081376, 0.016018639297837627, 0.0004763212657538173]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.4753313065948579, 0.26427231078629804, 0.2187953990431713, 0.025106023012081376, 0.016018639297837627, 0.0004763212657538173]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.659]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.644]] [[-0.256]
 [-0.141]
 [-0.324]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.258]] [[0.646]
 [0.659]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.644]]
first move QE:  0.28932728598511237
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.227]
 [0.188]
 [0.193]
 [0.185]
 [0.203]
 [0.189]] [[1.607]
 [2.326]
 [2.402]
 [1.22 ]
 [0.847]
 [2.231]
 [1.598]] [[-0.309]
 [ 0.025]
 [-0.028]
 [-0.412]
 [-0.554]
 [-0.054]
 [-0.294]]
Printing some Q and Qe and total Qs values:  [[0.57]
 [0.57]
 [0.58]
 [0.57]
 [0.57]
 [0.57]
 [0.57]] [[2.716]
 [2.716]
 [3.217]
 [2.716]
 [2.716]
 [2.716]
 [2.716]] [[0.453]
 [0.453]
 [0.807]
 [0.453]
 [0.453]
 [0.453]
 [0.453]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.676]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[3.232]
 [3.583]
 [3.232]
 [3.232]
 [3.232]
 [3.232]
 [3.232]] [[0.152]
 [0.441]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]]
Printing some Q and Qe and total Qs values:  [[0.843]
 [1.014]
 [0.92 ]
 [0.878]
 [0.802]
 [0.984]
 [0.701]] [[3.815]
 [2.715]
 [3.2  ]
 [3.417]
 [3.33 ]
 [3.279]
 [3.262]] [[1.626]
 [1.321]
 [1.426]
 [1.473]
 [1.31 ]
 [1.567]
 [1.117]]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.302]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.273]] [[ 0.227]
 [-0.117]
 [-0.047]
 [ 0.   ]
 [ 0.   ]
 [ 0.066]
 [ 0.394]] [[-0.1  ]
 [-0.291]
 [-0.284]
 [-0.252]
 [-0.252]
 [-0.209]
 [-0.009]]
actor:  1 policy actor:  1  step number:  71 total reward:  0.36999999999999955  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  71 total reward:  0.4099999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.47671400199612257, 0.27210222293196157, 0.21105459971183513, 0.02421779278880285, 0.015451913155935755, 0.0004594694153420962]
siam score:  -0.7245467
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.515]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[0.896]
 [1.574]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]] [[0.29 ]
 [0.804]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
siam score:  -0.72486734
actor:  1 policy actor:  1  step number:  85 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]] [[4.212]
 [4.123]
 [4.123]
 [4.123]
 [4.123]
 [4.123]
 [4.123]] [[1.553]
 [1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]]
siam score:  -0.71101916
from probs:  [0.47091806230194727, 0.2809520817608301, 0.20848857537232338, 0.023923350280417966, 0.015264047147309128, 0.00045388313717218743]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.43 ]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[-0.643]
 [-0.306]
 [-0.643]
 [-0.643]
 [-0.643]
 [-0.643]
 [-0.643]] [[0.395]
 [0.43 ]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]]
siam score:  -0.71440524
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.665]
 [0.664]
 [0.667]
 [0.671]
 [0.664]
 [0.656]] [[2.709]
 [3.819]
 [2.909]
 [2.674]
 [2.864]
 [2.778]
 [3.179]] [[0.674]
 [0.665]
 [0.664]
 [0.667]
 [0.671]
 [0.664]
 [0.656]]
line 256 mcts: sample exp_bonus 3.255057756127457
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]] [[3.459]
 [3.459]
 [3.459]
 [3.459]
 [3.459]
 [3.459]
 [3.459]] [[0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]]
line 256 mcts: sample exp_bonus 1.6048694643035206
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.47091806230194727, 0.2809520817608301, 0.20848857537232338, 0.023923350280417966, 0.015264047147309128, 0.00045388313717218743]
line 256 mcts: sample exp_bonus -1.4180101608377633
from probs:  [0.47091806230194727, 0.2809520817608301, 0.20848857537232338, 0.023923350280417966, 0.015264047147309128, 0.00045388313717218743]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.81939922388275
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.188]
 [0.189]
 [0.187]
 [0.188]
 [0.189]
 [0.19 ]] [[2.166]
 [2.009]
 [2.039]
 [1.942]
 [2.009]
 [1.957]
 [1.982]] [[ 0.021]
 [-0.023]
 [-0.01 ]
 [-0.046]
 [-0.023]
 [-0.037]
 [-0.027]]
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.182]
 [0.184]] [[2.316]
 [1.914]
 [1.914]
 [1.914]
 [1.914]
 [2.313]
 [2.302]] [[0.201]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.211]
 [0.211]]
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.435]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[0.161]
 [0.615]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]] [[0.808]
 [1.175]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.47091806230194727, 0.2809520817608301, 0.20848857537232338, 0.023923350280417966, 0.015264047147309128, 0.00045388313717218743]
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.885]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]] [[2.245]
 [1.271]
 [2.245]
 [2.245]
 [2.245]
 [2.245]
 [2.245]] [[0.914]
 [0.885]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.47091806230194727, 0.2809520817608301, 0.20848857537232338, 0.023923350280417966, 0.015264047147309128, 0.00045388313717218743]
maxi score, test score, baseline:  -0.9973 -1.0 -0.9973
probs:  [0.47091806230194727, 0.2809520817608301, 0.20848857537232338, 0.023923350280417966, 0.015264047147309128, 0.00045388313717218743]
actor:  0 policy actor:  1  step number:  90 total reward:  0.06499999999999928  reward:  1.0 rdn_beta:  0.667
from probs:  [0.47091806230194727, 0.2809520817608301, 0.20848857537232338, 0.023923350280417966, 0.015264047147309128, 0.00045388313717218743]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.4709339107734925, 0.280957436206592, 0.20847955440232183, 0.0239111535700434, 0.015262631835849284, 0.000455313211701091]
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.733]
 [0.758]
 [0.716]
 [0.716]
 [0.716]
 [0.716]] [[2.987]
 [3.371]
 [3.087]
 [2.987]
 [2.987]
 [2.987]
 [2.987]] [[0.763]
 [1.053]
 [0.914]
 [0.763]
 [0.763]
 [0.763]
 [0.763]]
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.364]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]] [[1.989]
 [2.31 ]
 [1.989]
 [1.989]
 [1.989]
 [1.989]
 [1.989]] [[-0.471]
 [ 0.029]
 [-0.471]
 [-0.471]
 [-0.471]
 [-0.471]
 [-0.471]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.4709339107734925, 0.280957436206592, 0.20847955440232183, 0.0239111535700434, 0.015262631835849284, 0.000455313211701091]
actor:  1 policy actor:  1  step number:  54 total reward:  0.6049999999999998  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.506]
 [0.498]
 [0.498]
 [0.498]
 [0.499]
 [0.498]] [[0.647]
 [1.009]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.668]
 [0.   ]] [[0.502]
 [0.506]
 [0.498]
 [0.498]
 [0.498]
 [0.499]
 [0.498]]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.621]
 [0.448]
 [0.626]
 [0.554]
 [0.626]
 [0.626]] [[1.449]
 [2.536]
 [0.923]
 [1.888]
 [0.959]
 [1.888]
 [1.888]] [[0.37 ]
 [0.906]
 [0.021]
 [0.701]
 [0.246]
 [0.701]
 [0.701]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.46020335354618125, 0.29734132193481433, 0.20372920252053844, 0.02336632127853788, 0.014914862136944046, 0.00044493858298411294]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.651]
 [0.656]
 [0.588]
 [0.66 ]
 [0.659]
 [0.655]] [[-0.731]
 [-0.543]
 [-1.489]
 [-0.775]
 [-1.385]
 [-1.311]
 [-1.196]] [[1.027]
 [1.246]
 [0.939]
 [1.043]
 [0.981]
 [1.005]
 [1.036]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.46020335354618125, 0.29734132193481433, 0.20372920252053844, 0.02336632127853788, 0.014914862136944046, 0.00044493858298411294]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.46020335354618125, 0.29734132193481433, 0.20372920252053844, 0.02336632127853788, 0.014914862136944046, 0.00044493858298411294]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.478]
 [0.465]
 [0.417]
 [0.465]
 [0.418]
 [0.419]] [[ 0.603]
 [ 0.897]
 [ 0.603]
 [ 0.048]
 [ 0.603]
 [-0.017]
 [ 0.07 ]] [[0.303]
 [0.427]
 [0.303]
 [0.022]
 [0.303]
 [0.001]
 [0.032]]
using another actor
from probs:  [0.46020335354618125, 0.29734132193481433, 0.20372920252053844, 0.02336632127853788, 0.014914862136944046, 0.00044493858298411294]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.433]
 [0.499]
 [0.499]
 [0.487]
 [0.499]
 [0.49 ]] [[0.767]
 [0.719]
 [1.   ]
 [1.   ]
 [0.621]
 [1.   ]
 [0.719]] [[0.368]
 [0.268]
 [0.682]
 [0.682]
 [0.278]
 [0.682]
 [0.381]]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.01 ]
 [0.015]
 [0.015]
 [0.016]
 [0.011]
 [0.013]] [[1.712]
 [0.881]
 [1.593]
 [1.599]
 [1.582]
 [1.52 ]
 [0.815]] [[1.151]
 [0.487]
 [1.059]
 [1.063]
 [1.051]
 [0.997]
 [0.437]]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.387]
 [0.344]
 [0.341]
 [0.325]
 [0.333]
 [0.339]] [[-1.963]
 [-2.454]
 [-2.113]
 [-2.098]
 [-1.992]
 [-2.068]
 [-2.065]] [[ 0.425]
 [-0.048]
 [ 0.288]
 [ 0.301]
 [ 0.403]
 [ 0.326]
 [ 0.337]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [0.226]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[1.64 ]
 [1.031]
 [1.718]
 [1.718]
 [1.718]
 [1.718]
 [1.718]] [[-0.388]
 [-0.742]
 [-0.329]
 [-0.329]
 [-0.329]
 [-0.329]
 [-0.329]]
siam score:  -0.7214502
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.052]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]] [[-1.756]
 [-2.196]
 [-2.299]
 [-2.343]
 [-2.343]
 [-2.349]
 [-2.266]] [[ 0.557]
 [-0.009]
 [-0.147]
 [-0.205]
 [-0.206]
 [-0.213]
 [-0.105]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.47253622778543697, 0.2876513028062154, 0.20918889031765817, 0.014852154923954154, 0.015314561786269417, 0.0004568623804659257]
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.216]
 [0.258]
 [0.31 ]
 [0.31 ]
 [0.229]
 [0.264]] [[1.989]
 [2.219]
 [2.678]
 [2.919]
 [2.919]
 [2.741]
 [2.623]] [[1.027]
 [1.25 ]
 [1.545]
 [1.747]
 [1.747]
 [1.533]
 [1.527]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.47253622778543697, 0.2876513028062154, 0.20918889031765817, 0.014852154923954154, 0.015314561786269417, 0.0004568623804659257]
from probs:  [0.47253622778543697, 0.2876513028062154, 0.20918889031765817, 0.014852154923954154, 0.015314561786269417, 0.0004568623804659257]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]] [[3.532]
 [3.532]
 [3.532]
 [3.532]
 [3.532]
 [3.532]
 [3.532]] [[-0.339]
 [-0.339]
 [-0.339]
 [-0.339]
 [-0.339]
 [-0.339]
 [-0.339]]
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]] [[4.382]
 [4.251]
 [4.251]
 [4.251]
 [4.251]
 [4.251]
 [4.318]] [[0.303]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.245]]
4185 5815
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.47253622778543697, 0.2876513028062154, 0.20918889031765817, 0.014852154923954154, 0.015314561786269417, 0.0004568623804659257]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.47253622778543697, 0.2876513028062154, 0.20918889031765817, 0.014852154923954154, 0.015314561786269417, 0.0004568623804659257]
first move QE:  0.2872600602100223
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]] [[0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]]
first move QE:  0.2872491292362338
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.031]
 [ 0.005]
 [-0.004]
 [-0.005]
 [-0.005]
 [-0.   ]] [[1.696]
 [0.882]
 [0.738]
 [1.205]
 [1.157]
 [1.068]
 [0.882]] [[-0.385]
 [-0.583]
 [-0.684]
 [-0.546]
 [-0.563]
 [-0.593]
 [-0.646]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
siam score:  -0.7154377
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.47253622778543697, 0.2876513028062154, 0.20918889031765814, 0.014852154923954154, 0.015314561786269417, 0.0004568623804659257]
actor:  1 policy actor:  1  step number:  59 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.4825480445320165, 0.28219137876532946, 0.20521826532764695, 0.014570245414290332, 0.01502387530838193, 0.0004481906523348602]
actor:  1 policy actor:  1  step number:  47 total reward:  0.6199999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]] [[1.665]
 [1.643]
 [1.643]
 [1.643]
 [1.643]
 [1.643]
 [1.643]] [[0.338]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.453]
 [0.546]
 [0.54 ]
 [0.54 ]
 [0.536]
 [0.537]] [[-0.026]
 [ 0.599]
 [-0.025]
 [-0.376]
 [-0.376]
 [-0.178]
 [-0.192]] [[0.545]
 [0.453]
 [0.546]
 [0.54 ]
 [0.54 ]
 [0.536]
 [0.537]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
Printing some Q and Qe and total Qs values:  [[0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]] [[-0.5]
 [-0.5]
 [-0.5]
 [-0.5]
 [-0.5]
 [-0.5]
 [-0.5]] [[0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]]
siam score:  -0.7062702
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.4943719361355059, 0.2757432433612554, 0.20052898258622665, 0.014237312084742857, 0.014680576435446005, 0.000437949396823299]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.4943719361355059, 0.2757432433612554, 0.20052898258622665, 0.014237312084742857, 0.014680576435446005, 0.000437949396823299]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.4943719361355059, 0.2757432433612554, 0.20052898258622665, 0.014237312084742857, 0.014680576435446005, 0.000437949396823299]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.4943719361355059, 0.2757432433612554, 0.20052898258622665, 0.014237312084742857, 0.014680576435446005, 0.000437949396823299]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.375]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[0.935]
 [1.421]
 [0.935]
 [0.935]
 [0.935]
 [0.935]
 [0.935]] [[0.077]
 [0.324]
 [0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.077]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
siam score:  -0.7046001
first move QE:  0.2873131085443437
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.629]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[-0.396]
 [-0.151]
 [-0.396]
 [-0.396]
 [-0.396]
 [-0.396]
 [-0.396]] [[0.382]
 [0.541]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]]
line 256 mcts: sample exp_bonus 0.14938528039898477
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.8203189320162645
first move QE:  0.28750511611359464
UNIT TEST: sample policy line 217 mcts : [0.429 0.224 0.02  0.02  0.02  0.265 0.02 ]
Printing some Q and Qe and total Qs values:  [[0.966]
 [1.284]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]] [[0.378]
 [1.025]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]] [[2.016]
 [2.867]
 [2.016]
 [2.016]
 [2.016]
 [2.016]
 [2.016]]
from probs:  [0.4943719361355059, 0.2757432433612554, 0.20052898258622665, 0.014237312084742857, 0.014680576435446005, 0.000437949396823299]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.459]
 [0.462]
 [0.443]
 [0.442]
 [0.448]
 [0.453]] [[0.634]
 [0.397]
 [0.753]
 [0.395]
 [0.436]
 [0.421]
 [0.212]] [[0.437]
 [0.459]
 [0.462]
 [0.443]
 [0.442]
 [0.448]
 [0.453]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.4943719361355059, 0.2757432433612554, 0.20052898258622665, 0.014237312084742857, 0.014680576435446005, 0.000437949396823299]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.4943719361355059, 0.2757432433612554, 0.20052898258622665, 0.014237312084742857, 0.014680576435446005, 0.000437949396823299]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[1.043]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]] [[0.361]
 [0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]] [[1.735]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.592]
 [0.572]
 [0.58 ]
 [0.585]
 [0.554]
 [0.588]] [[2.027]
 [2.422]
 [2.751]
 [2.742]
 [2.414]
 [2.708]
 [2.529]] [[0.577]
 [0.592]
 [0.572]
 [0.58 ]
 [0.585]
 [0.554]
 [0.588]]
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.665]
 [0.673]
 [0.649]
 [0.638]
 [0.663]
 [0.647]] [[0.523]
 [0.194]
 [0.518]
 [0.678]
 [0.916]
 [1.207]
 [0.169]] [[1.733]
 [1.663]
 [1.774]
 [1.778]
 [1.83 ]
 [1.959]
 [1.625]]
Printing some Q and Qe and total Qs values:  [[0.967]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[0.178]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[1.246]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.678]
 [0.621]
 [0.621]
 [0.505]
 [0.37 ]
 [0.621]] [[2.028]
 [3.052]
 [2.028]
 [2.028]
 [2.082]
 [2.28 ]
 [2.028]] [[ 0.198]
 [ 0.654]
 [ 0.198]
 [ 0.198]
 [-0.017]
 [-0.22 ]
 [ 0.198]]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.019]
 [-0.026]
 [-0.021]
 [-0.026]
 [-0.025]
 [-0.003]] [[-0.131]
 [-0.591]
 [-0.137]
 [-0.124]
 [-0.307]
 [-0.331]
 [-0.209]] [[-0.419]
 [-0.905]
 [-0.465]
 [-0.444]
 [-0.636]
 [-0.658]
 [-0.493]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.4943719361355059, 0.2757432433612554, 0.20052898258622665, 0.014237312084742857, 0.014680576435446005, 0.000437949396823299]
using explorer policy with actor:  1
siam score:  -0.7053171
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
actor:  1 policy actor:  1  step number:  86 total reward:  0.3149999999999995  reward:  1.0 rdn_beta:  0.167
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.25 ]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]] [[-2.436]
 [-0.672]
 [-2.278]
 [-2.278]
 [-2.278]
 [-2.278]
 [-2.278]] [[0.342]
 [0.25 ]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.5018615477436256, 0.2716587987982801, 0.19755864865289346, 0.014026422014592617, 0.014463120515684652, 0.00043146227492356235]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.849]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]] [[-3.216]
 [-3.495]
 [-3.495]
 [-3.495]
 [-3.495]
 [-3.495]
 [-3.495]] [[0.849]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[-3.823]
 [-3.183]
 [-3.183]
 [-3.183]
 [-3.183]
 [-3.183]
 [-3.183]] [[0.516]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.657]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[-2.156]
 [-1.26 ]
 [-2.156]
 [-2.156]
 [-2.156]
 [-2.156]
 [-2.156]] [[0.653]
 [0.657]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
start point for exploration sampling:  10935
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.5018615477436256, 0.2716587987982801, 0.1975586486528935, 0.014026422014592617, 0.014463120515684652, 0.00043146227492356235]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.45 ]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.49 ]] [[-4.805]
 [-2.07 ]
 [-1.952]
 [-1.952]
 [-1.952]
 [-1.952]
 [-2.124]] [[0.674]
 [0.45 ]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.49 ]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.5018615477436256, 0.2716587987982801, 0.1975586486528935, 0.014026422014592617, 0.014463120515684652, 0.00043146227492356235]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.616]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]] [[-1.263]
 [-2.777]
 [-2.909]
 [-2.909]
 [-2.909]
 [-2.909]
 [-2.909]] [[0.694]
 [0.616]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]]
start point for exploration sampling:  10935
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.739]
 [0.775]
 [0.775]
 [0.775]
 [0.723]
 [0.716]] [[ 0.   ]
 [-1.92 ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-1.87 ]
 [-1.703]] [[0.775]
 [0.739]
 [0.775]
 [0.775]
 [0.775]
 [0.723]
 [0.716]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]] [[-2.]
 [-2.]
 [-2.]
 [-2.]
 [-2.]
 [-2.]
 [-2.]] [[0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.777]
 [0.821]
 [0.77 ]
 [0.723]
 [0.735]
 [0.765]] [[0.251]
 [1.096]
 [1.333]
 [0.643]
 [0.11 ]
 [1.106]
 [0.644]] [[0.647]
 [0.777]
 [0.821]
 [0.77 ]
 [0.723]
 [0.735]
 [0.765]]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.0532897043681095
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[-1.063]
 [-2.392]
 [-2.392]
 [-2.392]
 [-2.392]
 [-2.392]
 [-2.392]] [[0.531]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
maxi score, test score, baseline:  -0.99517 -1.0 -0.99517
probs:  [0.5018615477436256, 0.2716587987982801, 0.1975586486528935, 0.014026422014592617, 0.014463120515684652, 0.00043146227492356235]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  66 total reward:  0.4049999999999996  reward:  1.0 rdn_beta:  1
line 256 mcts: sample exp_bonus -0.06934088578040654
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.925]
 [0.794]
 [0.822]
 [0.572]
 [0.794]
 [0.635]] [[-1.329]
 [ 1.296]
 [-1.329]
 [ 0.27 ]
 [ 0.383]
 [-1.329]
 [-0.435]] [[0.794]
 [0.925]
 [0.794]
 [0.822]
 [0.572]
 [0.794]
 [0.635]]
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.735]
 [0.735]
 [0.735]
 [0.759]
 [0.755]
 [0.735]] [[-1.568]
 [-1.503]
 [-1.503]
 [-1.503]
 [-1.596]
 [-1.593]
 [-1.503]] [[0.766]
 [0.735]
 [0.735]
 [0.735]
 [0.759]
 [0.755]
 [0.735]]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.503]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]] [[-4.024]
 [-3.261]
 [-3.843]
 [-3.843]
 [-3.843]
 [-3.843]
 [-3.843]] [[0.634]
 [0.503]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]]
actor:  0 policy actor:  1  step number:  72 total reward:  0.3949999999999996  reward:  1.0 rdn_beta:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.619]
 [0.53 ]
 [0.519]
 [0.519]
 [0.526]
 [0.519]] [[2.709]
 [2.501]
 [2.587]
 [2.881]
 [2.881]
 [2.49 ]
 [2.881]] [[0.496]
 [0.591]
 [0.441]
 [0.517]
 [0.517]
 [0.401]
 [0.517]]
maxi score, test score, baseline:  -0.9895700000000001 -1.0 -0.9895700000000001
probs:  [0.5018945437249929, 0.2716732894197323, 0.19751885141254463, 0.014019974615918265, 0.014458322969722059, 0.0004350178570899125]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.989582075563288
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -0.21112260584035655
maxi score, test score, baseline:  -0.9895700000000001 -1.0 -0.9895700000000001
maxi score, test score, baseline:  -0.9895700000000001 -1.0 -0.9895700000000001
probs:  [0.5018945437249929, 0.2716732894197323, 0.19751885141254463, 0.014019974615918265, 0.014458322969722059, 0.0004350178570899125]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[-3.039]
 [-2.87 ]
 [-2.87 ]
 [-2.87 ]
 [-2.87 ]
 [-2.87 ]
 [-2.87 ]] [[0.512]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]]
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.78 ]
 [0.79 ]
 [0.78 ]
 [0.796]
 [0.798]
 [0.783]] [[-1.496]
 [ 0.   ]
 [-1.446]
 [ 0.   ]
 [-1.434]
 [-1.474]
 [-1.431]] [[0.791]
 [0.78 ]
 [0.79 ]
 [0.78 ]
 [0.796]
 [0.798]
 [0.783]]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[-2.623]
 [-2.211]
 [-2.211]
 [-2.211]
 [-2.211]
 [-2.211]
 [-2.211]] [[0.462]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]]
Printing some Q and Qe and total Qs values:  [[ 0.059]
 [ 0.045]
 [ 0.113]
 [ 0.06 ]
 [-0.034]
 [ 0.09 ]
 [ 0.136]] [[ 0.964]
 [-0.109]
 [-0.181]
 [-0.191]
 [ 1.72 ]
 [-0.018]
 [-0.188]] [[-0.633]
 [-1.374]
 [-1.287]
 [-1.4  ]
 [-0.315]
 [-1.223]
 [-1.245]]
from probs:  [0.5018945437249929, 0.2716732894197323, 0.19751885141254463, 0.014019974615918265, 0.014458322969722059, 0.0004350178570899125]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.681]
 [0.814]
 [0.787]
 [0.786]
 [0.795]
 [0.818]] [[-1.88 ]
 [-1.025]
 [-2.651]
 [ 0.   ]
 [-2.157]
 [-2.129]
 [-2.492]] [[0.774]
 [0.681]
 [0.814]
 [0.787]
 [0.786]
 [0.795]
 [0.818]]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.47 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[-1.422]
 [-1.723]
 [-2.039]
 [-2.039]
 [-2.039]
 [-2.039]
 [-2.039]] [[0.458]
 [0.47 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[-2.708]
 [-2.708]
 [-2.708]
 [-2.708]
 [-2.708]
 [-2.708]
 [-2.708]] [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]]
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.844]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]] [[-1.592]
 [-0.77 ]
 [-1.855]
 [-1.855]
 [-1.855]
 [-1.855]
 [-1.855]] [[0.833]
 [0.844]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[-2.543]
 [-2.274]
 [-2.274]
 [-2.274]
 [-2.274]
 [-2.274]
 [-2.274]] [[0.509]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]]
maxi score, test score, baseline:  -0.9895700000000001 -1.0 -0.9895700000000001
probs:  [0.5018945437249929, 0.2716732894197323, 0.19751885141254463, 0.014019974615918265, 0.014458322969722059, 0.0004350178570899125]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.819]
 [0.821]
 [0.805]
 [0.812]
 [0.815]
 [0.772]
 [0.81 ]] [[-1.59 ]
 [-1.401]
 [-2.195]
 [-1.823]
 [-1.804]
 [-1.447]
 [-1.82 ]] [[0.819]
 [0.821]
 [0.805]
 [0.812]
 [0.815]
 [0.772]
 [0.81 ]]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[-2.177]
 [-2.439]
 [-2.439]
 [-2.439]
 [-2.439]
 [-2.439]
 [-2.439]] [[0.569]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.509]
 [0.502]
 [0.507]
 [0.495]
 [0.505]
 [0.504]] [[-1.347]
 [-0.819]
 [-1.181]
 [-1.604]
 [-1.819]
 [-1.537]
 [-1.496]] [[0.513]
 [0.509]
 [0.502]
 [0.507]
 [0.495]
 [0.505]
 [0.504]]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[-1.883]
 [-3.001]
 [-3.001]
 [-3.001]
 [-3.001]
 [-3.001]
 [-3.001]] [[0.689]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.6749315958476754
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.383]
 [0.414]
 [0.446]
 [0.503]
 [0.462]
 [0.453]] [[ 0.401]
 [-3.752]
 [-3.965]
 [-3.425]
 [-3.689]
 [-3.173]
 [-3.763]] [[0.566]
 [0.383]
 [0.414]
 [0.446]
 [0.503]
 [0.462]
 [0.453]]
maxi score, test score, baseline:  -0.9895700000000001 -1.0 -0.9895700000000001
maxi score, test score, baseline:  -0.9895700000000001 -1.0 -0.9895700000000001
probs:  [0.5018945437249929, 0.2716732894197323, 0.19751885141254463, 0.014019974615918265, 0.014458322969722059, 0.0004350178570899125]
line 256 mcts: sample exp_bonus 1.8971890579300175
maxi score, test score, baseline:  -0.9895700000000001 -1.0 -0.9895700000000001
probs:  [0.5018945437249929, 0.2716732894197323, 0.19751885141254463, 0.014019974615918265, 0.014458322969722059, 0.0004350178570899125]
rdn probs:  [0.5018945437249929, 0.2716732894197323, 0.19751885141254463, 0.014019974615918265, 0.014458322969722059, 0.0004350178570899125]
actor:  0 policy actor:  0  step number:  93 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.361]
 [0.209]
 [0.204]
 [0.218]
 [0.221]
 [0.21 ]] [[1.961]
 [2.125]
 [1.937]
 [2.321]
 [2.232]
 [2.275]
 [2.299]] [[-0.638]
 [-0.325]
 [-0.692]
 [-0.573]
 [-0.576]
 [-0.555]
 [-0.569]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5027192303074925, 0.2720645964606979, 0.1964886922727964, 0.013858034823571409, 0.014338242715341901, 0.0005312034201001004]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5027192303074925, 0.2720645964606979, 0.1964886922727964, 0.013858034823571409, 0.014338242715341901, 0.0005312034201001004]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.459]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[-6.24 ]
 [-6.24 ]
 [-5.087]
 [-6.24 ]
 [-6.24 ]
 [-6.24 ]
 [-6.24 ]] [[0.475]
 [0.475]
 [0.459]
 [0.475]
 [0.475]
 [0.475]
 [0.475]]
actor:  1 policy actor:  1  step number:  76 total reward:  0.3249999999999995  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[-0.929]
 [-0.929]
 [-0.929]
 [-0.929]
 [-0.929]
 [-0.929]
 [-0.929]] [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.571]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]] [[-0.281]
 [ 0.452]
 [-0.281]
 [-0.281]
 [-0.281]
 [-0.281]
 [-0.281]] [[0.449]
 [0.883]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
from probs:  [0.5100289044947863, 0.26806544009023336, 0.19360044802621165, 0.013654331552481481, 0.014127480729246404, 0.0005233951070408422]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5100289044947863, 0.26806544009023336, 0.19360044802621165, 0.013654331552481481, 0.014127480729246404, 0.0005233951070408422]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5100289044947863, 0.26806544009023336, 0.19360044802621165, 0.013654331552481481, 0.014127480729246404, 0.0005233951070408422]
actor:  1 policy actor:  1  step number:  69 total reward:  0.4099999999999996  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.266]
 [0.208]
 [0.208]
 [0.208]
 [0.207]
 [0.206]] [[-0.04 ]
 [ 0.3  ]
 [-0.545]
 [-0.645]
 [-0.64 ]
 [-0.595]
 [-0.353]] [[-0.429]
 [-0.103]
 [-0.777]
 [-0.844]
 [-0.842]
 [-0.813]
 [-0.654]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.212]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]] [[1.258]
 [1.79 ]
 [1.258]
 [1.258]
 [1.258]
 [1.258]
 [1.258]] [[-1.165]
 [-0.746]
 [-1.165]
 [-1.165]
 [-1.165]
 [-1.165]
 [-1.165]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
siam score:  -0.7012192
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.145]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]] [[2.97 ]
 [2.936]
 [2.846]
 [2.846]
 [2.846]
 [2.846]
 [2.846]] [[-0.764]
 [-0.63 ]
 [-0.863]
 [-0.863]
 [-0.863]
 [-0.863]
 [-0.863]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
line 256 mcts: sample exp_bonus 3.977575833547938
start point for exploration sampling:  10935
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.04080109666932984
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
from probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
from probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]] [[2.167]
 [2.25 ]
 [2.25 ]
 [2.25 ]
 [2.25 ]
 [2.25 ]
 [2.25 ]] [[1.406]
 [1.434]
 [1.434]
 [1.434]
 [1.434]
 [1.434]
 [1.434]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
using another actor
from probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
line 256 mcts: sample exp_bonus 0.21135798071556716
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[-0.754]
 [ 0.484]
 [ 0.484]
 [ 0.484]
 [ 0.484]
 [ 0.484]
 [ 0.484]] [[0.603]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
line 256 mcts: sample exp_bonus 2.7229644335822196
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]] [[5.972]
 [0.904]
 [1.597]
 [1.597]
 [1.597]
 [1.597]
 [1.597]] [[1.464]
 [0.05 ]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]]
first move QE:  0.2846320140734695
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[-0.656]
 [-0.656]
 [-0.656]
 [-0.656]
 [-0.656]
 [-0.656]
 [-0.656]] [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.54 ]
 [0.512]
 [0.514]
 [0.52 ]
 [0.507]
 [0.502]] [[-0.391]
 [-1.295]
 [-0.689]
 [-0.945]
 [-1.041]
 [-0.905]
 [-0.491]] [[0.51 ]
 [0.54 ]
 [0.512]
 [0.514]
 [0.52 ]
 [0.507]
 [0.502]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.27604040769977556
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.113]
 [-0.082]
 [-0.082]
 [-0.082]
 [-0.082]
 [-0.082]] [[4.307]
 [0.749]
 [2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]] [[ 0.815]
 [-0.045]
 [ 0.3  ]
 [ 0.3  ]
 [ 0.3  ]
 [ 0.3  ]
 [ 0.3  ]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.087]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.098]
 [-0.097]] [[2.048]
 [1.902]
 [1.953]
 [1.924]
 [2.029]
 [2.508]
 [2.587]] [[-0.911]
 [-0.941]
 [-0.944]
 [-0.954]
 [-0.92 ]
 [-0.76 ]
 [-0.733]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.202]
 [0.282]
 [0.2  ]
 [0.214]
 [0.214]
 [0.212]
 [0.196]] [[-0.929]
 [-1.462]
 [-0.89 ]
 [-1.019]
 [-1.036]
 [-1.023]
 [-0.89 ]] [[ 0.267]
 [-0.105]
 [ 0.302]
 [ 0.201]
 [ 0.184]
 [ 0.194]
 [ 0.293]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.641]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.624]] [[-0.383]
 [ 0.041]
 [-0.383]
 [-0.383]
 [-0.383]
 [-0.383]
 [-0.42 ]] [[0.627]
 [0.641]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.624]]
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]] [[ 1.338]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[0.857]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]] [[0.484]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[0.602]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
first move QE:  0.28403253259897776
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
siam score:  -0.7316274
first move QE:  0.2837537307118091
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.065]
 [-0.071]
 [-0.072]
 [-0.072]
 [-0.073]
 [-0.074]] [[2.948]
 [2.379]
 [2.563]
 [3.102]
 [3.072]
 [2.831]
 [2.81 ]] [[-0.839]
 [-1.016]
 [-0.967]
 [-0.79 ]
 [-0.8  ]
 [-0.882]
 [-0.89 ]]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
maxi score, test score, baseline:  -0.9874499999999999 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.73 ]
 [0.612]
 [0.654]
 [0.612]
 [0.612]
 [0.612]] [[3.871]
 [3.502]
 [3.871]
 [3.579]
 [3.871]
 [3.871]
 [3.871]] [[0.426]
 [0.54 ]
 [0.426]
 [0.414]
 [0.426]
 [0.426]
 [0.426]]
actor:  0 policy actor:  1  step number:  88 total reward:  0.14499999999999935  reward:  1.0 rdn_beta:  0.333
4248 5874
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.396]
 [0.325]
 [0.367]
 [0.374]
 [0.346]
 [0.354]] [[3.884]
 [3.87 ]
 [3.973]
 [3.854]
 [4.314]
 [4.04 ]
 [4.118]] [[0.529]
 [0.55 ]
 [0.475]
 [0.482]
 [0.801]
 [0.563]
 [0.632]]
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
siam score:  -0.74952716
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.076]] [[0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [1.44 ]] [[-1.76 ]
 [-1.76 ]
 [-1.76 ]
 [-1.76 ]
 [-1.76 ]
 [-1.76 ]
 [-0.105]]
from probs:  [0.5015607142149424, 0.2802180084468237, 0.19038603131825474, 0.013427623856679366, 0.013892917170326948, 0.0005147049929729905]
Printing some Q and Qe and total Qs values:  [[0.966]
 [0.886]
 [0.886]
 [0.886]
 [0.966]
 [0.966]
 [0.886]] [[0.951]
 [1.496]
 [1.496]
 [1.496]
 [1.009]
 [1.209]
 [1.496]] [[0.639]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.658]
 [0.726]
 [0.66 ]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.3349999999999995  reward:  1.0 rdn_beta:  0.167
4252 5875
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.075]
 [-0.068]
 [-0.082]
 [-0.082]
 [-0.079]
 [-0.081]
 [-0.08 ]] [[3.412]
 [3.845]
 [3.368]
 [3.513]
 [3.628]
 [3.372]
 [3.461]] [[-0.329]
 [ 0.005]
 [-0.371]
 [-0.264]
 [-0.174]
 [-0.368]
 [-0.3  ]]
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.639]
 [0.423]
 [0.564]
 [0.556]
 [0.492]
 [0.78 ]] [[1.498]
 [1.388]
 [1.044]
 [1.361]
 [0.917]
 [0.639]
 [1.734]] [[0.624]
 [0.639]
 [0.423]
 [0.564]
 [0.556]
 [0.492]
 [0.78 ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.5087817366255103, 0.27615841568087185, 0.18762785828090503, 0.013233094301014344, 0.013691646786759399, 0.0005072483249392193]
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.5087817366255103, 0.27615841568087185, 0.18762785828090503, 0.013233094301014344, 0.013691646786759399, 0.0005072483249392193]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.429]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.392]] [[0.714]
 [0.35 ]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.4  ]] [[0.53 ]
 [0.253]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.23 ]]
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
actor:  1 policy actor:  1  step number:  84 total reward:  0.1849999999999994  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.5032069299487298, 0.2731325016812897, 0.19652915585837588, 0.013088097071056659, 0.013541625120437033, 0.0005016903201110344]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.5032069299487298, 0.2731325016812897, 0.19652915585837588, 0.013088097071056659, 0.013541625120437033, 0.0005016903201110344]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.595]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]] [[0.352]
 [0.644]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[0.379]
 [0.909]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]]
actor:  1 policy actor:  1  step number:  101 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
using another actor
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.497804474529299, 0.2809361884387438, 0.19441920875718743, 0.012947582589352736, 0.01339624153832448, 0.0004963041470926381]
4259 5885
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.497804474529299, 0.2809361884387438, 0.19441920875718743, 0.012947582589352736, 0.01339624153832448, 0.0004963041470926381]
siam score:  -0.7450168
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[-0.106]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]] [[-0.981]
 [-0.924]
 [-0.924]
 [-0.924]
 [-0.924]
 [-0.924]
 [-0.924]] [[-0.545]
 [-0.466]
 [-0.466]
 [-0.466]
 [-0.466]
 [-0.466]
 [-0.466]]
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.497804474529299, 0.2809361884387438, 0.19441920875718743, 0.012947582589352736, 0.01339624153832448, 0.0004963041470926381]
Printing some Q and Qe and total Qs values:  [[-0.128]
 [-0.124]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]] [[1.011]
 [0.685]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[0.536]
 [0.374]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]]
siam score:  -0.7440007
Printing some Q and Qe and total Qs values:  [[-0.15 ]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]] [[ 3.385]
 [-0.537]
 [-0.537]
 [-0.537]
 [-0.537]
 [-0.537]
 [-0.537]] [[ 1.172]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.426]
 [0.375]
 [0.426]
 [0.426]
 [0.418]
 [0.426]] [[0.267]
 [0.267]
 [0.62 ]
 [0.267]
 [0.267]
 [1.632]
 [0.267]] [[0.127]
 [0.127]
 [0.273]
 [0.127]
 [0.127]
 [0.911]
 [0.127]]
from probs:  [0.49780447452929905, 0.28093618843874385, 0.1944192087571874, 0.012947582589352738, 0.013396241538324482, 0.0004963041470926382]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]] [[0.418]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]] [[1.767]
 [1.878]
 [1.878]
 [1.878]
 [1.878]
 [1.878]
 [1.878]]
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.319]
 [0.273]
 [0.268]
 [0.268]
 [0.269]
 [0.27 ]] [[-2.686]
 [-2.585]
 [-2.655]
 [-2.53 ]
 [-2.549]
 [-2.515]
 [-2.459]] [[0.083]
 [0.213]
 [0.105]
 [0.218]
 [0.2  ]
 [0.232]
 [0.286]]
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.664]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[0.313]
 [2.497]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[0.635]
 [0.664]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]]
using explorer policy with actor:  1
siam score:  -0.75625765
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.49780447452929905, 0.28093618843874385, 0.19441920875718743, 0.01294758258935274, 0.013396241538324484, 0.0004963041470926383]
actor:  1 policy actor:  1  step number:  60 total reward:  0.4349999999999996  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus -0.4644783754211351
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.48965355974368724, 0.2927099433030306, 0.191235841623406, 0.012735582400995104, 0.013176895130621737, 0.0004881777982595489]
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[-0.019]
 [ 0.404]
 [ 0.404]
 [ 0.404]
 [-0.035]
 [ 0.404]
 [ 0.404]] [[ 0.124]
 [-1.382]
 [-1.382]
 [-1.382]
 [ 0.298]
 [-1.382]
 [-1.382]] [[1.377]
 [0.782]
 [0.782]
 [0.782]
 [1.464]
 [0.782]
 [0.782]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.48965355974368724, 0.2927099433030306, 0.19123584162340604, 0.012735582400995104, 0.013176895130621737, 0.0004881777982595489]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.48965355974368724, 0.2927099433030306, 0.19123584162340604, 0.012735582400995104, 0.013176895130621737, 0.0004881777982595489]
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.534]
 [0.532]
 [0.532]
 [0.532]
 [0.537]
 [0.535]] [[-0.265]
 [ 0.485]
 [-0.808]
 [-0.853]
 [-0.767]
 [-0.285]
 [-0.134]] [[0.528]
 [0.534]
 [0.532]
 [0.532]
 [0.532]
 [0.537]
 [0.535]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.701]
 [0.691]
 [0.691]
 [0.683]
 [0.682]
 [0.685]] [[2.42 ]
 [2.389]
 [2.43 ]
 [2.462]
 [2.329]
 [2.239]
 [2.444]] [[0.682]
 [0.701]
 [0.691]
 [0.691]
 [0.683]
 [0.682]
 [0.685]]
siam score:  -0.7569306
actor:  1 policy actor:  1  step number:  50 total reward:  0.6349999999999998  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.805]
 [0.548]
 [0.548]
 [0.811]
 [0.78 ]
 [0.548]
 [0.548]] [[3.12 ]
 [4.166]
 [4.166]
 [3.847]
 [2.416]
 [4.166]
 [4.166]] [[0.837]
 [0.672]
 [0.672]
 [1.09 ]
 [0.551]
 [0.672]
 [0.672]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.509]
 [0.453]
 [0.457]
 [0.457]
 [0.457]
 [0.457]] [[0.983]
 [0.839]
 [0.659]
 [0.983]
 [0.983]
 [0.983]
 [0.983]] [[-0.134]
 [-0.127]
 [-0.358]
 [-0.134]
 [-0.134]
 [-0.134]
 [-0.134]]
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]] [[1.993]
 [1.993]
 [1.993]
 [1.993]
 [1.993]
 [1.993]
 [1.993]] [[0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.5005524011954321, 0.2864588969318502, 0.18715185287901948, 0.012463604226102048, 0.012895492382357654, 0.0004777523852385026]
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.682]
 [0.684]
 [0.674]
 [0.673]
 [0.676]
 [0.706]] [[ 0.006]
 [ 0.343]
 [ 0.108]
 [-0.152]
 [-0.28 ]
 [-0.249]
 [ 0.323]] [[0.679]
 [0.682]
 [0.684]
 [0.674]
 [0.673]
 [0.676]
 [0.706]]
siam score:  -0.7591542
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.5005524011954321, 0.2864588969318502, 0.18715185287901948, 0.012463604226102048, 0.012895492382357654, 0.0004777523852385026]
Printing some Q and Qe and total Qs values:  [[ 0.054]
 [ 0.476]
 [-0.001]
 [-0.001]
 [ 0.318]
 [ 0.464]
 [-0.001]] [[ 0.236]
 [-1.341]
 [ 0.   ]
 [ 0.   ]
 [ 0.9  ]
 [-1.492]
 [ 0.   ]] [[1.234]
 [0.378]
 [1.018]
 [1.018]
 [1.922]
 [0.258]
 [1.018]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.4249999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
line 256 mcts: sample exp_bonus -0.1636054966858612
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.49277758624825335, 0.29754195241277814, 0.18424492241643264, 0.012270013672542832, 0.012695193539147352, 0.00047033171084570835]
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.49277758624825335, 0.29754195241277814, 0.18424492241643264, 0.012270013672542832, 0.012695193539147352, 0.00047033171084570835]
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.49277758624825335, 0.29754195241277814, 0.18424492241643264, 0.012270013672542832, 0.012695193539147352, 0.00047033171084570835]
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.176]
 [0.25 ]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[1.372]
 [2.088]
 [1.571]
 [1.372]
 [1.372]
 [1.372]
 [1.372]] [[0.016]
 [0.244]
 [0.22 ]
 [0.016]
 [0.016]
 [0.016]
 [0.016]]
using another actor
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
probs:  [0.49277758624825335, 0.29754195241277814, 0.18424492241643264, 0.012270013672542832, 0.012695193539147352, 0.00047033171084570835]
siam score:  -0.75868547
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  86 total reward:  0.22499999999999942  reward:  1.0 rdn_beta:  0.167
using another actor
4292 5907
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.98516 -0.8600000000000001 -0.8600000000000001
actor:  0 policy actor:  1  step number:  94 total reward:  0.0849999999999993  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  75 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.98039 -0.8600000000000001 -0.8600000000000001
probs:  [0.4983329096314911, 0.2942831418379326, 0.18222699083850405, 0.012135627076013041, 0.012556150185362772, 0.0004651804306966057]
using another actor
first move QE:  0.2798315545479226
using explorer policy with actor:  1
from probs:  [0.4983329096314911, 0.2942831418379326, 0.18222699083850405, 0.012135627076013041, 0.012556150185362772, 0.0004651804306966057]
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.824]
 [0.82 ]
 [0.85 ]
 [0.85 ]
 [0.845]
 [0.839]] [[2.653]
 [1.057]
 [1.802]
 [2.653]
 [2.653]
 [2.246]
 [1.098]] [[0.85 ]
 [0.824]
 [0.82 ]
 [0.85 ]
 [0.85 ]
 [0.845]
 [0.839]]
actor:  0 policy actor:  1  step number:  73 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  10935
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.832]
 [0.832]
 [0.864]
 [0.878]
 [0.832]
 [0.832]] [[3.342]
 [3.342]
 [3.342]
 [4.493]
 [4.094]
 [3.342]
 [3.342]] [[0.832]
 [0.832]
 [0.832]
 [0.864]
 [0.878]
 [0.832]
 [0.832]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.307]
 [0.284]
 [0.279]
 [0.281]
 [0.278]
 [0.274]] [[-2.135]
 [-2.388]
 [-2.204]
 [-2.284]
 [-2.283]
 [-2.257]
 [-2.235]] [[ 0.235]
 [-0.211]
 [-0.047]
 [-0.142]
 [-0.139]
 [-0.115]
 [-0.097]]
first move QE:  0.2791265726074174
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.49833290963149107, 0.2942831418379325, 0.18222699083850405, 0.012135627076013041, 0.012556150185362772, 0.0004651804306966057]
siam score:  -0.75777686
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.591]
 [0.464]
 [0.627]
 [0.4  ]
 [0.402]
 [0.506]] [[ 1.439]
 [ 0.631]
 [ 0.43 ]
 [-0.641]
 [ 1.328]
 [ 1.087]
 [ 0.515]] [[0.479]
 [0.591]
 [0.464]
 [0.627]
 [0.4  ]
 [0.402]
 [0.506]]
actor:  1 policy actor:  1  step number:  84 total reward:  0.2149999999999994  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]] [[0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]]
siam score:  -0.7508616
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.631]
 [0.537]
 [0.667]
 [0.923]
 [0.663]
 [0.665]] [[0.234]
 [0.263]
 [1.088]
 [0.103]
 [1.235]
 [0.352]
 [0.417]] [[0.605]
 [0.532]
 [0.87 ]
 [0.499]
 [1.684]
 [0.648]
 [0.692]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.5036692016259087, 0.29115281735775145, 0.1802886208496256, 0.012006538979830783, 0.012422588935280301, 0.0004602322516034495]
4304 5925
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.5036692016259087, 0.29115281735775145, 0.1802886208496256, 0.012006538979830783, 0.012422588935280301, 0.0004602322516034495]
actor:  1 policy actor:  1  step number:  62 total reward:  0.5449999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.22 ]
 [0.425]
 [0.269]
 [0.269]
 [0.172]
 [0.269]] [[2.095]
 [1.755]
 [1.766]
 [2.095]
 [2.095]
 [1.828]
 [2.095]] [[ 0.18 ]
 [-0.032]
 [ 0.381]
 [ 0.18 ]
 [ 0.18 ]
 [-0.104]
 [ 0.18 ]]
from probs:  [0.49468075368740133, 0.30380285825159514, 0.1770712018031941, 0.01179227106312377, 0.012200896218025024, 0.00045201897666067843]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[-0.119]
 [-0.203]
 [-0.203]
 [-0.203]
 [-0.203]
 [-0.203]
 [-0.203]] [[0.441]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]] [[ 0.461]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]] [[1.064]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]]
siam score:  -0.7491423
first move QE:  0.2788642214935241
actor:  1 policy actor:  1  step number:  69 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.003]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.1  ]] [[ 1.349]
 [-0.085]
 [ 0.13 ]
 [ 0.13 ]
 [ 0.13 ]
 [ 0.13 ]
 [-0.063]] [[ 0.732]
 [-0.248]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.161]]
siam score:  -0.7556463
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.559]
 [0.498]
 [0.491]
 [0.492]
 [0.492]
 [0.492]] [[0.216]
 [0.184]
 [0.262]
 [0.153]
 [0.141]
 [0.108]
 [0.061]] [[ 0.047]
 [ 0.139]
 [ 0.094]
 [-0.029]
 [-0.04 ]
 [-0.073]
 [-0.118]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
siam score:  -0.75164104
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.5005487451684625, 0.3002749645543906, 0.1750149723773321, 0.01165533397504388, 0.012059213993191674, 0.00044676993157941915]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]] [[0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]] [[1.892]
 [1.892]
 [1.892]
 [1.892]
 [1.892]
 [1.892]
 [1.892]]
in main func line 156:  4313
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.5005487451684625, 0.3002749645543906, 0.1750149723773321, 0.01165533397504388, 0.012059213993191674, 0.00044676993157941915]
siam score:  -0.7554236
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.592]
 [0.538]
 [0.526]
 [0.534]
 [0.551]
 [0.537]] [[3.239]
 [2.899]
 [2.668]
 [2.821]
 [2.673]
 [3.684]
 [2.842]] [[0.788]
 [0.619]
 [0.383]
 [0.473]
 [0.381]
 [1.105]
 [0.503]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.5005487451684625, 0.3002749645543906, 0.1750149723773321, 0.01165533397504388, 0.012059213993191674, 0.00044676993157941915]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.5005487451684625, 0.3002749645543906, 0.1750149723773321, 0.01165533397504388, 0.012059213993191674, 0.00044676993157941915]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.257]
 [0.123]
 [0.144]
 [0.123]
 [0.123]
 [0.123]] [[3.974]
 [3.707]
 [3.974]
 [3.735]
 [3.974]
 [3.974]
 [3.974]] [[0.146]
 [0.231]
 [0.146]
 [0.039]
 [0.146]
 [0.146]
 [0.146]]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.371]
 [0.348]
 [0.358]
 [0.358]
 [0.351]
 [0.348]] [[2.634]
 [2.602]
 [2.761]
 [2.845]
 [2.845]
 [2.69 ]
 [2.53 ]] [[-0.126]
 [-0.114]
 [-0.053]
 [ 0.022]
 [ 0.022]
 [-0.094]
 [-0.207]]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.738]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[0.264]
 [0.314]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[0.256]
 [0.515]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]]
actor:  1 policy actor:  1  step number:  127 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.49593830985086307, 0.3067199652604482, 0.17340295113555357, 0.011547979468783207, 0.011948139444242422, 0.00044265484010975217]
actor:  1 policy actor:  1  step number:  80 total reward:  0.4049999999999996  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  10935
UNIT TEST: sample policy line 217 mcts : [0.041 0.735 0.    0.041 0.163 0.    0.02 ]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.511]
 [0.492]
 [0.489]
 [0.513]
 [0.513]
 [0.513]] [[3.204]
 [3.284]
 [4.165]
 [2.863]
 [3.204]
 [3.204]
 [3.204]] [[ 0.18 ]
 [ 0.227]
 [ 0.777]
 [-0.096]
 [ 0.18 ]
 [ 0.18 ]
 [ 0.18 ]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.493]
 [0.479]
 [0.479]
 [0.48 ]
 [0.479]
 [0.478]] [[2.947]
 [3.272]
 [2.799]
 [3.739]
 [2.882]
 [2.875]
 [2.783]] [[ 0.066]
 [ 0.309]
 [-0.035]
 [ 0.591]
 [ 0.023]
 [ 0.016]
 [-0.047]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
actor:  1 policy actor:  1  step number:  59 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7738571
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[7.531]
 [1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]] [[1.733]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.116]
 [0.111]
 [0.161]
 [0.126]
 [0.177]
 [0.188]] [[0.83 ]
 [0.365]
 [0.53 ]
 [0.598]
 [0.642]
 [0.452]
 [0.331]] [[-0.048]
 [-0.664]
 [-0.453]
 [-0.263]
 [-0.275]
 [-0.427]
 [-0.566]]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.68 ]
 [0.704]
 [0.708]
 [0.708]
 [0.708]
 [0.696]] [[ 0.213]
 [ 0.793]
 [ 0.145]
 [-0.089]
 [-0.032]
 [-0.083]
 [ 0.396]] [[0.702]
 [0.68 ]
 [0.704]
 [0.708]
 [0.708]
 [0.708]
 [0.696]]
siam score:  -0.7831195
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.56 ]
 [0.508]] [[0.921]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.986]
 [0.   ]] [[0.432]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [1.014]
 [0.253]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.134]
 [0.496]
 [0.492]
 [0.494]
 [0.43 ]
 [0.495]] [[-0.359]
 [ 0.395]
 [-0.035]
 [-0.068]
 [-0.028]
 [ 0.027]
 [-0.087]] [[0.857]
 [1.158]
 [1.161]
 [1.126]
 [1.165]
 [1.145]
 [1.112]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.49621579357528334, 0.3119429279361273, 0.16856974295883292, 0.011226106118718396, 0.011615112555828234, 0.0004303168552099971]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.49621579357528334, 0.3119429279361273, 0.16856974295883292, 0.011226106118718396, 0.011615112555828234, 0.0004303168552099971]
line 256 mcts: sample exp_bonus -1.7455846716972865
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.36 ]
 [0.289]
 [0.289]
 [0.289]
 [0.315]
 [0.314]] [[2.072]
 [2.299]
 [2.168]
 [2.168]
 [2.168]
 [2.652]
 [2.639]] [[-0.539]
 [-0.333]
 [-0.519]
 [-0.519]
 [-0.519]
 [-0.306]
 [-0.314]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.49621579357528334, 0.3119429279361273, 0.16856974295883292, 0.011226106118718396, 0.011615112555828234, 0.0004303168552099971]
using explorer policy with actor:  1
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.49621579357528334, 0.3119429279361272, 0.16856974295883287, 0.011226106118718394, 0.011615112555828231, 0.000430316855209997]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.508]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[-1.216]
 [-0.633]
 [-0.248]
 [-0.248]
 [-0.248]
 [-0.248]
 [-0.248]] [[0.653]
 [0.508]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.49621579357528334, 0.3119429279361272, 0.16856974295883287, 0.011226106118718394, 0.011615112555828231, 0.000430316855209997]
line 256 mcts: sample exp_bonus -0.5484724660548838
from probs:  [0.4962157935752834, 0.3119429279361272, 0.16856974295883292, 0.011226106118718396, 0.011615112555828234, 0.0004303168552099971]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.141]
 [0.357]
 [0.357]
 [0.388]
 [0.357]
 [0.388]] [[0.841]
 [1.181]
 [0.91 ]
 [0.91 ]
 [0.95 ]
 [0.91 ]
 [0.978]] [[0.217]
 [0.132]
 [0.293]
 [0.293]
 [0.396]
 [0.293]
 [0.424]]
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.612]
 [0.607]
 [0.622]
 [0.604]
 [0.588]
 [0.609]] [[-0.855]
 [-0.276]
 [ 0.197]
 [-0.855]
 [ 0.528]
 [ 0.107]
 [ 0.361]] [[0.622]
 [0.612]
 [0.607]
 [0.622]
 [0.604]
 [0.588]
 [0.609]]
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.636]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[-0.693]
 [-0.653]
 [-0.784]
 [-0.784]
 [-0.784]
 [-0.784]
 [-0.784]] [[0.688]
 [0.636]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.431]
 [0.536]
 [0.518]
 [0.355]
 [0.518]
 [0.537]] [[1.383]
 [1.395]
 [0.859]
 [1.527]
 [2.281]
 [1.527]
 [1.359]] [[1.038]
 [0.89 ]
 [0.641]
 [1.123]
 [1.45 ]
 [1.123]
 [1.025]]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.484]
 [0.478]
 [0.476]
 [0.473]
 [0.471]
 [0.473]] [[ 0.148]
 [-0.079]
 [-0.021]
 [-0.067]
 [-0.083]
 [-0.238]
 [-0.115]] [[ 0.239]
 [ 0.034]
 [ 0.08 ]
 [ 0.03 ]
 [ 0.008]
 [-0.152]
 [-0.025]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
start point for exploration sampling:  10935
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.4962157935752834, 0.3119429279361272, 0.16856974295883292, 0.011226106118718396, 0.011615112555828234, 0.0004303168552099971]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5449999999999997  reward:  1.0 rdn_beta:  0.333
from probs:  [0.4962157935752834, 0.3119429279361272, 0.16856974295883292, 0.011226106118718396, 0.011615112555828234, 0.0004303168552099971]
UNIT TEST: sample policy line 217 mcts : [0.02  0.837 0.041 0.061 0.    0.    0.041]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.48792635198475726, 0.3234371386881204, 0.16575373215006922, 0.011038570469581413, 0.01142107843126884, 0.00042312827620290635]
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]] [[-0.636]
 [-0.636]
 [-0.636]
 [-0.636]
 [-0.636]
 [-0.636]
 [-0.636]]
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[1.034]
 [1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]] [[-0.05 ]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.788]
 [0.676]
 [0.728]
 [0.594]
 [0.776]
 [0.651]] [[0.773]
 [1.284]
 [1.236]
 [1.296]
 [0.722]
 [1.301]
 [0.953]] [[0.602]
 [0.788]
 [0.676]
 [0.728]
 [0.594]
 [0.776]
 [0.651]]
in main func line 156:  4340
from probs:  [0.48792635198475726, 0.3234371386881204, 0.16575373215006922, 0.011038570469581413, 0.01142107843126884, 0.00042312827620290635]
actor:  1 policy actor:  1  step number:  65 total reward:  0.3299999999999995  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 2.272415079384484
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]] [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[4.317]
 [2.628]
 [2.628]
 [2.628]
 [2.628]
 [2.628]
 [2.628]] [[1.614]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]] [[1.107]
 [1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]] [[-0.224]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.77772444
siam score:  -0.77800554
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.472]
 [0.474]
 [0.475]
 [0.477]
 [0.476]
 [0.475]] [[0.806]
 [0.562]
 [0.787]
 [0.85 ]
 [0.853]
 [0.728]
 [0.602]] [[0.31 ]
 [0.211]
 [0.292]
 [0.315]
 [0.319]
 [0.276]
 [0.231]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.49399029089070146, 0.3196070196095075, 0.1637908885062184, 0.010907852520716162, 0.01128583085093417, 0.00041811762192257623]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.266]
 [0.496]
 [0.472]
 [0.237]
 [0.491]
 [0.408]] [[7.082]
 [1.523]
 [0.814]
 [1.236]
 [1.938]
 [0.635]
 [1.386]] [[ 1.589]
 [ 0.088]
 [ 0.027]
 [ 0.119]
 [ 0.175]
 [-0.019]
 [ 0.124]]
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.255]
 [0.275]
 [0.255]
 [0.255]
 [0.255]
 [0.255]] [[2.629]
 [2.629]
 [3.342]
 [2.629]
 [2.629]
 [2.629]
 [2.629]] [[0.073]
 [0.073]
 [0.349]
 [0.073]
 [0.073]
 [0.073]
 [0.073]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.49399029089070146, 0.3196070196095075, 0.1637908885062184, 0.010907852520716162, 0.01128583085093417, 0.00041811762192257623]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.145]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[2.324]
 [2.944]
 [2.655]
 [2.655]
 [2.655]
 [2.655]
 [2.655]] [[-0.572]
 [-0.59 ]
 [-0.638]
 [-0.638]
 [-0.638]
 [-0.638]
 [-0.638]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.49399029089070146, 0.3196070196095075, 0.1637908885062184, 0.010907852520716162, 0.01128583085093417, 0.00041811762192257623]
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[4.077]
 [1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]] [[1.244]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]]
line 256 mcts: sample exp_bonus 3.0295528388123367
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.49399029089070146, 0.3196070196095075, 0.1637908885062184, 0.010907852520716162, 0.01128583085093417, 0.00041811762192257623]
in main func line 156:  4349
4349 5965
using explorer policy with actor:  1
siam score:  -0.77512217
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.294]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[3.49 ]
 [3.5  ]
 [3.866]
 [3.866]
 [3.866]
 [3.866]
 [3.866]] [[0.783]
 [0.765]
 [1.051]
 [1.051]
 [1.051]
 [1.051]
 [1.051]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.4939902908907014, 0.31960701960950744, 0.16379088850621837, 0.010907852520716159, 0.011285830850934167, 0.0004181176219225761]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
actor:  1 policy actor:  1  step number:  81 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  0.167
from probs:  [0.4939902908907014, 0.31960701960950744, 0.16379088850621837, 0.010907852520716159, 0.011285830850934167, 0.0004181176219225761]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.463]
 [0.412]
 [0.395]
 [0.401]
 [0.4  ]
 [0.4  ]] [[0.392]
 [0.377]
 [0.803]
 [0.867]
 [0.927]
 [0.385]
 [0.583]] [[0.504]
 [0.528]
 [0.71 ]
 [0.718]
 [0.771]
 [0.408]
 [0.539]]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.71 ]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[0.953]
 [0.747]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]] [[0.942]
 [1.146]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]]
line 256 mcts: sample exp_bonus 4.245182463990804
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [-0.045]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]] [[1.118]
 [1.319]
 [1.118]
 [1.118]
 [1.118]
 [1.118]
 [1.118]] [[-1.324]
 [-1.111]
 [-1.324]
 [-1.324]
 [-1.324]
 [-1.324]
 [-1.324]]
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.036]
 [-0.036]
 [-0.037]] [[0.26 ]
 [0.337]
 [0.337]
 [0.337]
 [0.257]
 [0.266]
 [0.259]] [[1.412]
 [1.471]
 [1.471]
 [1.471]
 [1.41 ]
 [1.416]
 [1.411]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.023]
 [-0.026]
 [-0.028]
 [-0.027]
 [-0.026]
 [-0.027]] [[-0.029]
 [-0.103]
 [ 0.078]
 [ 0.127]
 [ 0.09 ]
 [-0.085]
 [ 0.07 ]] [[-0.673]
 [-0.692]
 [-0.637]
 [-0.624]
 [-0.635]
 [-0.692]
 [-0.641]]
siam score:  -0.7733704
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.4981866311394401, 0.3169565175024875, 0.16243257010749604, 0.01081739366244532, 0.011192237417077741, 0.0004146501710535621]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
probs:  [0.4981866311394401, 0.3169565175024875, 0.16243257010749604, 0.01081739366244532, 0.011192237417077741, 0.0004146501710535621]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.519]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[-2.666]
 [-2.833]
 [-2.662]
 [-2.662]
 [-2.662]
 [-2.818]
 [-2.662]] [[0.512]
 [0.519]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.44316717131612
siam score:  -0.77257
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]] [[-0.783]
 [-2.041]
 [-2.041]
 [-2.041]
 [-2.041]
 [-2.041]
 [-2.041]] [[0.774]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.512]
 [0.5  ]
 [0.497]
 [0.496]
 [0.496]
 [0.497]] [[-2.73 ]
 [-2.88 ]
 [-2.84 ]
 [-2.837]
 [-2.802]
 [-2.943]
 [-2.85 ]] [[0.515]
 [0.512]
 [0.5  ]
 [0.497]
 [0.496]
 [0.496]
 [0.497]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.099]
 [-0.104]
 [-0.098]
 [-0.099]
 [-0.097]
 [-0.097]] [[0.955]
 [1.264]
 [1.151]
 [1.063]
 [1.249]
 [0.737]
 [0.863]] [[-1.009]
 [-0.911]
 [-0.96 ]
 [-0.977]
 [-0.917]
 [-1.083]
 [-1.041]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.014]
 [-0.004]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[2.356]
 [4.   ]
 [4.617]
 [2.356]
 [2.356]
 [2.356]
 [2.356]] [[-0.444]
 [ 0.118]
 [ 0.344]
 [-0.444]
 [-0.444]
 [-0.444]
 [-0.444]]
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]] [[0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]] [[0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.495]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[-2.862]
 [-3.198]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.493]
 [0.495]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]]
maxi score, test score, baseline:  -0.9775900000000001 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.484]
 [0.483]
 [0.485]
 [0.482]
 [0.482]
 [0.481]] [[-2.792]
 [-3.289]
 [-2.916]
 [-2.871]
 [-2.783]
 [-2.826]
 [-2.707]] [[0.574]
 [0.484]
 [0.483]
 [0.485]
 [0.482]
 [0.482]
 [0.481]]
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[-1.506]
 [-2.152]
 [-2.152]
 [-2.152]
 [-2.152]
 [-2.152]
 [-2.152]] [[0.754]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]]
line 256 mcts: sample exp_bonus 2.127308053201691
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.646]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]] [[ 0.963]
 [-0.978]
 [-0.86 ]
 [-0.86 ]
 [-0.86 ]
 [-0.86 ]
 [-0.86 ]] [[0.789]
 [0.646]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]]
Printing some Q and Qe and total Qs values:  [[ 0.43 ]
 [-0.035]
 [ 0.43 ]
 [ 0.43 ]
 [-0.033]
 [ 0.43 ]
 [ 0.43 ]] [[0.148]
 [1.494]
 [0.148]
 [0.148]
 [0.576]
 [0.148]
 [0.148]] [[0.223]
 [1.767]
 [0.223]
 [0.223]
 [0.16 ]
 [0.223]
 [0.223]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[0.301]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[0.794]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.493]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[-3.26 ]
 [-3.168]
 [-3.26 ]
 [-3.26 ]
 [-3.26 ]
 [-3.26 ]
 [-3.26 ]] [[0.471]
 [0.493]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]] [[ 1.28 ]
 [-1.084]
 [-1.084]
 [-1.084]
 [-1.084]
 [-1.084]
 [-1.084]] [[0.787]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]]
actor:  0 policy actor:  1  step number:  56 total reward:  0.5549999999999997  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]] [[-1.61 ]
 [-2.419]
 [-2.419]
 [-2.419]
 [-2.419]
 [-2.419]
 [-2.419]] [[0.769]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[-2.226]
 [-2.363]
 [-2.363]
 [-2.363]
 [-2.363]
 [-2.363]
 [-2.363]] [[0.787]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]]
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.491]
 [0.472]
 [0.465]
 [0.463]
 [0.461]
 [0.46 ]] [[-2.621]
 [-3.302]
 [-3.074]
 [-2.991]
 [-2.998]
 [-3.017]
 [-2.824]] [[0.712]
 [0.491]
 [0.472]
 [0.465]
 [0.463]
 [0.461]
 [0.46 ]]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[-1.02 ]
 [-1.363]
 [-1.363]
 [-1.363]
 [-1.363]
 [-1.363]
 [-1.363]] [[0.745]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[-0.93 ]
 [-1.742]
 [-1.742]
 [-1.742]
 [-1.742]
 [-1.742]
 [-1.742]] [[0.788]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]]
maxi score, test score, baseline:  -0.97448 -0.8600000000000001 -0.8600000000000001
probs:  [0.4981866311394401, 0.3169565175024875, 0.16243257010749604, 0.01081739366244532, 0.011192237417077741, 0.0004146501710535621]
maxi score, test score, baseline:  -0.97448 -0.8600000000000001 -0.8600000000000001
actor:  0 policy actor:  1  step number:  68 total reward:  0.47499999999999964  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]] [[-0.99 ]
 [-2.137]
 [-2.137]
 [-2.137]
 [-2.137]
 [-2.137]
 [-2.137]] [[0.839]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]]
maxi score, test score, baseline:  -0.97153 -0.8600000000000001 -0.8600000000000001
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[-0.431]
 [-1.934]
 [-1.934]
 [-1.934]
 [-1.934]
 [-1.934]
 [-1.934]] [[0.844]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]]
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[-1.012]
 [-1.273]
 [-1.273]
 [-1.273]
 [-1.273]
 [-1.273]
 [-1.273]] [[0.724]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97153 -0.8600000000000001 -0.8600000000000001
rdn probs:  [0.4981866311394401, 0.3169565175024875, 0.16243257010749604, 0.01081739366244532, 0.011192237417077741, 0.0004146501710535621]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.395]
 [0.445]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[2.894]
 [2.894]
 [5.102]
 [2.894]
 [2.894]
 [2.894]
 [2.894]] [[0.311]
 [0.311]
 [1.715]
 [0.311]
 [0.311]
 [0.311]
 [0.311]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.49821149481689997, 0.317047426309788, 0.16232913792283907, 0.010805691529737328, 0.011183746839461517, 0.00042250258127423946]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.49821149481689997, 0.317047426309788, 0.16232913792283907, 0.010805691529737328, 0.011183746839461517, 0.00042250258127423946]
Printing some Q and Qe and total Qs values:  [[-0.051]
 [-0.039]
 [-0.113]
 [-0.052]
 [-0.052]
 [-0.105]
 [-0.052]] [[1.684]
 [3.263]
 [1.916]
 [2.555]
 [2.555]
 [2.094]
 [2.555]] [[-0.933]
 [-0.381]
 [-0.98 ]
 [-0.643]
 [-0.643]
 [-0.903]
 [-0.643]]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.026]
 [-0.049]
 [-0.052]
 [-0.052]
 [-0.022]
 [-0.034]] [[2.737]
 [2.596]
 [3.15 ]
 [2.737]
 [2.737]
 [3.278]
 [3.13 ]] [[-0.995]
 [-0.991]
 [-0.852]
 [-0.995]
 [-0.995]
 [-0.754]
 [-0.828]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[3.157]
 [3.157]
 [3.157]
 [3.157]
 [3.157]
 [3.157]
 [3.157]] [[0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.49821149481689997, 0.317047426309788, 0.16232913792283907, 0.010805691529737328, 0.011183746839461517, 0.00042250258127423946]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.49821149481689997, 0.317047426309788, 0.16232913792283907, 0.010805691529737328, 0.011183746839461517, 0.00042250258127423946]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.49821149481689997, 0.317047426309788, 0.16232913792283907, 0.010805691529737328, 0.011183746839461517, 0.00042250258127423946]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.49821149481689997, 0.317047426309788, 0.16232913792283907, 0.010805691529737328, 0.011183746839461517, 0.00042250258127423946]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.49821149481689997, 0.317047426309788, 0.16232913792283907, 0.010805691529737328, 0.011183746839461517, 0.00042250258127423946]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]] [[0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.2949999999999995  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
actor:  1 policy actor:  1  step number:  64 total reward:  0.3949999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]] [[0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]] [[0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]]
first move QE:  0.26879643793915686
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.48647980501278554, 0.32234028767535544, 0.16929571399843257, 0.010551243323574573, 0.010920396334438491, 0.0004125536554134086]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.48647980501278554, 0.32234028767535544, 0.16929571399843257, 0.010551243323574573, 0.010920396334438491, 0.0004125536554134086]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.48647980501278554, 0.32234028767535544, 0.16929571399843257, 0.010551243323574573, 0.010920396334438491, 0.0004125536554134086]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.48647980501278554, 0.32234028767535544, 0.16929571399843257, 0.010551243323574573, 0.010920396334438491, 0.0004125536554134086]
Printing some Q and Qe and total Qs values:  [[-0.1  ]
 [-0.093]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.097]
 [-0.1  ]] [[-0.384]
 [-0.357]
 [-0.441]
 [ 0.162]
 [ 0.088]
 [-0.372]
 [-0.553]] [[-0.88 ]
 [-0.84 ]
 [-0.939]
 [-0.336]
 [-0.409]
 [-0.863]
 [-1.05 ]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.528]
 [0.605]
 [0.603]
 [0.586]
 [0.585]
 [0.577]] [[1.402]
 [1.737]
 [1.573]
 [1.174]
 [1.596]
 [1.344]
 [1.89 ]] [[0.622]
 [0.528]
 [0.605]
 [0.603]
 [0.586]
 [0.585]
 [0.577]]
siam score:  -0.7771788
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.994]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.79 ]] [[0.211]
 [0.56 ]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[0.924]
 [1.564]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.4864798050127856, 0.3223402876753554, 0.16929571399843257, 0.010551243323574575, 0.010920396334438491, 0.0004125536554134087]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
line 256 mcts: sample exp_bonus 0.266556065340775
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
actor:  1 policy actor:  1  step number:  71 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.041]
 [1.038]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]] [[0.678]
 [0.753]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]] [[2.866]
 [2.879]
 [2.866]
 [2.866]
 [2.866]
 [2.866]
 [2.866]]
line 256 mcts: sample exp_bonus 2.630495859605594
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.571]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[2.235]
 [2.235]
 [2.512]
 [2.235]
 [2.235]
 [2.235]
 [2.235]] [[0.426]
 [0.426]
 [0.515]
 [0.426]
 [0.426]
 [0.426]
 [0.426]]
Printing some Q and Qe and total Qs values:  [[0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]] [[2.774]
 [2.774]
 [2.774]
 [2.774]
 [2.774]
 [2.774]
 [2.774]] [[1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.321]]
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.82 ]
 [0.638]
 [0.636]
 [0.635]
 [0.637]
 [0.634]] [[0.426]
 [2.26 ]
 [0.221]
 [0.225]
 [0.305]
 [0.296]
 [0.593]] [[0.483]
 [1.77 ]
 [0.354]
 [0.353]
 [0.395]
 [0.394]
 [0.552]]
actor:  1 policy actor:  1  step number:  83 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7697836
using another actor
actor:  1 policy actor:  1  step number:  56 total reward:  0.4349999999999996  reward:  1.0 rdn_beta:  0.167
from probs:  [0.47654640576952606, 0.33617737691160066, 0.16583887591391042, 0.010335798177926465, 0.0106974134776645, 0.00040412974937196507]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]] [[2.174]
 [2.174]
 [2.174]
 [2.174]
 [2.174]
 [2.174]
 [2.174]] [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.517]
 [0.484]
 [0.411]
 [0.456]
 [0.45 ]
 [0.462]] [[-0.462]
 [-0.448]
 [-0.527]
 [ 0.284]
 [-0.478]
 [-0.363]
 [-0.253]] [[0.487]
 [0.517]
 [0.484]
 [0.411]
 [0.456]
 [0.45 ]
 [0.462]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
actor:  1 policy actor:  1  step number:  52 total reward:  0.5249999999999997  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.5895911138136447
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.4910825518967582, 0.326841834030019, 0.16123358107893193, 0.010048776225432684, 0.010400349578956627, 0.00039290718990159094]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]] [[1.552]
 [2.019]
 [2.019]
 [2.019]
 [2.019]
 [2.019]
 [2.019]] [[-0.424]
 [ 0.519]
 [ 0.519]
 [ 0.519]
 [ 0.519]
 [ 0.519]
 [ 0.519]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  75 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[ 4.215]
 [-0.222]
 [-0.222]
 [-0.222]
 [-0.222]
 [-0.222]
 [-0.222]] [[1.368]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.17499999999999938  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.542]
 [0.686]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[0.99 ]
 [0.99 ]
 [1.713]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]] [[0.51 ]
 [0.51 ]
 [1.484]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.532]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]] [[3.081]
 [2.656]
 [2.934]
 [2.934]
 [2.934]
 [2.934]
 [2.934]] [[0.424]
 [0.432]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.645]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[1.477]
 [2.041]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]] [[0.166]
 [0.859]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.35499999999999954  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.49499999999999966  reward:  1.0 rdn_beta:  0.5
from probs:  [0.49952589255460583, 0.33261226918768405, 0.1473655909801967, 0.009882059125867655, 0.010227799600992385, 0.0003863885506534751]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
from probs:  [0.4926540742957316, 0.3280366283678648, 0.15909501039043514, 0.009746114792753856, 0.010087099026520116, 0.0003810731266945254]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.484]
 [0.533]
 [0.496]
 [0.464]
 [0.534]
 [0.508]] [[-0.097]
 [ 1.002]
 [ 0.865]
 [-0.377]
 [ 0.137]
 [ 0.385]
 [ 0.1  ]] [[0.579]
 [0.484]
 [0.533]
 [0.496]
 [0.464]
 [0.534]
 [0.508]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -1.0812228467164926
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[2.114]
 [2.053]
 [2.114]
 [2.114]
 [2.114]
 [2.114]
 [2.114]] [[0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
from probs:  [0.49918928189187045, 0.3238111337755005, 0.1570456809926124, 0.009620573460500065, 0.009957165419406565, 0.00037616446011008123]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.519]
 [0.424]
 [0.482]
 [0.416]
 [0.43 ]
 [0.417]] [[-0.298]
 [-1.125]
 [-1.208]
 [-0.872]
 [-0.775]
 [-0.883]
 [-1.043]] [[0.637]
 [0.517]
 [0.271]
 [0.611]
 [0.545]
 [0.5  ]
 [0.368]]
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.569]
 [0.435]
 [0.428]
 [0.429]
 [0.432]
 [0.432]] [[-0.11 ]
 [-0.487]
 [-0.762]
 [-0.704]
 [-0.744]
 [-0.826]
 [-0.766]] [[-0.038]
 [-0.193]
 [-0.643]
 [-0.62 ]
 [-0.644]
 [-0.694]
 [-0.653]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.49918928189187045, 0.3238111337755005, 0.1570456809926124, 0.009620573460500065, 0.009957165419406565, 0.00037616446011008123]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.448]
 [0.624]
 [0.525]
 [0.528]
 [0.527]
 [0.535]] [[0.076]
 [0.315]
 [0.133]
 [0.179]
 [0.116]
 [0.146]
 [0.16 ]] [[0.096]
 [0.108]
 [0.277]
 [0.127]
 [0.07 ]
 [0.098]
 [0.128]]
Printing some Q and Qe and total Qs values:  [[0.987]
 [0.959]
 [0.959]
 [0.959]
 [0.959]
 [0.959]
 [0.959]] [[0.492]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]] [[1.317]
 [1.425]
 [1.425]
 [1.425]
 [1.425]
 [1.425]
 [1.425]]
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.6  ]
 [0.551]
 [0.614]
 [0.619]
 [0.565]
 [0.627]] [[1.777]
 [1.623]
 [1.611]
 [1.762]
 [1.733]
 [1.532]
 [1.694]] [[ 0.126]
 [ 0.056]
 [-0.051]
 [ 0.176]
 [ 0.167]
 [-0.076]
 [ 0.156]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
actor:  1 policy actor:  1  step number:  69 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.4937640224842108, 0.3311600495894299, 0.1553388864175998, 0.009516015713434967, 0.009848949543535738, 0.0003720762517889709]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.4937640224842108, 0.3311600495894299, 0.1553388864175998, 0.009516015713434967, 0.009848949543535738, 0.0003720762517889709]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.4937640224842108, 0.3311600495894299, 0.1553388864175998, 0.009516015713434967, 0.009848949543535738, 0.0003720762517889709]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.838]
 [0.926]
 [0.838]
 [0.838]
 [0.838]
 [0.838]
 [0.838]] [[2.529]
 [2.864]
 [2.529]
 [2.529]
 [2.529]
 [2.529]
 [2.529]] [[0.85 ]
 [1.362]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.4937640224842108, 0.3311600495894299, 0.1553388864175998, 0.009516015713434967, 0.009848949543535738, 0.0003720762517889709]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]] [[0.74]
 [0.74]
 [0.74]
 [0.74]
 [0.74]
 [0.74]
 [0.74]] [[1.077]
 [1.077]
 [1.077]
 [1.077]
 [1.077]
 [1.077]
 [1.077]]
4396 6007
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.4937640224842108, 0.3311600495894299, 0.1553388864175998, 0.009516015713434967, 0.009848949543535738, 0.0003720762517889709]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
using explorer policy with actor:  1
siam score:  -0.8155199
actor:  1 policy actor:  1  step number:  70 total reward:  0.4249999999999996  reward:  1.0 rdn_beta:  0.167
from probs:  [0.4937640224842108, 0.3311600495894299, 0.1553388864175998, 0.009516015713434967, 0.009848949543535738, 0.0003720762517889709]
actor:  1 policy actor:  1  step number:  71 total reward:  0.4299999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.49386520583720755, 0.3352009885625474, 0.15166372286333482, 0.009290876246181348, 0.009615933192994319, 0.0003632732977346575]
actor:  1 policy actor:  1  step number:  76 total reward:  0.23499999999999943  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
siam score:  -0.815481
using explorer policy with actor:  1
start point for exploration sampling:  10935
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]] [[-0.301]
 [-0.301]
 [-0.301]
 [-0.301]
 [-0.301]
 [-0.301]
 [-0.301]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0000],
        [-0.1954],
        [-0.1205],
        [-0.0000],
        [-0.7485],
        [-0.0556],
        [-0.0000],
        [-0.3606],
        [-0.0000]], dtype=torch.float64)
-0.5147999999999997 -0.5147999999999997
-0.9752489999999999 -0.9752489999999999
-0.0727797758985 -0.2681656389520194
-0.0727797758985 -0.19327348494587204
-0.1924122568499994 -0.1924122568499994
-0.024259925299500003 -0.7727333120225032
-0.0339629152995 -0.08959206425245134
-0.9602999999999999 -0.9602999999999999
-0.024259925299500003 -0.38484012234576165
-0.97044651 -0.97044651
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
using explorer policy with actor:  1
start point for exploration sampling:  10935
line 256 mcts: sample exp_bonus 2.8895456210133545
actor:  1 policy actor:  1  step number:  85 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.167
using another actor
first move QE:  0.2553628157706631
actor:  1 policy actor:  1  step number:  87 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.167
siam score:  -0.82473874
line 256 mcts: sample exp_bonus 0.2556906917499983
line 256 mcts: sample exp_bonus 5.8393508096967945
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.5055367430717121, 0.32747120814798025, 0.14816633677382277, 0.009076627375526544, 0.00939418846490822, 0.00035489616605015296]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.575]
 [0.762]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[1.2  ]
 [1.343]
 [1.137]
 [1.131]
 [1.131]
 [1.131]
 [1.131]] [[1.619]
 [1.301]
 [1.605]
 [1.583]
 [1.583]
 [1.583]
 [1.583]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
line 256 mcts: sample exp_bonus 0.38506649510541957
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]] [[3.139]
 [3.139]
 [3.139]
 [3.139]
 [3.139]
 [3.139]
 [3.139]] [[-0.8]
 [-0.8]
 [-0.8]
 [-0.8]
 [-0.8]
 [-0.8]
 [-0.8]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]] [[0.339]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[0.697]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.299]
 [0.613]
 [0.611]
 [0.595]
 [0.573]
 [0.632]] [[2.787]
 [2.227]
 [3.095]
 [3.066]
 [3.074]
 [2.778]
 [3.031]] [[1.292]
 [0.644]
 [1.435]
 [1.419]
 [1.402]
 [1.244]
 [1.431]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
probs:  [0.5055367430717121, 0.32747120814798025, 0.14816633677382277, 0.009076627375526544, 0.00939418846490822, 0.00035489616605015296]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.469]
 [0.469]
 [0.475]
 [0.426]
 [0.469]
 [0.459]] [[3.394]
 [3.299]
 [3.299]
 [3.384]
 [3.294]
 [3.299]
 [3.411]] [[0.618]
 [0.765]
 [0.765]
 [0.833]
 [0.675]
 [0.765]
 [0.821]]
maxi score, test score, baseline:  -0.97153 -0.8484999999999999 -0.8484999999999999
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  64 total reward:  0.4049999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.96872 -0.8484999999999999 -0.8484999999999999
probs:  [0.5055367430717121, 0.32747120814798025, 0.14816633677382277, 0.009076627375526544, 0.00939418846490822, 0.00035489616605015296]
maxi score, test score, baseline:  -0.96872 -0.8484999999999999 -0.8484999999999999
probs:  [0.5055367430717121, 0.32747120814798025, 0.14816633677382277, 0.009076627375526544, 0.00939418846490822, 0.00035489616605015296]
using another actor
from probs:  [0.5055367430717121, 0.32747120814798025, 0.14816633677382277, 0.009076627375526544, 0.00939418846490822, 0.00035489616605015296]
Printing some Q and Qe and total Qs values:  [[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]] [[0.069]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]] [[0.69 ]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]]
from probs:  [0.5055367430717121, 0.32747120814798025, 0.14816633677382277, 0.009076627375526544, 0.00939418846490822, 0.00035489616605015296]
maxi score, test score, baseline:  -0.96872 -0.8484999999999999 -0.8484999999999999
siam score:  -0.81374305
maxi score, test score, baseline:  -0.96872 -0.8484999999999999 -0.8484999999999999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.313]
 [0.258]
 [0.21 ]
 [0.167]
 [0.258]
 [0.258]] [[3.451]
 [3.132]
 [3.737]
 [2.724]
 [2.597]
 [3.737]
 [3.737]] [[-0.006]
 [-0.156]
 [ 0.136]
 [-0.634]
 [-0.805]
 [ 0.136]
 [ 0.136]]
Printing some Q and Qe and total Qs values:  [[0.99 ]
 [0.994]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]] [[3.967]
 [3.919]
 [3.967]
 [3.967]
 [3.967]
 [3.967]
 [3.967]] [[0.99 ]
 [0.994]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96872 -0.8484999999999999 -0.8484999999999999
actor:  0 policy actor:  1  step number:  72 total reward:  0.37499999999999956  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
line 256 mcts: sample exp_bonus 1.450598912174885
actor:  1 policy actor:  1  step number:  73 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.167
siam score:  -0.81680435
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
actor:  1 policy actor:  1  step number:  82 total reward:  0.09499999999999931  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
line 256 mcts: sample exp_bonus 2.800120803087759
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.038]
 [-0.   ]
 [ 0.108]
 [ 0.015]
 [-0.009]
 [ 0.058]] [[2.178]
 [2.487]
 [1.933]
 [2.85 ]
 [2.966]
 [2.726]
 [2.452]] [[0.243]
 [0.391]
 [0.105]
 [0.726]
 [0.713]
 [0.553]
 [0.454]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.5066385057410772, 0.328843831846158, 0.1459708983291803, 0.008942135444890881, 0.00925499110765993, 0.0003496375310338057]
siam score:  -0.81427675
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]] [[2.837]
 [2.837]
 [2.837]
 [2.837]
 [2.837]
 [2.837]
 [2.837]] [[0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
using explorer policy with actor:  1
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.5066385057410772, 0.32884383184615806, 0.14597089832918034, 0.008942135444890883, 0.009254991107659932, 0.0003496375310338058]
Printing some Q and Qe and total Qs values:  [[1.153]
 [1.153]
 [1.153]
 [1.153]
 [1.153]
 [1.153]
 [1.153]] [[0.252]
 [0.249]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[1.891]
 [1.889]
 [1.891]
 [1.891]
 [1.891]
 [1.891]
 [1.891]]
from probs:  [0.5066385057410772, 0.32884383184615806, 0.14597089832918034, 0.008942135444890883, 0.009254991107659932, 0.0003496375310338058]
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.65 ]
 [0.676]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[0.914]
 [1.375]
 [0.984]
 [1.193]
 [1.193]
 [1.193]
 [1.193]] [[-0.141]
 [ 0.203]
 [-0.067]
 [ 0.035]
 [ 0.035]
 [ 0.035]
 [ 0.035]]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.706]
 [0.737]
 [0.713]
 [0.692]
 [0.728]
 [0.754]] [[1.592]
 [2.43 ]
 [2.846]
 [1.946]
 [2.224]
 [3.014]
 [2.506]] [[0.674]
 [0.706]
 [0.737]
 [0.713]
 [0.692]
 [0.728]
 [0.754]]
4427 6022
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
siam score:  -0.8078205
siam score:  -0.80594164
first move QE:  0.2516070616993578
4428 6022
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]] [[0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]] [[1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
actor:  1 policy actor:  1  step number:  101 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]] [[1.715]
 [1.715]
 [1.715]
 [1.715]
 [1.715]
 [1.715]
 [1.715]] [[0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]]
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]] [[0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[2.043]
 [1.846]
 [1.846]
 [1.846]
 [1.846]
 [1.846]
 [1.846]] [[0.868]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.5036025198350244, 0.32687326461239247, 0.1510885912930004, 0.008888550498478266, 0.009199531401685945, 0.0003475423594185077]
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]] [[5.301]
 [2.068]
 [2.068]
 [2.068]
 [2.068]
 [2.068]
 [2.068]] [[1.125]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]]
siam score:  -0.8066136
first move QE:  0.25007401639475263
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
using another actor
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.5036025198350245, 0.3268732646123925, 0.1510885912930004, 0.008888550498478268, 0.009199531401685947, 0.00034754235941850776]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.5036025198350245, 0.3268732646123925, 0.1510885912930004, 0.008888550498478268, 0.009199531401685947, 0.00034754235941850776]
Printing some Q and Qe and total Qs values:  [[0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.36]
 [0.31]
 [0.31]] [[2.71 ]
 [2.71 ]
 [2.71 ]
 [2.71 ]
 [3.188]
 [2.71 ]
 [2.71 ]] [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.439]
 [0.021]
 [0.021]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.5036025198350244, 0.32687326461239247, 0.1510885912930004, 0.008888550498478266, 0.009199531401685945, 0.0003475423594185077]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
start point for exploration sampling:  10935
actor:  1 policy actor:  1  step number:  87 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.500205177812178, 0.32466815198035787, 0.15681541411235173, 0.008828587640983897, 0.009137470642673697, 0.00034519781145485085]
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.685]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]] [[1.102]
 [1.263]
 [1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.102]] [[0.687]
 [0.685]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.500205177812178, 0.32466815198035787, 0.15681541411235173, 0.008828587640983897, 0.009137470642673697, 0.00034519781145485085]
using another actor
from probs:  [0.500205177812178, 0.32466815198035787, 0.15681541411235173, 0.008828587640983897, 0.009137470642673697, 0.00034519781145485085]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.500205177812178, 0.32466815198035787, 0.15681541411235173, 0.008828587640983897, 0.009137470642673697, 0.00034519781145485085]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.705]
 [0.673]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[1.924]
 [2.473]
 [2.412]
 [1.924]
 [1.924]
 [1.924]
 [1.924]] [[0.862]
 [1.353]
 [1.255]
 [0.862]
 [0.862]
 [0.862]
 [0.862]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
in main func line 156:  4444
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.627]
 [0.61 ]
 [0.596]
 [0.601]
 [0.598]
 [0.601]] [[ 0.172]
 [-0.202]
 [ 0.103]
 [-0.313]
 [ 0.085]
 [ 0.176]
 [ 0.072]] [[0.602]
 [0.627]
 [0.61 ]
 [0.596]
 [0.601]
 [0.598]
 [0.601]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.493541005585006, 0.3203426380163917, 0.15472618158562965, 0.022033842787483066, 0.009015733242133693, 0.00034059878335594186]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.493541005585006, 0.3203426380163917, 0.15472618158562965, 0.022033842787483066, 0.009015733242133693, 0.00034059878335594186]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.364]
 [0.395]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[0.236]
 [0.362]
 [0.254]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[0.696]
 [0.81 ]
 [0.73 ]
 [1.271]
 [1.271]
 [1.271]
 [1.271]]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 0.1059512190888614
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.493541005585006, 0.3203426380163917, 0.15472618158562965, 0.022033842787483066, 0.009015733242133693, 0.00034059878335594186]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.493541005585006, 0.3203426380163917, 0.15472618158562965, 0.022033842787483066, 0.009015733242133693, 0.00034059878335594186]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.493541005585006, 0.3203426380163917, 0.15472618158562965, 0.022033842787483066, 0.009015733242133693, 0.00034059878335594186]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.273]
 [0.231]
 [0.229]
 [0.229]
 [0.228]
 [0.225]] [[0.121]
 [0.776]
 [0.326]
 [0.755]
 [0.949]
 [0.926]
 [2.33 ]] [[-0.605]
 [ 0.023]
 [-0.428]
 [-0.053]
 [ 0.117]
 [ 0.097]
 [ 1.325]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[0.421]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]] [[0.115]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.493541005585006, 0.3203426380163917, 0.15472618158562965, 0.022033842787483066, 0.009015733242133693, 0.00034059878335594186]
line 256 mcts: sample exp_bonus 1.643492077869144
UNIT TEST: sample policy line 217 mcts : [0.163 0.388 0.082 0.041 0.041 0.163 0.122]
siam score:  -0.7882938
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
4454 6046
line 256 mcts: sample exp_bonus 3.1938222511405354
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
from probs:  [0.493541005585006, 0.3203426380163917, 0.15472618158562965, 0.022033842787483066, 0.009015733242133693, 0.00034059878335594186]
from probs:  [0.493541005585006, 0.3203426380163917, 0.15472618158562965, 0.022033842787483066, 0.009015733242133693, 0.00034059878335594186]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.442]] [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.685]] [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.442]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
actor:  1 policy actor:  1  step number:  110 total reward:  0.1949999999999994  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48990711587326774, 0.32534688258223465, 0.15358694923606203, 0.021871609956351668, 0.008949351361192447, 0.0003380909908915636]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48990711587326774, 0.32534688258223465, 0.15358694923606203, 0.021871609956351668, 0.008949351361192447, 0.0003380909908915636]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.754]
 [0.715]
 [0.655]
 [0.634]
 [0.7  ]
 [0.759]] [[3.118]
 [2.545]
 [2.984]
 [2.876]
 [3.131]
 [3.779]
 [2.669]] [[0.615]
 [0.754]
 [0.715]
 [0.655]
 [0.634]
 [0.7  ]
 [0.759]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48990711587326774, 0.32534688258223465, 0.15358694923606203, 0.021871609956351668, 0.008949351361192447, 0.0003380909908915636]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[0.882]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]] [[2.005]
 [1.854]
 [1.854]
 [1.854]
 [1.854]
 [1.854]
 [1.854]]
line 256 mcts: sample exp_bonus 0.8890229148716338
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48990711587326774, 0.32534688258223465, 0.153586949236062, 0.021871609956351668, 0.008949351361192447, 0.0003380909908915636]
Printing some Q and Qe and total Qs values:  [[0.942]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[5.321]
 [2.367]
 [2.367]
 [2.367]
 [2.367]
 [2.367]
 [2.367]] [[2.108]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]]
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]] [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48990711587326774, 0.32534688258223465, 0.153586949236062, 0.021871609956351668, 0.008949351361192447, 0.0003380909908915636]
using explorer policy with actor:  1
4461 6047
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.908]
 [0.528]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[1.351]
 [1.346]
 [0.685]
 [1.351]
 [1.351]
 [1.351]
 [1.351]] [[0.941]
 [1.824]
 [0.625]
 [0.941]
 [0.941]
 [0.941]
 [0.941]]
actor:  1 policy actor:  1  step number:  97 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.575]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[2.786]
 [1.146]
 [1.589]
 [1.589]
 [1.589]
 [1.589]
 [1.589]] [[2.137]
 [1.339]
 [1.568]
 [1.568]
 [1.568]
 [1.568]
 [1.568]]
siam score:  -0.77870107
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]] [[5.211]
 [1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]] [[1.44 ]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]]
Printing some Q and Qe and total Qs values:  [[ 0.204]
 [-0.014]
 [ 0.204]
 [-0.014]
 [-0.014]
 [ 0.204]
 [-0.014]] [[ 3.342]
 [ 1.216]
 [ 3.342]
 [-0.589]
 [-0.532]
 [ 3.342]
 [-0.401]] [[ 0.8  ]
 [-0.345]
 [ 0.8  ]
 [-0.949]
 [-0.93 ]
 [ 0.8  ]
 [-0.886]]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.167]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]] [[1.212]
 [1.571]
 [1.212]
 [1.212]
 [1.212]
 [1.212]
 [1.212]] [[-0.213]
 [ 0.306]
 [-0.213]
 [-0.213]
 [-0.213]
 [-0.213]
 [-0.213]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.3263],
        [-0.0000],
        [-0.0000],
        [-0.3729],
        [-0.1471],
        [-0.0000],
        [ 0.4935],
        [-0.4635],
        [-0.0000]], dtype=torch.float64)
-0.95104053 -0.95104053
-0.0727797758985 -0.39912551954240033
-0.9605475 -0.9605475
-0.7806871144709998 -0.7806871144709998
-0.0337698257985 -0.4066901018578114
-0.024259925299500003 -0.17133854925793313
-0.37619999999999953 -0.37619999999999953
-0.024259925299500003 0.46920256281351885
-0.024259925299500003 -0.48779796693810457
-0.894415820958 -0.894415820958
actor:  1 policy actor:  1  step number:  87 total reward:  0.06999999999999929  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  67 total reward:  0.3299999999999995  reward:  1.0 rdn_beta:  0.833
4466 6050
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.24 ]
 [0.344]
 [0.344]
 [0.344]
 [0.35 ]
 [0.345]] [[1.272]
 [1.437]
 [1.255]
 [1.255]
 [1.255]
 [1.32 ]
 [1.396]] [[0.33 ]
 [0.24 ]
 [0.344]
 [0.344]
 [0.344]
 [0.35 ]
 [0.345]]
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.245]
 [0.237]] [[2.545]
 [2.585]
 [2.585]
 [2.585]
 [2.585]
 [2.547]
 [3.805]] [[0.808]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.815]
 [1.166]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.005]
 [-0.001]
 [-0.005]
 [-0.005]
 [ 0.064]
 [-0.005]] [[2.014]
 [2.014]
 [2.339]
 [2.014]
 [2.014]
 [2.175]
 [2.014]] [[-0.969]
 [-0.969]
 [-0.853]
 [-0.969]
 [-0.969]
 [-0.777]
 [-0.969]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48398869073132517, 0.32642693762352465, 0.14998505999757786, 0.021358681501675334, 0.017910467994713325, 0.00033016215118364633]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48398869073132517, 0.32642693762352465, 0.14998505999757786, 0.021358681501675334, 0.017910467994713325, 0.00033016215118364633]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.54 ]
 [0.53 ]
 [0.534]
 [0.528]
 [0.521]
 [0.521]] [[2.402]
 [1.829]
 [2.085]
 [2.416]
 [2.511]
 [0.928]
 [0.928]] [[0.534]
 [0.54 ]
 [0.53 ]
 [0.534]
 [0.528]
 [0.521]
 [0.521]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
siam score:  -0.7801619
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.121]
 [0.29 ]
 [0.228]
 [0.214]
 [0.185]
 [0.182]] [[1.19 ]
 [1.24 ]
 [1.052]
 [1.3  ]
 [1.314]
 [1.405]
 [1.438]] [[-0.622]
 [-0.817]
 [-0.729]
 [-0.523]
 [-0.532]
 [-0.468]
 [-0.429]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.299]
 [0.295]
 [0.24 ]
 [0.24 ]
 [0.29 ]
 [0.24 ]] [[2.934]
 [2.937]
 [3.381]
 [2.934]
 [2.934]
 [3.017]
 [2.934]] [[-0.091]
 [ 0.028]
 [ 0.317]
 [-0.091]
 [-0.091]
 [ 0.064]
 [-0.091]]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.406]
 [0.373]
 [0.373]
 [0.393]
 [0.373]
 [0.39 ]] [[1.235]
 [1.194]
 [1.224]
 [1.224]
 [1.193]
 [1.224]
 [1.229]] [[0.374]
 [0.402]
 [0.366]
 [0.366]
 [0.376]
 [0.366]
 [0.405]]
line 256 mcts: sample exp_bonus 1.7333546507718025
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48398869073132517, 0.32642693762352465, 0.14998505999757786, 0.021358681501675334, 0.017910467994713325, 0.00033016215118364633]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
actor:  1 policy actor:  1  step number:  118 total reward:  0.17499999999999938  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.487]
 [0.414]
 [0.429]
 [0.442]
 [0.434]
 [0.433]] [[1.464]
 [0.756]
 [0.94 ]
 [1.026]
 [0.948]
 [0.891]
 [1.079]] [[0.338]
 [0.122]
 [0.036]
 [0.096]
 [0.095]
 [0.06 ]
 [0.122]]
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]] [[2.487]
 [2.317]
 [2.317]
 [2.317]
 [2.317]
 [2.317]
 [2.317]] [[0.51 ]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48066382270731817, 0.33105419768107675, 0.1489547042483326, 0.02121195328566565, 0.017787428048799216, 0.000327894028807586]
from probs:  [0.48066382270731817, 0.33105419768107675, 0.1489547042483326, 0.02121195328566565, 0.017787428048799216, 0.000327894028807586]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48066382270731817, 0.33105419768107675, 0.14895470424833257, 0.02121195328566565, 0.017787428048799216, 0.000327894028807586]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]] [[0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]] [[0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48066382270731817, 0.33105419768107675, 0.14895470424833257, 0.02121195328566565, 0.017787428048799216, 0.000327894028807586]
siam score:  -0.7805685
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48066382270731817, 0.33105419768107675, 0.14895470424833257, 0.02121195328566565, 0.017787428048799216, 0.000327894028807586]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48066382270731817, 0.33105419768107675, 0.14895470424833257, 0.02121195328566565, 0.017787428048799216, 0.000327894028807586]
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.149]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[0.626]
 [0.946]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[-0.219]
 [-0.048]
 [-0.219]
 [-0.219]
 [-0.219]
 [-0.219]
 [-0.219]]
line 256 mcts: sample exp_bonus 1.0086138760301318
line 256 mcts: sample exp_bonus 2.6586965775944558
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.54 ]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.469]] [[0.289]
 [0.093]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.321]] [[0.927]
 [0.784]
 [0.978]
 [0.978]
 [0.978]
 [0.978]
 [0.943]]
4483 6057
first move QE:  0.23517359359252224
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48066382270731817, 0.33105419768107675, 0.1489547042483326, 0.02121195328566565, 0.017787428048799216, 0.000327894028807586]
Printing some Q and Qe and total Qs values:  [[0.962]
 [0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.912]] [[1.396]
 [1.124]
 [1.124]
 [1.124]
 [1.124]
 [1.124]
 [1.124]] [[0.628]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.48066382270731817, 0.33105419768107675, 0.1489547042483326, 0.02121195328566565, 0.017787428048799216, 0.000327894028807586]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.38 ]
 [0.298]
 [0.611]
 [0.565]
 [0.355]
 [0.372]] [[1.242]
 [2.513]
 [2.9  ]
 [2.631]
 [2.513]
 [2.758]
 [2.545]] [[-0.323]
 [ 0.547]
 [ 0.688]
 [ 0.795]
 [ 0.695]
 [ 0.658]
 [ 0.558]]
siam score:  -0.7854585
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
actor:  1 policy actor:  1  step number:  67 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.4756447080610057, 0.3275973139011712, 0.14739931209603996, 0.0314325049244996, 0.01760169087362113, 0.0003244701436623362]
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[0.723]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]] [[0.786]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.482]
 [0.518]
 [0.493]
 [0.477]
 [0.466]
 [0.485]] [[2.601]
 [2.481]
 [2.565]
 [2.477]
 [2.634]
 [2.704]
 [2.482]] [[1.155]
 [1.089]
 [1.215]
 [1.107]
 [1.178]
 [1.204]
 [1.096]]
maxi score, test score, baseline:  -0.96597 -0.8484999999999999 -0.8484999999999999
probs:  [0.4756447080610057, 0.3275973139011712, 0.14739931209603996, 0.0314325049244996, 0.01760169087362113, 0.0003244701436623362]
line 256 mcts: sample exp_bonus 2.162216267657796
4494 6064
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.5049999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.96597 -0.8484999999999999 -0.8484999999999999
probs:  [0.47005651586932445, 0.3237484815258019, 0.1456675663813191, 0.04281188373681061, 0.01739489443536928, 0.0003206580513747142]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.479]
 [0.508]
 [0.479]
 [0.487]
 [0.464]
 [0.499]] [[3.325]
 [3.643]
 [2.831]
 [3.643]
 [3.223]
 [4.531]
 [3.461]] [[1.12 ]
 [1.277]
 [0.855]
 [1.277]
 [1.051]
 [1.756]
 [1.197]]
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.061]
 [0.014]
 [0.061]
 [0.153]
 [0.027]
 [0.061]] [[2.478]
 [2.478]
 [2.23 ]
 [2.478]
 [2.881]
 [2.908]
 [2.478]] [[-0.707]
 [-0.707]
 [-0.965]
 [-0.707]
 [-0.254]
 [-0.488]
 [-0.707]]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [ 0.108]
 [ 0.274]
 [ 0.335]
 [-0.009]
 [ 0.426]
 [ 0.077]] [[0.915]
 [1.456]
 [1.716]
 [1.741]
 [1.968]
 [1.664]
 [1.836]] [[0.048]
 [0.698]
 [1.203]
 [1.34 ]
 [0.804]
 [1.472]
 [0.889]]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.481]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[1.386]
 [2.289]
 [1.386]
 [1.386]
 [1.386]
 [1.386]
 [1.386]] [[0.419]
 [1.162]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]]
maxi score, test score, baseline:  -0.96597 -0.8484999999999999 -0.8484999999999999
probs:  [0.47005651586932445, 0.3237484815258019, 0.1456675663813191, 0.04281188373681061, 0.01739489443536928, 0.0003206580513747142]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.96597 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.96597 -0.8484999999999999 -0.8484999999999999
probs:  [0.47005651586932445, 0.3237484815258019, 0.1456675663813191, 0.04281188373681061, 0.01739489443536928, 0.0003206580513747142]
maxi score, test score, baseline:  -0.96597 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.96597 -0.8484999999999999 -0.8484999999999999
in main func line 156:  4505
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[1.166]
 [1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]] [[0.755]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 7.867738693247697
maxi score, test score, baseline:  -0.96597 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.96597 -0.8484999999999999 -0.8484999999999999
maxi score, test score, baseline:  -0.96597 -0.8484999999999999 -0.8484999999999999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.616]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]] [[2.064]
 [1.785]
 [2.25 ]
 [2.25 ]
 [2.25 ]
 [2.25 ]
 [2.25 ]] [[0.642]
 [0.365]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]]
Printing some Q and Qe and total Qs values:  [[ 0.06 ]
 [ 0.06 ]
 [-0.044]
 [ 0.06 ]
 [ 0.06 ]
 [-0.037]
 [ 0.06 ]] [[1.136]
 [1.136]
 [1.375]
 [1.136]
 [1.136]
 [2.259]
 [1.136]] [[-0.275]
 [-0.275]
 [-0.324]
 [-0.275]
 [-0.275]
 [ 0.278]
 [-0.275]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.36999999999999955  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]] [[0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]] [[-0.507]
 [-0.507]
 [-0.507]
 [-0.507]
 [-0.507]
 [-0.507]
 [-0.507]]
Starting evaluation
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.656]
 [0.656]
 [0.656]
 [0.653]
 [0.648]
 [0.658]] [[-1.724]
 [-2.203]
 [-1.596]
 [-1.596]
 [-2.149]
 [-1.935]
 [-1.832]] [[0.659]
 [0.656]
 [0.656]
 [0.656]
 [0.653]
 [0.648]
 [0.658]]
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[0.675]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]] [[1.002]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]]
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[6.21 ]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[1.708]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.682]
 [0.666]
 [0.649]
 [0.654]
 [0.665]
 [0.662]] [[-1.515]
 [-1.404]
 [-2.451]
 [ 0.   ]
 [-2.511]
 [-2.781]
 [-2.914]] [[0.671]
 [0.682]
 [0.666]
 [0.649]
 [0.654]
 [0.665]
 [0.662]]
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.752]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]] [[0.015]
 [0.08 ]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[0.757]
 [0.752]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.653]
 [0.686]
 [0.687]
 [0.688]
 [0.688]
 [0.685]] [[-2.334]
 [-2.579]
 [-2.567]
 [-2.608]
 [ 0.   ]
 [ 0.   ]
 [-2.506]] [[0.698]
 [0.653]
 [0.686]
 [0.687]
 [0.688]
 [0.688]
 [0.685]]
Printing some Q and Qe and total Qs values:  [[0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]] [[-2.219]
 [-2.219]
 [-2.219]
 [-2.219]
 [-2.219]
 [-2.219]
 [-2.219]] [[0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]]
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]] [[0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.39 ]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[ 0.728]
 [-0.147]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]] [[0.521]
 [0.333]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]]
Printing some Q and Qe and total Qs values:  [[0.76 ]
 [0.785]
 [0.442]
 [0.707]
 [0.711]
 [0.707]
 [0.789]] [[1.285]
 [0.661]
 [1.058]
 [0.283]
 [0.955]
 [0.283]
 [0.357]] [[0.76 ]
 [0.785]
 [0.442]
 [0.707]
 [0.711]
 [0.707]
 [0.789]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]]
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[-1.633]
 [-2.352]
 [-2.352]
 [-2.352]
 [-2.352]
 [-2.352]
 [-2.352]] [[0.765]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]]
line 256 mcts: sample exp_bonus -3.0148876610865316
maxi score, test score, baseline:  -0.96597 -0.8484999999999999 -0.8484999999999999
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.96597 -0.8484999999999999 -0.8484999999999999
probs:  [0.4656229052561766, 0.32069485998195685, 0.1442936182570064, 0.051840158524566624, 0.01723082440127857, 0.0003176335790149662]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[-1.812]
 [-2.03 ]
 [-2.03 ]
 [-2.03 ]
 [-2.03 ]
 [-2.03 ]
 [-2.03 ]] [[0.702]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.642]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[-2.626]
 [-2.389]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.654]
 [0.642]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]]
maxi score, test score, baseline:  -0.9659700000000001 -0.8484999999999999 -0.8484999999999999
probs:  [0.4656229052561766, 0.32069485998195685, 0.1442936182570064, 0.051840158524566624, 0.01723082440127857, 0.0003176335790149662]
actor:  0 policy actor:  1  step number:  56 total reward:  0.48499999999999965  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  57 total reward:  0.5299999999999997  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.897]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]] [[0.314]
 [0.302]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]] [[0.877]
 [0.897]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]]
actor:  0 policy actor:  1  step number:  60 total reward:  0.4449999999999996  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.043]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]] [[-0.684]
 [-0.668]
 [-0.684]
 [-0.684]
 [-0.684]
 [-0.684]
 [-0.684]] [[-1.448]
 [-1.423]
 [-1.448]
 [-1.448]
 [-1.448]
 [-1.448]
 [-1.448]]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  63 total reward:  0.4099999999999996  reward:  1.0 rdn_beta:  1
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  66 total reward:  0.4549999999999996  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.736]
 [0.909]
 [0.941]
 [0.871]
 [0.929]
 [0.911]] [[1.856]
 [1.431]
 [1.18 ]
 [1.305]
 [1.707]
 [1.611]
 [1.564]] [[0.81 ]
 [0.736]
 [0.909]
 [0.941]
 [0.871]
 [0.929]
 [0.911]]
actor:  0 policy actor:  1  step number:  69 total reward:  0.4499999999999996  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  71 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1
maxi score, test score, baseline:  -0.9455800000000001 -0.8484999999999999 -0.8484999999999999
line 256 mcts: sample exp_bonus 0.703338373379771
actor:  0 policy actor:  1  step number:  72 total reward:  0.3949999999999996  reward:  1.0 rdn_beta:  1
actor:  0 policy actor:  1  step number:  74 total reward:  0.3349999999999995  reward:  1.0 rdn_beta:  1
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  77 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  89 total reward:  0.3299999999999995  reward:  1.0 rdn_beta:  1
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]] [[0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]] [[1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.201]]
actor:  0 policy actor:  1  step number:  112 total reward:  0.2149999999999994  reward:  1.0 rdn_beta:  1
rdn probs:  [0.4656229052561766, 0.32069485998195685, 0.1442936182570064, 0.051840158524566624, 0.01723082440127857, 0.0003176335790149662]
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[2.779]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[1.102]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]]
maxi score, test score, baseline:  -0.9323100000000001 -0.15850000000000025 -0.15850000000000025
probs:  [0.4651157060600162, 0.3259783871352594, 0.1314717971580527, 0.0596543950972328, 0.016105272191070615, 0.0016744423583683408]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]] [[-0.875]
 [-0.875]
 [-0.875]
 [-0.875]
 [-0.875]
 [-0.875]
 [-0.875]]
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]] [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[-0.627]
 [-0.627]
 [-0.627]
 [-0.627]
 [-0.627]
 [-0.627]
 [-0.627]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9323100000000001 -0.15850000000000025 -0.15850000000000025
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.036]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]] [[2.657]
 [2.752]
 [2.657]
 [2.657]
 [2.657]
 [2.657]
 [2.657]] [[-1.246]
 [-1.191]
 [-1.246]
 [-1.246]
 [-1.246]
 [-1.246]
 [-1.246]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9323100000000001 -0.15850000000000025 -0.15850000000000025
probs:  [0.4651157060600162, 0.3259783871352594, 0.1314717971580527, 0.0596543950972328, 0.016105272191070615, 0.0016744423583683408]
start point for exploration sampling:  10935
maxi score, test score, baseline:  -0.9323100000000001 -0.15850000000000025 -0.15850000000000025
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9323100000000001 -0.15850000000000025 -0.15850000000000025
maxi score, test score, baseline:  -0.9323100000000001 -0.15850000000000025 -0.15850000000000025
4521 6075
from probs:  [0.4651157060600162, 0.3259783871352594, 0.13147179715805268, 0.0596543950972328, 0.016105272191070615, 0.0016744423583683408]
line 256 mcts: sample exp_bonus 1.332445182800679
siam score:  -0.81132555
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.044]
 [-0.04 ]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]] [[1.285]
 [1.285]
 [1.515]
 [1.285]
 [1.285]
 [1.285]
 [1.285]] [[-1.182]
 [-1.182]
 [-1.098]
 [-1.182]
 [-1.182]
 [-1.182]
 [-1.182]]
start point for exploration sampling:  10935
using explorer policy with actor:  1
using another actor
siam score:  -0.809299
maxi score, test score, baseline:  -0.9323100000000001 -0.15850000000000025 -0.15850000000000025
probs:  [0.4651157060600162, 0.3259783871352594, 0.13147179715805268, 0.0596543950972328, 0.016105272191070615, 0.0016744423583683408]
maxi score, test score, baseline:  -0.9323100000000001 -0.15850000000000025 -0.15850000000000025
line 256 mcts: sample exp_bonus 0.36509305514555346
Printing some Q and Qe and total Qs values:  [[-0.138]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.148]] [[0.322]
 [1.308]
 [1.308]
 [1.308]
 [1.308]
 [1.308]
 [0.112]] [[-0.808]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.97 ]]
in main func line 156:  4526
actor:  0 policy actor:  1  step number:  74 total reward:  0.2849999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]] [[1.154]
 [1.154]
 [1.154]
 [1.154]
 [1.154]
 [1.154]
 [1.154]] [[0.949]
 [0.949]
 [0.949]
 [0.949]
 [0.949]
 [0.949]
 [0.949]]
maxi score, test score, baseline:  -0.92974 -0.15850000000000025 -0.15850000000000025
first move QE:  0.21785542304823358
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10935
maxi score, test score, baseline:  -0.92974 -0.15850000000000025 -0.15850000000000025
maxi score, test score, baseline:  -0.92974 -0.15850000000000025 -0.15850000000000025
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.032]
 [-0.044]
 [-0.042]
 [-0.04 ]
 [-0.044]
 [-0.042]] [[3.586]
 [3.521]
 [3.627]
 [3.61 ]
 [3.413]
 [3.576]
 [3.551]] [[-0.048]
 [-0.087]
 [-0.009]
 [-0.021]
 [-0.205]
 [-0.057]
 [-0.077]]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.565]
 [0.543]
 [0.549]
 [0.55 ]
 [0.555]
 [0.54 ]] [[1.694]
 [1.778]
 [1.427]
 [1.401]
 [1.406]
 [1.32 ]
 [1.576]] [[-0.099]
 [-0.015]
 [-0.178]
 [-0.174]
 [-0.17 ]
 [-0.19 ]
 [-0.133]]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.511]
 [0.412]
 [0.412]
 [0.568]
 [0.618]
 [0.558]] [[0.607]
 [1.018]
 [0.607]
 [0.607]
 [0.66 ]
 [0.499]
 [0.564]] [[-0.572]
 [-0.237]
 [-0.572]
 [-0.572]
 [-0.241]
 [-0.196]
 [-0.295]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  74 total reward:  0.34499999999999953  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.92974 -0.15850000000000025 -0.15850000000000025
probs:  [0.46961855594386165, 0.32323418290403017, 0.1303650199105822, 0.05915220277437393, 0.01596969217154774, 0.0016603462956042868]
using another actor
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.733]
 [0.734]
 [0.733]
 [0.733]
 [0.733]
 [0.733]] [[2.849]
 [2.849]
 [2.788]
 [2.849]
 [2.849]
 [2.849]
 [2.849]] [[0.733]
 [0.733]
 [0.734]
 [0.733]
 [0.733]
 [0.733]
 [0.733]]
maxi score, test score, baseline:  -0.92974 -0.15850000000000025 -0.15850000000000025
probs:  [0.46961855594386165, 0.32323418290403017, 0.13036501991058222, 0.05915220277437393, 0.01596969217154774, 0.0016603462956042868]
maxi score, test score, baseline:  -0.92974 -0.15850000000000025 -0.15850000000000025
maxi score, test score, baseline:  -0.92974 -0.15850000000000025 -0.15850000000000025
probs:  [0.46961855594386165, 0.32323418290403017, 0.13036501991058222, 0.05915220277437393, 0.01596969217154774, 0.0016603462956042868]
first move QE:  0.21576534613123463
maxi score, test score, baseline:  -0.92974 -0.15850000000000025 -0.15850000000000025
probs:  [0.46961855594386165, 0.32323418290403017, 0.13036501991058222, 0.05915220277437393, 0.01596969217154774, 0.0016603462956042868]
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.971]
 [0.938]
 [0.942]
 [0.934]
 [0.94 ]
 [0.937]] [[1.538]
 [2.172]
 [1.558]
 [1.384]
 [1.35 ]
 [1.97 ]
 [2.622]] [[0.914]
 [0.971]
 [0.938]
 [0.942]
 [0.934]
 [0.94 ]
 [0.937]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.126]
 [-0.116]
 [-0.125]
 [-0.124]
 [-0.124]
 [-0.125]
 [-0.125]] [[0.227]
 [0.065]
 [0.193]
 [0.879]
 [0.879]
 [0.294]
 [0.608]] [[-1.077]
 [-1.112]
 [-1.087]
 [-0.855]
 [-0.855]
 [-1.052]
 [-0.947]]
maxi score, test score, baseline:  -0.92974 -0.15850000000000025 -0.15850000000000025
Printing some Q and Qe and total Qs values:  [[-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]] [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]] [[-0.862]
 [-0.862]
 [-0.862]
 [-0.862]
 [-0.862]
 [-0.862]
 [-0.862]]
actor:  0 policy actor:  1  step number:  103 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.098]
 [-0.105]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.093]] [[1.111]
 [0.292]
 [1.111]
 [1.111]
 [1.111]
 [1.111]
 [2.134]] [[-0.074]
 [-0.461]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [ 0.407]]
siam score:  -0.8115976
maxi score, test score, baseline:  -0.92726 -0.15850000000000025 -0.15850000000000025
probs:  [0.46961855594386165, 0.32323418290403017, 0.1303650199105822, 0.05915220277437393, 0.01596969217154774, 0.0016603462956042868]
