dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[9, 10]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.  0.2 0.2 0.  0.  0.6 0. ]
6 7
Starting evaluation
main train batch thing paused
add a thread
Adding thread: now have 4 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
siam score:  0.0024589410644363274
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.4659101392510575, 0.19496408215169123, 0.042556925033297756, 0.2965688535639535]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.36654353267433937, 0.23335092390366577, 0.03356201074765554, 0.36654353267433937]
deleting a thread, now have 3 threads
Frames:  720 train batches done:  11 episodes:  25
siam score:  -0.013976511414953027
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.36654353267433937, 0.23335092390366577, 0.03356201074765554, 0.36654353267433937]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.36654353267433937, 0.23335092390366577, 0.03356201074765554, 0.36654353267433937]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.36654353267433937, 0.23335092390366577, 0.03356201074765554, 0.36654353267433937]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.36654353267433937, 0.23335092390366577, 0.03356201074765554, 0.36654353267433937]
deleting a thread, now have 2 threads
Frames:  720 train batches done:  42 episodes:  25
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.36654353267433937, 0.23335092390366577, 0.03356201074765554, 0.36654353267433937]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.36654353267433937, 0.23335092390366577, 0.03356201074765554, 0.36654353267433937]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.36654353267433937, 0.23335092390366577, 0.03356201074765554, 0.36654353267433937]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.30214006646079394, 0.49465723493141794, 0.15775219010782607, 0.04545050849996207]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.30214006646079394, 0.49465723493141794, 0.15775219010782607, 0.04545050849996207]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.30214006646079394, 0.49465723493141794, 0.15775219010782607, 0.04545050849996207]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.30214006646079394, 0.49465723493141794, 0.15775219010782607, 0.04545050849996207]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.30214006646079394, 0.49465723493141794, 0.15775219010782607, 0.04545050849996207]
deleting a thread, now have 2 threads
Frames:  1014 train batches done:  63 episodes:  37
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.30214006646079394, 0.49465723493141794, 0.15775219010782607, 0.04545050849996207]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.30214006646079394, 0.49465723493141794, 0.15775219010782607, 0.04545050849996207]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.30214006646079394, 0.49465723493141794, 0.15775219010782607, 0.04545050849996207]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.30214006646079394, 0.49465723493141794, 0.15775219010782607, 0.04545050849996207]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.21222295089182014, 0.6655475401899776, 0.06111475445910102, 0.06111475445910102]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.21222295089182014, 0.6655475401899776, 0.06111475445910102, 0.06111475445910102]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
deleting a thread, now have 2 threads
Frames:  1482 train batches done:  86 episodes:  51
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.2930428232731407, 0.03478588363429654, 0.3791284698194221, 0.2930428232731407]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.2930428232731407, 0.03478588363429654, 0.3791284698194221, 0.2930428232731407]
siam score:  -0.2233754
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.2930428232731407, 0.03478588363429654, 0.3791284698194221, 0.2930428232731407]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.2930428232731407, 0.03478588363429654, 0.3791284698194221, 0.2930428232731407]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.2930428232731407, 0.03478588363429654, 0.3791284698194221, 0.2930428232731407]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.2930428232731407, 0.03478588363429654, 0.3791284698194221, 0.2930428232731407]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.2930428232731407, 0.03478588363429654, 0.3791284698194221, 0.2930428232731407]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.2930428232731407, 0.03478588363429654, 0.3791284698194221, 0.2930428232731407]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.2930428232731407, 0.03478588363429654, 0.3791284698194221, 0.2930428232731407]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.339569861582852, 0.04148136223512074, 0.45135304883825117, 0.1675957273437761]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.3641778841019607, 0.044479808616470745, 0.48406466240901935, 0.10727764487254919]
siam score:  -0.2915392
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.2589853908152827, 0.050117438278896274, 0.5438053260921736, 0.1470918448136474]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.47111776348341705, 0.04413173744647327, 0.24237524953505477, 0.24237524953505477]
UNIT TEST: sample policy line 217 mcts : [0.  0.2 0.2 0.2 0.  0.  0.4]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.47111776348341705, 0.04413173744647327, 0.24237524953505477, 0.24237524953505477]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.3229211037141573, 0.03123668885752816, 0.3229211037141573, 0.3229211037141573]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.45594074724169553, 0.04405925275830443, 0.04405925275830443, 0.45594074724169553]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.3228801583356667, 0.3228801583356667, 0.03135952499299997, 0.3228801583356667]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.32288016013345894, 0.32288016013345894, 0.03135951959962321, 0.32288016013345894]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.4557775046600551, 0.4557775046600551, 0.04422249533994482, 0.04422249533994482]
Printing some Q and Qe and total Qs values:  [[0.955]
 [0.955]
 [0.955]
 [0.955]
 [1.14 ]
 [0.955]
 [1.13 ]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.709]
 [ 0.   ]
 [-0.171]] [[1.94 ]
 [1.94 ]
 [1.94 ]
 [1.94 ]
 [2.072]
 [1.94 ]
 [2.231]]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.4557775046600551, 0.4557775046600551, 0.04422249533994482, 0.04422249533994482]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.3228392522959428, 0.03148224311217165, 0.3228392522959428, 0.3228392522959428]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.3228392522959428, 0.03148224311217165, 0.3228392522959428, 0.3228392522959428]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.38 ]
 [0.388]
 [0.608]
 [0.608]
 [0.442]
 [0.461]] [[ 0.   ]
 [-0.093]
 [-0.093]
 [ 0.   ]
 [ 0.   ]
 [-0.093]
 [-0.091]] [[1.247]
 [0.761]
 [0.776]
 [1.247]
 [1.247]
 [0.885]
 [0.923]]
siam score:  -0.30862486
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.19906460008069307, 0.03607132033891111, 0.3824320397901979, 0.3824320397901979]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.3045619764677842, 0.05513579832934172, 0.05513579832934172, 0.5851664268735324]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
from probs:  [0.3227575483820075, 0.3227575483820075, 0.3227575483820075, 0.03172735485397754]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.03617537288158321, 0.38193434439221446, 0.38193434439221446, 0.19995593833398786]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.044201029269160294, 0.4669232394189933, 0.24443786565592324, 0.24443786565592324]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.05524424302909955, 0.5838670119501147, 0.05524424302909955, 0.30564450199168613]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.05524424302909955, 0.5838670119501147, 0.05524424302909955, 0.30564450199168613]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.05524424302909955, 0.5838670119501147, 0.05524424302909955, 0.30564450199168613]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.05524424302909955, 0.5838670119501147, 0.05524424302909955, 0.30564450199168613]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.07603094811835999, 0.77190715564492, 0.07603094811835999, 0.07603094811835999]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  50
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.4565941067925162, 0.04340589320748386, 0.4565941067925162, 0.04340589320748386]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.5827555981881607, 0.05536936709749114, 0.306505667616857, 0.05536936709749114]
using explorer policy with actor:  1
from probs:  [0.04431833895373335, 0.2452167055570635, 0.46524824993213965, 0.2452167055570635]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.04535891129013688, 0.4546410887098631, 0.4546410887098631, 0.04535891129013688]
Printing some Q and Qe and total Qs values:  [[1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]] [[-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]] [[1.992]
 [1.992]
 [1.992]
 [1.992]
 [1.992]
 [1.992]
 [1.992]]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.7691204621052018, 0.07695984596493265, 0.07695984596493265, 0.07695984596493265]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.4643336844636909, 0.24562584317421063, 0.044414629187888015, 0.24562584317421063]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.3225133492379651, 0.3225133492379651, 0.032459952286104696, 0.3225133492379651]
siam score:  -0.34413743
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.04568161294186398, 0.45431838705813604, 0.04568161294186398, 0.45431838705813604]
first move QE:  -0.21164878748084331
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.04568161294186398, 0.45431838705813604, 0.04568161294186398, 0.45431838705813604]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.0771906590452399, 0.0771906590452399, 0.0771906590452399, 0.7684280228642802]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.0771906590452399, 0.0771906590452399, 0.0771906590452399, 0.7684280228642802]
62 60
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.3080289356636398, 0.05572921538694263, 0.580512633562475, 0.05572921538694263]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.3080289356636398, 0.05572921538694263, 0.580512633562475, 0.05572921538694263]
67 65
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.06310151655722418, 0.6733822788193491, 0.2004146880662026, 0.06310151655722418]
siam score:  -0.39053866
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.06310151655722418, 0.6733822788193491, 0.2004146880662026, 0.06310151655722418]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.06310151655722418, 0.6733822788193491, 0.2004146880662026, 0.06310151655722418]
from probs:  [0.06310151655722418, 0.6733822788193491, 0.2004146880662026, 0.06310151655722418]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.046644357145577534, 0.4533556428544225, 0.4533556428544225, 0.046644357145577534]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.37996582191313805, 0.37996582191313805, 0.03696324036918588, 0.20310511580453808]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.37996582191313805, 0.37996582191313805, 0.03696324036918588, 0.20310511580453808]
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.24674419119177485, 0.4616275725346299, 0.044884045081820485, 0.24674419119177485]
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.05089444243928708, 0.5347965570172222, 0.05089444243928708, 0.36341455810420353]
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.05629615799618267, 0.5781105895166708, 0.05629615799618267, 0.30929709449096393]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.056296148830928296, 0.5781106050414896, 0.056296148830928296, 0.3092970972966537]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.24694051902096217, 0.04501477440446471, 0.24694051902096217, 0.46110418755361093]
siam score:  -0.38224524
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.24703016649096574, 0.04508148787662771, 0.46085817914144095, 0.24703016649096574]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.05646594209858925, 0.05646594209858925, 0.5775191749100794, 0.3095489408927421]
87 81
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.04744058492845542, 0.04744058492845542, 0.4525594150715445, 0.4525594150715445]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.04744058492845542, 0.04744058492845542, 0.4525594150715445, 0.4525594150715445]
first move QE:  -0.12455780520289025
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.421310743258062, 0.03989680023219486, 0.2205665627181317, 0.3182258937916115]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.40356449465229327, 0.03875138045293685, 0.15411963024247666, 0.40356449465229327]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.5377339077599183, 0.051598641219695526, 0.20533372551019313, 0.20533372551019313]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
siam score:  -0.4217118
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.08058545020575353, 0.08058545020575353, 0.08058545020575353, 0.7582436493827394]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.08058545020575353, 0.08058545020575353, 0.08058545020575353, 0.7582436493827394]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
Printing some Q and Qe and total Qs values:  [[0.991]
 [0.991]
 [0.991]
 [1.036]
 [0.991]
 [1.063]
 [1.025]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.001]
 [ 0.   ]
 [-0.009]
 [-0.099]] [[1.902]
 [1.902]
 [1.902]
 [1.993]
 [1.902]
 [2.043]
 [1.937]]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.24753695350876273, 0.45935895175515756, 0.24753695350876273, 0.0455671412273169]
in main func line 156:  105
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.04854621967712563, 0.45145378032287437, 0.45145378032287437, 0.04854621967712563]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.4512966746872797, 0.04870332531272035, 0.04870332531272035, 0.4512966746872797]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.3788986117486456, 0.037753413737192626, 0.20444936276551615, 0.3788986117486456]
Printing some Q and Qe and total Qs values:  [[1.208]
 [1.208]
 [1.208]
 [1.178]
 [1.208]
 [1.208]
 [1.208]] [[ 0.719]
 [ 0.719]
 [ 0.719]
 [-0.129]
 [ 0.719]
 [ 0.719]
 [ 0.719]] [[1.968]
 [1.968]
 [1.968]
 [1.626]
 [1.968]
 [1.968]
 [1.968]]
from probs:  [0.3788986117486456, 0.037753413737192626, 0.20444936276551615, 0.3788986117486456]
siam score:  -0.44734123
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.4547139682108009, 0.045286031789199085, 0.045286031789199085, 0.4547139682108009]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
siam score:  -0.44839343
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.4547139682108009, 0.045286031789199085, 0.045286031789199085, 0.4547139682108009]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.4547139682108009, 0.045286031789199085, 0.045286031789199085, 0.4547139682108009]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.4547139682108009, 0.045286031789199085, 0.045286031789199085, 0.4547139682108009]
siam score:  -0.48433226
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.4547139682108009, 0.045286031789199085, 0.045286031789199085, 0.4547139682108009]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.4547139682108009, 0.045286031789199085, 0.045286031789199085, 0.4547139682108009]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.4547139682108009, 0.045286031789199085, 0.045286031789199085, 0.4547139682108009]
using another actor
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.3103251925211465, 0.05725365316414154, 0.05725365316414154, 0.5751675011505705]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.3103251925211465, 0.05725365316414154, 0.05725365316414154, 0.5751675011505705]
from probs:  [0.3103251925211465, 0.05725365316414154, 0.05725365316414154, 0.5751675011505705]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3162906423333075, 0.04467096437682116, 0.17746280693332636, 0.46157558635654505]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3162906423333075, 0.04467096437682116, 0.17746280693332636, 0.46157558635654505]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3162906423333075, 0.04467096437682116, 0.17746280693332636, 0.46157558635654505]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3162906423333075, 0.04467096437682116, 0.17746280693332636, 0.46157558635654505]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3788234034002089, 0.03782027675259629, 0.2045329164469859, 0.3788234034002089]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3788234034002089, 0.03782027675259629, 0.2045329164469859, 0.3788234034002089]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.928]
 [0.934]
 [0.99 ]
 [1.021]
 [0.964]
 [0.964]
 [1.004]] [[-0.065]
 [-0.042]
 [-0.032]
 [-0.053]
 [-0.298]
 [-0.298]
 [-0.02 ]] [[1.448]
 [1.482]
 [1.605]
 [1.645]
 [1.287]
 [1.287]
 [1.644]]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3788234034002089, 0.03782027675259629, 0.2045329164469859, 0.3788234034002089]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.3788234034002089, 0.03782027675259629, 0.2045329164469859, 0.3788234034002089]
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]] [[0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 5 threads
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 6 threads
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.504]
 [0.515]
 [0.438]
 [0.438]
 [0.438]
 [0.506]] [[-0.022]
 [-0.218]
 [-0.335]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.149]] [[0.482]
 [0.504]
 [0.515]
 [0.438]
 [0.438]
 [0.438]
 [0.506]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.657]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[-0.238]
 [-0.214]
 [-0.238]
 [-0.238]
 [-0.238]
 [-0.238]
 [-0.238]] [[0.881]
 [1.006]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]] [[-0.215]
 [-0.215]
 [-0.215]
 [-0.215]
 [-0.215]
 [-0.215]
 [-0.215]] [[0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]]
main train batch thing paused
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.24033935615238994
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.777]
 [0.951]
 [0.985]
 [0.934]
 [0.962]
 [0.94 ]] [[-0.12 ]
 [-0.134]
 [-0.08 ]
 [-0.239]
 [-0.28 ]
 [-0.313]
 [-0.077]] [[1.312]
 [1.236]
 [1.603]
 [1.617]
 [1.501]
 [1.547]
 [1.583]]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.925]
 [0.917]] [[-0.102]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.054]
 [-0.102]] [[1.406]
 [1.406]
 [1.406]
 [1.406]
 [1.406]
 [1.437]
 [1.406]]
main train batch thing paused
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.464]
 [0.621]
 [0.66 ]
 [0.617]
 [0.591]
 [0.662]] [[ 0.142]
 [-0.067]
 [-0.153]
 [-0.198]
 [-0.257]
 [-0.093]
 [-0.207]] [[0.85 ]
 [0.561]
 [0.848]
 [0.909]
 [0.804]
 [0.807]
 [0.911]]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.32162937801266134, 0.32162937801266134, 0.32162937801266134, 0.03511186596201599]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.32162937801266134, 0.32162937801266134, 0.32162937801266134, 0.03511186596201599]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.32162938204618874, 0.32162938204618874, 0.32162938204618874, 0.03511185386143387]
siam score:  -0.5261551
main train batch thing paused
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]]
using another actor
from probs:  [0.32162938204618874, 0.32162938204618874, 0.32162938204618874, 0.03511185386143387]
main train batch thing paused
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.04592808360025698, 0.4584605597631788, 0.2478056783182821, 0.2478056783182821]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.04592808360025698, 0.4584605597631788, 0.2478056783182821, 0.2478056783182821]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.328]
 [0.331]
 [0.336]
 [0.323]
 [0.322]
 [0.322]] [[ 0.147]
 [ 0.149]
 [ 0.001]
 [-0.031]
 [ 0.208]
 [-0.005]
 [ 0.052]] [[0.335]
 [0.328]
 [0.331]
 [0.336]
 [0.323]
 [0.322]
 [0.322]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.04592808360025698, 0.4584605597631788, 0.2478056783182821, 0.2478056783182821]
using explorer policy with actor:  1
using another actor
from probs:  [0.03523113363301158, 0.3215896221223295, 0.3215896221223295, 0.3215896221223295]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.03240609889838706, 0.32253130036720434, 0.32253130036720434, 0.32253130036720434]
using explorer policy with actor:  1
siam score:  -0.55170107
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.19 ]
 [0.214]
 [0.226]
 [0.274]
 [0.262]
 [0.273]] [[1.367]
 [1.42 ]
 [0.763]
 [0.478]
 [1.007]
 [0.484]
 [0.674]] [[0.182]
 [0.19 ]
 [0.214]
 [0.226]
 [0.274]
 [0.262]
 [0.273]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.03802142392859764, 0.20476607790945003, 0.3786062490809762, 0.3786062490809762]
siam score:  -0.5613164
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.03802141681567475, 0.2047660763916291, 0.3786062533963481, 0.3786062533963481]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.03802141681567475, 0.2047660763916291, 0.3786062533963481, 0.3786062533963481]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.03497634177361924, 0.2466226650540364, 0.35920049658617215, 0.35920049658617215]
Printing some Q and Qe and total Qs values:  [[1.102]
 [1.133]
 [1.044]
 [1.044]
 [1.044]
 [1.044]
 [1.158]] [[ 0.627]
 [ 0.029]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [-0.123]] [[2.426]
 [1.891]
 [1.684]
 [1.684]
 [1.684]
 [1.684]
 [1.788]]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.03940059509478984, 0.27791076450551, 0.27791076450551, 0.4047778758941901]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.03940059509478984, 0.27791076450551, 0.27791076450551, 0.4047778758941901]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.03940059509478984, 0.27791076450551, 0.27791076450551, 0.4047778758941901]
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.909]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.843]] [[ 0.929]
 [-0.001]
 [ 0.089]
 [ 0.089]
 [ 0.089]
 [ 0.089]
 [ 0.675]] [[2.3  ]
 [1.712]
 [1.56 ]
 [1.56 ]
 [1.56 ]
 [1.56 ]
 [2.255]]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.03940059509478984, 0.27791076450551, 0.27791076450551, 0.4047778758941901]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.03940059509478984, 0.27791076450551, 0.27791076450551, 0.4047778758941901]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.039400590232216225, 0.2779107651499472, 0.2779107651499472, 0.4047778794678893]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.875]] [[-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [ 0.138]] [[1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.889]]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.039400590232216225, 0.2779107651499472, 0.2779107651499472, 0.4047778794678893]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.039400590232216225, 0.2779107651499472, 0.2779107651499472, 0.4047778794678893]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.039400590232216225, 0.2779107651499472, 0.2779107651499472, 0.4047778794678893]
main train batch thing paused
134 110
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.081]
 [-0.086]
 [-0.088]
 [-0.089]
 [-0.088]
 [-0.087]] [[0.26 ]
 [0.241]
 [0.213]
 [0.153]
 [0.171]
 [0.139]
 [0.187]] [[-0.179]
 [-0.199]
 [-0.238]
 [-0.302]
 [-0.285]
 [-0.315]
 [-0.266]]
siam score:  -0.5429344
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.3214705595515722, 0.03558832134528348, 0.3214705595515722, 0.3214705595515722]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.3214705595515722, 0.03558832134528348, 0.3214705595515722, 0.3214705595515722]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.3214705595515722, 0.03558832134528348, 0.3214705595515722, 0.3214705595515722]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.45020310278990405, 0.04979689721009592, 0.04979689721009592, 0.45020310278990405]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.45020310278990405, 0.04979689721009592, 0.04979689721009592, 0.45020310278990405]
main train batch thing paused
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.007021193300124499
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.45004771262483034, 0.04995228737516961, 0.04995228737516961, 0.45004771262483034]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.45004771262483034, 0.04995228737516961, 0.04995228737516961, 0.45004771262483034]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.22 ]] [[-0.333]
 [-0.333]
 [-0.333]
 [-0.333]
 [-0.333]
 [-0.333]
 [-0.459]] [[2.02 ]
 [2.02 ]
 [2.02 ]
 [2.02 ]
 [2.02 ]
 [2.02 ]
 [2.211]]
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.084]
 [0.254]
 [0.302]
 [0.289]
 [0.302]
 [0.315]] [[-0.176]
 [-0.019]
 [-0.203]
 [-0.302]
 [-0.348]
 [-0.315]
 [-0.4  ]] [[0.15 ]
 [0.084]
 [0.254]
 [0.302]
 [0.289]
 [0.302]
 [0.315]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.03582591771559904, 0.32139136076146696, 0.32139136076146696, 0.32139136076146696]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.03582591771559904, 0.32139136076146696, 0.32139136076146696, 0.32139136076146696]
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.294]] [[0.085]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.175]] [[0.29 ]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.294]]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.050107484048192916, 0.4498925159518071, 0.050107484048192916, 0.4498925159518071]
main train batch thing paused
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.014]
 [0.919]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.87 ]] [[ 0.062]
 [ 0.468]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [ 0.118]] [[1.764]
 [1.71 ]
 [1.324]
 [1.324]
 [1.324]
 [1.324]
 [1.495]]
Printing some Q and Qe and total Qs values:  [[1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.077]] [[0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.071]] [[2.148]
 [2.148]
 [2.148]
 [2.148]
 [2.148]
 [2.148]
 [2.069]]
Training Flag: False
Self play flag: True
resampling flag: False
add more workers flag:  True
expV_train_flag:  False
expV_train_start_flag:  255000
using explorer policy with actor:  1
main train batch thing paused
main train batch thing paused
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.035186629715839815, 0.35893379440950296, 0.24694578146515425, 0.35893379440950296]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.03677465608086483, 0.38184162486602946, 0.2906918595265529, 0.2906918595265529]
Printing some Q and Qe and total Qs values:  [[1.046]
 [0.895]
 [1.24 ]
 [1.266]
 [1.285]
 [1.283]
 [1.082]] [[-0.24 ]
 [-0.   ]
 [-0.033]
 [-0.196]
 [-0.244]
 [-0.244]
 [-0.007]] [[2.238]
 [2.255]
 [2.902]
 [2.738]
 [2.712]
 [2.708]
 [2.621]]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.03677465608086483, 0.38184162486602946, 0.2906918595265529, 0.2906918595265529]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.04441238514106552, 0.46138082995845575, 0.3512382218934846, 0.142968563006994]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.688]
 [0.731]
 [0.746]
 [0.745]
 [0.726]
 [0.726]] [[0.381]
 [0.352]
 [0.242]
 [0.148]
 [0.196]
 [0.06 ]
 [0.06 ]] [[1.414]
 [1.445]
 [1.386]
 [1.293]
 [1.353]
 [1.135]
 [1.135]]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.04466753488560711, 0.4553324651143929, 0.4553324651143929, 0.04466753488560711]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.04466753488560711, 0.4553324651143929, 0.4553324651143929, 0.04466753488560711]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.04466753488560711, 0.4553324651143929, 0.4553324651143929, 0.04466753488560711]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.04466753488560711, 0.4553324651143929, 0.4553324651143929, 0.04466753488560711]
main train batch thing paused
from probs:  [0.04466753488560711, 0.4553324651143929, 0.4553324651143929, 0.04466753488560711]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.20243265327968069, 0.38042659584603705, 0.38042659584603705, 0.03671415502824513]
Printing some Q and Qe and total Qs values:  [[0.943]
 [0.515]
 [0.825]
 [0.891]
 [0.868]
 [0.86 ]
 [0.871]] [[ 0.068]
 [ 0.017]
 [-0.056]
 [-0.199]
 [-0.286]
 [-0.217]
 [-0.082]] [[1.775]
 [0.866]
 [1.413]
 [1.402]
 [1.271]
 [1.322]
 [1.479]]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.20243265327968069, 0.38042659584603705, 0.38042659584603705, 0.03671415502824513]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[1.117]
 [1.117]
 [1.117]
 [1.117]
 [1.117]
 [1.117]
 [1.117]] [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.20243265327968069, 0.38042659584603705, 0.38042659584603705, 0.03671415502824513]
main train batch thing paused
using explorer policy with actor:  1
main train batch thing paused
using explorer policy with actor:  0
main train batch thing paused
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.098]
 [0.398]
 [0.466]
 [0.48 ]
 [0.444]
 [0.477]] [[0.464]
 [0.971]
 [0.545]
 [0.449]
 [0.547]
 [0.997]
 [1.049]] [[0.33 ]
 [0.098]
 [0.398]
 [0.466]
 [0.48 ]
 [0.444]
 [0.477]]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.321]
 [0.335]
 [0.342]
 [0.351]
 [0.337]
 [0.347]] [[0.22 ]
 [0.317]
 [0.054]
 [0.101]
 [0.116]
 [0.253]
 [0.354]] [[-0.141]
 [-0.119]
 [-0.265]
 [-0.22 ]
 [-0.192]
 [-0.129]
 [-0.042]]
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.196]
 [0.204]
 [0.208]
 [0.211]
 [0.213]
 [0.203]] [[-0.007]
 [-0.065]
 [-0.133]
 [-0.271]
 [-0.15 ]
 [-0.16 ]
 [-0.069]] [[-0.4  ]
 [-0.435]
 [-0.441]
 [-0.479]
 [-0.434]
 [-0.431]
 [-0.422]]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.24711184617863957, 0.35878712727123874, 0.35878712727123874, 0.03531389927888296]
line 256 mcts: sample exp_bonus 0.018709829077124596
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.2053440457820763, 0.3780137354247132, 0.3780137354247132, 0.038628483368497134]
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.2053440457820763, 0.3780137354247132, 0.3780137354247132, 0.038628483368497134]
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.2053440457820763, 0.3780137354247132, 0.3780137354247132, 0.038628483368497134]
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.2053440457820763, 0.3780137354247132, 0.3780137354247132, 0.038628483368497134]
siam score:  -0.62384814
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.2053440457820763, 0.3780137354247132, 0.3780137354247132, 0.038628483368497134]
main train batch thing paused
main train batch thing paused
Starting evaluation
siam score:  -0.63814294
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.239]] [[ 0.001]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [-0.002]] [[0.246]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.239]]
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -7.683995144521682e-05
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.08449972171218006, 0.08449972171218006, 0.7465008348634598, 0.08449972171218006]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9872417721518988 -1.0 -0.9872417721518988
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.091]
 [0.194]
 [0.194]
 [0.194]
 [0.208]
 [0.201]] [[-0.004]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.002]
 [-0.002]] [[0.207]
 [0.091]
 [0.194]
 [0.194]
 [0.194]
 [0.208]
 [0.201]]
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.134]] [[ 0.002]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [ 0.002]] [[0.161]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.134]]
main train batch thing paused
line 256 mcts: sample exp_bonus -0.0003381834394045948
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.219]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.003]] [[0.221]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.219]]
main train batch thing paused
line 256 mcts: sample exp_bonus 0.17678798282023794
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]
 [1.016]] [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.009]] [[1.882]
 [1.882]
 [1.882]
 [1.882]
 [1.882]
 [1.882]
 [1.94 ]]
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
probs:  [0.32115469008373765, 0.03653592974878709, 0.32115469008373765, 0.32115469008373765]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.4489660874893701, 0.05103391251062992, 0.05103391251062992, 0.4489660874893701]
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]] [[1.26]
 [1.26]
 [1.26]
 [1.26]
 [1.26]
 [1.26]
 [1.26]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.322287080423271, 0.322287080423271, 0.03313875873018706, 0.322287080423271]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.322287080423271, 0.322287080423271, 0.03313875873018706, 0.322287080423271]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.32228708192752703, 0.32228708192752703, 0.03313875421741902, 0.32228708192752703]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
Printing some Q and Qe and total Qs values:  [[0.845]
 [0.804]
 [0.804]
 [0.821]
 [0.825]
 [0.835]
 [0.849]] [[ 0.   ]
 [-0.001]
 [-0.   ]
 [-0.   ]
 [-0.001]
 [-0.002]
 [-0.001]] [[1.656]
 [1.574]
 [1.574]
 [1.607]
 [1.614]
 [1.635]
 [1.662]]
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.751]
 [0.763]
 [0.765]
 [0.763]
 [0.757]
 [0.774]] [[-0.014]
 [-0.011]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.013]
 [-0.013]] [[1.287]
 [1.21 ]
 [1.233]
 [1.238]
 [1.233]
 [1.222]
 [1.255]]
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.702]
 [0.705]
 [0.703]
 [0.703]
 [0.707]
 [0.708]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.006]] [[0.712]
 [0.702]
 [0.705]
 [0.703]
 [0.703]
 [0.707]
 [0.708]]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.408]
 [0.44 ]
 [0.462]
 [0.466]
 [0.465]
 [0.475]] [[-0.005]
 [-0.009]
 [-0.006]
 [-0.006]
 [-0.005]
 [-0.006]
 [-0.006]] [[0.604]
 [0.477]
 [0.546]
 [0.591]
 [0.6  ]
 [0.596]
 [0.616]]
first move QE:  -0.07761988132059486
rdn probs:  [0.45342133609960417, 0.45342133609960417, 0.04657866390039584, 0.04657866390039584]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.4534213419360114, 0.4534213419360114, 0.046578658063988615, 0.046578658063988615]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[-0.014]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[0.776]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[1.336]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]] [[-0.029]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]] [[2.655]
 [1.989]
 [1.989]
 [1.989]
 [1.989]
 [1.989]
 [1.989]]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.5716842811530412, 0.31091695689608945, 0.05869938097543467, 0.05869938097543467]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.358560260475323, 0.358560260475323, 0.2473521887688946, 0.03552729028045939]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.2783032695501336, 0.4034335138770395, 0.2783032695501336, 0.03995994702269322]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.20560793689871415, 0.3777125507683177, 0.3777125507683177, 0.03896696156465043]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.20560793581073886, 0.3777125538983361, 0.3777125538983361, 0.03896695639258892]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.12 ]
 [0.122]
 [0.134]
 [0.135]
 [0.135]
 [0.136]] [[-0.016]
 [-0.016]
 [-0.014]
 [-0.013]
 [-0.011]
 [-0.011]
 [-0.012]] [[0.134]
 [0.12 ]
 [0.122]
 [0.134]
 [0.135]
 [0.135]
 [0.136]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.27832500219324297, 0.27832500219324297, 0.40334570152893584, 0.040004294084578264]
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.256]] [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.008]] [[0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.256]]
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]] [[-0.045]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[1.749]
 [1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.479]]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.31666129109312513, 0.17892872669642795, 0.45890967727332227, 0.04550030493712461]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.36542703902560775, 0.0524915110006269, 0.5295899389731384, 0.0524915110006269]
using explorer policy with actor:  1
siam score:  -0.7091775
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.726]
 [0.722]
 [0.722]
 [0.722]
 [0.729]
 [0.728]] [[-0.015]
 [-0.013]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.013]
 [-0.013]] [[1.046]
 [1.055]
 [1.046]
 [1.046]
 [1.046]
 [1.061]
 [1.058]]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.733]
 [0.716]
 [0.749]
 [0.748]
 [0.745]
 [0.767]] [[-0.015]
 [-0.015]
 [-0.015]
 [-0.016]
 [-0.015]
 [-0.015]
 [-0.014]] [[1.171]
 [1.158]
 [1.125]
 [1.19 ]
 [1.187]
 [1.183]
 [1.228]]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.7426985430651456, 0.08576715231161808, 0.08576715231161808, 0.08576715231161808]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.624]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[-0.052]
 [-0.051]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]] [[0.585]
 [0.924]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.4559948429789938, 0.24840314075209977, 0.047198875516806765, 0.24840314075209977]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.5708916738327077, 0.05906445575686322, 0.05906445575686322, 0.31097941465356604]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.5708916738327077, 0.05906445575686322, 0.05906445575686322, 0.31097941465356604]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.37753892376343556, 0.20575180195962484, 0.03917035051350416, 0.37753892376343556]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.3109914367449757, 0.05915582695927099, 0.05915582695927099, 0.5706969093364824]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.44789445702187125, 0.052105542978128715, 0.052105542978128715, 0.44789445702187125]
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.796]
 [0.795]] [[-0.01 ]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.011]
 [-0.011]] [[1.257]
 [1.255]
 [1.255]
 [1.255]
 [1.255]
 [1.268]
 [1.266]]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.08618580986860348, 0.08618580986860348, 0.08618580986860348, 0.7414425703941895]
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.24845306020626745, 0.047350887021022685, 0.24845306020626745, 0.4557429925664425]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[-0.538]
 [-0.538]
 [-0.538]
 [-0.538]
 [-0.538]
 [-0.538]
 [-0.538]]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
STARTED EXPV TRAINING ON FRAME NO.  11893
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.3221248424043343, 0.3221248424043343, 0.3221248424043343, 0.03362547278699692]
from probs:  [0.3221248424043343, 0.3221248424043343, 0.3221248424043343, 0.03362547278699692]
first move QE:  -0.06950981863071441
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.3773697886373423, 0.3773697886373423, 0.20588656100852984, 0.03937386171678563]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.24850002357776454, 0.4554967698463225, 0.24850002357776454, 0.047503182998148485]
siam score:  -0.8316811
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.3207630508442269, 0.3207630508442269, 0.3207630508442269, 0.037710847467319314]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.3207630508442269, 0.3207630508442269, 0.3207630508442269, 0.037710847467319314]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.3207630537218952, 0.3207630537218952, 0.3207630537218952, 0.03771083883431449]
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.231]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.279]] [[-0.006]
 [-0.006]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.002]] [[0.26 ]
 [0.231]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.279]]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.3225631575879634, 0.3225631575879634, 0.3225631575879634, 0.03231052723610982]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.35823223041615204, 0.35823223041615204, 0.247664052580946, 0.03587148658674985]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.35823223041615204, 0.35823223041615204, 0.247664052580946, 0.03587148658674985]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.32208436168294685, 0.32208436168294685, 0.32208436168294685, 0.03374691495115936]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.3220843631616139, 0.3220843631616139, 0.3220843631616139, 0.03374691051515828]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.010046277174484936
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11893
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.5697409641030483, 0.3110331754840587, 0.05961293020644655, 0.05961293020644655]
in main func line 156:  210
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.7383286785043587, 0.08722377383188047, 0.08722377383188047, 0.08722377383188047]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.32064624390664104, 0.03806126828007694, 0.32064624390664104, 0.32064624390664104]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.32064624390664104, 0.03806126828007694, 0.32064624390664104, 0.32064624390664104]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.0475402117156552, 0.0475402117156552, 0.4524597882843448, 0.4524597882843448]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.045646627200866007, 0.045646627200866007, 0.454353372799134, 0.454353372799134]
siam score:  -0.85287356
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.591]
 [0.59 ]
 [0.589]
 [0.59 ]
 [0.587]
 [0.586]] [[ 0.003]
 [ 0.004]
 [-0.001]
 [-0.001]
 [-0.002]
 [-0.002]
 [-0.003]] [[1.163]
 [1.08 ]
 [1.072]
 [1.071]
 [1.071]
 [1.064]
 [1.062]]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.04068874450953554, 0.13034846782671783, 0.41448139383187327, 0.41448139383187327]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.0033381867509408485
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.003964420701780823
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.02 ]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.027]] [[0.005]
 [0.007]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.004]] [[-0.046]
 [-0.1  ]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.091]]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.06309547352152654, 0.24618562190860246, 0.06309547352152654, 0.6276234310483445]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.497]
 [0.506]
 [0.507]
 [0.507]
 [0.507]
 [0.508]] [[-0.007]
 [-0.001]
 [-0.007]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[0.711]
 [0.701]
 [0.713]
 [0.714]
 [0.715]
 [0.715]
 [0.716]]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.0599787969372851, 0.31104753582557887, 0.0599787969372851, 0.568994870299851]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.0599787969372851, 0.31104753582557887, 0.0599787969372851, 0.568994870299851]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.512]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.608]] [[-0.011]
 [-0.011]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.012]] [[0.941]
 [0.965]
 [1.062]
 [1.062]
 [1.062]
 [1.062]
 [1.157]]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.2486445507871523, 0.45467283113993884, 0.048038067285756526, 0.2486445507871523]
siam score:  -0.8422931
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.2486630110050899, 0.45455931622133244, 0.2486630110050899, 0.04811466176848775]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.84357864
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.32045225179557607, 0.32045225179557607, 0.32045225179557607, 0.038643244613271684]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
using another actor
siam score:  -0.8471519
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.45729446482391595, 0.17974596330542517, 0.3167179510677974, 0.04624162080286146]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.45729446901603543, 0.17974596188467654, 0.3167179524170357, 0.04624161668225227]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11893
siam score:  -0.8504544
UNIT TEST: sample policy line 217 mcts : [0.143 0.082 0.082 0.184 0.143 0.102 0.265]
UNIT TEST: sample policy line 217 mcts : [0.102 0.122 0.163 0.122 0.245 0.082 0.163]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11893
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.717]] [[ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [-0.001]] [[0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]
 [1.241]]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.45864022860194503, 0.35132676419422904, 0.04528466199444876, 0.14474834520937724]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.45864022860194503, 0.35132676419422904, 0.04528466199444876, 0.14474834520937724]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.783]] [[-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.017]] [[0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [1.451]]
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.5122310604569061, 0.27555334097145134, 0.0505634101025641, 0.16165218846907825]
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.5780946444918885, 0.18242715758603792, 0.057051040336035676, 0.18242715758603792]
start point for exploration sampling:  11893
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.5522490954218787, 0.24562203153285, 0.053668503732401014, 0.14846036931287013]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.5677222763751568, 0.06061874166800881, 0.06061874166800881, 0.3110402402888256]
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
probs:  [0.527322926358879, 0.05358323493386908, 0.05358323493386908, 0.3655106037733826]
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.096]
 [0.105]
 [0.104]
 [0.104]
 [0.104]
 [0.104]] [[-0.09 ]
 [-0.073]
 [-0.107]
 [-0.11 ]
 [-0.113]
 [-0.123]
 [-0.127]] [[0.13 ]
 [0.134]
 [0.14 ]
 [0.138]
 [0.137]
 [0.134]
 [0.132]]
from probs:  [0.5675434718305364, 0.0607100958674412, 0.0607100958674412, 0.3110363364345812]
first move QE:  -0.055494350670228815
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
probs:  [0.45378905857843765, 0.04865152894944633, 0.248779706236058, 0.248779706236058]
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
probs:  [0.5673653356572954, 0.06080143451199651, 0.31103179531871156, 0.06080143451199651]
using another actor
from probs:  [0.41582881926000215, 0.22409218495565278, 0.31880546214213973, 0.04127353364220545]
first move QE:  -0.054421844958296765
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.3576947930234416, 0.24808825811200907, 0.3576947930234416, 0.036522155841107753]
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.222]
 [0.27 ]
 [0.272]
 [0.275]
 [0.267]
 [0.26 ]] [[0.481]
 [0.608]
 [0.394]
 [0.404]
 [0.393]
 [0.402]
 [0.398]] [[0.228]
 [0.222]
 [0.27 ]
 [0.272]
 [0.275]
 [0.267]
 [0.26 ]]
from probs:  [0.566834840629563, 0.0610752806454812, 0.0610752806454812, 0.3110145980794746]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.37637417895718156, 0.20658902249562489, 0.040662619590011974, 0.37637417895718156]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.37637417895718156, 0.20658902249562489, 0.040662619590011974, 0.37637417895718156]
siam score:  -0.85805774
from probs:  [0.3575958586780264, 0.24815549956551922, 0.036652783078428015, 0.3575958586780264]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.45637353169131323, 0.3166926346419781, 0.046747530232031834, 0.18018630343467673]
using explorer policy with actor:  1
siam score:  -0.85312706
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.5285409784487416, 0.20866811287534898, 0.05412279580056055, 0.20866811287534898]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.6251888141225597, 0.06399987968428267, 0.06399987968428267, 0.2468114265088749]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.692]
 [0.695]
 [0.693]
 [0.693]
 [0.693]
 [0.694]] [[1.313]
 [2.22 ]
 [1.02 ]
 [0.972]
 [0.992]
 [1.04 ]
 [1.09 ]] [[0.695]
 [0.692]
 [0.695]
 [0.693]
 [0.693]
 [0.693]
 [0.694]]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.4562862980177268, 0.04679845463289814, 0.1802270777582921, 0.31668816959108304]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.4562862980177268, 0.04679845463289814, 0.1802270777582921, 0.31668816959108304]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.51 ]] [[1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.585]] [[1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.087]]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.3109924833394109, 0.06134883060136042, 0.06134883060136042, 0.5663098554578683]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]] [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]] [[0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.31668340196571054, 0.04684942900956087, 0.18026733786010052, 0.45619983116462814]
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.223]
 [0.223]
 [0.196]
 [0.223]
 [0.188]
 [0.223]] [[1.462]
 [1.462]
 [1.462]
 [0.837]
 [1.462]
 [0.829]
 [1.462]] [[0.43 ]
 [0.43 ]
 [0.43 ]
 [0.165]
 [0.43 ]
 [0.147]
 [0.43 ]]
using explorer policy with actor:  0
start point for exploration sampling:  11893
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.20874014415362435, 0.05423515204804844, 0.20874014415362435, 0.5282845596447029]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.20874014415362435, 0.05423515204804844, 0.20874014415362435, 0.5282845596447029]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[1.888]
 [1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]] [[1.625]
 [0.96 ]
 [0.96 ]
 [0.96 ]
 [0.96 ]
 [0.96 ]
 [0.96 ]]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.06412972552293975, 0.06412972552293975, 0.24688485573502014, 0.6248556932191003]
start point for exploration sampling:  11893
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.06412972552293975, 0.06412972552293975, 0.24688485573502014, 0.6248556932191003]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.06412972552293975, 0.06412972552293975, 0.24688485573502014, 0.6248556932191003]
Printing some Q and Qe and total Qs values:  [[-0.031]
 [ 0.016]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]] [[0.652]
 [1.188]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[-0.187]
 [ 0.443]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]]
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.601]
 [0.604]
 [0.607]
 [0.61 ]
 [0.61 ]
 [0.675]] [[0.699]
 [0.581]
 [0.497]
 [0.474]
 [0.471]
 [0.471]
 [0.508]] [[0.659]
 [0.601]
 [0.604]
 [0.607]
 [0.61 ]
 [0.61 ]
 [0.675]]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.04151543410742226, 0.22433206563849797, 0.31878732526288633, 0.4153651749911935]
using another actor
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.142]
 [0.115]
 [0.096]
 [0.102]
 [0.11 ]
 [0.12 ]] [[0.865]
 [1.075]
 [0.939]
 [0.839]
 [0.841]
 [0.922]
 [0.943]] [[0.085]
 [0.142]
 [0.115]
 [0.096]
 [0.102]
 [0.11 ]
 [0.12 ]]
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
probs:  [0.05422241614828625, 0.3654147328383464, 0.05422241614828625, 0.5261404348650812]
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.162326152907937, 0.2759305907407225, 0.05111338745058107, 0.5106298689007596]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9930034482758621 -1.0 -0.9930034482758621
probs:  [0.18042355831001536, 0.3166615106726155, 0.04705377336557498, 0.45586115765179414]
maxi score, test score, baseline:  -0.9930034482758621 -1.0 -0.9930034482758621
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
maxi score, test score, baseline:  -0.9930034482758621 -1.0 -0.9930034482758621
probs:  [0.18042355831001536, 0.3166615106726155, 0.04705377336557498, 0.45586115765179414]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9930506849315068 -1.0 -0.9930506849315068
probs:  [0.18042355677505073, 0.31666151214327326, 0.04705376888826645, 0.45586116219340944]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.55 ]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[0.905]
 [1.493]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]] [[0.356]
 [0.789]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]]
maxi score, test score, baseline:  -0.9930506849315068 -1.0 -0.9930506849315068
maxi score, test score, baseline:  -0.9930506849315068 -1.0 -0.9930506849315068
probs:  [0.04928546204885202, 0.450714537951148, 0.04928546204885202, 0.450714537951148]
maxi score, test score, baseline:  -0.9930506849315068 -1.0 -0.9930506849315068
probs:  [0.04710496555399385, 0.31665537784382547, 0.1804614853184372, 0.4557781712837436]
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.04710496104586042, 0.3166553793248453, 0.1804614837733588, 0.45577817585593544]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
from probs:  [0.05658653812201469, 0.4434134618779853, 0.4434134618779853, 0.05658653812201469]
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.485]] [[2.   ]
 [2.47 ]
 [2.47 ]
 [2.47 ]
 [2.47 ]
 [2.47 ]
 [1.748]] [[1.186]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.081]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.05673284863514599, 0.44326715136485395, 0.44326715136485395, 0.05673284863514599]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.661]
 [0.661]
 [0.664]
 [0.664]
 [0.663]
 [0.954]] [[1.064]
 [1.047]
 [1.047]
 [1.14 ]
 [1.117]
 [1.093]
 [1.009]] [[1.16 ]
 [1.149]
 [1.149]
 [1.216]
 [1.2  ]
 [1.183]
 [1.708]]
start point for exploration sampling:  11893
using explorer policy with actor:  1
siam score:  -0.8366005
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.09243159675084349, 0.7227052097474694, 0.09243159675084349, 0.09243159675084349]
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.09243159675084349, 0.7227052097474694, 0.09243159675084349, 0.09243159675084349]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.996]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[1.062]
 [1.383]
 [1.062]
 [1.062]
 [1.062]
 [1.062]
 [1.062]] [[1.088]
 [2.284]
 [1.088]
 [1.088]
 [1.088]
 [1.088]
 [1.088]]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.3196469990947446, 0.041059002715766346, 0.3196469990947446, 0.3196469990947446]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
from probs:  [0.06216759312600929, 0.06216759312600929, 0.31090129233115543, 0.564763521416826]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.18060908098260275, 0.04731008609943192, 0.3166284635164524, 0.45545236940151307]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.20694412973410045, 0.04147451067299342, 0.3757906797964531, 0.3757906797964531]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.2787200221394974, 0.04170426048301936, 0.40085569523798575, 0.2787200221394974]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.4426837942205702, 0.057316205779429794, 0.4426837942205702, 0.057316205779429794]
rdn beta is 0 so we're just using the maxi policy
289 227
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.336]
 [0.328]
 [0.329]
 [0.336]
 [0.342]
 [0.345]] [[1.991]
 [1.999]
 [2.061]
 [2.084]
 [1.999]
 [2.141]
 [2.145]] [[0.832]
 [0.855]
 [0.901]
 [0.926]
 [0.855]
 [1.008]
 [1.02 ]]
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.20702572898017013, 0.3756485828865485, 0.041677105246732876, 0.3756485828865485]
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.2070257277552241, 0.3756485864680567, 0.041677099308662494, 0.3756485864680567]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.20702572535259708, 0.37564859349288376, 0.041677087661635276, 0.37564859349288376]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.2490153237566358, 0.2490153237566358, 0.05011072259694697, 0.4518586298897813]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.574]] [[2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.099]] [[1.083]
 [1.083]
 [1.083]
 [1.083]
 [1.083]
 [1.083]
 [1.114]]
siam score:  -0.8203792
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.3194574965284648, 0.3194574965284648, 0.04162751041460567, 0.3194574965284648]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[1.035]
 [1.035]
 [1.035]
 [1.035]
 [1.035]
 [1.035]
 [1.035]] [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]]
Printing some Q and Qe and total Qs values:  [[0.623]
 [1.061]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[2.059]
 [1.842]
 [2.059]
 [2.059]
 [2.059]
 [2.059]
 [2.059]] [[1.538]
 [2.342]
 [1.538]
 [1.538]
 [1.538]
 [1.538]
 [1.538]]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.3194574965284648, 0.3194574965284648, 0.04162751041460567, 0.3194574965284648]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.296]
 [0.321]
 [0.321]
 [0.324]
 [0.326]
 [0.321]] [[2.08 ]
 [2.072]
 [1.873]
 [1.841]
 [1.899]
 [1.91 ]
 [1.8  ]] [[1.076]
 [1.027]
 [0.812]
 [0.769]
 [0.852]
 [0.872]
 [0.714]]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.3194574965284648, 0.3194574965284648, 0.04162751041460567, 0.3194574965284648]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.4423932697878292, 0.4423932697878292, 0.0576067302121707, 0.0576067302121707]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.7192299321544796, 0.09359002261517353, 0.09359002261517353, 0.09359002261517353]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.782]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[0.557]
 [1.077]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[0.809]
 [1.754]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]]
using another actor
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.3194196929324126, 0.3194196929324126, 0.041740921202762085, 0.3194196929324126]
line 256 mcts: sample exp_bonus 0.5937743872407817
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.07524478692682605
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.08348784258108004, 0.08348784258108004, 0.7495364722567599, 0.08348784258108004]
from probs:  [0.08348784258108004, 0.08348784258108004, 0.7495364722567599, 0.08348784258108004]
from probs:  [0.20925007941566745, 0.05514128884219353, 0.5263585523264718, 0.20925007941566745]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.06289251038937904, 0.06289251038937904, 0.563419801208961, 0.310795178012281]
from probs:  [0.06289251038937904, 0.06289251038937904, 0.563419801208961, 0.310795178012281]
siam score:  -0.81939054
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08359649710737345, 0.08359649710737345, 0.7492105086778797, 0.08359649710737345]
first move QE:  0.09918044623889256
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.24906335343657782, 0.24906335343657782, 0.4513790111358032, 0.050494281991041216]
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.122]
 [0.089]
 [0.095]
 [0.081]
 [0.059]
 [0.126]] [[1.055]
 [0.974]
 [0.751]
 [0.716]
 [0.74 ]
 [0.612]
 [0.512]] [[0.128]
 [0.122]
 [0.089]
 [0.095]
 [0.081]
 [0.059]
 [0.126]]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.24906335340472815, 0.24906335340472815, 0.4513790179833613, 0.05049427520718251]
start point for exploration sampling:  11893
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
siam score:  -0.8563873
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.06530133851897582, 0.2474228093746843, 0.6219745135873641, 0.06530133851897582]
from probs:  [0.06530133286385599, 0.24742280929577729, 0.6219745249765107, 0.06530133286385599]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.05847374689014554, 0.44152625310985455, 0.44152625310985455, 0.05847374689014554]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.042149042600186315, 0.37532190078518074, 0.37532190078518074, 0.2072071558294523]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.042149042600186315, 0.37532190078518074, 0.37532190078518074, 0.2072071558294523]
siam score:  -0.849245
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.06316353292145774, 0.3107505077003836, 0.5629224264567009, 0.06316353292145774]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.105]
 [0.097]
 [0.105]
 [0.105]
 [0.105]
 [0.105]] [[3.243]
 [3.054]
 [3.29 ]
 [3.054]
 [3.054]
 [3.054]
 [3.054]] [[0.624]
 [0.446]
 [0.665]
 [0.446]
 [0.446]
 [0.446]
 [0.446]]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.08392170335537856, 0.08392170335537856, 0.7482348899338643, 0.08392170335537856]
using another actor
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.20936280235322086, 0.05536921127069145, 0.525905184022867, 0.20936280235322086]
from probs:  [0.3191560579901295, 0.042531826029611454, 0.3191560579901295, 0.3191560579901295]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.682]
 [0.703]
 [0.678]
 [0.703]
 [0.688]
 [0.696]] [[1.824]
 [1.687]
 [1.824]
 [1.518]
 [1.824]
 [1.485]
 [1.47 ]] [[1.37 ]
 [1.191]
 [1.37 ]
 [1.015]
 [1.37 ]
 [1.002]
 [1.002]]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.4412387927668459, 0.058761207233154075, 0.058761207233154075, 0.4412387927668459]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.4412387927668459, 0.058761207233154075, 0.058761207233154075, 0.4412387927668459]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.44123880449853087, 0.0587611955014691, 0.0587611955014691, 0.44123880449853087]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.4410953199716114, 0.4410953199716114, 0.05890468002838855, 0.05890468002838855]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.2491070733791328, 0.4509084896951965, 0.05087736354653809, 0.2491070733791328]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.2491070733791328, 0.4509084896951965, 0.05087736354653809, 0.2491070733791328]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.31070351744092434, 0.562428195269345, 0.0634341436448653, 0.0634341436448653]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.3568119463461948, 0.3568119463461948, 0.03778771893333219, 0.24858838837427816]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.543]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[1.533]
 [1.395]
 [1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]] [[0.888]
 [0.976]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.3219505457248223, 0.3219505457248223, 0.03414836282553317, 0.3219505457248223]
UNIT TEST: sample policy line 217 mcts : [0.143 0.714 0.02  0.02  0.041 0.02  0.041]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.35678374938211466, 0.24860108625263572, 0.037831414983134866, 0.35678374938211466]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.35678374938211466, 0.24860108625263572, 0.037831414983134866, 0.35678374938211466]
actor:  1 policy actor:  1  step number:  60 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.035]
 [-0.036]
 [-0.036]
 [-0.035]
 [-0.037]
 [-0.036]] [[1.188]
 [1.169]
 [1.186]
 [1.171]
 [1.094]
 [1.145]
 [1.11 ]] [[-0.866]
 [-0.882]
 [-0.877]
 [-0.882]
 [-0.908]
 [-0.893]
 [-0.904]]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.07878043336142021, 0.07878043336142021, 0.07046684881182246, 0.771972284465337]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
start point for exploration sampling:  11893
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.0787805142038133, 0.0787805142038133, 0.07046683732551005, 0.7719721342668633]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.07490123668539335, 0.07911188520574211, 0.0707631855533264, 0.7752236925555381]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.06994012301413362, 0.08598109069605976, 0.07782232265094215, 0.7662564636388645]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.06994012301413362, 0.08598109069605976, 0.07782232265094215, 0.7662564636388645]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.06994012301413362, 0.08598109069605976, 0.07782232265094215, 0.7662564636388645]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.06994012301413362, 0.08598109069605976, 0.07782232265094215, 0.7662564636388645]
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.197]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.255]
 [0.361]] [[0.552]
 [1.166]
 [0.535]
 [0.535]
 [0.535]
 [0.759]
 [0.874]] [[1.358]
 [1.922]
 [1.337]
 [1.337]
 [1.337]
 [1.584]
 [1.874]]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.06994012301413362, 0.08598109069605976, 0.07782232265094215, 0.7662564636388645]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.398]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]] [[1.095]
 [1.398]
 [1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.095]] [[1.5  ]
 [1.882]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.5  ]]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.07018264548355893, 0.08674689264419776, 0.07421752620217609, 0.7688529356700672]
siam score:  -0.854908
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.07016422695132687, 0.08696187801855902, 0.07425596246770393, 0.7686179325624102]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.037]
 [-0.064]
 [-0.053]
 [-0.064]
 [-0.052]
 [-0.055]] [[1.178]
 [1.184]
 [1.187]
 [1.043]
 [1.187]
 [0.855]
 [0.851]] [[1.148]
 [1.18 ]
 [1.137]
 [0.995]
 [1.137]
 [0.788]
 [0.78 ]]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.07016422695132687, 0.08696187801855902, 0.07425596246770393, 0.7686179325624102]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.07016422695132687, 0.08696187801855902, 0.07425596246770393, 0.7686179325624102]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.07016422695132687, 0.08696187801855902, 0.07425596246770393, 0.7686179325624102]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.0704521458608314, 0.08731884412314028, 0.0704521458608314, 0.7717768641551969]
Printing some Q and Qe and total Qs values:  [[ 0.005]
 [ 0.005]
 [ 0.005]
 [ 0.005]
 [ 0.005]
 [ 0.005]
 [-0.017]] [[1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.468]] [[0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [1.17 ]]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.07043750782400586, 0.08754214790637364, 0.07043750782400586, 0.7715828364456148]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]] [[1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]] [[1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]]
from probs:  [0.07042270535350051, 0.08776796013136241, 0.07042270535350051, 0.7713866291616366]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.0703925833777141, 0.08822746975758532, 0.0703925833777141, 0.7709873634869865]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.0703925833777141, 0.08822746975758532, 0.0703925833777141, 0.7709873634869865]
Starting evaluation
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.909631295163024
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.0703925833777141, 0.08822746975758532, 0.0703925833777141, 0.7709873634869865]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.07037727038641937, 0.08846106996493507, 0.07037727038641937, 0.7707843892622263]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.032]
 [ 0.054]
 [ 0.037]
 [ 0.054]
 [ 0.054]
 [ 0.041]] [[ 0.869]
 [-0.561]
 [ 0.   ]
 [-1.034]
 [ 0.   ]
 [ 0.   ]
 [-0.721]] [[-0.001]
 [ 0.032]
 [ 0.054]
 [ 0.037]
 [ 0.054]
 [ 0.054]
 [ 0.041]]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.128]
 [0.163]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.155]] [[1.105]
 [1.017]
 [0.78 ]
 [0.   ]
 [0.   ]
 [0.   ]
 [1.008]] [[0.134]
 [0.128]
 [0.163]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.155]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.32593778716018706
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.07037723293141249, 0.08846163477694788, 0.07037723293141249, 0.7707838993602271]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.07037722072500617, 0.0884618188465736, 0.07037722072500617, 0.7707837397034141]
line 256 mcts: sample exp_bonus 0.8493096089334109
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.583]] [[1.783]
 [1.783]
 [1.783]
 [1.783]
 [1.783]
 [1.783]
 [1.783]]
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.07036167057792567, 0.08869902625979946, 0.07036167057792567, 0.7705776325843493]
Printing some Q and Qe and total Qs values:  [[-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]] [[0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]] [[-0.509]
 [-0.509]
 [-0.509]
 [-0.509]
 [-0.509]
 [-0.509]
 [-0.509]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.07029783434703105, 0.08967284061516688, 0.07029783434703105, 0.769731490690771]
Printing some Q and Qe and total Qs values:  [[1.224]
 [1.224]
 [1.224]
 [1.224]
 [1.224]
 [1.224]
 [1.391]] [[0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.351]] [[2.943]
 [2.943]
 [2.943]
 [2.943]
 [2.943]
 [2.943]
 [3.274]]
Printing some Q and Qe and total Qs values:  [[1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.396]] [[0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.545]] [[2.77 ]
 [2.77 ]
 [2.77 ]
 [2.77 ]
 [2.77 ]
 [2.77 ]
 [3.192]]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]] [[0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]] [[0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]]
from probs:  [0.07026475074396103, 0.0901775229988772, 0.07026475074396103, 0.7692929755132007]
from probs:  [0.07058882734030306, 0.08609325216362142, 0.07058882734030306, 0.7727290931557725]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.519]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[0.935]
 [1.046]
 [1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.337]] [[1.834]
 [1.979]
 [2.23 ]
 [2.23 ]
 [2.23 ]
 [2.23 ]
 [2.23 ]]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.574]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]] [[1.381]
 [1.646]
 [1.381]
 [1.381]
 [1.381]
 [1.381]
 [1.381]] [[0.587]
 [0.574]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.07056391338211661, 0.08649708071763514, 0.07056391338211661, 0.7723750925181315]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.07055122749411102, 0.08670270649395366, 0.07055122749411102, 0.7721948385178242]
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.161]
 [0.157]
 [0.157]
 [0.156]
 [0.156]
 [0.157]] [[0.902]
 [0.358]
 [0.369]
 [0.47 ]
 [0.488]
 [0.534]
 [0.581]] [[0.145]
 [0.161]
 [0.157]
 [0.157]
 [0.156]
 [0.156]
 [0.157]]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.07055121804069327, 0.08670285743397418, 0.07055121804069327, 0.7721947064846393]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.07055119942136377, 0.08670315472363238, 0.07055119942136377, 0.7721944464336401]
siam score:  -0.8514018
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.0705511902525521, 0.08670330111956968, 0.0705511902525521, 0.7721943183753261]
rdn probs:  [0.0705511902525521, 0.08670330111956968, 0.0705511902525521, 0.7721943183753261]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.07093923879045652, 0.08167340462125705, 0.07093923879045652, 0.77644811779783]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.07093166163169026, 0.08181356431797546, 0.07093166163169026, 0.7763231124186439]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.07055034299468621, 0.08674963398272838, 0.07055034299468621, 0.7721496800278993]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.07568906164559713, 0.0871437087014263, 0.07010492120588047, 0.7670623084470961]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.07568906164559713, 0.0871437087014263, 0.07010492120588047, 0.7670623084470961]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.07625343269833545, 0.08211932979693773, 0.0704853005513767, 0.7711419369533502]
siam score:  -0.8530819
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.0763172733919334, 0.08226178314434954, 0.07047183880205757, 0.7709491046616596]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.07086096026638618, 0.0832058311754441, 0.07086096026638618, 0.7750722482917836]
from probs:  [0.07086096026638618, 0.0832058311754441, 0.07086096026638618, 0.7750722482917836]
line 256 mcts: sample exp_bonus 1.2443465267251237
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.06993024485940434, 0.0891450598279737, 0.07622843421021322, 0.7646962611024087]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.06993024485940434, 0.0891450598279737, 0.07622843421021322, 0.7646962611024087]
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.934]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]] [[1.731]
 [2.974]
 [1.731]
 [1.731]
 [1.731]
 [1.731]
 [1.731]] [[0.973]
 [1.691]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.0690629152012577, 0.09427031513291821, 0.08145655350099075, 0.7552102161648334]
from probs:  [0.0690629152012577, 0.09427031513291821, 0.08145655350099075, 0.7552102161648334]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.06899980362811206, 0.0948627332728962, 0.08171574403679761, 0.7544217190621941]
Printing some Q and Qe and total Qs values:  [[1.312]
 [1.389]
 [1.327]
 [1.327]
 [1.327]
 [1.327]
 [1.327]] [[0.606]
 [1.174]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]] [[1.869]
 [2.592]
 [1.96 ]
 [1.96 ]
 [1.96 ]
 [1.96 ]
 [1.96 ]]
362 298
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.06857514528042093, 0.10057997123988115, 0.08105967243154913, 0.7497852110481488]
Printing some Q and Qe and total Qs values:  [[1.167]
 [1.375]
 [1.167]
 [1.167]
 [1.167]
 [1.167]
 [1.167]] [[1.597]
 [2.521]
 [1.597]
 [1.597]
 [1.597]
 [1.597]
 [1.597]] [[1.359]
 [2.057]
 [1.359]
 [1.359]
 [1.359]
 [1.359]
 [1.359]]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.06857514528042093, 0.10057997123988115, 0.08105967243154913, 0.7497852110481488]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.06846229547086248, 0.10170629374911543, 0.0814302022372554, 0.7484012085427666]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.0683450249336404, 0.10287674178589931, 0.08181524836691828, 0.7469629849135421]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.0683450249336404, 0.10287674178589931, 0.08181524836691828, 0.7469629849135421]
line 256 mcts: sample exp_bonus 2.4344323947711737
first move QE:  0.2585486453406079
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.06739311879377341, 0.1086086213884344, 0.08748993410852549, 0.7365083257092667]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.441]
 [0.321]
 [0.489]
 [0.486]
 [0.335]
 [0.458]] [[1.326]
 [1.307]
 [0.77 ]
 [0.852]
 [0.824]
 [0.587]
 [0.985]] [[1.704]
 [1.7  ]
 [1.219]
 [1.444]
 [1.421]
 [1.111]
 [1.502]]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
367 310
UNIT TEST: sample policy line 217 mcts : [0.184 0.265 0.082 0.163 0.143 0.082 0.082]
first move QE:  0.26620965499632265
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.06761976058173925, 0.11179794334729153, 0.08186299983402097, 0.7387192962369483]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.06731637031622693, 0.11495790731719574, 0.08267621011435336, 0.735049512252224]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.396]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.474]
 [0.49 ]] [[4.888]
 [4.744]
 [4.888]
 [4.888]
 [4.888]
 [5.368]
 [4.888]] [[1.438]
 [1.252]
 [1.438]
 [1.438]
 [1.438]
 [1.734]
 [1.438]]
first move QE:  0.2736722467701493
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.0663737322003387, 0.12014080805883803, 0.0886612554484753, 0.7248242042923478]
from probs:  [0.0663737322003387, 0.12014080805883803, 0.0886612554484753, 0.7248242042923478]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.06691289570443595, 0.11298357281346916, 0.0893817915076118, 0.7307217399744831]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.06600987954980746, 0.11804949694373466, 0.09501556793330769, 0.7209250555731502]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.06600987954980746, 0.11804949694373466, 0.09501556793330769, 0.7209250555731502]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.0669958874268679, 0.11981425002697865, 0.08147834168818861, 0.7317115208579649]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.06682669941562497, 0.12163359060889084, 0.0818543953879721, 0.7296853145875121]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.0672232781203027, 0.11598566343491554, 0.082953079834694, 0.7338379786100878]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.49418678412775713
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
probs:  [0.06751603039645956, 0.11988017955196566, 0.07589429426134066, 0.7367094957902341]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.019]
 [-0.   ]
 [-0.029]
 [-0.   ]
 [-0.   ]
 [-0.018]] [[5.052]
 [5.153]
 [4.853]
 [7.083]
 [4.853]
 [4.853]
 [5.461]] [[0.513]
 [0.542]
 [0.459]
 [1.129]
 [0.459]
 [0.459]
 [0.637]]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.06808566339725221, 0.12089362219129288, 0.06808566339725221, 0.7429350510142028]
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]] [[3.392]
 [3.499]
 [3.499]
 [3.499]
 [3.499]
 [3.499]
 [3.499]] [[2.13 ]
 [2.065]
 [2.065]
 [2.065]
 [2.065]
 [2.065]
 [2.065]]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[3.613]
 [3.411]
 [3.411]
 [3.411]
 [3.411]
 [3.411]
 [3.411]] [[2.054]
 [1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]]
using explorer policy with actor:  1
siam score:  -0.8206884
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.06795164323221208, 0.12283865592209837, 0.06795164323221208, 0.7412580576134774]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
Printing some Q and Qe and total Qs values:  [[-0.12 ]
 [-0.143]
 [-0.143]
 [-0.111]
 [-0.143]
 [-0.143]
 [-0.11 ]] [[3.089]
 [3.618]
 [3.618]
 [1.822]
 [3.618]
 [3.618]
 [2.407]] [[ 0.16 ]
 [ 0.373]
 [ 0.373]
 [-0.413]
 [ 0.373]
 [ 0.373]
 [-0.14 ]]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
actor:  1 policy actor:  1  step number:  64 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06771390261625009, 0.12628898034823163, 0.06771390261625009, 0.7382832144192683]
line 256 mcts: sample exp_bonus 7.1046447329707885
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.05298706331115385, 0.5796925261683014, 0.05298706331115385, 0.3143333472093909]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[5.863]
 [3.471]
 [3.471]
 [3.471]
 [3.471]
 [3.471]
 [3.471]] [[1.584]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.168]
 [0.27 ]
 [0.266]
 [0.266]
 [0.269]
 [0.273]] [[5.298]
 [1.94 ]
 [1.935]
 [1.843]
 [1.843]
 [1.864]
 [1.886]] [[ 1.252]
 [-0.121]
 [-0.016]
 [-0.053]
 [-0.052]
 [-0.041]
 [-0.03 ]]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[3.993]
 [2.92 ]
 [2.92 ]
 [2.92 ]
 [2.92 ]
 [2.92 ]
 [2.92 ]] [[1.402]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.05275874803312576, 0.577120661009321, 0.05275874803312576, 0.3173618429244273]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.025]
 [-0.006]
 [-0.006]
 [-0.007]
 [-0.003]
 [-0.   ]] [[1.63 ]
 [1.522]
 [1.455]
 [1.467]
 [1.495]
 [1.251]
 [1.314]] [[ 0.122]
 [ 0.012]
 [ 0.005]
 [ 0.012]
 [ 0.03 ]
 [-0.126]
 [-0.079]]
siam score:  -0.8076605
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.05594229892941651, 0.5716082427316429, 0.05225595440940918, 0.32019350392953133]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.05580429477315899, 0.5689665309524673, 0.05202128420858718, 0.32320789006578665]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.05580429477315899, 0.5689665309524673, 0.05202128420858718, 0.32320789006578665]
using another actor
from probs:  [0.05580429477315899, 0.5689665309524673, 0.05202128420858718, 0.32320789006578665]
390 345
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.499]
 [0.563]
 [0.563]
 [0.563]
 [0.577]
 [0.595]] [[1.659]
 [1.742]
 [1.714]
 [1.67 ]
 [1.68 ]
 [1.751]
 [1.853]] [[1.877]
 [1.197]
 [1.306]
 [1.277]
 [1.284]
 [1.359]
 [1.464]]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.182]
 [0.253]
 [0.248]
 [0.258]
 [0.262]
 [0.263]] [[2.048]
 [1.79 ]
 [1.806]
 [1.62 ]
 [1.767]
 [1.597]
 [1.628]] [[0.244]
 [0.182]
 [0.253]
 [0.248]
 [0.258]
 [0.262]
 [0.263]]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.538]
 [0.632]
 [0.637]
 [0.64 ]
 [0.643]
 [0.647]] [[-0.153]
 [ 1.276]
 [-0.207]
 [-0.294]
 [-0.204]
 [-0.043]
 [ 0.114]] [[1.646]
 [2.22 ]
 [1.626]
 [1.591]
 [1.64 ]
 [1.724]
 [1.807]]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
from probs:  [0.050804580197298485, 0.5554169104103371, 0.054816611258921503, 0.33896189813344296]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.0505635611595208, 0.552701043404807, 0.05467843132214105, 0.34205696411353115]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.0505635611595208, 0.552701043404807, 0.05467843132214105, 0.34205696411353115]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.329]
 [0.308]
 [0.304]
 [0.304]
 [0.304]
 [0.305]] [[0.281]
 [1.148]
 [0.548]
 [0.392]
 [0.672]
 [0.536]
 [0.671]] [[0.282]
 [0.329]
 [0.308]
 [0.304]
 [0.304]
 [0.304]
 [0.305]]
siam score:  -0.8061598
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.05053451260189079, 0.5523052601254775, 0.05053451260189079, 0.34662571467074094]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.05074180515891729, 0.554575710644289, 0.05074180515891729, 0.34394067903787634]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.007]
 [0.014]] [[1.915]
 [1.317]
 [1.317]
 [1.317]
 [1.317]
 [1.427]
 [1.572]] [[0.018]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.007]
 [0.014]]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.523]
 [0.469]
 [0.472]
 [0.466]
 [0.472]
 [0.471]] [[1.885]
 [1.883]
 [1.776]
 [1.766]
 [1.789]
 [1.779]
 [1.848]] [[-0.131]
 [-0.022]
 [-0.167]
 [-0.165]
 [-0.168]
 [-0.159]
 [-0.138]]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.05074196061045924, 0.5545774109401983, 0.05074196061045924, 0.34393866783888316]
actor:  1 policy actor:  1  step number:  39 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]] [[1.216]
 [1.216]
 [1.216]
 [1.216]
 [1.216]
 [1.216]
 [1.216]]
Printing some Q and Qe and total Qs values:  [[1.164]
 [0.948]
 [0.948]
 [0.948]
 [0.948]
 [0.948]
 [0.948]] [[1.601]
 [1.364]
 [1.364]
 [1.364]
 [1.364]
 [1.364]
 [1.364]] [[1.819]
 [1.317]
 [1.317]
 [1.317]
 [1.317]
 [1.317]
 [1.317]]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.398]
 [0.436]
 [0.443]
 [0.439]
 [0.441]
 [0.443]] [[4.959]
 [4.003]
 [4.574]
 [4.6  ]
 [6.378]
 [5.053]
 [5.528]] [[1.014]
 [0.473]
 [0.809]
 [0.828]
 [1.776]
 [1.069]
 [1.325]]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.04788170920849922, 0.25479246842031605, 0.5246342242464095, 0.17269159812477516]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.04796340475517808, 0.25522808068169894, 0.5255313954333686, 0.17127711912975446]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.04796340475517808, 0.25522808068169894, 0.5255313954333686, 0.17127711912975446]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.042251475872156635, 0.4630206049681537, 0.3682895492020534, 0.12643836995763616]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.041838768168158846, 0.45845699298735315, 0.37358988094825296, 0.12611435789623493]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.041838768168158846, 0.45845699298735315, 0.37358988094825296, 0.12611435789623493]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.041770484141396835, 0.45769942490096904, 0.3761556930277182, 0.12437439792991595]
using another actor
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]] [[1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]] [[0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]]
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[2.118]
 [2.118]
 [2.118]
 [2.118]
 [2.118]
 [2.118]
 [2.118]] [[1.248]
 [1.248]
 [1.248]
 [1.248]
 [1.248]
 [1.248]
 [1.248]]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.04159930900039845, 0.45580404588366197, 0.3771578646272376, 0.12543878048870188]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.04164796021989412, 0.4563385185424726, 0.37760009304649145, 0.12441342819114184]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.463]
 [0.464]
 [0.464]
 [0.465]
 [0.465]
 [0.464]] [[0.637]
 [2.369]
 [0.611]
 [0.682]
 [0.858]
 [0.826]
 [0.705]] [[0.466]
 [0.463]
 [0.464]
 [0.464]
 [0.465]
 [0.465]
 [0.464]]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.476]
 [0.542]
 [0.544]
 [0.544]
 [0.545]
 [0.546]] [[-1.514]
 [ 1.176]
 [-1.644]
 [-1.678]
 [-1.727]
 [-1.705]
 [-1.618]] [[0.831]
 [1.76 ]
 [0.781]
 [0.77 ]
 [0.752]
 [0.76 ]
 [0.793]]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.04164186760731806, 0.4562641087850289, 0.376915336146218, 0.125178687461435]
Printing some Q and Qe and total Qs values:  [[1.44 ]
 [1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]] [[0.511]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]] [[1.486]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]]
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.0421077984767539, 0.46138264300400217, 0.3699278466286099, 0.12658171189063397]
siam score:  -0.81515723
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.04241259164127414, 0.46473098539184293, 0.36535671509997514, 0.12749970786690779]
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.04240164835667521, 0.46460311786001807, 0.3647261093552807, 0.1282691244280259]
using another actor
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.042431127582142014, 0.4649115229577094, 0.3639087612491781, 0.12874858821097052]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.679]
 [0.677]
 [0.663]
 [0.66 ]
 [0.657]
 [0.643]] [[0.654]
 [1.364]
 [0.757]
 [0.577]
 [0.64 ]
 [0.89 ]
 [1.23 ]] [[0.667]
 [1.606]
 [0.841]
 [0.591]
 [0.663]
 [0.971]
 [1.37 ]]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.042431127582142014, 0.4649115229577094, 0.3639087612491781, 0.12874858821097052]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.042482769228020245, 0.4654788040878478, 0.36435137665103456, 0.12768705003309735]
actor:  1 policy actor:  1  step number:  29 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.033430631493176176, 0.36604038531263605, 0.2837686954072902, 0.3167602877868976]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.033356559478815516, 0.3652205891008674, 0.28273206366016623, 0.3186907877601508]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.493]
 [0.516]
 [0.501]
 [0.516]
 [0.511]
 [0.523]] [[1.422]
 [1.438]
 [0.47 ]
 [1.422]
 [0.462]
 [0.726]
 [0.484]] [[0.501]
 [0.493]
 [0.516]
 [0.501]
 [0.516]
 [0.511]
 [0.523]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
actor:  1 policy actor:  1  step number:  31 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]] [[0.77 ]
 [2.434]
 [2.434]
 [2.434]
 [2.434]
 [2.434]
 [2.434]] [[0.391]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.044429143896277126, 0.48726012741142183, 0.22102489880621304, 0.24728582988608805]
actor:  1 policy actor:  1  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.050038693550732025, 0.5490889095442855, 0.19054473272277953, 0.21032766418220292]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.050038693550732025, 0.5490889095442855, 0.19054473272277953, 0.21032766418220292]
Printing some Q and Qe and total Qs values:  [[0.897]
 [1.391]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]] [[0.792]
 [0.318]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[1.234]
 [1.906]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.046249116403205115, 0.5074087851069565, 0.1780266214110364, 0.26831547707880216]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.541]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[0.568]
 [1.103]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[0.34 ]
 [0.584]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]]
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.362]
 [0.399]
 [0.399]
 [0.397]
 [0.395]
 [0.391]] [[5.261]
 [4.554]
 [5.085]
 [5.256]
 [5.244]
 [5.086]
 [5.14 ]] [[1.124]
 [0.568]
 [0.995]
 [1.108]
 [1.097]
 [0.987]
 [1.016]]
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.595]
 [0.433]
 [0.43 ]
 [0.595]
 [0.508]
 [0.595]] [[2.457]
 [2.456]
 [2.361]
 [2.234]
 [2.456]
 [2.415]
 [2.456]] [[0.484]
 [0.712]
 [0.326]
 [0.235]
 [0.712]
 [0.511]
 [0.712]]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.046009487289417186, 0.5047683586709666, 0.17903805817983215, 0.27018409585978403]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.046009487289417186, 0.5047683586709666, 0.17903805817983215, 0.27018409585978403]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.8204856
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.04542692816995374, 0.49834643540016693, 0.18254591878558307, 0.27368071764429625]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5871],
        [-0.3271],
        [-0.5858],
        [-0.5266],
        [-0.6159],
        [-0.3060],
        [-0.4831],
        [-0.0000],
        [-0.0000],
        [-0.5864]], dtype=torch.float64)
-0.032346567066 -0.6194085790413908
-0.09703970119800001 -0.4241483344229324
-0.032346567066 -0.6181694580810766
-0.032346567066 -0.5589159318179904
-0.032346567066 -0.6482499688376312
-0.032346567066 -0.3382995090916754
-0.045026434398 -0.5281370461940388
-0.8118000000000001 -0.8118000000000001
-0.9389570813039999 -0.9389570813039999
-0.032346567066 -0.6187759595131366
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]] [[3.809]
 [3.809]
 [3.809]
 [3.809]
 [3.809]
 [3.809]
 [3.809]] [[1.483]
 [1.483]
 [1.483]
 [1.483]
 [1.483]
 [1.483]
 [1.483]]
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.85 ]
 [0.781]
 [0.781]
 [0.85 ]
 [0.85 ]
 [0.85 ]] [[2.541]
 [2.541]
 [3.669]
 [3.45 ]
 [2.541]
 [2.541]
 [2.541]] [[0.824]
 [0.824]
 [1.437]
 [1.293]
 [0.824]
 [0.824]
 [0.824]]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.873]
 [0.542]
 [0.537]
 [0.535]
 [0.873]
 [0.519]] [[1.8  ]
 [1.244]
 [1.444]
 [1.307]
 [1.298]
 [1.244]
 [1.419]] [[1.391]
 [1.462]
 [1.   ]
 [0.852]
 [0.84 ]
 [1.462]
 [0.929]]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.04537739705295666, 0.497799154350762, 0.181828971704908, 0.27499447689137335]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.822]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.679]] [[1.079]
 [1.109]
 [1.009]
 [1.009]
 [1.009]
 [1.009]
 [0.818]] [[2.026]
 [2.184]
 [1.671]
 [1.671]
 [1.671]
 [1.671]
 [1.606]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.04537739705295666, 0.497799154350762, 0.181828971704908, 0.27499447689137335]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.04544386753262803, 0.49853010522311825, 0.18062783692474982, 0.2753981903195038]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.04532774117115527, 0.4972505056523413, 0.1811144318374048, 0.2763073213390986]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.0450965693758524, 0.49470321728220973, 0.18208309233892292, 0.278117121003015]
siam score:  -0.8109219
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
actor:  1 policy actor:  1  step number:  50 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  0.333
in main func line 156:  426
Printing some Q and Qe and total Qs values:  [[-0.034]
 [-0.043]
 [-0.034]
 [-0.031]
 [-0.033]
 [-0.03 ]
 [-0.031]] [[2.901]
 [3.1  ]
 [2.804]
 [2.808]
 [2.877]
 [2.811]
 [2.847]] [[-0.428]
 [-0.313]
 [-0.492]
 [-0.482]
 [-0.442]
 [-0.478]
 [-0.456]]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.04856130058724171, 0.5328811857875858, 0.1675651089990746, 0.2509924046260978]
actor:  1 policy actor:  1  step number:  37 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.043003078204092535, 0.47174957861639283, 0.2618540028614868, 0.22339334031802796]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.043084555633198775, 0.47264566910863864, 0.26045221823772846, 0.2238175570204341]
actor:  1 policy actor:  1  step number:  57 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.42875317999305307
from probs:  [0.04473050581350014, 0.49077253267842225, 0.24846545386626676, 0.21603150764181095]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 4.044087040349234
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6370610733021518
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.04132700555342107, 0.4533356233150304, 0.3057054691809536, 0.19963190195059483]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.638]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[1.453]
 [1.   ]
 [1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.103]] [[1.62 ]
 [1.042]
 [1.176]
 [1.176]
 [1.176]
 [1.176]
 [1.176]]
line 256 mcts: sample exp_bonus 2.633520075771384
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.041344587779127695, 0.4535273699427171, 0.3053771396317306, 0.19975090264642453]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.939]
 [1.322]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]] [[0.889]
 [0.848]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]] [[1.631]
 [2.37 ]
 [1.631]
 [1.631]
 [1.631]
 [1.631]
 [1.631]]
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.626]
 [0.65 ]
 [0.689]
 [0.686]
 [0.68 ]
 [0.663]] [[2.063]
 [1.709]
 [1.699]
 [1.484]
 [1.503]
 [1.791]
 [2.028]] [[1.364]
 [1.137]
 [1.178]
 [1.113]
 [1.118]
 [1.299]
 [1.423]]
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.739]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.804]] [[3.163]
 [3.061]
 [3.163]
 [3.163]
 [3.163]
 [3.163]
 [2.662]] [[1.1  ]
 [0.989]
 [1.1  ]
 [1.1  ]
 [1.1  ]
 [1.1  ]
 [0.853]]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.04144229351186832, 0.4545970622107579, 0.30269163029977814, 0.2012690139775956]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.624]
 [0.569]
 [0.569]
 [0.571]
 [0.582]
 [0.559]] [[1.028]
 [1.143]
 [0.926]
 [0.545]
 [0.57 ]
 [0.643]
 [0.895]] [[1.093]
 [1.397]
 [0.996]
 [0.489]
 [0.526]
 [0.644]
 [0.936]]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.04105691699414074, 0.4503520301592952, 0.3062584536027541, 0.20233259924380984]
Printing some Q and Qe and total Qs values:  [[0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]] [[1.18]
 [1.18]
 [1.18]
 [1.18]
 [1.18]
 [1.18]
 [1.18]] [[0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.04105691699414074, 0.4503520301592952, 0.3062584536027541, 0.20233259924380984]
Printing some Q and Qe and total Qs values:  [[1.021]
 [1.171]
 [1.021]
 [1.021]
 [1.021]
 [1.021]
 [1.021]] [[0.789]
 [1.085]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]] [[0.512]
 [0.987]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.0410361963112697, 0.4501225025021726, 0.30563677252876725, 0.2032045286577904]
from probs:  [0.0410361963112697, 0.4501225025021726, 0.30563677252876725, 0.2032045286577904]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.04113379967211996, 0.4511943056239047, 0.3059013172067438, 0.20177057749723162]
437 448
using another actor
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.008]
 [0.057]
 [0.058]
 [0.057]
 [0.061]
 [0.065]] [[-0.794]
 [ 1.139]
 [-0.035]
 [-0.008]
 [-0.496]
 [-0.061]
 [ 0.124]] [[0.067]
 [0.008]
 [0.057]
 [0.058]
 [0.057]
 [0.061]
 [0.065]]
438 450
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.313]
 [0.301]
 [0.311]
 [0.341]
 [0.44 ]
 [0.634]] [[0.587]
 [0.517]
 [0.455]
 [0.459]
 [0.46 ]
 [0.547]
 [0.776]] [[0.563]
 [0.5  ]
 [0.435]
 [0.457]
 [0.518]
 [0.774]
 [1.315]]
using another actor
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.04106925665962243, 0.45047620608987254, 0.3031603195039916, 0.20529421774651335]
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.808]
 [0.724]
 [0.719]
 [0.524]
 [0.7  ]
 [0.689]] [[0.411]
 [0.583]
 [0.353]
 [0.261]
 [1.049]
 [0.355]
 [0.483]] [[1.303]
 [1.476]
 [1.153]
 [1.084]
 [1.218]
 [1.108]
 [1.172]]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.04106925665962243, 0.45047620608987254, 0.3031603195039916, 0.20529421774651335]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[0.405]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.04114990558315336, 0.45136317449339713, 0.30178861520378303, 0.20569830471966666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.041229898286108735, 0.4522429232445134, 0.3004283058889566, 0.2060988725804212]
line 256 mcts: sample exp_bonus 2.434238469574053
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]] [[0.421]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]]
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.041269859244728875, 0.4526824096742687, 0.3007202208237932, 0.20532751025720933]
Printing some Q and Qe and total Qs values:  [[-0.117]
 [-0.126]
 [-0.122]
 [-0.135]
 [-0.137]
 [-0.131]
 [-0.11 ]] [[3.368]
 [3.298]
 [3.269]
 [3.183]
 [3.101]
 [3.243]
 [4.011]] [[-0.236]
 [-0.3  ]
 [-0.312]
 [-0.395]
 [-0.454]
 [-0.347]
 [ 0.206]]
using explorer policy with actor:  0
from probs:  [0.04114624748072496, 0.45131961717348296, 0.300905966110868, 0.20662816923492403]
using explorer policy with actor:  1
siam score:  -0.79820985
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.196]] [[0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.585]] [[0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.196]]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.05 ]
 [0.114]
 [0.114]
 [0.11 ]
 [0.115]
 [0.109]] [[1.191]
 [1.827]
 [1.122]
 [1.154]
 [1.079]
 [1.312]
 [1.43 ]] [[0.106]
 [0.05 ]
 [0.114]
 [0.114]
 [0.11 ]
 [0.115]
 [0.109]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
from probs:  [0.041002746961947036, 0.44973643821396964, 0.304368680538445, 0.20489213428563835]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.04080340108799081, 0.44754077664598907, 0.3058914084673763, 0.20576441379864377]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[1.662]
 [1.662]
 [1.662]
 [1.662]
 [1.662]
 [1.662]
 [1.662]] [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.04080340108799081, 0.44754077664598907, 0.3058914084673763, 0.20576441379864377]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.040922502609663576, 0.4488506061749714, 0.30482141943469354, 0.20540547178067145]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.04092251692883518, 0.44885076105016736, 0.3048214082325347, 0.2054053137884628]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.040823464123553595, 0.4477597513028085, 0.30557761401685657, 0.20583917055678133]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.040823464123553595, 0.4477597513028085, 0.30557761401685657, 0.20583917055678133]
start point for exploration sampling:  11893
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.04072473359037117, 0.44667229400755704, 0.30633126245232595, 0.20627170994974584]
in main func line 156:  453
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.040626337823268535, 0.4455885239588336, 0.30708235548852664, 0.20670278272937123]
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.313]
 [0.291]
 [0.281]
 [0.279]
 [0.279]
 [0.282]] [[-0.043]
 [ 0.677]
 [-0.062]
 [-0.336]
 [-0.315]
 [-0.238]
 [-0.007]] [[0.297]
 [0.313]
 [0.291]
 [0.281]
 [0.279]
 [0.279]
 [0.282]]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.04060611200431068, 0.44536442938426185, 0.3084236368644853, 0.20560582174694225]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.813]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[2.013]
 [1.798]
 [2.013]
 [2.013]
 [2.013]
 [2.013]
 [2.013]] [[0.848]
 [1.28 ]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]]
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.859]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[2.372]
 [2.955]
 [2.372]
 [2.372]
 [2.372]
 [2.372]
 [2.372]] [[1.348]
 [1.805]
 [1.348]
 [1.348]
 [1.348]
 [1.348]
 [1.348]]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.040547005275186315, 0.44471274490828705, 0.3094680911450049, 0.20527215867152168]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.476]] [[2.576]
 [2.576]
 [2.576]
 [2.576]
 [2.576]
 [2.576]
 [2.504]] [[0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [1.242]]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.04064818868579532, 0.44582382928600744, 0.3077775898906956, 0.20575039213750168]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.04043791112421685, 0.44350633508523035, 0.30863723262071746, 0.20741852116983528]
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.804]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.803]] [[1.436]
 [2.278]
 [2.715]
 [2.715]
 [2.715]
 [2.715]
 [1.596]] [[1.06 ]
 [1.622]
 [1.817]
 [1.817]
 [1.817]
 [1.817]
 [1.166]]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.512]
 [0.642]
 [0.639]
 [0.638]
 [0.629]
 [0.63 ]] [[0.903]
 [1.472]
 [0.855]
 [0.855]
 [0.855]
 [0.843]
 [1.003]] [[0.631]
 [0.731]
 [0.581]
 [0.575]
 [0.572]
 [0.548]
 [0.654]]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.04043791112421685, 0.44350633508523035, 0.30863723262071746, 0.20741852116983528]
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.242]
 [0.223]
 [0.233]
 [0.25 ]
 [0.235]
 [0.261]] [[0.433]
 [0.963]
 [0.258]
 [0.229]
 [0.417]
 [0.349]
 [0.619]] [[0.227]
 [0.242]
 [0.223]
 [0.233]
 [0.25 ]
 [0.235]
 [0.261]]
siam score:  -0.81063104
using another actor
actor:  1 policy actor:  1  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4295514771150093
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.03575573206919704, 0.38810332585894874, 0.39201385303882225, 0.18412708903303196]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.035755713888857406, 0.3881036257452315, 0.39201365041011216, 0.18412700995579886]
actor:  1 policy actor:  1  step number:  70 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.03432817468921118, 0.3730280921857156, 0.3763129475735901, 0.2163307855514832]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.03415529267645583, 0.3725159072062935, 0.37440643986290867, 0.218922360254342]
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.03415529267645583, 0.3725159072062935, 0.37440643986290867, 0.218922360254342]
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.03418912933431359, 0.3728866078793796, 0.37477853870441846, 0.21814572408188837]
siam score:  -0.8077584
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.03413620591475185, 0.37419505791934565, 0.37382669667014057, 0.21784203949576197]
line 256 mcts: sample exp_bonus 0.4421040118596527
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.759]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.92 ]] [[1.587]
 [1.025]
 [1.685]
 [1.685]
 [1.685]
 [1.685]
 [1.117]] [[1.033]
 [1.308]
 [1.216]
 [1.216]
 [1.216]
 [1.216]
 [1.692]]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.03428865141059665, 0.3758714910047415, 0.37102209643800754, 0.21881776114665427]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.03422777130718989, 0.3752005689295456, 0.37215208872200856, 0.218419571041256]
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[3.505]
 [3.505]
 [3.505]
 [3.505]
 [3.505]
 [3.505]
 [3.505]] [[3.468]
 [3.468]
 [3.468]
 [3.468]
 [3.468]
 [3.468]
 [3.468]]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
using explorer policy with actor:  1
siam score:  -0.8045139
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.548]
 [0.559]
 [0.54 ]
 [0.536]
 [0.548]
 [0.543]] [[5.064]
 [5.609]
 [6.53 ]
 [6.196]
 [6.131]
 [5.609]
 [6.14 ]] [[0.781]
 [1.131]
 [1.677]
 [1.453]
 [1.409]
 [1.131]
 [1.426]]
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.019]
 [-0.044]
 [-0.036]
 [-0.038]
 [-0.039]
 [-0.046]] [[3.564]
 [2.716]
 [2.321]
 [1.786]
 [1.657]
 [1.564]
 [1.819]] [[ 0.484]
 [-0.059]
 [-0.361]
 [-0.692]
 [-0.779]
 [-0.842]
 [-0.691]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11893
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.03662789080343256, 0.4016281200867839, 0.35192261006431924, 0.2098213790454642]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
line 256 mcts: sample exp_bonus 0.9896044146916816
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.637]
 [0.592]
 [0.544]
 [0.541]
 [0.399]
 [0.538]] [[2.621]
 [2.014]
 [1.543]
 [1.081]
 [0.928]
 [2.279]
 [1.294]] [[2.196]
 [1.769]
 [1.207]
 [0.65 ]
 [0.492]
 [1.557]
 [0.851]]
using explorer policy with actor:  0
siam score:  -0.8063797
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.036521404682799984, 0.4004544119286725, 0.352050489320109, 0.21097369406841854]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.325]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[0.727]
 [2.563]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]] [[0.431]
 [1.277]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]]
from probs:  [0.0365395228850507, 0.4006523682483966, 0.35177784633102827, 0.21103026253552432]
start point for exploration sampling:  11893
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.036539584715370575, 0.4006530421245109, 0.3517769027124958, 0.2110304704476228]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.03652967363969497, 0.40054628435951567, 0.3517378104592277, 0.2111862315415616]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.02  0.776 0.041 0.02  0.082 0.02  0.041]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.03649766456627226, 0.400191680657714, 0.3505496302731972, 0.21276102450281656]
siam score:  -0.81408614
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.555]] [[3.118]
 [3.118]
 [3.118]
 [3.118]
 [3.118]
 [3.118]
 [3.199]] [[0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [1.032]]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[5.561]
 [5.561]
 [5.561]
 [5.561]
 [5.561]
 [5.561]
 [5.561]] [[0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.036408650601857065, 0.39921144393615154, 0.35125951097977887, 0.21312039448221254]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.728]] [[3.016]
 [3.016]
 [3.016]
 [3.016]
 [3.016]
 [3.016]
 [6.875]] [[0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [1.785]]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.036408650601857065, 0.39921144393615154, 0.35125951097977887, 0.21312039448221254]
actor:  1 policy actor:  1  step number:  59 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.03778938502491057, 0.4144163232277799, 0.3402482417449087, 0.20754605000240087]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.642]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]] [[1.736]
 [1.577]
 [1.736]
 [1.736]
 [1.736]
 [1.736]
 [1.736]] [[1.813]
 [1.756]
 [1.813]
 [1.813]
 [1.813]
 [1.813]
 [1.813]]
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.793]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]] [[2.593]
 [1.924]
 [2.593]
 [2.593]
 [2.593]
 [2.593]
 [2.593]] [[1.6  ]
 [1.148]
 [1.6  ]
 [1.6  ]
 [1.6  ]
 [1.6  ]
 [1.6  ]]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.03784752892176456, 0.4150545900071836, 0.3384031085075193, 0.2086947725635325]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.03784755819750283, 0.4150549101679071, 0.3384027289624102, 0.20869480267217988]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.754]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]] [[1.82 ]
 [1.729]
 [1.82 ]
 [1.82 ]
 [1.82 ]
 [1.82 ]
 [1.82 ]] [[1.054]
 [1.003]
 [1.054]
 [1.054]
 [1.054]
 [1.054]
 [1.054]]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.646]
 [0.681]
 [0.656]
 [0.681]
 [0.659]
 [0.663]] [[3.588]
 [3.685]
 [3.996]
 [3.164]
 [3.996]
 [3.152]
 [3.337]] [[1.874]
 [1.949]
 [2.329]
 [1.447]
 [2.329]
 [1.442]
 [1.633]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9488538565341471
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.03776140870512273, 0.41410621230575123, 0.3390862277841165, 0.2090461512050096]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
actor:  1 policy actor:  1  step number:  59 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.039428865038030725, 0.4324686218169825, 0.32585684700308065, 0.20224566614190628]
Printing some Q and Qe and total Qs values:  [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]] [[-0.555]
 [-0.555]
 [-0.555]
 [-0.555]
 [-0.555]
 [-0.555]
 [-0.555]]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.03957044907067755, 0.43402586799602894, 0.3234299944197551, 0.20297368851353845]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.687]
 [0.669]
 [0.636]
 [0.636]
 [0.633]
 [0.625]] [[2.887]
 [3.231]
 [2.853]
 [2.569]
 [2.447]
 [2.428]
 [2.426]] [[1.23 ]
 [1.536]
 [1.247]
 [0.991]
 [0.911]
 [0.892]
 [0.876]]
from probs:  [0.03957044907067755, 0.43402586799602894, 0.3234299944197551, 0.20297368851353845]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.03948694447422424, 0.4331062892111985, 0.32408895737109344, 0.20331780894348403]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  0.11333333333333317  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]] [[3.539]
 [3.539]
 [3.539]
 [3.539]
 [3.539]
 [3.539]
 [3.539]] [[3.7]
 [3.7]
 [3.7]
 [3.7]
 [3.7]
 [3.7]
 [3.7]]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.431]
 [0.419]
 [0.412]
 [0.41 ]
 [0.408]
 [0.412]] [[4.839]
 [3.825]
 [4.823]
 [4.822]
 [4.842]
 [4.826]
 [4.683]] [[1.362]
 [0.71 ]
 [1.351]
 [1.336]
 [1.344]
 [1.33 ]
 [1.242]]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.03802632625284802, 0.41704136897747746, 0.34915366545934107, 0.1957786393103335]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.0379424922319781, 0.416118214269724, 0.3498492805650845, 0.19609001293321324]
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.865]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]] [[3.562]
 [3.515]
 [3.562]
 [3.562]
 [3.562]
 [3.562]
 [3.562]] [[1.937]
 [1.941]
 [1.937]
 [1.937]
 [1.937]
 [1.937]
 [1.937]]
maxi score, test score, baseline:  -0.996168656716418 -1.0 -0.996168656716418
probs:  [0.03794251627454977, 0.4161184771079644, 0.34984903042913146, 0.19608997618835455]
maxi score, test score, baseline:  -0.996168656716418 -1.0 -0.996168656716418
probs:  [0.03794251627454977, 0.4161184771079644, 0.34984903042913146, 0.19608997618835455]
