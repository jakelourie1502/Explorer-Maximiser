dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:32
res_block_kernel_size:3
res_block_channels:[32, 32]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 16, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[32, 32]
reward_conv_channels:16
reward_hidden_dim:128
terminal_conv_channels:16
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[32]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[9, 10]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:False
channels:1
timesteps_in_obs:1
store_prev_actions:False
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[4, 4]
observable_size:[4, 4]
game_modes:1
env_map:[['S' 'F' 'F' 'F']
 ['F' 'F' 'H' 'H']
 ['F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'G']]
max_steps:30
actions_size:4
optimal_score:1
total_frames:255000
exp_gamma:0.95
atari_env:False
state_size:[4, 4]
reward_clipping:False
memory_size:100
image_size:[48, 48]
running_reward_in_obs:False
deque_length:1
PRESET_CONFIG:5
VK_ceiling:False
VK:False
use_two_heads:False
use_siam:False
exploration_type:none
rdn_beta:[0, 0.0, 1]
explorer_percentage:0.0
follow_better_policy:0.0
reward_exploration:False
train_dones:False
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 16)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'F' 'F']
 ['F' 'F' 'H' 'H']
 ['F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
2 10
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
7 21
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.   ]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
Starting evaluation
27 88
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
rdn probs:  [1.0]
maxi score, test score, baseline:  0.028269014084507042 0.05 0.05
probs:  [1.0]
28 100
maxi score, test score, baseline:  0.02641578947368421 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.025257232704402516 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
deleting a thread, now have 2 threads
Frames:  2052 train batches done:  132 episodes:  148
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.023768639053254437 0.05 0.05
probs:  [1.0]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.02346448598130841 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.02346448598130841 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.02346448598130841 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.02346448598130841 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.02346448598130841 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.02335581395348837 0.05 0.05
45 153
maxi score, test score, baseline:  0.02242142857142857 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022029824561403507 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
48 165
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.033572803347280335 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0372900826446281 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.035814285714285715 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03498372093023256 0.05 0.05
maxi score, test score, baseline:  0.03498372093023256 0.05 0.05
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.2 0.4 0.2 0.2]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.04145338345864662 0.05 0.05
probs:  [1.0]
deleting a thread, now have 2 threads
Frames:  3432 train batches done:  217 episodes:  249
maxi score, test score, baseline:  0.04084074074074075 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.04054117647058824 0.05 0.05
maxi score, test score, baseline:  0.04054117647058824 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.04054117647058824 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.04054117647058824 0.05 0.05
maxi score, test score, baseline:  0.04054117647058824 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.04054117647058824 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.04054117647058824 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.04054117647058824 0.05 0.05
maxi score, test score, baseline:  0.04054117647058824 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.04054117647058824 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.04054117647058824 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.04054117647058824 0.05 0.05
probs:  [1.0]
54 204
maxi score, test score, baseline:  0.03952652329749104 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03938571428571429 0.05 0.05
maxi score, test score, baseline:  0.03924590747330961 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03777123287671233 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.037388135593220344 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03676666666666667 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03640363036303631 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.036047712418300654 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03546977491961415 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.035131847133757964 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.035131847133757964 0.05 0.05
probs:  [1.0]
62 232
maxi score, test score, baseline:  0.03436791277258567 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03405061728395062 0.05 0.05
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
63 241
maxi score, test score, baseline:  0.03613603603603604 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03529061583577713 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03419090909090909 0.05 0.05
probs:  [1.0]
65 267
maxi score, test score, baseline:  0.032886885245901644 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.032797547683923706 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032797547683923706 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032797547683923706 0.05 0.05
maxi score, test score, baseline:  0.032797547683923706 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032797547683923706 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032797547683923706 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032797547683923706 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032797547683923706 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03253243243243244 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.03334808184143223 0.05 0.05
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.4 0.2]
maxi score, test score, baseline:  0.032358064516129034 0.05 0.05
maxi score, test score, baseline:  0.032041031941031944 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.030978859857482183 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0368816091954023 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0368816091954023 0.05 0.05
78 339
maxi score, test score, baseline:  0.03654646924829157 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  8 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.03830224719101124 0.05 0.05
maxi score, test score, baseline:  0.03830224719101124 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03787777777777778 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03787777777777778 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.037380701754385966 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0369763557483731 0.05 0.05
maxi score, test score, baseline:  0.036817062634989205 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.035814285714285715 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.035739412997903564 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.035739412997903564 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.035223966942148764 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03515154639175258 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03507942386831276 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.03683469387755103 0.05 0.05
maxi score, test score, baseline:  0.03683469387755103 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.03683469387755103 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.04042258064516129 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.04042258064516129 0.05 0.05
maxi score, test score, baseline:  0.04042258064516129 0.05 0.05
maxi score, test score, baseline:  0.04042258064516129 0.05 0.05
siam score:  0.0
maxi score, test score, baseline:  0.04034144869215292 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
main train batch thing paused
add a thread
Adding thread: now have 3 threads
95 396
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.01 ]
 [0.025]
 [0.042]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.01 ]
 [0.025]
 [0.042]]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.448]
 [0.657]
 [0.657]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.657]
 [0.448]
 [0.657]
 [0.657]]
siam score:  0.0
in main func line 156:  98
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
siam score:  0.0
main train batch thing paused
add a thread
Adding thread: now have 4 threads
101 418
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.025]
 [0.03 ]
 [0.028]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.025]
 [0.03 ]
 [0.028]]
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.007]
 [0.007]
 [0.104]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.053]
 [0.007]
 [0.007]
 [0.104]]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.585]
 [0.021]
 [0.024]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.024]
 [0.585]
 [0.021]
 [0.024]]
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.333 0.25  0.25  0.167]
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
107 470
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.007]
 [0.008]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.007]
 [0.008]
 [0.009]]
actor:  0 policy actor:  0  step number:  8 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.006]
 [0.008]]
108 484
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  7 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
111 519
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
116 546
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
main train batch thing paused
add a thread
Adding thread: now have 6 threads
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.004]
 [0.004]
 [0.004]]
UNIT TEST: sample policy line 217 mcts : [0.458 0.208 0.167 0.167]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.301]
 [0.47 ]
 [0.042]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.042]
 [0.301]
 [0.47 ]
 [0.042]]
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
Starting evaluation
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.475]
 [0.437]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.491]
 [0.491]
 [0.475]
 [0.437]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
rdn probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.061]
 [0.866]
 [0.062]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.059]
 [0.061]
 [0.866]
 [0.062]]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.068]
 [0.067]
 [0.288]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.068]
 [0.067]
 [0.288]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
140 679
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
141 681
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.067]
 [0.067]
 [0.067]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.067]
 [0.067]
 [0.067]
 [0.067]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.015]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.015]
 [0.015]
 [0.015]]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.044]
 [0.058]
 [0.044]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.044]
 [0.044]
 [0.058]
 [0.044]]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  8 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
160 774
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.  ]
 [0.  ]
 [0.01]
 [0.  ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.  ]
 [0.  ]
 [0.01]
 [0.  ]]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.719]
 [0.413]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.411]
 [0.411]
 [0.719]
 [0.413]]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.397]
 [0.69 ]
 [0.394]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.397]
 [0.397]
 [0.69 ]
 [0.394]]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
176 859
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
176 871
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
178 891
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.068]
 [0.876]
 [0.068]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.068]
 [0.876]
 [0.068]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.541]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.541]]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.692]
 [0.005]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.692]
 [0.005]
 [0.006]]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  8 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
193 943
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
actor:  0 policy actor:  0  step number:  7 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
siam score:  0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0381 0.05 0.05
197 1013
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.262]
 [0.36 ]
 [0.183]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.183]
 [0.262]
 [0.36 ]
 [0.183]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
201 1023
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  7 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
201 1029
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
siam score:  0.0
202 1037
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.293]
 [0.293]
 [0.293]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.293]
 [0.293]
 [0.293]
 [0.293]]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
UNIT TEST: sample policy line 217 mcts : [0.042 0.208 0.708 0.042]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  8 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
207 1059
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  8 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  8 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
207 1079
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.   ]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  8 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
213 1159
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.989]
 [0.036]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.036]
 [0.036]
 [0.989]
 [0.036]]
213 1165
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
