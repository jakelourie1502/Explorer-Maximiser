dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:32
res_block_kernel_size:3
res_block_channels:[32, 32]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 16, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[32, 32]
reward_conv_channels:16
reward_hidden_dim:128
terminal_conv_channels:16
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[32]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[9, 10]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:False
channels:1
timesteps_in_obs:1
store_prev_actions:False
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[4, 4]
observable_size:[4, 4]
game_modes:1
env_map:[['S' 'F' 'F' 'F']
 ['F' 'F' 'H' 'H']
 ['F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'G']]
max_steps:30
actions_size:4
optimal_score:1
total_frames:255000
exp_gamma:0.95
atari_env:False
state_size:[4, 4]
reward_clipping:False
memory_size:100
image_size:[48, 48]
running_reward_in_obs:False
deque_length:1
PRESET_CONFIG:5
VK_ceiling:False
VK:False
use_two_heads:False
use_siam:False
exploration_type:none
rdn_beta:[0, 0.0, 1]
explorer_percentage:0.0
follow_better_policy:0.0
reward_exploration:False
train_dones:False
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 16)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'F' 'F']
 ['F' 'F' 'H' 'H']
 ['F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
actor:  0 policy actor:  0  step number:  7 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  5
5 28
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
11 60
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.003]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.003]
 [0.001]]
maxi score, test score, baseline:  0.02574102564102564 0.0 0.02574102564102564
probs:  [1.0]
rdn probs:  [1.0]
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
deleting a thread, now have 2 threads
Frames:  1491 train batches done:  81 episodes:  117
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
26 121
27 126
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
probs:  [1.0]
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
probs:  [1.0]
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
probs:  [1.0]
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
probs:  [1.0]
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
probs:  [1.0]
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
probs:  [1.0]
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
probs:  [1.0]
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
probs:  [1.0]
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
probs:  [1.0]
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
maxi score, test score, baseline:  0.015406122448979591 0.0 0.015406122448979591
main train batch thing paused
add a thread
Adding thread: now have 4 threads
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[ 0.001]
 [ 0.   ]
 [-0.   ]
 [-0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[ 0.001]
 [ 0.   ]
 [-0.   ]
 [-0.   ]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  39
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
deleting a thread, now have 3 threads
Frames:  3116 train batches done:  179 episodes:  241
maxi score, test score, baseline:  0.022741509433962262 0.0 0.022741509433962262
maxi score, test score, baseline:  0.022741509433962262 0.0 0.022741509433962262
probs:  [1.0]
maxi score, test score, baseline:  0.022741509433962262 0.0 0.022741509433962262
probs:  [1.0]
maxi score, test score, baseline:  0.022741509433962262 0.0 0.022741509433962262
probs:  [1.0]
maxi score, test score, baseline:  0.022741509433962262 0.0 0.022741509433962262
maxi score, test score, baseline:  0.022741509433962262 0.0 0.022741509433962262
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.022741509433962262 0.0 0.022741509433962262
maxi score, test score, baseline:  0.022741509433962262 0.0 0.022741509433962262
probs:  [1.0]
maxi score, test score, baseline:  0.022741509433962262 0.0 0.022741509433962262
probs:  [1.0]
maxi score, test score, baseline:  0.021997810218978103 0.0 0.021997810218978103
probs:  [1.0]
maxi score, test score, baseline:  0.02191818181818182 0.0 0.02191818181818182
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026855852842809363 0.0 0.026855852842809363
siam score:  0.0
maxi score, test score, baseline:  0.025659105431309902 0.0 0.025659105431309902
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.028225 0.0 0.028225
probs:  [1.0]
maxi score, test score, baseline:  0.028225 0.0 0.028225
probs:  [1.0]
maxi score, test score, baseline:  0.028225 0.0 0.028225
probs:  [1.0]
maxi score, test score, baseline:  0.028225 0.0 0.028225
maxi score, test score, baseline:  0.028225 0.0 0.028225
probs:  [1.0]
maxi score, test score, baseline:  0.028225 0.0 0.028225
probs:  [1.0]
maxi score, test score, baseline:  0.028225 0.0 0.028225
probs:  [1.0]
maxi score, test score, baseline:  0.028137383177570092 0.0 0.028137383177570092
maxi score, test score, baseline:  0.028137383177570092 0.0 0.028137383177570092
probs:  [1.0]
maxi score, test score, baseline:  0.027792307692307693 0.0 0.027792307692307693
probs:  [1.0]
51 258
maxi score, test score, baseline:  0.026885714285714284 0.0 0.026885714285714284
probs:  [1.0]
maxi score, test score, baseline:  0.02664867256637168 0.0 0.02664867256637168
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
60 284
maxi score, test score, baseline:  0.030154644808743167 0.0 0.030154644808743167
probs:  [1.0]
maxi score, test score, baseline:  0.030072752043596728 0.0 0.030072752043596728
deleting a thread, now have 2 threads
Frames:  4462 train batches done:  308 episodes:  347
maxi score, test score, baseline:  0.02982972972972973 0.0 0.02982972972972973
maxi score, test score, baseline:  0.02982972972972973 0.0 0.02982972972972973
probs:  [1.0]
maxi score, test score, baseline:  0.02982972972972973 0.0 0.02982972972972973
probs:  [1.0]
maxi score, test score, baseline:  0.02982972972972973 0.0 0.02982972972972973
probs:  [1.0]
maxi score, test score, baseline:  0.02982972972972973 0.0 0.02982972972972973
probs:  [1.0]
maxi score, test score, baseline:  0.02982972972972973 0.0 0.02982972972972973
probs:  [1.0]
maxi score, test score, baseline:  0.02982972972972973 0.0 0.02982972972972973
probs:  [1.0]
maxi score, test score, baseline:  0.02982972972972973 0.0 0.02982972972972973
probs:  [1.0]
maxi score, test score, baseline:  0.02982972972972973 0.0 0.02982972972972973
probs:  [1.0]
maxi score, test score, baseline:  0.02982972972972973 0.0 0.02982972972972973
probs:  [1.0]
maxi score, test score, baseline:  0.029749595687331536 0.0 0.029749595687331536
maxi score, test score, baseline:  0.029123746701846965 0.0 0.029123746701846965
probs:  [1.0]
maxi score, test score, baseline:  0.029123746701846965 0.0 0.029123746701846965
probs:  [1.0]
maxi score, test score, baseline:  0.028971391076115485 0.0 0.028971391076115485
actor:  0 policy actor:  0  step number:  7 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.03086923076923077 0.0 0.03086923076923077
probs:  [1.0]
maxi score, test score, baseline:  0.030790537084398977 0.0 0.030790537084398977
probs:  [1.0]
maxi score, test score, baseline:  0.030634351145038166 0.0 0.030634351145038166
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.03165339805825243 0.0 0.03165339805825243
probs:  [1.0]
maxi score, test score, baseline:  0.03150096618357488 0.0 0.03150096618357488
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
72 324
maxi score, test score, baseline:  0.033673141486810554 0.0 0.033673141486810554
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0333541567695962 0.0 0.0333541567695962
probs:  [1.0]
maxi score, test score, baseline:  0.03304117647058824 0.0 0.03304117647058824
probs:  [1.0]
maxi score, test score, baseline:  0.032810280373831775 0.0 0.032810280373831775
siam score:  0.0
maxi score, test score, baseline:  0.032283908045977014 0.0 0.032283908045977014
probs:  [1.0]
77 339
maxi score, test score, baseline:  0.03213661327231122 0.0 0.03213661327231122
probs:  [1.0]
maxi score, test score, baseline:  0.031490134529147985 0.0 0.031490134529147985
probs:  [1.0]
maxi score, test score, baseline:  0.03121111111111111 0.0 0.03121111111111111
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.03321258278145696 0.0 0.03321258278145696
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.03729912472647703 0.0 0.03729912472647703
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0389768898488121 0.0 0.0389768898488121
probs:  [1.0]
maxi score, test score, baseline:  0.038397872340425535 0.0 0.038397872340425535
maxi score, test score, baseline:  0.03831656050955414 0.0 0.03831656050955414
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.04001596638655462 0.0 0.04001596638655462
maxi score, test score, baseline:  0.0398489539748954 0.0 0.0398489539748954
siam score:  0.0
maxi score, test score, baseline:  0.0398489539748954 0.0 0.0398489539748954
probs:  [1.0]
maxi score, test score, baseline:  0.0398489539748954 0.0 0.0398489539748954
probs:  [1.0]
maxi score, test score, baseline:  0.0398489539748954 0.0 0.0398489539748954
probs:  [1.0]
maxi score, test score, baseline:  0.0398489539748954 0.0 0.0398489539748954
maxi score, test score, baseline:  0.0398489539748954 0.0 0.0398489539748954
probs:  [1.0]
86 373
maxi score, test score, baseline:  0.03935619834710744 0.0 0.03935619834710744
probs:  [1.0]
maxi score, test score, baseline:  0.039275257731958765 0.0 0.039275257731958765
probs:  [1.0]
maxi score, test score, baseline:  0.03879653767820774 0.0 0.03879653767820774
probs:  [1.0]
maxi score, test score, baseline:  0.0386395537525355 0.0 0.0386395537525355
probs:  [1.0]
maxi score, test score, baseline:  0.038483838383838384 0.0 0.038483838383838384
probs:  [1.0]
maxi score, test score, baseline:  0.03840645161290323 0.0 0.03840645161290323
probs:  [1.0]
maxi score, test score, baseline:  0.03840645161290323 0.0 0.03840645161290323
probs:  [1.0]
maxi score, test score, baseline:  0.03840645161290323 0.0 0.03840645161290323
probs:  [1.0]
maxi score, test score, baseline:  0.038329376257545275 0.0 0.038329376257545275
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.038252610441767074 0.0 0.038252610441767074
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.0 0.0381
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.0 0.0381
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.009]
 [0.008]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.009]
 [0.008]
 [0.003]]
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
actor:  0 policy actor:  0  step number:  7 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [1.0]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
94 437
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [1.0]
98 457
99 457
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.0 0.0381
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.283]
 [0.028]
 [0.295]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.028]
 [0.283]
 [0.028]
 [0.295]]
maxi score, test score, baseline:  0.0381 0.0 0.0381
maxi score, test score, baseline:  0.0381 0.0 0.0381
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.0 0.0381
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
actor:  0 policy actor:  0  step number:  7 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [1.0]
112 520
maxi score, test score, baseline:  0.0461 0.0 0.0461
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.0 0.0461
113 535
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
117 544
UNIT TEST: sample policy line 217 mcts : [0.167 0.333 0.333 0.167]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
siam score:  0.0
Training Flag: False
Self play flag: True
resampling flag: False
add more workers flag:  True
expV_train_flag:  False
expV_train_start_flag:  255000
main train batch thing paused
add a thread
Adding thread: now have 6 threads
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
126 565
actor:  0 policy actor:  0  step number:  7 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.014]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.014]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.131]
 [0.131]
 [0.131]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.131]
 [0.131]
 [0.131]
 [0.131]]
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
in main func line 156:  131
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
135 607
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
141 623
maxi score, test score, baseline:  0.0461 0.0 0.0461
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
141 638
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.024]
 [0.024]
 [0.025]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.024]
 [0.024]
 [0.025]]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
Starting evaluation
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
rdn probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.036]
 [0.062]
 [0.025]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.04 ]
 [0.036]
 [0.062]
 [0.025]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0381 0.0 0.0381
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.0 0.0381
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.0 0.0381
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
154 710
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.017]
 [0.017]
 [0.017]]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.011]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.011]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.013]
 [0.014]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.013]
 [0.014]
 [0.015]]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.025]
 [0.012]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.025]
 [0.012]
 [0.012]]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.021]
 [0.022]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.021]
 [0.022]
 [0.016]]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
172 791
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
178 817
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
183 867
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.021]
 [0.021]
 [0.021]]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.016]
 [0.016]
 [0.025]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.016]
 [0.016]
 [0.025]]
maxi score, test score, baseline:  0.0201 0.0 0.0201
184 881
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.028]
 [0.018]
 [0.028]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.028]
 [0.028]
 [0.018]
 [0.028]]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.002]
 [0.002]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.005]
 [0.006]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.005]
 [0.006]
 [0.008]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  8 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
195 950
195 953
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
197 971
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.009]
 [0.005]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.009]
 [0.005]
 [0.009]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.25  0.208 0.292 0.25 ]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.001]
 [0.002]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.001]
 [0.002]
 [0.003]]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
