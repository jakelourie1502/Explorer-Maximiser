dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[9, 10]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[5, 5]
observable_size:[5, 5]
game_modes:1
env_map:[['S' 'H' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'H']
 ['F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'H']
 ['F' 'H' 'F' 'F' 'G']]
max_steps:40
actions_size:4
optimal_score:1
total_frames:305000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:5
VK_ceiling:False
VK:False
use_two_heads:False
use_siam:False
exploration_type:none
rdn_beta:[0, 0.0, 1]
explorer_percentage:0.0
follow_better_policy:0.0
reward_exploration:False
train_dones:False
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 25)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'H' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'H']
 ['F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'H']
 ['F' 'H' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1 32
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Starting evaluation
3 42
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
5 84
maxi score, test score, baseline:  0.016360162601626017 0.0 0.016360162601626017
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.01597301587301587 0.0 0.01597301587301587
probs:  [1.0]
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
deleting a thread, now have 3 threads
Frames:  844 train batches done:  16 episodes:  110
siam score:  0.0
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
deleting a thread, now have 2 threads
Frames:  844 train batches done:  58 episodes:  110
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.015367175572519083 0.0 0.015367175572519083
probs:  [1.0]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
6 106
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.012838853503184713 0.0 0.012838853503184713
maxi score, test score, baseline:  0.012758227848101265 0.0 0.012758227848101265
probs:  [1.0]
6 145
maxi score, test score, baseline:  0.011528571428571428 0.0 0.011528571428571428
probs:  [1.0]
maxi score, test score, baseline:  0.011335955056179774 0.0 0.011335955056179774
probs:  [1.0]
maxi score, test score, baseline:  0.011211111111111111 0.0 0.011211111111111111
maxi score, test score, baseline:  0.011211111111111111 0.0 0.011211111111111111
probs:  [1.0]
6 154
7 159
7 162
maxi score, test score, baseline:  0.010626315789473683 0.0 0.010626315789473683
probs:  [1.0]
maxi score, test score, baseline:  0.0092324200913242 0.0 0.0092324200913242
maxi score, test score, baseline:  0.0092324200913242 0.0 0.0092324200913242
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.011]
 [0.011]
 [0.047]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.047]
 [0.011]
 [0.011]
 [0.047]]
maxi score, test score, baseline:  0.008988888888888888 0.0 0.008988888888888888
siam score:  0.0
maxi score, test score, baseline:  0.008758008658008657 0.0 0.008758008658008657
probs:  [1.0]
maxi score, test score, baseline:  0.008468200836820082 0.0 0.008468200836820082
probs:  [1.0]
maxi score, test score, baseline:  0.008263265306122449 0.0 0.008263265306122449
probs:  [1.0]
maxi score, test score, baseline:  0.008197165991902833 0.0 0.008197165991902833
probs:  [1.0]
maxi score, test score, baseline:  0.008132128514056224 0.0 0.008132128514056224
probs:  [1.0]
maxi score, test score, baseline:  0.00806812749003984 0.0 0.00806812749003984
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.01186470588235294 0.0 0.01186470588235294
probs:  [1.0]
maxi score, test score, baseline:  0.01181875 0.0 0.01181875
probs:  [1.0]
maxi score, test score, baseline:  0.011506844106463878 0.0 0.011506844106463878
probs:  [1.0]
10 245
maxi score, test score, baseline:  0.01089136690647482 0.0 0.01089136690647482
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.012838853503184713 0.0 0.012838853503184713
probs:  [1.0]
maxi score, test score, baseline:  0.012639184952978055 0.0 0.012639184952978055
probs:  [1.0]
maxi score, test score, baseline:  0.012332415902140672 0.0 0.012332415902140672
probs:  [1.0]
maxi score, test score, baseline:  0.011795906432748537 0.0 0.011795906432748537
13 309
maxi score, test score, baseline:  0.011119283746556474 0.0 0.011119283746556474
probs:  [1.0]
13 333
maxi score, test score, baseline:  0.01094010840108401 0.0 0.01094010840108401
probs:  [1.0]
maxi score, test score, baseline:  0.01088167115902965 0.0 0.01088167115902965
probs:  [1.0]
maxi score, test score, baseline:  0.010766666666666666 0.0 0.010766666666666666
probs:  [1.0]
maxi score, test score, baseline:  0.010710079575596816 0.0 0.010710079575596816
probs:  [1.0]
maxi score, test score, baseline:  0.010654089709762532 0.0 0.010654089709762532
probs:  [1.0]
maxi score, test score, baseline:  0.010571204188481675 0.0 0.010571204188481675
probs:  [1.0]
siam score:  0.0
siam score:  0.0
16 354
maxi score, test score, baseline:  0.010201010101010101 0.0 0.010201010101010101
probs:  [1.0]
maxi score, test score, baseline:  0.009976543209876542 0.0 0.009976543209876542
maxi score, test score, baseline:  0.009976543209876542 0.0 0.009976543209876542
probs:  [1.0]
maxi score, test score, baseline:  0.009952216748768472 0.0 0.009952216748768472
maxi score, test score, baseline:  0.00973855421686747 0.0 0.00973855421686747
probs:  [1.0]
maxi score, test score, baseline:  0.009692326139088728 0.0 0.009692326139088728
maxi score, test score, baseline:  0.009692326139088728 0.0 0.009692326139088728
probs:  [1.0]
maxi score, test score, baseline:  0.00955626477541371 0.0 0.00955626477541371
probs:  [1.0]
maxi score, test score, baseline:  0.009445794392523363 0.0 0.009445794392523363
probs:  [1.0]
21 391
maxi score, test score, baseline:  0.009337875288683602 0.0 0.009337875288683602
probs:  [1.0]
21 392
maxi score, test score, baseline:  0.009316589861751151 0.0 0.009316589861751151
maxi score, test score, baseline:  0.00919090909090909 0.0 0.00919090909090909
probs:  [1.0]
maxi score, test score, baseline:  0.00889120879120879 0.0 0.00889120879120879
probs:  [1.0]
maxi score, test score, baseline:  0.008739308855291577 0.0 0.008739308855291577
maxi score, test score, baseline:  0.008739308855291577 0.0 0.008739308855291577
probs:  [1.0]
maxi score, test score, baseline:  0.00861063829787234 0.0 0.00861063829787234
probs:  [1.0]
maxi score, test score, baseline:  0.008592569002123141 0.0 0.008592569002123141
25 452
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
26 479
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
27 484
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
27 492
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
30 546
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
34 624
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
38 666
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
main train batch thing paused
add a thread
Adding thread: now have 4 threads
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
main train batch thing paused
add a thread
Adding thread: now have 5 threads
main train batch thing paused
add a thread
Adding thread: now have 6 threads
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
main train batch thing paused
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
54 840
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
57 856
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.002]
 [0.002]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.002]
 [0.002]
 [0.   ]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
main train batch thing paused
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
main train batch thing paused
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  0.0
main train batch thing paused
65 919
66 921
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
66 931
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
main train batch thing paused
main train batch thing paused
73 979
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
74 1008
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
main train batch thing paused
maxi score, test score, baseline:  0.0061 0.0 0.0061
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
77 1042
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
77 1050
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Training Flag: False
Self play flag: True
resampling flag: False
add more workers flag:  True
expV_train_flag:  False
expV_train_start_flag:  305000
main train batch thing paused
78 1056
78 1065
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
main train batch thing paused
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
main train batch thing paused
85 1123
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
main train batch thing paused
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
main train batch thing paused
89 1201
Starting evaluation
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
97 1273
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.008]
 [0.007]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.008]
 [0.007]
 [0.006]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
101 1348
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
107 1424
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.002]
 [0.002]]
126 1603
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.002]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.022]
 [0.103]
 [0.103]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.103]
 [0.022]
 [0.103]
 [0.103]]
128 1647
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
128 1655
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
130 1728
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
130 1741
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
136 1800
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.375 0.167 0.167 0.292]
maxi score, test score, baseline:  0.0061 0.0 0.0061
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
139 1833
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
142 1907
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
143 1956
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
146 2001
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.5   0.167 0.167 0.167]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
147 2018
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.125 0.208 0.5  ]
maxi score, test score, baseline:  0.0041 0.0 0.0041
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
151 2069
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
154 2130
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.006]
 [0.007]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.006]
 [0.007]
 [0.008]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.007]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.009]
 [0.007]
 [0.009]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.009]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.007]
 [0.007]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.007]
 [0.007]
 [0.008]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.012]
 [0.012]
 [0.012]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]]
163 2232
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.004]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.003]
 [0.004]
 [0.003]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
siam score:  0.0
169 2298
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
173 2345
173 2349
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
173 2362
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
175 2412
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
STARTED EXPV TRAINING ON FRAME NO.  20001
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Starting evaluation
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
179 2468
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0001 0.0 0.0001
179 2493
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
179 2510
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  0.0
179 2522
179 2527
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
maxi score, test score, baseline:  0.0001 0.0 0.0001
181 2534
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.333 0.25  0.167 0.25 ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
185 2598
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.005]
 [0.004]
 [0.017]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.005]
 [0.004]
 [0.017]]
siam score:  0.0
190 2681
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.013]
 [0.011]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.013]
 [0.011]
 [0.004]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.   ]
 [0.012]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.   ]
 [0.012]
 [0.   ]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.   ]
 [0.013]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.   ]
 [0.013]
 [0.   ]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.019]
 [0.02 ]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.026]
 [0.019]
 [0.02 ]
 [0.013]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
197 2765
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
201 2783
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
205 2843
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
206 2871
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.003]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.003]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.003]
 [0.001]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
223 3162
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
226 3270
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.005]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.004]
 [0.005]
 [0.001]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
227 3337
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.002]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.002]
 [0.   ]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.017]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.004]
 [0.017]
 [0.007]]
in main func line 156:  229
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.002]
 [0.002]
 [0.002]]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
229 3383
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
230 3407
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
234 3481
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
234 3499
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
235 3504
deleting a thread, now have 5 threads
Frames:  27963 train batches done:  1965 episodes:  3740
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.064]
 [0.022]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.064]
 [0.022]
 [0.006]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
240 3543
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
241 3558
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]]
243 3591
243 3595
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
243 3596
line 256 mcts: sample exp_bonus 0.0
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
245 3612
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
deleting a thread, now have 4 threads
Frames:  29266 train batches done:  2057 episodes:  3904
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
252 3702
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
256 3730
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
256 3743
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
siam score:  0.0
Starting evaluation
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
258 3778
maxi score, test score, baseline:  0.0121 0.05 0.05
maxi score, test score, baseline:  0.0121 0.05 0.05
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
262 3798
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
264 3834
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.04 ]
 [0.028]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.046]
 [0.04 ]
 [0.028]
 [0.012]]
UNIT TEST: sample policy line 217 mcts : [0.208 0.292 0.333 0.167]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
266 3887
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
269 3955
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
270 3987
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
271 4007
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
277 4050
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
283 4127
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
286 4146
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.009]
 [0.009]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.009]
 [0.009]
 [0.004]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.003]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.003]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
289 4164
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.004]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
289 4263
289 4273
289 4279
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.05 0.05
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.05 0.05
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
296 4361
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.032]
 [0.032]
 [0.032]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.032]
 [0.032]
 [0.032]]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.25  0.417 0.167]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0016],
        [0.0028],
        [0.0000],
        [0.0000],
        [0.0216],
        [0.0022],
        [0.0084],
        [0.0041]], dtype=torch.float64)
0.99 0.99
0.0 0.0
0.0 0.0015710342809895173
0.0 0.0027901390060858207
0.0 0.0
0.0 0.0
0.0 0.021550384578269802
0.0 0.0021639722345371502
0.0 0.008423744500692054
0.0 0.004115098335712152
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.058]
 [0.018]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.084]
 [0.058]
 [0.018]
 [0.004]]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
maxi score, test score, baseline:  0.0161 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.05]
 [0.05]
 [0.05]
 [0.05]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.05]
 [0.05]
 [0.05]
 [0.05]]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.005]
 [0.044]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.005]
 [0.044]
 [0.008]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
313 4563
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
314 4570
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0000],
        [0.0004],
        [0.0003],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0021],
        [0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.99 0.99
0.0 0.00039343431148918835
0.0 0.00031054941465698624
0.99 0.99
0.0 0.0
0.0 0.0
0.0 0.0021487866203064843
0.9801 0.9801
316 4600
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.013]
 [0.029]
 [0.026]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.013]
 [0.029]
 [0.026]]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
317 4623
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
318 4629
318 4640
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
322 4678
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
325 4733
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
327 4744
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
327 4761
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
Printing some Q and Qe and total Qs values:  [[1]
 [1]
 [1]
 [1]] [[0]
 [0]
 [0]
 [0]] [[1]
 [1]
 [1]
 [1]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.014]
 [0.007]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.014]
 [0.007]
 [0.002]]
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
331 4844
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
331 4864
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
siam score:  0.0
337 4933
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [1.0]
341 4978
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [1.0]
341 4995
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [1.0]
341 5002
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
345 5049
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
Starting evaluation
345 5078
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
353 5184
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
354 5205
354 5209
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.001]]
357 5256
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
358 5292
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
358 5308
361 5321
line 256 mcts: sample exp_bonus 0.0
line 256 mcts: sample exp_bonus 0.0
362 5328
362 5337
362 5342
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
369 5392
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
371 5429
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.011]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.011]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
380 5525
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
383 5553
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
383 5571
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.   ]
 [0.001]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  0.0
UNIT TEST: sample policy line 217 mcts : [0.292 0.375 0.167 0.167]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
389 5650
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.002]
 [0.001]]
389 5674
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
395 5745
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
396 5752
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
398 5785
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.01 ]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.01 ]
 [0.003]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.006]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.006]
 [0.009]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
410 5893
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.043]
 [0.114]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.043]
 [0.114]
 [0.01 ]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.005]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.005]
 [0.002]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
415 5922
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
415 5933
in main func line 156:  417
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
419 5968
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
419 5972
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.208 0.458 0.167]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.001]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.001]
 [0.001]
 [0.003]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0021 0.0 0.0021
427 6072
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.004]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.004]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
434 6171
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
437 6201
437 6202
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [1.0]
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
442 6255
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Starting evaluation
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
446 6294
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
447 6317
maxi score, test score, baseline:  0.0141 0.1 0.1
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.012]
 [0.011]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.012]
 [0.011]
 [0.011]]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
451 6393
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
451 6402
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
452 6419
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.003]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.003]
 [0.004]]
UNIT TEST: sample policy line 217 mcts : [0.333 0.333 0.167 0.167]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.004]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.004]
 [0.002]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.003]
 [0.003]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.003]
 [0.003]
 [0.002]]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
459 6497
maxi score, test score, baseline:  0.0141 0.1 0.1
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.001]
 [0.001]
 [0.001]]
460 6516
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.001]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.001]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
maxi score, test score, baseline:  0.0121 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
471 6688
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.25  0.375 0.167 0.208]
maxi score, test score, baseline:  0.0081 0.1 0.1
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.005]
 [0.004]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.005]
 [0.004]
 [0.002]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.1 0.1
maxi score, test score, baseline:  0.0101 0.1 0.1
maxi score, test score, baseline:  0.0081 0.1 0.1
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.033]
 [0.033]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.033]
 [0.033]
 [0.033]
 [0.033]]
484 6855
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
484 6859
maxi score, test score, baseline:  0.0041 0.1 0.1
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
deleting a thread, now have 3 threads
Frames:  54818 train batches done:  3854 episodes:  7357
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
siam score:  0.0
485 6888
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.013]
 [0.013]
 [0.018]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0041 0.1 0.1
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
UNIT TEST: sample policy line 217 mcts : [0.208 0.208 0.167 0.417]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.004]]
490 6980
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
490 6994
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.006]
 [0.002]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.006]
 [0.002]
 [0.004]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.006]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.006]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
492 7044
492 7046
maxi score, test score, baseline:  0.0041 0.1 0.1
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.011]
 [0.007]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.011]
 [0.007]
 [0.011]]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
495 7077
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
496 7113
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.1 0.1
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
498 7148
499 7153
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
502 7191
maxi score, test score, baseline:  0.0081 0.1 0.1
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.001]
 [0.003]]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.1 0.1
probs:  [1.0]
504 7223
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.1 0.1
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
505 7234
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
509 7263
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0000],
        [0.0015],
        [0.0000],
        [0.0009],
        [0.0000],
        [0.0000],
        [0.0008],
        [0.0000]], dtype=torch.float64)
0.970299 0.970299
0.99 0.99
0.0 0.0
0.0 0.0014557135403605423
0.0 0.0
0.0 0.0009001042969692977
0.0 0.0
0.0 0.0
0.0 0.0008144792878068824
0.0 0.0
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
511 7295
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.009]
 [0.017]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.009]
 [0.017]
 [0.004]]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
519 7419
maxi score, test score, baseline:  0.0141 0.1 0.1
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.007]
 [0.015]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.007]
 [0.015]
 [0.005]]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.004]
 [0.017]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.004]
 [0.017]
 [0.007]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.011]
 [0.021]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.011]
 [0.021]
 [0.013]]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
520 7439
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
521 7453
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
528 7579
maxi score, test score, baseline:  0.0101 0.1 0.1
probs:  [1.0]
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
528 7591
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
531 7605
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.006]
 [0.012]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.006]
 [0.012]
 [0.006]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
543 7681
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.052]
 [0.052]
 [0.052]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.052]
 [0.052]
 [0.052]
 [0.052]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
548 7758
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.001]
 [0.002]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.02 ]
 [0.019]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.003]
 [0.02 ]
 [0.019]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
551 7793
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.016]
 [0.005]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.016]
 [0.005]
 [0.002]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
554 7841
in main func line 156:  555
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
556 7858
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
556 7882
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.009]
 [0.011]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.009]
 [0.011]
 [0.004]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
563 7938
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.011]
 [0.022]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.011]
 [0.022]
 [0.006]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
566 7985
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
566 7995
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.057]
 [0.227]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.057]
 [0.227]
 [0.004]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
569 8072
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
570 8084
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
571 8099
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.005]
 [0.005]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.005]
 [0.005]
 [0.003]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.03 ]
 [0.053]
 [0.048]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.03 ]
 [0.053]
 [0.048]]
Printing some Q and Qe and total Qs values:  [[0.03]
 [0.03]
 [0.03]
 [0.03]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.03]
 [0.03]
 [0.03]
 [0.03]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
576 8186
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
577 8209
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.003]
 [0.003]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.002]
 [0.005]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.002]
 [0.005]
 [0.001]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
582 8309
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  584
584 8353
UNIT TEST: sample policy line 217 mcts : [0.167 0.417 0.167 0.25 ]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.045]
 [0.038]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.045]
 [0.038]
 [0.002]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
591 8450
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
591 8458
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.002]
 [0.001]]
line 256 mcts: sample exp_bonus 0.0
592 8503
maxi score, test score, baseline:  0.0081 0.0 0.0081
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
593 8510
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
593 8522
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
595 8535
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
596 8557
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
599 8569
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0000],
        [0.0040],
        [0.0000],
        [0.0000],
        [0.0049],
        [0.0000],
        [0.0052],
        [0.0000]], dtype=torch.float64)
0.0 0.0
0.99 0.99
0.99 0.99
0.0 0.003960848120483504
0.970299 0.970299
0.0 0.0
0.0 0.004874172694187637
0.0 0.0
0.0 0.0051720824791385055
0.99 0.99
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
606 8647
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.   ]
 [0.   ]
 [0.001]]
608 8664
608 8669
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
608 8670
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
in main func line 156:  609
609 8679
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.083]
 [0.045]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.083]
 [0.045]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
610 8710
611 8720
611 8726
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.015]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.015]
 [0.015]
 [0.015]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.015]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.015]
 [0.018]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.005]
 [0.027]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.005]
 [0.027]
 [0.005]]
615 8762
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
617 8802
617 8803
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.   ]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.002]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.002]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.005]
 [0.008]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.005]
 [0.008]
 [0.004]]
Starting evaluation
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.998]
 [0.006]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.998]
 [0.006]
 [0.001]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.008]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.004]
 [0.008]
 [0.003]]
in main func line 156:  622
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
624 8960
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.022]
 [0.01 ]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.022]
 [0.01 ]
 [0.003]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.   ]
 [0.005]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.   ]
 [0.005]
 [0.002]]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
625 8976
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.008]
 [0.008]
 [0.008]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.03 ]
 [0.062]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.03 ]
 [0.062]
 [0.011]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.023]
 [0.016]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.023]
 [0.016]
 [0.006]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.013]
 [0.013]
 [0.013]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
632 9144
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
633 9148
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
634 9168
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
635 9207
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.004]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
638 9238
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
639 9286
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.012]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.012]
 [0.002]]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
642 9341
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
642 9368
642 9382
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
644 9405
siam score:  0.0
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.016]
 [0.008]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.016]
 [0.008]
 [0.003]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.005]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.005]
 [0.005]
 [0.005]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0201 0.05 0.05
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.167 0.208 0.458]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.021]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.021]
 [0.006]]
654 9549
654 9558
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
657 9601
657 9603
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
659 9615
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.375 0.25  0.208 0.167]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.002]
 [0.998]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.002]
 [0.998]]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.006]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.004]
 [0.006]
 [0.001]]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.006]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.006]
 [0.001]]
665 9701
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
665 9710
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
666 9733
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.021]
 [0.048]
 [0.045]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.021]
 [0.048]
 [0.045]]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.126]
 [0.064]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.025]
 [0.126]
 [0.064]
 [0.013]]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.007]
 [0.007]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.007]
 [0.007]
 [0.007]]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.011]
 [0.011]
 [0.011]]
siam score:  0.0
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
671 9823
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
672 9836
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
673 9840
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.002]]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0281 0.05 0.05
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.017]
 [0.006]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.017]
 [0.006]
 [0.001]]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
682 9934
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.018]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.018]
 [0.018]
 [0.018]]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.03 ]
 [0.004]
 [0.03 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.02 ]
 [0.03 ]
 [0.004]
 [0.03 ]]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
689 10041
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
691 10079
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
692 10093
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
693 10116
maxi score, test score, baseline:  0.0201 0.05 0.05
line 256 mcts: sample exp_bonus 0.0
693 10126
siam score:  0.0
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.013]
 [0.013]
 [0.001]]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.007]
 [0.005]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.007]
 [0.005]
 [0.001]]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.003]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.003]
 [0.002]]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.208 0.167 0.333 0.292]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
700 10218
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.023]
 [0.033]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.033]
 [0.033]
 [0.023]
 [0.033]]
siam score:  0.0
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.031]
 [0.096]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.031]
 [0.096]
 [0.013]]
maxi score, test score, baseline:  0.0161 0.05 0.05
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.152]
 [0.152]
 [0.152]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.152]
 [0.152]
 [0.152]
 [0.152]]
in main func line 156:  703
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
707 10365
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
708 10372
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.021]
 [0.008]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.021]
 [0.008]
 [0.004]]
712 10403
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
719 10479
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.005]
 [0.005]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.005]
 [0.005]
 [0.002]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
721 10550
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.008]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.008]
 [0.002]]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.004]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.004]
 [0.001]]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
723 10605
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
723 10615
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.002]
 [0.   ]]
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
725 10643
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
727 10678
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
735 10746
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
in main func line 156:  737
737 10777
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
742 10834
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
743 10849
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
maxi score, test score, baseline:  0.0121 0.05 0.05
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.084]
 [0.048]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.084]
 [0.048]
 [0.015]]
maxi score, test score, baseline:  0.0121 0.05 0.05
743 10873
maxi score, test score, baseline:  0.0121 0.05 0.05
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.01 ]
 [0.034]
 [0.014]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.01 ]
 [0.034]
 [0.014]]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.375 0.25  0.208 0.167]
752 10923
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.024]
 [0.026]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.024]
 [0.026]
 [0.012]]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
752 10948
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
754 10969
maxi score, test score, baseline:  0.0121 0.05 0.05
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
in main func line 156:  756
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
757 11040
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.008]
 [0.011]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.008]
 [0.011]
 [0.001]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
762 11109
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.033]
 [0.051]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.033]
 [0.051]
 [0.009]]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.167 0.167 0.5  ]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
764 11146
maxi score, test score, baseline:  0.0061 0.05 0.05
UNIT TEST: sample policy line 217 mcts : [0.25  0.208 0.208 0.333]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
767 11169
767 11176
maxi score, test score, baseline:  0.0061 0.05 0.05
768 11184
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.135]
 [0.055]
 [0.135]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.135]
 [0.055]
 [0.135]]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.023]
 [0.038]
 [0.023]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.023]
 [0.023]
 [0.038]
 [0.023]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
770 11198
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
776 11266
UNIT TEST: sample policy line 217 mcts : [0.125 0.167 0.208 0.5  ]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.122]
 [0.12 ]
 [0.115]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.075]
 [0.122]
 [0.12 ]
 [0.115]]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
779 11298
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
779 11318
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.113]
 [0.114]
 [0.101]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.109]
 [0.113]
 [0.114]
 [0.101]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
siam score:  0.0
784 11383
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.031]
 [0.038]
 [0.042]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.031]
 [0.038]
 [0.042]]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.029]
 [0.035]
 [0.038]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.029]
 [0.029]
 [0.035]
 [0.038]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.015]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.015]
 [0.015]
 [0.015]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.015]
 [0.031]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.015]
 [0.031]
 [0.018]]
maxi score, test score, baseline:  0.0041 0.05 0.05
Starting evaluation
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.031]
 [0.019]
 [0.031]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.031]
 [0.019]
 [0.031]]
rdn probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.013]
 [0.013]
 [0.013]]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.008]
 [0.01 ]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.008]
 [0.01 ]
 [0.01 ]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
794 11499
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.177]
 [0.176]
 [0.2  ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.16 ]
 [0.177]
 [0.176]
 [0.2  ]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.1  ]
 [0.111]
 [0.12 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.104]
 [0.1  ]
 [0.111]
 [0.12 ]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
803 11640
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
siam score:  0.0
804 11656
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
805 11707
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
siam score:  0.0
siam score:  0.0
806 11737
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
807 11751
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.333 0.292 0.208]
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.007]
 [0.009]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.007]
 [0.009]
 [0.009]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
815 11785
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.003]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.003]
 [0.005]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.009]
 [0.007]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.009]
 [0.007]
 [0.004]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
817 11832
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
820 11849
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.003]
 [0.015]
 [0.03 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.003]
 [0.015]
 [0.03 ]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.01 ]
 [0.01 ]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.01 ]
 [0.01 ]
 [0.005]]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.011]
 [0.016]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.011]
 [0.016]
 [0.013]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
832 11922
832 11926
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
siam score:  0.0
833 11947
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
834 11979
UNIT TEST: sample policy line 217 mcts : [0.25  0.167 0.375 0.208]
maxi score, test score, baseline:  0.0101 0.0 0.0101
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.027]
 [0.007]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.027]
 [0.007]
 [0.002]]
siam score:  0.0
836 11995
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
836 12000
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
837 12043
837 12051
siam score:  0.0
in main func line 156:  838
in main func line 156:  839
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
845 12106
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.019]
 [0.021]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.019]
 [0.021]
 [0.009]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
846 12112
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.292 0.208 0.333]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.008]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.01 ]
 [0.008]
 [0.01 ]]
847 12164
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.011]
 [0.014]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.011]
 [0.014]
 [0.012]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.017]
 [0.017]
 [0.017]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.01 ]
 [0.01 ]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.01 ]
 [0.01 ]
 [0.009]]
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.009]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.009]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.005]
 [0.005]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.007]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
860 12389
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
860 12402
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
862 12425
862 12430
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
in main func line 156:  870
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
876 12573
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.002]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.024]
 [0.038]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.024]
 [0.038]
 [0.007]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.05 ]
 [0.065]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.05 ]
 [0.065]
 [0.012]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.004]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.004]
 [0.004]
 [0.001]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.004]]
rdn probs:  [1.0]
882 12664
883 12666
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
885 12700
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.014]
 [0.024]
 [0.025]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.014]
 [0.024]
 [0.025]]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
889 12749
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.027]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.004]
 [0.027]
 [0.002]]
892 12809
maxi score, test score, baseline:  0.0141 0.1 0.1
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.1 0.1
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.009]
 [0.015]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.009]
 [0.015]
 [0.001]]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.   ]
 [0.001]]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.   ]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.   ]
 [0.004]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.1 0.1
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
896 12934
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.022]
 [0.014]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.022]
 [0.014]
 [0.004]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.059]
 [0.01 ]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.059]
 [0.01 ]
 [0.002]]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.02 ]
 [0.004]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.02 ]
 [0.004]
 [0.001]]
900 13020
900 13022
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.035]
 [0.023]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.035]
 [0.023]
 [0.003]]
UNIT TEST: sample policy line 217 mcts : [0.208 0.417 0.208 0.167]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.1 0.1
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
902 13063
maxi score, test score, baseline:  0.0201 0.1 0.1
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.007]
 [0.041]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.007]
 [0.041]
 [0.01 ]]
maxi score, test score, baseline:  0.0161 0.1 0.1
908 13184
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
908 13187
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
912 13232
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
siam score:  0.0
916 13253
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.022]
 [0.013]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.022]
 [0.013]
 [0.011]]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
917 13290
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
917 13294
siam score:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
917 13298
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.004]
 [0.034]
 [0.04 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.004]
 [0.034]
 [0.04 ]]
maxi score, test score, baseline:  0.0241 0.1 0.1
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.045]
 [0.045]
 [0.045]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.045]
 [0.045]
 [0.045]
 [0.045]]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
920 13402
920 13403
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.061]
 [0.08 ]
 [0.071]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.025]
 [0.061]
 [0.08 ]
 [0.071]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0201 0.1 0.1
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
922 13433
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
922 13447
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
924 13475
924 13478
siam score:  0.0
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
924 13499
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
933 13590
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
938 13658
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.054]
 [0.1  ]
 [0.113]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.024]
 [0.054]
 [0.1  ]
 [0.113]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
939 13678
siam score:  0.0
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
942 13692
943 13696
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.234]
 [0.234]
 [0.234]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.234]
 [0.234]
 [0.234]
 [0.234]]
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.191]
 [0.268]
 [0.919]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.289]
 [0.191]
 [0.268]
 [0.919]]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
947 13798
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
948 13800
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
951 13819
952 13823
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.221]
 [0.251]
 [0.252]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.18 ]
 [0.221]
 [0.251]
 [0.252]]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.198]
 [0.032]
 [0.177]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.198]
 [0.198]
 [0.032]
 [0.177]]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.13 ]
 [0.089]
 [0.089]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.089]
 [0.13 ]
 [0.089]
 [0.089]]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.054]
 [0.082]
 [0.095]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.039]
 [0.054]
 [0.082]
 [0.095]]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.125 0.167 0.5   0.208]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
siam score:  0.0
in main func line 156:  962
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
962 13884
963 13890
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
965 13904
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
Starting evaluation
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.066]
 [0.066]
 [0.066]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.052]
 [0.066]
 [0.066]
 [0.066]]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
rdn probs:  [1.0]
966 13934
966 13943
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.03]
 [0.03]
 [0.03]
 [0.02]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.03]
 [0.03]
 [0.03]
 [0.02]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
970 14001
971 14016
UNIT TEST: sample policy line 217 mcts : [0.25  0.292 0.292 0.167]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
972 14029
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
975 14048
976 14050
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.004]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.004]
 [0.   ]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.006]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.006]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
982 14111
982 14112
maxi score, test score, baseline:  0.0101 0.0 0.0101
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
982 14130
982 14132
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
982 14171
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
siam score:  0.0
985 14216
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
986 14223
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
988 14271
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
990 14344
990 14350
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.003]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.003]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
995 14426
UNIT TEST: sample policy line 217 mcts : [0.167 0.167 0.292 0.375]
siam score:  0.0
995 14444
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
997 14470
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
999 14496
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
1004 14604
siam score:  0.0
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.45 ]
 [0.107]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.025]
 [0.45 ]
 [0.107]
 [0.003]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1004 14662
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
in main func line 156:  1007
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.003]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.003]
 [0.001]]
1009 14714
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1009 14728
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
1010 14787
maxi score, test score, baseline:  0.0201 0.0 0.0201
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.003]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.004]
 [0.003]
 [0.001]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
1012 14807
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
1013 14820
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.123]
 [0.123]
 [0.123]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.123]
 [0.123]
 [0.123]
 [0.123]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
1015 14887
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
1015 14902
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.003]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.003]
 [0.002]]
1015 14919
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.069]
 [0.069]
 [0.069]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.069]
 [0.069]
 [0.069]
 [0.069]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
1019 14995
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
1026 15089
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
1029 15214
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.011]
 [0.011]
 [0.011]]
1032 15248
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1035 15285
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Starting evaluation
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
1038 15353
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
1040 15404
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1043 15459
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.033]
 [0.031]
 [0.033]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.036]
 [0.033]
 [0.031]
 [0.033]]
1045 15474
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.014]
 [0.014]
 [0.014]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.009]
 [0.005]
 [0.705]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.705]
 [0.009]
 [0.005]
 [0.705]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.022]
 [0.024]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.022]
 [0.024]
 [0.021]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
1049 15564
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.013]
 [0.013]
 [0.013]]
1049 15573
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
1053 15615
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.126]
 [0.237]
 [0.153]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.088]
 [0.126]
 [0.237]
 [0.153]]
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[1.01]
 [1.01]
 [1.01]
 [1.01]] [[0.]
 [0.]
 [0.]
 [0.]] [[1.01]
 [1.01]
 [1.01]
 [1.01]]
Printing some Q and Qe and total Qs values:  [[0.999]
 [0.066]
 [0.072]
 [0.999]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.999]
 [0.066]
 [0.072]
 [0.999]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
1061 15699
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1061 15704
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
1063 15720
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.118]
 [0.129]
 [0.121]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.08 ]
 [0.118]
 [0.129]
 [0.121]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
1065 15767
1065 15769
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
1066 15782
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.064]
 [0.053]
 [0.069]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.045]
 [0.064]
 [0.053]
 [0.069]]
1067 15797
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
1068 15810
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
1069 15845
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.011]
 [0.016]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.011]
 [0.016]
 [0.011]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.02 ]
 [0.023]
 [0.02 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.02 ]
 [0.02 ]
 [0.023]
 [0.02 ]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
1072 15906
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.007]
 [0.02 ]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.007]
 [0.02 ]
 [0.018]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.03]
 [0.03]
 [0.03]
 [0.03]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.03]
 [0.03]
 [0.03]
 [0.03]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
1074 15932
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.015]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.015]
 [0.015]
 [0.015]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
siam score:  0.0
1075 15957
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
1077 15989
1078 15990
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.029]
 [0.027]
 [0.025]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.029]
 [0.029]
 [0.027]
 [0.025]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
1080 16009
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
1085 16045
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
1086 16067
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
1086 16079
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.008]
 [0.008]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.008]
 [0.008]
 [0.007]]
UNIT TEST: sample policy line 217 mcts : [0.5   0.167 0.167 0.167]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
1088 16126
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.073]
 [0.032]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.073]
 [0.032]
 [0.005]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0000],
        [0.0119],
        [0.0000],
        [0.0030],
        [0.0205],
        [0.0000],
        [0.0031],
        [0.0090]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.9801 0.9801
0.0 0.011946808704300755
0.99 0.99
0.0 0.00295451694038163
0.0 0.02054604813228465
0.970299 0.970299
0.0 0.0030929597694524733
0.0 0.009004740027294225
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.006]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.   ]
 [0.006]
 [0.005]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1099 16304
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
1101 16322
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.009]
 [0.009]
 [0.009]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.054]
 [0.033]
 [0.037]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.091]
 [0.054]
 [0.033]
 [0.037]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.001]
 [0.013]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.001]
 [0.013]
 [0.009]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.031]
 [0.034]
 [0.039]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.019]
 [0.031]
 [0.034]
 [0.039]]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
1112 16500
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.074]
 [0.074]
 [0.074]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.074]
 [0.074]
 [0.074]
 [0.074]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.03 ]
 [0.079]
 [0.15 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.048]
 [0.03 ]
 [0.079]
 [0.15 ]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
1114 16551
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1114 16595
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.065]
 [0.045]
 [0.094]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.067]
 [0.065]
 [0.045]
 [0.094]]
1114 16624
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
1115 16638
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
1115 16648
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
1116 16668
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.098]
 [0.115]
 [0.061]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.04 ]
 [0.098]
 [0.115]
 [0.061]]
1117 16680
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1118 16694
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
1118 16719
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.027]
 [0.029]
 [0.027]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.027]
 [0.027]
 [0.029]
 [0.027]]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.05 0.05
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
1125 16835
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1126 16856
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
UNIT TEST: sample policy line 217 mcts : [0.292 0.375 0.167 0.167]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.128]
 [0.042]
 [0.039]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.056]
 [0.128]
 [0.042]
 [0.039]]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
1132 16942
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.004]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
1139 17019
siam score:  0.0
1139 17023
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1140 17100
maxi score, test score, baseline:  0.0201 0.05 0.05
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
1142 17129
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.05 0.05
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
1143 17215
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
1147 17251
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.05 0.05
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
maxi score, test score, baseline:  0.0301 0.05 0.05
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.013]
 [0.003]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.013]
 [0.003]
 [0.002]]
1150 17306
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
1151 17341
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.099]
 [0.062]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.049]
 [0.099]
 [0.062]
 [0.006]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
1153 17381
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.004]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.004]
 [0.004]
 [0.003]]
1153 17387
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
1156 17453
maxi score, test score, baseline:  0.0461 0.05 0.05
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
1157 17484
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
1157 17510
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.05 0.05
probs:  [1.0]
1161 17558
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
1161 17580
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
1161 17594
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.015]
 [0.006]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.015]
 [0.006]
 [0.002]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.007]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.007]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
1167 17693
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.032]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.032]
 [0.007]]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.009]
 [0.01 ]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.009]
 [0.01 ]
 [0.006]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1171 17738
maxi score, test score, baseline:  0.0301 0.05 0.05
1171 17743
maxi score, test score, baseline:  0.0281 0.05 0.05
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.05 0.05
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.031]
 [0.007]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.031]
 [0.007]
 [0.001]]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
1178 17879
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.005]
 [0.015]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.005]
 [0.015]
 [0.01 ]]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
1179 17887
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1179 17897
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
1182 17925
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.011]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.   ]
 [0.011]
 [0.004]]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.02]
 [0.02]
 [0.02]
 [0.02]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.02]
 [0.02]
 [0.02]
 [0.02]]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.007]
 [0.006]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.007]
 [0.006]
 [0.007]]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.05 0.05
1183 17992
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0241 0.0 0.0241
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
1189 18034
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
1191 18075
maxi score, test score, baseline:  0.0281 0.0 0.0281
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
1194 18158
siam score:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.859]
 [0.859]
 [0.859]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.859]
 [0.859]
 [0.859]
 [0.859]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.019]
 [0.045]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.019]
 [0.045]
 [0.011]]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
in main func line 156:  1199
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.0 0.0361
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.025]
 [0.068]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.025]
 [0.068]
 [0.011]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.005]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.004]
 [0.005]
 [0.004]]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
1201 18261
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.009]
 [0.009]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.009]
 [0.009]
 [0.009]]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
1205 18322
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.013]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.004]
 [0.013]
 [0.004]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.003]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.002]
 [0.001]]
siam score:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.005]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.005]
 [0.004]]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.008]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.004]
 [0.008]
 [0.004]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.005]
 [0.007]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.005]
 [0.007]
 [0.004]]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
1214 18477
maxi score, test score, baseline:  0.0301 0.0 0.0301
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.722]
 [0.702]
 [0.758]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.758]
 [0.722]
 [0.702]
 [0.758]]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
siam score:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.78 ]
 [0.032]
 [0.023]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.046]
 [0.78 ]
 [0.032]
 [0.023]]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
siam score:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.019]
 [0.022]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.023]
 [0.019]
 [0.022]
 [0.018]]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1219 18589
1220 18599
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.006]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.005]
 [0.005]]
UNIT TEST: sample policy line 217 mcts : [0.167 0.292 0.292 0.25 ]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1225
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
siam score:  0.0
siam score:  0.0
1227 18641
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
line 256 mcts: sample exp_bonus 0.0
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.004]
 [0.004]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.004]
 [0.004]
 [0.002]]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
1232 18721
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.   ]
 [0.001]
 [0.001]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
1236 18787
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.054]
 [0.041]
 [0.023]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.02 ]
 [0.054]
 [0.041]
 [0.023]]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
1239 18859
line 256 mcts: sample exp_bonus 0.0
siam score:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.116]
 [0.035]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.241]
 [0.116]
 [0.035]
 [0.005]]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.027]
 [0.027]
 [0.027]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.027]
 [0.027]
 [0.027]
 [0.027]]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.01 ]
 [0.048]
 [0.052]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.01 ]
 [0.048]
 [0.052]]
1239 18941
1239 18942
maxi score, test score, baseline:  0.0241 0.0 0.0241
1239 18947
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.149]
 [0.034]
 [0.034]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.034]
 [0.149]
 [0.034]
 [0.034]]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.012]
 [0.025]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.012]
 [0.025]
 [0.016]]
1240 18979
UNIT TEST: sample policy line 217 mcts : [0.25  0.167 0.208 0.375]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.01 ]
 [0.01 ]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.01 ]
 [0.01 ]
 [0.006]]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.028]
 [0.11 ]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.028]
 [0.11 ]
 [0.01 ]]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
1243 19053
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.061]
 [0.03 ]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.061]
 [0.03 ]
 [0.001]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1244 19105
1244 19111
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.019]
 [0.005]
 [0.045]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.045]
 [0.019]
 [0.005]
 [0.045]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.036]
 [0.021]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.039]
 [0.036]
 [0.021]
 [0.008]]
1249 19177
1249 19178
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
1249 19191
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
siam score:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.0 0.0201
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
Starting evaluation
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.012]
 [0.011]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.012]
 [0.011]
 [0.002]]
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
1258 19360
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
1258 19395
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.823]
 [0.735]
 [0.831]
 [0.823]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.823]
 [0.735]
 [0.831]
 [0.823]]
maxi score, test score, baseline:  0.0241 0.05 0.05
1258 19408
maxi score, test score, baseline:  0.0241 0.05 0.05
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.02 ]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.02 ]
 [0.008]]
maxi score, test score, baseline:  0.0241 0.05 0.05
1258 19418
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.643]
 [0.345]
 [0.049]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.643]
 [0.345]
 [0.049]]
maxi score, test score, baseline:  0.0241 0.05 0.05
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
1258 19440
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1260 19477
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.167 0.208 0.458]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
1261 19558
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.126]
 [0.171]
 [0.259]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.259]
 [0.126]
 [0.171]
 [0.259]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [1.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [1.]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.002]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.002]
 [0.   ]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.05 0.05
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1268 19701
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
1269 19710
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1270 19753
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.938]
 [0.412]
 [0.412]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.412]
 [0.938]
 [0.412]
 [0.412]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.003]
 [0.019]
 [0.201]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.003]
 [0.019]
 [0.201]]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
maxi score, test score, baseline:  0.0441 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.018]
 [0.015]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.018]
 [0.015]
 [0.004]]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.818]
 [0.701]
 [0.701]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.701]
 [0.818]
 [0.701]
 [0.701]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.008]
 [0.009]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.008]
 [0.009]
 [0.005]]
1276 19826
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.444]
 [0.173]
 [0.35 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.35 ]
 [0.444]
 [0.173]
 [0.35 ]]
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
1279 19861
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
1285 19896
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.005]
 [0.008]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.005]
 [0.008]
 [0.001]]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.002]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.002]
 [0.003]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
1289 19950
1289 19952
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.063]
 [0.181]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.063]
 [0.181]
 [0.006]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
1291 19979
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.05 0.05
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.002]
 [0.001]]
1292 20002
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.014]
 [0.   ]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.014]
 [0.   ]
 [0.018]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.006]
 [0.002]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.006]
 [0.002]
 [0.   ]]
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.05 0.05
probs:  [1.0]
1297 20160
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.05 0.05
maxi score, test score, baseline:  0.0161 0.05 0.05
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.048]
 [0.017]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.048]
 [0.017]
 [0.016]]
maxi score, test score, baseline:  0.0161 0.05 0.05
UNIT TEST: sample policy line 217 mcts : [0.333 0.167 0.333 0.167]
maxi score, test score, baseline:  0.0161 0.05 0.05
probs:  [1.0]
1300 20204
maxi score, test score, baseline:  0.0141 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.081]
 [0.057]
 [0.023]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.081]
 [0.057]
 [0.023]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.004]
 [0.004]]
1305 20249
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
1305 20258
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.008]
 [0.003]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.008]
 [0.003]
 [0.005]]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.019]
 [0.022]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.022]
 [0.019]
 [0.022]]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.375 0.25  0.208]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
1312 20313
maxi score, test score, baseline:  0.0101 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
1314 20361
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.13 ]
 [0.019]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.019]
 [0.019]
 [0.13 ]
 [0.019]]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
1320 20422
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.005]
 [0.003]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.005]
 [0.003]
 [0.001]]
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.003]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
siam score:  0.0
1330 20571
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
1333 20608
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
1333 20624
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
1334 20654
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.05 0.05
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.05 0.05
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.014]
 [0.015]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.014]
 [0.015]
 [0.003]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.1 0.1
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.1 0.1
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1340 20894
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.002]
 [0.012]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.002]
 [0.012]
 [0.006]]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
1344 21019
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
1345 21170
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
siam score:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.011]
 [0.011]
 [0.011]]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.047]
 [0.045]
 [0.027]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.019]
 [0.047]
 [0.045]
 [0.027]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
1353 21286
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
siam score:  0.0
1356 21304
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.048]
 [0.201]
 [0.162]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.162]
 [0.048]
 [0.201]
 [0.162]]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.044]
 [0.071]
 [0.055]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.038]
 [0.044]
 [0.071]
 [0.055]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.111]
 [0.107]
 [0.107]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.107]
 [0.111]
 [0.107]
 [0.107]]
1356 21333
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
1357 21350
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.017]
 [0.028]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.017]
 [0.028]
 [0.018]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
1359 21396
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.831]
 [0.101]
 [0.101]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.101]
 [0.831]
 [0.101]
 [0.101]]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.099]
 [0.099]
 [0.099]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.099]
 [0.099]
 [0.099]
 [0.099]]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
1363 21437
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.014]
 [0.071]
 [0.022]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.014]
 [0.071]
 [0.022]]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
1367 21480
1367 21484
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.005]
 [0.009]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.005]
 [0.009]
 [0.002]]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
1370 21519
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.165]
 [0.165]
 [0.165]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.165]
 [0.165]
 [0.165]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.1 0.1
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.005]
 [0.005]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
1375 21598
siam score:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
1375 21611
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.64 ]
 [0.748]
 [0.64 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.64 ]
 [0.64 ]
 [0.748]
 [0.64 ]]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
1383 21684
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.75 ]
 [0.344]
 [0.75 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.75 ]
 [0.75 ]
 [0.344]
 [0.75 ]]
1383 21701
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.1 0.1
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.034]
 [0.026]
 [0.02 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.02 ]
 [0.034]
 [0.026]
 [0.02 ]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
1391 21828
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
1398 21914
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
1399 21922
1399 21929
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1400
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.007]
 [0.01 ]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.007]
 [0.01 ]
 [0.004]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.003]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.003]
 [0.002]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
1403 22041
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
1403 22094
1403 22099
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
Starting evaluation
UNIT TEST: sample policy line 217 mcts : [0.333 0.167 0.333 0.167]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.058]
 [0.058]
 [0.061]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.053]
 [0.058]
 [0.058]
 [0.061]]
maxi score, test score, baseline:  0.0441 0.1 0.1
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
1405 22155
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
1406 22174
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1407 22183
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1411 22247
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.201]
 [0.338]
 [0.096]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.096]
 [0.201]
 [0.338]
 [0.096]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.014]
 [0.012]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.014]
 [0.012]
 [0.01 ]]
in main func line 156:  1416
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
1416 22298
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
1417 22305
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.009]
 [0.011]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.009]
 [0.011]
 [0.01 ]]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
1420 22356
1420 22358
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
1422 22373
maxi score, test score, baseline:  0.0361 0.1 0.1
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.999]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.999]
 [0.002]
 [0.001]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1424 22393
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
1425 22419
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.007]
 [0.006]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.007]
 [0.006]
 [0.002]]
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
1430 22644
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
1430 22652
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1430 22654
1430 22656
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1431 22659
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0541 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1431 22678
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
1431 22682
1431 22685
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0621 0.1 0.1
1432 22703
maxi score, test score, baseline:  0.0601 0.1 0.1
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0641 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0621 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
siam score:  0.0
maxi score, test score, baseline:  0.0641 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0661 0.1 0.1
1439 22802
maxi score, test score, baseline:  0.0661 0.1 0.1
1439 22815
maxi score, test score, baseline:  0.0661 0.1 0.1
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0781 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.08410000000000001 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0821 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0821 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0821 0.1 0.1
maxi score, test score, baseline:  0.0801 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.08410000000000001 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.08410000000000001 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.08410000000000001 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.036]
 [0.06 ]
 [0.065]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.019]
 [0.036]
 [0.06 ]
 [0.065]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0781 0.1 0.1
probs:  [1.0]
1450 22955
maxi score, test score, baseline:  0.0781 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0781 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0781 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0781 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0781 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0821 0.1 0.1
maxi score, test score, baseline:  0.0741 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1453 23019
maxi score, test score, baseline:  0.0721 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.1 0.1
1454 23044
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.1 0.1
maxi score, test score, baseline:  0.0741 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.1 0.1
maxi score, test score, baseline:  0.0741 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
probs:  [1.0]
1456 23123
maxi score, test score, baseline:  0.0641 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
1457 23144
maxi score, test score, baseline:  0.0541 0.1 0.1
maxi score, test score, baseline:  0.0541 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0541 0.1 0.1
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
line 256 mcts: sample exp_bonus 0.0
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
1458 23303
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.004]
 [0.003]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.004]
 [0.003]
 [0.   ]]
siam score:  0.0
maxi score, test score, baseline:  0.0281 0.1 0.1
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.004]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.004]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.008]
 [0.009]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.008]
 [0.009]
 [0.002]]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
1461 23414
UNIT TEST: sample policy line 217 mcts : [0.125 0.25  0.208 0.417]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
1462 23464
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.1 0.1
maxi score, test score, baseline:  0.0121 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1462 23521
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.216]
 [0.216]
 [0.216]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.216]
 [0.216]
 [0.216]
 [0.216]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.182]
 [0.075]
 [0.365]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.246]
 [0.182]
 [0.075]
 [0.365]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
1462 23558
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [1.0]
1463 23579
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.25 0.25
maxi score, test score, baseline:  0.0361 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.492]
 [0.492]
 [0.492]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.492]
 [0.492]
 [0.492]
 [0.492]]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.015]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.015]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
1465 23724
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.333]
 [0.503]
 [0.463]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.23 ]
 [0.333]
 [0.503]
 [0.463]]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.999]
 [0.999]
 [0.001]
 [0.999]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.999]
 [0.999]
 [0.001]
 [0.999]]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
1467 23772
1467 23780
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1469 23793
maxi score, test score, baseline:  0.0381 0.25 0.25
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
1469 23804
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1470 23826
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
1470 23852
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
1470 23865
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.241]
 [0.105]
 [0.075]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.075]
 [0.241]
 [0.105]
 [0.075]]
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [1.0]
siam score:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0541 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0601 0.25 0.25
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0621 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.25 0.25
probs:  [1.0]
1473 23954
maxi score, test score, baseline:  0.0601 0.25 0.25
probs:  [1.0]
1473 23992
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [1.0]
1473 23997
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [1.0]
1474 24022
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.143]
 [0.157]
 [0.157]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.157]
 [0.143]
 [0.157]
 [0.157]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1481 24095
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1481 24123
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [1.0]
1481 24190
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.208 0.208 0.375 0.208]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.175]
 [0.101]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.175]
 [0.101]
 [0.006]]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.106]
 [0.753]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.106]
 [0.753]
 [0.011]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1483 24307
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [1.0]
1483 24370
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.557]
 [0.557]
 [0.557]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.557]
 [0.557]
 [0.557]
 [0.557]]
maxi score, test score, baseline:  0.048100000000000004 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.007]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.007]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0461 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.887]
 [0.105]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.887]
 [0.105]
 [0.008]]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.202]
 [0.012]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.202]
 [0.012]
 [0.012]]
maxi score, test score, baseline:  0.0441 0.25 0.25
maxi score, test score, baseline:  0.0441 0.25 0.25
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [1.0]
1485 24411
1485 24414
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
1485 24419
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.25 0.25
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.006]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.006]
 [0.   ]]
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.25 0.25
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [1.0]
1486 24474
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.202]
 [0.021]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.119]
 [0.202]
 [0.021]
 [0.006]]
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [1.0]
1487 24524
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.186]
 [0.157]
 [0.143]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.121]
 [0.186]
 [0.157]
 [0.143]]
siam score:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
Sims:  25 1 epoch:  185797 pick best:  False frame count:  185797
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.009]
 [0.046]
 [0.031]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.009]
 [0.046]
 [0.031]]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.833]
 [0.576]
 [0.62 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.62 ]
 [0.833]
 [0.576]
 [0.62 ]]
maxi score, test score, baseline:  0.032100000000000004 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.53 ]
 [0.333]
 [0.066]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.088]
 [0.53 ]
 [0.333]
 [0.066]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
1491 24607
maxi score, test score, baseline:  0.032100000000000004 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.004]
 [0.026]
 [0.027]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.004]
 [0.026]
 [0.027]]
1491 24609
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
probs:  [1.0]
in main func line 156:  1494
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.333 0.208 0.292]
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1496 24671
1496 24680
1496 24683
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
probs:  [1.0]
1496 24686
maxi score, test score, baseline:  0.042100000000000005 0.25 0.25
probs:  [1.0]
1497 24697
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.641]
 [0.641]
 [0.641]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.641]
 [0.641]
 [0.641]
 [0.641]]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.067]
 [0.156]
 [0.014]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.067]
 [0.156]
 [0.014]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.25 0.25
maxi score, test score, baseline:  0.0361 0.25 0.25
probs:  [1.0]
1497 24770
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1498 24820
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.25 0.25
probs:  [1.0]
1499 24839
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.012]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.011]
 [0.012]
 [0.011]]
1499 24873
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.002]
 [0.013]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.002]
 [0.013]
 [0.   ]]
1500 24902
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.047]
 [0.042]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.047]
 [0.042]
 [0.01 ]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.001]
 [0.01 ]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.001]
 [0.01 ]
 [0.007]]
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0541 0.25 0.25
1502 24927
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.25 0.25
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1503 24968
maxi score, test score, baseline:  0.0601 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.898]
 [0.491]
 [0.49 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.49 ]
 [0.898]
 [0.491]
 [0.49 ]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1503 24984
siam score:  0.0
maxi score, test score, baseline:  0.0621 0.25 0.25
maxi score, test score, baseline:  0.0621 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.25 0.25
maxi score, test score, baseline:  0.0621 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0641 0.25 0.25
maxi score, test score, baseline:  0.0641 0.25 0.25
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.005]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.005]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0641 0.25 0.25
maxi score, test score, baseline:  0.0641 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.25 0.25
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.25 0.25
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.25 0.25
maxi score, test score, baseline:  0.0721 0.25 0.25
maxi score, test score, baseline:  0.0721 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0721 0.25 0.25
probs:  [1.0]
1504 25046
maxi score, test score, baseline:  0.0721 0.25 0.25
1504 25056
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.25 0.25
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.264]
 [0.065]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.264]
 [0.065]
 [0.006]]
siam score:  0.0
maxi score, test score, baseline:  0.0781 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0821 0.25 0.25
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1506 25098
maxi score, test score, baseline:  0.0821 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0801 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.08410000000000001 0.25 0.25
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0861 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.637]
 [0.637]
 [0.637]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.637]
 [0.637]
 [0.637]
 [0.637]]
maxi score, test score, baseline:  0.08410000000000001 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0861 0.25 0.25
probs:  [1.0]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0027],
        [0.0000]], dtype=torch.float64)
0.970299 0.970299
0.9801 0.9801
0.0 0.0
0.0 0.0
0.99 0.99
0.970299 0.970299
0.9801 0.9801
0.0 0.0
0.0 0.002690350250654326
0.99 0.99
maxi score, test score, baseline:  0.0861 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.0861 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0861 0.25 0.25
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0881 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.021]
 [0.024]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.021]
 [0.024]
 [0.001]]
1509 25177
siam score:  0.0
Starting evaluation
1509 25184
maxi score, test score, baseline:  0.0881 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0921 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0941 0.15 0.15
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0961 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0961 0.15 0.15
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.333 0.375 0.167 0.125]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.088]
 [0.108]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.088]
 [0.108]
 [0.003]]
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.001]
 [0.004]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.001]
 [0.004]
 [0.003]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0961 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.15 0.15
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1512 25334
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.011]
 [0.005]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.011]
 [0.005]
 [0.002]]
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.046]
 [0.02 ]
 [0.02 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.02 ]
 [0.046]
 [0.02 ]
 [0.02 ]]
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1514 25410
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.273]
 [1.031]
 [0.273]
 [0.273]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.273]
 [1.031]
 [0.273]
 [0.273]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [1.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [1.]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.417]
 [0.166]
 [0.166]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.166]
 [0.417]
 [0.166]
 [0.166]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.001]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.001]
 [0.   ]
 [0.   ]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1061 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.15 0.15
1516 25532
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1516 25601
maxi score, test score, baseline:  0.0901 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0921 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.18 ]
 [0.259]
 [0.028]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.18 ]
 [0.259]
 [0.028]]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.378]
 [0.135]
 [0.027]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.027]
 [0.378]
 [0.135]
 [0.027]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0961 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0921 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0921 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0901 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0801 0.15 0.15
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.15 0.15
1518 25733
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.029]
 [0.074]
 [0.014]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.029]
 [0.074]
 [0.014]]
maxi score, test score, baseline:  0.0761 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0781 0.15 0.15
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0801 0.15 0.15
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0721 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0721 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0721 0.15 0.15
probs:  [1.0]
1523 25809
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.03 ]
 [0.024]
 [0.024]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.024]
 [0.03 ]
 [0.024]
 [0.024]]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.117]
 [0.014]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.117]
 [0.014]
 [0.012]]
maxi score, test score, baseline:  0.0741 0.15 0.15
probs:  [1.0]
1524 25886
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0075],
        [0.0000],
        [0.0000],
        [0.0088],
        [0.0095],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000]], dtype=torch.float64)
0.0 0.007484925093757048
0.0 0.0
0.0 0.0
0.0 0.008807604884640795
0.0 0.009520991882844335
0.0 0.0
0.0 0.0
0.0 0.0
0.99 0.99
0.99 0.99
1524 25893
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1524 25943
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.033]
 [0.003]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.033]
 [0.003]
 [0.002]]
1524 25953
maxi score, test score, baseline:  0.0661 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0541 0.15 0.15
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.123]
 [0.058]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.123]
 [0.058]
 [0.002]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1524 26017
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.15 0.15
probs:  [1.0]
1524 26037
siam score:  0.0
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.15 0.15
probs:  [1.0]
1524 26076
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.15 ]
 [0.149]
 [0.065]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.15 ]
 [0.149]
 [0.065]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.093]
 [0.123]
 [0.123]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.123]
 [0.093]
 [0.123]
 [0.123]]
1524 26095
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.15 0.15
maxi score, test score, baseline:  0.0601 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.047]
 [0.657]
 [0.024]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.024]
 [0.047]
 [0.657]
 [0.024]]
maxi score, test score, baseline:  0.0601 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.229]
 [0.229]
 [0.229]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.229]
 [0.229]
 [0.229]
 [0.229]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0621 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.377]
 [0.059]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.024]
 [0.377]
 [0.059]
 [0.002]]
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
1526 26196
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.15 0.15
maxi score, test score, baseline:  0.0661 0.15 0.15
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.447]
 [0.133]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.447]
 [0.133]
 [0.018]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.15 0.15
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.15 0.15
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.15 0.15
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.15 0.15
maxi score, test score, baseline:  0.0761 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0781 0.15 0.15
probs:  [1.0]
1529 26280
UNIT TEST: sample policy line 217 mcts : [0.25  0.292 0.25  0.208]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0090],
        [0.0000],
        [0.0103]], dtype=torch.float64)
0.970299 0.970299
0.96059601 0.96059601
0.0 0.0
0.0 0.0
0.99 0.99
0.99 0.99
0.970299 0.970299
0.0 0.009024568543584804
0.970299 0.970299
0.0 0.010347006608587664
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1529 26300
maxi score, test score, baseline:  0.08410000000000001 0.15 0.15
maxi score, test score, baseline:  0.08410000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.08410000000000001 0.15 0.15
maxi score, test score, baseline:  0.08410000000000001 0.15 0.15
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0881 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0901 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0921 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.092]
 [0.015]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.092]
 [0.015]
 [0.015]]
maxi score, test score, baseline:  0.0921 0.15 0.15
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.085]
 [0.085]
 [0.085]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.085]
 [0.085]
 [0.085]
 [0.085]]
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0961 0.15 0.15
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.111]
 [0.083]
 [0.083]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.083]
 [0.111]
 [0.083]
 [0.083]]
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
1532 26459
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.053]
 [0.172]
 [0.131]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.044]
 [0.053]
 [0.172]
 [0.131]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1041 0.15 0.15
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.208 0.375]
maxi score, test score, baseline:  0.0981 0.15 0.15
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.036]
 [0.036]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.036]
 [0.036]
 [0.036]
 [0.036]]
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.15 0.15
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.11 ]
 [0.089]
 [0.092]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.072]
 [0.11 ]
 [0.089]
 [0.092]]
maxi score, test score, baseline:  0.0901 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.163]
 [0.125]
 [0.095]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.163]
 [0.163]
 [0.125]
 [0.095]]
maxi score, test score, baseline:  0.0861 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.06 ]
 [0.035]
 [0.035]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.035]
 [0.06 ]
 [0.035]
 [0.035]]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.054]
 [0.066]
 [0.065]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.028]
 [0.054]
 [0.066]
 [0.065]]
Printing some Q and Qe and total Qs values:  [[0.42]
 [0.42]
 [0.42]
 [0.42]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.42]
 [0.42]
 [0.42]
 [0.42]]
Printing some Q and Qe and total Qs values:  [[0.296]
 [1.013]
 [0.296]
 [0.296]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.296]
 [1.013]
 [0.296]
 [0.296]]
siam score:  0.0
maxi score, test score, baseline:  0.0821 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.066]
 [0.05 ]
 [0.051]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.066]
 [0.05 ]
 [0.051]]
maxi score, test score, baseline:  0.08410000000000001 0.15 0.15
probs:  [1.0]
1534 26660
maxi score, test score, baseline:  0.0781 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0761 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0721 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0721 0.15 0.15
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.15 0.15
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.056100000000000004 0.15 0.15
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.011]
 [0.011]
 [0.011]]
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
1537 26811
1537 26818
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.292]
 [1.012]
 [0.292]
 [0.292]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.292]
 [1.012]
 [0.292]
 [0.292]]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [1.0]
1538 26879
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
1539 26888
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.554]
 [0.025]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.554]
 [0.025]
 [0.   ]]
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
1539 26913
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
1539 26960
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.361]
 [0.033]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.033]
 [0.361]
 [0.033]
 [0.013]]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.526]
 [0.947]
 [0.605]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.605]
 [0.526]
 [0.947]
 [0.605]]
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.15 0.15
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.114]
 [0.012]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.114]
 [0.012]
 [0.008]]
Starting evaluation
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
1543 27040
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
1543 27042
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1543 27048
1543 27066
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.047]
 [0.018]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.047]
 [0.018]
 [0.01 ]]
1543 27079
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.005]
 [0.239]
 [0.239]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.239]
 [0.005]
 [0.239]
 [0.239]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1543 27109
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.789]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.012]
 [0.789]
 [0.012]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.219]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.011]
 [0.219]
 [0.011]]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
Sims:  25 1 epoch:  200820 pick best:  False frame count:  200820
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.004]
 [0.008]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.004]
 [0.008]
 [0.   ]]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.222]
 [0.022]
 [0.022]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.222]
 [0.022]
 [0.022]]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.321]
 [0.408]
 [0.033]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.053]
 [0.321]
 [0.408]
 [0.033]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.041]
 [0.025]
 [0.014]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.041]
 [0.025]
 [0.014]]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.056]
 [0.263]
 [0.056]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.056]
 [0.056]
 [0.263]
 [0.056]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.349]
 [0.03 ]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.066]
 [0.349]
 [0.03 ]
 [0.004]]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.322]
 [0.32 ]
 [0.034]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.034]
 [0.322]
 [0.32 ]
 [0.034]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.04 ]
 [0.014]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.04 ]
 [0.014]
 [0.009]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0541 0.1 0.1
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.1 0.1
probs:  [1.0]
1544 27250
1544 27251
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1544 27267
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.017]
 [0.01 ]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.017]
 [0.01 ]
 [0.006]]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.86 ]
 [0.097]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.86 ]
 [0.097]
 [0.021]]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
1545 27320
siam score:  0.0
maxi score, test score, baseline:  0.0601 0.1 0.1
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.533]
 [0.939]
 [0.432]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.432]
 [0.533]
 [0.939]
 [0.432]]
1545 27347
1545 27348
1545 27349
maxi score, test score, baseline:  0.058100000000000006 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.008]
 [0.035]
 [0.029]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.008]
 [0.035]
 [0.029]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.058100000000000006 0.1 0.1
maxi score, test score, baseline:  0.058100000000000006 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0621 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0621 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.125 0.208 0.375 0.292]
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.255]
 [0.054]
 [0.041]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.024]
 [0.255]
 [0.054]
 [0.041]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0601 0.1 0.1
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0641 0.1 0.1
maxi score, test score, baseline:  0.0641 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1547 27508
1547 27514
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.278]
 [0.393]
 [0.393]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.393]
 [0.278]
 [0.393]
 [0.393]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.002]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.002]
 [0.   ]
 [0.   ]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.1 0.1
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.584]
 [0.579]
 [0.579]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.579]
 [0.584]
 [0.579]
 [0.579]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.058]
 [0.057]
 [0.048]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.031]
 [0.058]
 [0.057]
 [0.048]]
maxi score, test score, baseline:  0.0661 0.1 0.1
probs:  [1.0]
1554 27663
1554 27672
maxi score, test score, baseline:  0.0621 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.1 0.1
maxi score, test score, baseline:  0.0601 0.1 0.1
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.1 0.1
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0621 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.1 0.1
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.856]
 [0.085]
 [0.085]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.085]
 [0.856]
 [0.085]
 [0.085]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.269]
 [0.127]
 [0.055]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.055]
 [0.269]
 [0.127]
 [0.055]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1556 27756
maxi score, test score, baseline:  0.0641 0.1 0.1
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.708 0.167]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.1 0.1
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0781 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0781 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0781 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0801 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1556 27831
1556 27833
maxi score, test score, baseline:  0.0821 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0821 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0821 0.1 0.1
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1556 27852
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0881 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0901 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0941 0.1 0.1
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]], dtype=torch.float64)
0.96059601 0.96059601
0.0 0.0
0.0 0.0
0.9509900498999999 0.9509900498999999
0.970299 0.970299
0.9509900498999999 0.9509900498999999
0.99 0.99
0.9509900498999999 0.9509900498999999
0.96059601 0.96059601
0.96059601 0.96059601
maxi score, test score, baseline:  0.0981 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.1 0.1021
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.1 0.1021
maxi score, test score, baseline:  0.10010000000000001 0.1 0.10010000000000001
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.808]
 [0.365]
 [0.365]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.365]
 [0.808]
 [0.365]
 [0.365]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.   ]
 [0.361]
 [0.361]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.361]
 [0.   ]
 [0.361]
 [0.361]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.1 0.1081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1101 0.1 0.1101
probs:  [1.0]
maxi score, test score, baseline:  0.1081 0.1 0.1081
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1556 27969
maxi score, test score, baseline:  0.1121 0.1 0.1121
maxi score, test score, baseline:  0.1121 0.1 0.1121
probs:  [1.0]
maxi score, test score, baseline:  0.1101 0.1 0.1101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.618]
 [0.227]
 [0.227]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.227]
 [0.618]
 [0.227]
 [0.227]]
maxi score, test score, baseline:  0.1121 0.1 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.1 0.11410000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.1 0.1121
maxi score, test score, baseline:  0.1121 0.1 0.1121
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.1 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.1121 0.1 0.1121
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.1 0.1121
probs:  [1.0]
maxi score, test score, baseline:  0.1061 0.1 0.1061
maxi score, test score, baseline:  0.1041 0.1 0.1041
probs:  [1.0]
maxi score, test score, baseline:  0.10010000000000001 0.1 0.10010000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.10010000000000001 0.1 0.10010000000000001
maxi score, test score, baseline:  0.10010000000000001 0.1 0.10010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.1 0.1041
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.1 0.1021
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.169]
 [0.018]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.169]
 [0.018]
 [0.018]]
maxi score, test score, baseline:  0.10010000000000001 0.1 0.10010000000000001
maxi score, test score, baseline:  0.10010000000000001 0.1 0.10010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.1 0.1041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.1 0.1021
maxi score, test score, baseline:  0.1021 0.1 0.1021
maxi score, test score, baseline:  0.1021 0.1 0.1021
1558 28095
1558 28110
1558 28112
maxi score, test score, baseline:  0.0961 0.1 0.1
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.10010000000000001 0.1 0.10010000000000001
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.308]
 [1.052]
 [0.601]
 [0.308]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.308]
 [1.052]
 [0.601]
 [0.308]]
maxi score, test score, baseline:  0.10010000000000001 0.1 0.10010000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.10010000000000001 0.1 0.10010000000000001
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0941 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0901 0.1 0.1
probs:  [1.0]
1559 28223
maxi score, test score, baseline:  0.0861 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0861 0.1 0.1
maxi score, test score, baseline:  0.0861 0.1 0.1
probs:  [1.0]
siam score:  0.0
1559 28254
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0001],
        [    0.0000],
        [    0.0000],
        [    0.0003],
        [    0.0000],
        [    0.0001],
        [    0.0000]], dtype=torch.float64)
0.9801 0.9801
0.99 0.99
0.0 0.0
0.0 6.26647717565737e-05
0.0 0.0
0.0 0.0
0.0 0.0003366211327722671
0.0 0.0
0.0 8.492139003617741e-05
0.99 0.99
maxi score, test score, baseline:  0.0801 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0801 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0781 0.1 0.1
maxi score, test score, baseline:  0.0781 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0761 0.1 0.1
probs:  [1.0]
1561 28286
1561 28289
maxi score, test score, baseline:  0.0761 0.1 0.1
probs:  [1.0]
1562 28291
maxi score, test score, baseline:  0.0741 0.1 0.1
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0761 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.1 0.1
probs:  [1.0]
1565 28307
maxi score, test score, baseline:  0.0721 0.1 0.1
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.057]
 [0.085]
 [0.06 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.035]
 [0.057]
 [0.085]
 [0.06 ]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1566 28317
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.1 0.1
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0821 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.1 0.1
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0821 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1566 28413
maxi score, test score, baseline:  0.08410000000000001 0.1 0.1
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.08410000000000001 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1566 28432
maxi score, test score, baseline:  0.0781 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.323]
 [0.012]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.323]
 [0.012]
 [0.002]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0821 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1566 28453
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0861 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.08410000000000001 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.08410000000000001 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0821 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0821 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0821 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.654]
 [0.112]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.654]
 [0.112]
 [0.009]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0821 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0821 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0861 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.626]
 [0.101]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.626]
 [0.101]
 [0.006]]
1567 28572
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.785]
 [0.311]
 [0.196]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.196]
 [0.785]
 [0.311]
 [0.196]]
maxi score, test score, baseline:  0.0881 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0921 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0941 0.1 0.1
maxi score, test score, baseline:  0.0941 0.1 0.1
maxi score, test score, baseline:  0.0941 0.1 0.1
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.208 0.25  0.292 0.25 ]
maxi score, test score, baseline:  0.10010000000000001 0.1 0.10010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.1 0.1021
probs:  [1.0]
1570 28646
maxi score, test score, baseline:  0.10010000000000001 0.1 0.10010000000000001
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.10010000000000001 0.1 0.10010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.1 0.1021
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.1 0.1021
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.1021 0.1 0.1021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1570 28662
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.1 0.1061
probs:  [1.0]
maxi score, test score, baseline:  0.1061 0.1 0.1061
probs:  [1.0]
maxi score, test score, baseline:  0.1061 0.1 0.1061
probs:  [1.0]
1570 28692
maxi score, test score, baseline:  0.1061 0.1 0.1061
probs:  [1.0]
maxi score, test score, baseline:  0.1061 0.1 0.1061
probs:  [1.0]
maxi score, test score, baseline:  0.1041 0.1 0.1041
maxi score, test score, baseline:  0.1041 0.1 0.1041
maxi score, test score, baseline:  0.1041 0.1 0.1041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1121 0.1 0.1121
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.1 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.1 0.11410000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.11410000000000001 0.1 0.11410000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.11410000000000001 0.1 0.11410000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.1 0.11610000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.1 0.1221
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1241 0.1 0.1241
probs:  [1.0]
Starting evaluation
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.1221 0.2 0.2
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.2 0.2
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.2 0.2
maxi score, test score, baseline:  0.1181 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.2 0.2
probs:  [1.0]
1572 28871
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.017]
 [0.017]
 [0.017]]
maxi score, test score, baseline:  0.11410000000000001 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1572 28882
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.2 0.2
maxi score, test score, baseline:  0.1081 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1101 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1101 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1101 0.2 0.2
maxi score, test score, baseline:  0.1101 0.2 0.2
maxi score, test score, baseline:  0.1101 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.247]
 [0.247]
 [0.247]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.247]
 [0.247]
 [0.247]
 [0.247]]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1121 0.2 0.2
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1101 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1101 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1121 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.2 0.2
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.776]
 [0.803]
 [0.432]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.432]
 [0.776]
 [0.803]
 [0.432]]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.166]
 [0.304]
 [0.168]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.166]
 [0.304]
 [0.168]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1181 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0002],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0003],
        [0.0000],
        [0.0004],
        [0.0000]], dtype=torch.float64)
0.99 0.99
0.0 0.0002359271960427718
0.9801 0.9801
0.99 0.99
0.99 0.99
0.99 0.99
0.0 0.0003106780909208393
0.9801 0.9801
0.0 0.0003842587399276477
0.0 0.0
maxi score, test score, baseline:  0.1201 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1201 0.2 0.2
maxi score, test score, baseline:  0.1201 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1575 28998
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1261 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1241 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1241 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.465]
 [0.278]
 [0.161]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.161]
 [0.465]
 [0.278]
 [0.161]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.2 0.2
maxi score, test score, baseline:  0.1321 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.1301 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.2 0.2
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.702]
 [0.551]
 [0.148]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.148]
 [0.702]
 [0.551]
 [0.148]]
maxi score, test score, baseline:  0.1341 0.2 0.2
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1578 29066
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.14209999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1578 29100
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1341 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.2 0.2
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1579 29171
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.2 0.2
maxi score, test score, baseline:  0.15009999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1579 29209
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.073]
 [0.648]
 [0.146]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.073]
 [0.648]
 [0.146]]
maxi score, test score, baseline:  0.15209999999999999 0.2 0.2
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
1581 29235
maxi score, test score, baseline:  0.14409999999999998 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.14209999999999998 0.2 0.2
maxi score, test score, baseline:  0.14209999999999998 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.2 0.2
maxi score, test score, baseline:  0.1361 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1381 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1401 0.2 0.2
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.2 0.2
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.2 0.2
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.216]
 [0.436]
 [0.148]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.256]
 [0.216]
 [0.436]
 [0.148]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.2 0.2
probs:  [1.0]
1583 29347
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1541 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.278]
 [0.222]
 [0.121]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.147]
 [0.278]
 [0.222]
 [0.121]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.2 0.2
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.092]
 [0.092]
 [0.092]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.092]
 [0.092]
 [0.092]
 [0.092]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1581 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.2 0.2
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1541 0.2 0.2
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.066]
 [0.066]
 [0.066]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.066]
 [0.066]
 [0.066]
 [0.066]]
maxi score, test score, baseline:  0.1541 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1541 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.1561 0.2 0.2
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1585 29452
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.143]
 [0.172]
 [0.093]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.101]
 [0.143]
 [0.172]
 [0.093]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.022]
 [0.206]
 [0.165]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.087]
 [0.022]
 [0.206]
 [0.165]]
maxi score, test score, baseline:  0.1561 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.148]
 [0.148]
 [0.148]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.148]
 [0.148]
 [0.148]
 [0.148]]
maxi score, test score, baseline:  0.1561 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1587 29521
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.2 0.2
siam score:  0.0
maxi score, test score, baseline:  0.1541 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1589 29538
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.034]
 [0.107]
 [0.184]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.092]
 [0.034]
 [0.107]
 [0.184]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.14409999999999998 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.88 ]
 [0.444]
 [0.378]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.378]
 [0.88 ]
 [0.444]
 [0.378]]
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1401 0.2 0.2
siam score:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1593 29599
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.354]
 [0.44 ]
 [0.354]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.354]
 [0.354]
 [0.44 ]
 [0.354]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.14409999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1594 29621
maxi score, test score, baseline:  0.14409999999999998 0.2 0.2
maxi score, test score, baseline:  0.14409999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.047]
 [0.238]
 [0.058]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.038]
 [0.047]
 [0.238]
 [0.058]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.14609999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.063]
 [0.124]
 [0.046]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.028]
 [0.063]
 [0.124]
 [0.046]]
maxi score, test score, baseline:  0.14409999999999998 0.2 0.2
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1595 29662
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.367]
 [0.367]
 [0.367]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.367]
 [0.367]
 [0.367]
 [0.367]]
maxi score, test score, baseline:  0.1381 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1381 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.893]
 [0.43 ]
 [0.322]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.322]
 [0.893]
 [0.43 ]
 [0.322]]
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.022]
 [0.273]
 [0.207]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.112]
 [0.022]
 [0.273]
 [0.207]]
maxi score, test score, baseline:  0.1361 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.2 0.2
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1599 29725
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1381 0.2 0.2
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.082]
 [0.126]
 [0.123]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.135]
 [0.082]
 [0.126]
 [0.123]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1599 29762
1599 29768
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1599 29776
maxi score, test score, baseline:  0.1321 0.2 0.2
maxi score, test score, baseline:  0.1321 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.135]
 [0.305]
 [0.07 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.051]
 [0.135]
 [0.305]
 [0.07 ]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.2 0.2
1600 29809
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.2 0.2
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1341 0.2 0.2
maxi score, test score, baseline:  0.1341 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.2 0.2
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1602 29877
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.06 ]
 [0.126]
 [0.401]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.401]
 [0.06 ]
 [0.126]
 [0.401]]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.044]
 [0.554]
 [0.06 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.044]
 [0.554]
 [0.06 ]]
maxi score, test score, baseline:  0.1381 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1381 0.2 0.2
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
1603 29966
siam score:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.792 0.083]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1603 29980
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1604 29995
maxi score, test score, baseline:  0.14409999999999998 0.2 0.2
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1401 0.2 0.2
maxi score, test score, baseline:  0.1401 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.2 0.2
maxi score, test score, baseline:  0.14609999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.14609999999999998 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.14609999999999998 0.2 0.2
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.2 0.2
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1608 30114
maxi score, test score, baseline:  0.1581 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1601 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1601 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1641 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1641 0.2 0.2
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.347]
 [0.6  ]
 [0.603]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.603]
 [0.347]
 [0.6  ]
 [0.603]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.248]
 [0.278]
 [0.233]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.148]
 [0.248]
 [0.278]
 [0.233]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1610 30156
maxi score, test score, baseline:  0.1661 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.2 0.2
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.375]
 [0.168]
 [0.168]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.168]
 [0.375]
 [0.168]
 [0.168]]
maxi score, test score, baseline:  0.1621 0.2 0.2
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1641 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.2 0.2
maxi score, test score, baseline:  0.1661 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.2 0.2
maxi score, test score, baseline:  0.1661 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1613 30214
maxi score, test score, baseline:  0.1661 0.2 0.2
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.2 0.2
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.192]
 [0.177]
 [0.089]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.032]
 [0.192]
 [0.177]
 [0.089]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.2 0.2
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.015]
 [0.438]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.015]
 [0.438]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.14 ]
 [0.423]
 [0.226]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.226]
 [0.14 ]
 [0.423]
 [0.226]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1613 30258
maxi score, test score, baseline:  0.1681 0.2 0.2
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.2 0.2
maxi score, test score, baseline:  0.1661 0.2 0.2
probs:  [1.0]
Starting evaluation
rdn probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.058]
 [0.111]
 [0.068]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.055]
 [0.058]
 [0.111]
 [0.068]]
maxi score, test score, baseline:  0.1541 0.0 0.1541
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.651]
 [0.371]
 [0.05 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.05 ]
 [0.651]
 [0.371]
 [0.05 ]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.287]
 [0.067]
 [0.039]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.287]
 [0.067]
 [0.039]]
1613 30299
maxi score, test score, baseline:  0.1401 0.0 0.1401
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.0 0.1361
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.0 0.1361
probs:  [1.0]
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.1221 0.0 0.1221
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1614 30367
maxi score, test score, baseline:  0.1201 0.0 0.1201
probs:  [1.0]
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.862]
 [0.43 ]
 [0.43 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.43 ]
 [0.862]
 [0.43 ]
 [0.43 ]]
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1101 0.0 0.1101
probs:  [1.0]
maxi score, test score, baseline:  0.1101 0.0 0.1101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.125 0.208 0.417 0.25 ]
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1201 0.0 0.1201
UNIT TEST: sample policy line 217 mcts : [0.125 0.167 0.583 0.125]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1221 0.0 0.1221
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.0 0.1181
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
probs:  [1.0]
1614 30479
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1101 0.0 0.1101
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.1061 0.0 0.1061
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
1614 30551
maxi score, test score, baseline:  0.10010000000000001 0.0 0.10010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1614 30568
maxi score, test score, baseline:  0.0961 0.0 0.0961
probs:  [1.0]
1614 30574
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1614 30590
maxi score, test score, baseline:  0.08410000000000001 0.0 0.08410000000000001
probs:  [1.0]
1614 30604
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
1614 30631
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1614 30643
maxi score, test score, baseline:  0.08410000000000001 0.0 0.08410000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.08410000000000001 0.0 0.08410000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1615 30661
maxi score, test score, baseline:  0.08410000000000001 0.0 0.08410000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.08410000000000001 0.0 0.08410000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.0821 0.0 0.0821
probs:  [1.0]
maxi score, test score, baseline:  0.0821 0.0 0.0821
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.261]
 [0.154]
 [0.172]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.172]
 [0.261]
 [0.154]
 [0.172]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0781 0.0 0.0781
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.014]
 [0.132]
 [0.022]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.014]
 [0.132]
 [0.022]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1616 30730
maxi score, test score, baseline:  0.0801 0.0 0.0801
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.178]
 [0.178]
 [0.178]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.178]
 [0.178]
 [0.178]
 [0.178]]
maxi score, test score, baseline:  0.0801 0.0 0.0801
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1616 30740
1616 30747
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.0 0.0801
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.196]
 [0.354]
 [0.196]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.196]
 [0.196]
 [0.354]
 [0.196]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1616 30774
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0901 0.0 0.0901
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.325]
 [0.625]
 [0.323]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.323]
 [0.325]
 [0.625]
 [0.323]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0921 0.0 0.0921
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1617 30800
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0981 0.0 0.0981
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.0 0.1021
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.0 0.1021
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1041 0.0 0.1041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1101 0.0 0.1101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.459]
 [0.751]
 [0.751]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.751]
 [0.459]
 [0.751]
 [0.751]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1201 0.0 0.1201
probs:  [1.0]
maxi score, test score, baseline:  0.1201 0.0 0.1201
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1241 0.0 0.1241
maxi score, test score, baseline:  0.1241 0.0 0.1241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1241 0.0 0.1241
probs:  [1.0]
maxi score, test score, baseline:  0.1241 0.0 0.1241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.047]
 [0.139]
 [0.139]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.139]
 [0.047]
 [0.139]
 [0.139]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.0 0.1261
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1617 30895
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.101]
 [0.101]
 [0.102]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.102]
 [0.101]
 [0.101]
 [0.102]]
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1201 0.0 0.1201
probs:  [1.0]
1618 30987
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.0 0.1221
maxi score, test score, baseline:  0.1221 0.0 0.1221
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.0 0.1221
probs:  [1.0]
1618 30999
maxi score, test score, baseline:  0.1221 0.0 0.1221
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
1618 31020
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.0 0.1281
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.0 0.1301
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.151]
 [0.455]
 [0.289]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.289]
 [0.151]
 [0.455]
 [0.289]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.131]
 [0.154]
 [0.11 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.028]
 [0.131]
 [0.154]
 [0.11 ]]
maxi score, test score, baseline:  0.1341 0.0 0.1341
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1361 0.0 0.1361
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.0 0.14209999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.14209999999999998 0.0 0.14209999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.0 0.14409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.127]
 [0.127]
 [0.109]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.127]
 [0.127]
 [0.127]
 [0.109]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1619 31102
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.714]
 [0.15 ]
 [0.044]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.041]
 [0.714]
 [0.15 ]
 [0.044]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.0 0.15009999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1619 31130
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
siam score:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.0 0.14409999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[1]
 [1]
 [1]
 [1]] [[0]
 [0]
 [0]
 [0]] [[1]
 [1]
 [1]
 [1]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.0 0.14609999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.027]
 [0.235]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.027]
 [0.235]
 [0.007]]
maxi score, test score, baseline:  0.14609999999999998 0.0 0.14609999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.219]
 [1.023]
 [0.219]
 [0.219]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.219]
 [1.023]
 [0.219]
 [0.219]]
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
siam score:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.0 0.14609999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1619 31194
maxi score, test score, baseline:  0.14609999999999998 0.0 0.14609999999999998
1619 31205
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.0 0.14609999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.339]
 [0.548]
 [0.344]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.344]
 [0.339]
 [0.548]
 [0.344]]
maxi score, test score, baseline:  0.14209999999999998 0.0 0.14209999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.0 0.14209999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.0 0.1381
probs:  [1.0]
maxi score, test score, baseline:  0.1381 0.0 0.1381
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
1619 31278
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.0 0.1341
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.0 0.1321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1619 31311
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1619 31321
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1619 31326
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1101 0.0 0.1101
probs:  [1.0]
maxi score, test score, baseline:  0.1101 0.0 0.1101
probs:  [1.0]
1619 31387
maxi score, test score, baseline:  0.1061 0.0 0.1061
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0981 0.0 0.0981
probs:  [1.0]
maxi score, test score, baseline:  0.0961 0.0 0.0961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.10010000000000001 0.0 0.10010000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.10010000000000001 0.0 0.10010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.0 0.1041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.018]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.018]
 [0.018]
 [0.018]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.0 0.1041
probs:  [1.0]
maxi score, test score, baseline:  0.1041 0.0 0.1041
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.0 0.1041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.0 0.1041
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.0 0.1021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.0 0.1041
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
maxi score, test score, baseline:  0.1061 0.0 0.1061
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.014]
 [0.28 ]
 [0.121]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.014]
 [0.28 ]
 [0.121]]
siam score:  0.0
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.0 0.1061
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
probs:  [1.0]
1625 31537
1625 31538
1625 31546
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.083]
 [0.115]
 [0.08 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.036]
 [0.083]
 [0.115]
 [0.08 ]]
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1201 0.0 0.1201
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.0 0.1221
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1241 0.0 0.1241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
1627 31628
siam score:  0.0
maxi score, test score, baseline:  0.1261 0.0 0.1261
maxi score, test score, baseline:  0.1261 0.0 0.1261
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.918]
 [0.537]
 [0.515]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.515]
 [0.918]
 [0.537]
 [0.515]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.0 0.1341
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.0 0.1381
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.0 0.1541
probs:  [1.0]
maxi score, test score, baseline:  0.1541 0.0 0.1541
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.0 0.1581
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.0 0.1601
probs:  [1.0]
maxi score, test score, baseline:  0.1601 0.0 0.1601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17409999999999998 0.0 0.17409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.49 ]
 [0.562]
 [0.49 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.49 ]
 [0.49 ]
 [0.562]
 [0.49 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.0 0.18409999999999999
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.161]
 [0.148]
 [0.247]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.247]
 [0.161]
 [0.148]
 [0.247]]
maxi score, test score, baseline:  0.1861 0.0 0.1861
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1901 0.0 0.1901
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.159]
 [0.159]
 [0.159]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.159]
 [0.159]
 [0.159]
 [0.159]]
maxi score, test score, baseline:  0.1901 0.0 0.1901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1881 0.0 0.1881
probs:  [1.0]
maxi score, test score, baseline:  0.1881 0.0 0.1881
probs:  [1.0]
maxi score, test score, baseline:  0.1881 0.0 0.1881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1627 31785
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.0 0.1941
probs:  [1.0]
maxi score, test score, baseline:  0.1941 0.0 0.1941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.0 0.1961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2021 0.0 0.2021
probs:  [1.0]
maxi score, test score, baseline:  0.2001 0.0 0.2001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20409999999999998 0.0 0.20409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20409999999999998 0.0 0.20409999999999998
maxi score, test score, baseline:  0.20409999999999998 0.0 0.20409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1630 31848
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21009999999999998 0.0 0.21009999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.21009999999999998 0.0 0.21009999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21209999999999998 0.0 0.21209999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2161 0.0 0.2161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2201 0.0 0.2201
maxi score, test score, baseline:  0.2201 0.0 0.2201
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1630 31892
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1631 31900
maxi score, test score, baseline:  0.2261 0.0 0.2261
probs:  [1.0]
1631 31909
maxi score, test score, baseline:  0.2241 0.0 0.2241
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.802]
 [0.344]
 [0.695]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.695]
 [0.802]
 [0.344]
 [0.695]]
maxi score, test score, baseline:  0.2201 0.0 0.2201
probs:  [1.0]
maxi score, test score, baseline:  0.2161 0.0 0.2161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21409999999999998 0.0 0.21409999999999998
probs:  [1.0]
1631 31944
maxi score, test score, baseline:  0.20809999999999998 0.0 0.20809999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.2021 0.0 0.2021
probs:  [1.0]
maxi score, test score, baseline:  0.2001 0.0 0.2001
maxi score, test score, baseline:  0.1981 0.0 0.1981
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [1.013]
 [0.516]
 [0.2  ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.2  ]
 [1.013]
 [0.516]
 [0.2  ]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.67 ]
 [0.014]
 [0.095]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.67 ]
 [0.014]
 [0.095]]
1631 31994
maxi score, test score, baseline:  0.1981 0.0 0.1981
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.0 0.1981
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2001 0.0 0.2001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.046]
 [0.053]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.046]
 [0.053]
 [0.011]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1901 0.0 0.1901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.0 0.1921
probs:  [1.0]
maxi score, test score, baseline:  0.1901 0.0 0.1901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1881 0.0 0.1881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1881 0.0 0.1881
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1631 32072
maxi score, test score, baseline:  0.1861 0.0 0.1861
probs:  [1.0]
maxi score, test score, baseline:  0.18209999999999998 0.0 0.18209999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1631 32086
maxi score, test score, baseline:  0.18009999999999998 0.0 0.18009999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.327]
 [0.192]
 [0.04 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.327]
 [0.192]
 [0.04 ]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.0 0.18009999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.0 0.17609999999999998
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.   ]
 [0.096]
 [0.063]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.   ]
 [0.096]
 [0.063]]
maxi score, test score, baseline:  0.1561 0.0 0.1561
probs:  [1.0]
maxi score, test score, baseline:  0.15209999999999999 0.0 0.15209999999999999
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
probs:  [1.0]
1631 32212
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.076]
 [0.795]
 [0.051]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.051]
 [0.076]
 [0.795]
 [0.051]]
1631 32218
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
1631 32236
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.04 ]
 [0.179]
 [0.115]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.04 ]
 [0.179]
 [0.115]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
maxi score, test score, baseline:  0.1081 0.0 0.1081
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.1081 0.0 0.1081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.415]
 [0.27 ]
 [0.186]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.186]
 [0.415]
 [0.27 ]
 [0.186]]
maxi score, test score, baseline:  0.1021 0.0 0.1021
maxi score, test score, baseline:  0.1021 0.0 0.1021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1631 32293
maxi score, test score, baseline:  0.1021 0.0 0.1021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.0 0.1021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.0 0.1081
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.0 0.1021
probs:  [1.0]
1631 32320
1631 32334
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.086]
 [0.106]
 [0.023]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.034]
 [0.086]
 [0.106]
 [0.023]]
maxi score, test score, baseline:  0.0861 0.0 0.0861
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.992]
 [0.706]
 [0.377]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.377]
 [0.992]
 [0.706]
 [0.377]]
maxi score, test score, baseline:  0.0821 0.0 0.0821
maxi score, test score, baseline:  0.0821 0.0 0.0821
maxi score, test score, baseline:  0.0821 0.0 0.0821
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1632 32385
maxi score, test score, baseline:  0.0881 0.0 0.0881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0941 0.0 0.0941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0961 0.0 0.0961
probs:  [1.0]
maxi score, test score, baseline:  0.0961 0.0 0.0961
probs:  [1.0]
maxi score, test score, baseline:  0.0961 0.0 0.0961
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.39 ]
 [0.783]
 [0.783]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.783]
 [0.39 ]
 [0.783]
 [0.783]]
1632 32453
1632 32460
1632 32468
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.922]
 [0.304]
 [0.304]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.304]
 [0.922]
 [0.304]
 [0.304]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0901 0.0 0.0901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0881 0.0 0.0881
maxi score, test score, baseline:  0.0861 0.0 0.0861
probs:  [1.0]
maxi score, test score, baseline:  0.0861 0.0 0.0861
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0941 0.0 0.0941
probs:  [1.0]
maxi score, test score, baseline:  0.0941 0.0 0.0941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.579]
 [0.934]
 [0.491]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.491]
 [0.579]
 [0.934]
 [0.491]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.10010000000000001 0.0 0.10010000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.10010000000000001 0.0 0.10010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.257]
 [0.257]
 [0.257]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.257]
 [0.257]
 [0.257]
 [0.257]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.10010000000000001 0.0 0.10010000000000001
probs:  [1.0]
1633 32557
maxi score, test score, baseline:  0.0961 0.0 0.0961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.0 0.1021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.022]
 [0.022]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.022]
 [0.022]
 [0.022]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.0 0.1061
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
maxi score, test score, baseline:  0.1061 0.0 0.1061
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.0 0.1081
probs:  [1.0]
maxi score, test score, baseline:  0.1081 0.0 0.1081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.0 0.1081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1081 0.0 0.1081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.006]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.006]
 [0.   ]
 [0.   ]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1201 0.0 0.1201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.467]
 [0.215]
 [0.215]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.215]
 [0.467]
 [0.215]
 [0.215]]
siam score:  0.0
maxi score, test score, baseline:  0.1281 0.0 0.1281
maxi score, test score, baseline:  0.1281 0.0 0.1281
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.0 0.1301
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.0 0.1341
probs:  [1.0]
maxi score, test score, baseline:  0.1341 0.0 0.1341
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.0 0.1361
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1636 32687
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1636 32692
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.0 0.15009999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1637 32703
maxi score, test score, baseline:  0.1581 0.0 0.1581
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.0 0.1601
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.3  ]
 [0.158]
 [0.078]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.134]
 [0.3  ]
 [0.158]
 [0.078]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.0 0.1601
probs:  [1.0]
maxi score, test score, baseline:  0.1601 0.0 0.1601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.0 0.1621
maxi score, test score, baseline:  0.1621 0.0 0.1621
maxi score, test score, baseline:  0.1621 0.0 0.1621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.255]
 [0.382]
 [0.255]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.255]
 [0.255]
 [0.382]
 [0.255]]
maxi score, test score, baseline:  0.1641 0.0 0.1641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.0 0.1621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.0 0.1621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1641 0.0 0.1641
maxi score, test score, baseline:  0.1641 0.0 0.1641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.0 0.1661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.0 0.1661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.0 0.1681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17409999999999998 0.0 0.17409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1638 32799
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.0 0.17609999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.636]
 [1.041]
 [0.317]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.317]
 [0.636]
 [1.041]
 [0.317]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18209999999999998 0.0 0.18209999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1638 32868
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.0 0.1921
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1901 0.0 0.1901
maxi score, test score, baseline:  0.1901 0.0 0.1901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.0 0.1921
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.0 0.1941
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.0 0.1941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.097]
 [0.097]
 [0.097]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.097]
 [0.097]
 [0.097]
 [0.097]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2001 0.0 0.2001
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20609999999999998 0.0 0.20609999999999998
probs:  [1.0]
1639 32910
maxi score, test score, baseline:  0.20409999999999998 0.0 0.20409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20809999999999998 0.0 0.20809999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1639 32930
maxi score, test score, baseline:  0.21209999999999998 0.0 0.21209999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21409999999999998 0.0 0.21409999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.21409999999999998 0.0 0.21409999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.21409999999999998 0.0 0.21409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2161 0.0 0.2161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.2181 0.0 0.2181
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1639 32977
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2181 0.0 0.2181
probs:  [1.0]
maxi score, test score, baseline:  0.2161 0.0 0.2161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1639 33018
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21409999999999998 0.0 0.21409999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.21209999999999998 0.0 0.21209999999999998
maxi score, test score, baseline:  0.21209999999999998 0.0 0.21209999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.135]
 [0.209]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.135]
 [0.209]
 [0.001]]
maxi score, test score, baseline:  0.21209999999999998 0.0 0.21209999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.21209999999999998 0.0 0.21209999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.21209999999999998 0.0 0.21209999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.21209999999999998 0.0 0.21209999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.21009999999999998 0.0 0.21009999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.339]
 [0.493]
 [0.339]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.339]
 [0.339]
 [0.493]
 [0.339]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.0 0.1981
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.0 0.1981
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2001 0.0 0.2001
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1639 33102
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1639 33117
maxi score, test score, baseline:  0.20409999999999998 0.0 0.20409999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.20409999999999998 0.0 0.20409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20609999999999998 0.0 0.20609999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20609999999999998 0.0 0.20609999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.059]
 [0.059]
 [0.059]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.059]
 [0.059]
 [0.059]
 [0.059]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.33]
 [0.33]
 [0.33]
 [0.33]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.33]
 [0.33]
 [0.33]
 [0.33]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.0 0.1981
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20409999999999998 0.0 0.20409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2021 0.0 0.2021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.0 0.1961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.187]
 [0.056]
 [0.023]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.046]
 [0.187]
 [0.056]
 [0.023]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.0 0.1921
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.737]
 [0.196]
 [0.196]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.196]
 [0.737]
 [0.196]
 [0.196]]
1643 33216
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1901 0.0 0.1901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.0 0.1921
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.441]
 [0.356]
 [0.071]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.081]
 [0.441]
 [0.356]
 [0.071]]
maxi score, test score, baseline:  0.1921 0.0 0.1921
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.0 0.1921
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1643 33242
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.0 0.18409999999999999
probs:  [1.0]
maxi score, test score, baseline:  0.18209999999999998 0.0 0.18209999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1643 33279
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.066]
 [0.156]
 [0.035]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.066]
 [0.156]
 [0.035]]
1643 33293
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.073]
 [0.168]
 [0.019]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.034]
 [0.073]
 [0.168]
 [0.019]]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.118]
 [0.029]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.118]
 [0.029]
 [0.021]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.119]
 [0.137]
 [0.137]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.137]
 [0.119]
 [0.137]
 [0.137]]
maxi score, test score, baseline:  0.17409999999999998 0.0 0.17409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.007]
 [0.822]
 [0.043]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.043]
 [0.007]
 [0.822]
 [0.043]]
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
probs:  [1.0]
siam score:  0.0
1643 33339
maxi score, test score, baseline:  0.17409999999999998 0.0 0.17409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.0 0.1661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.0 0.1661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1643 33385
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.908]
 [0.157]
 [0.102]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.219]
 [0.908]
 [0.157]
 [0.102]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.0 0.1581
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.054]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.054]
 [0.054]
 [0.   ]
 [0.   ]]
1643 33403
maxi score, test score, baseline:  0.15009999999999998 0.0 0.15009999999999998
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.0 0.14609999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1643 33441
maxi score, test score, baseline:  0.15009999999999998 0.0 0.15009999999999998
maxi score, test score, baseline:  0.15009999999999998 0.0 0.15009999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.0 0.1581
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.209]
 [0.209]
 [0.209]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.209]
 [0.209]
 [0.209]
 [0.209]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.0 0.1601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.0 0.1621
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.238]
 [0.139]
 [0.034]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.034]
 [0.238]
 [0.139]
 [0.034]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.0 0.1621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.0 0.1601
probs:  [1.0]
maxi score, test score, baseline:  0.1601 0.0 0.1601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.0 0.1601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.0 0.1621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.0 0.1621
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.187]
 [0.167]
 [0.121]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.041]
 [0.187]
 [0.167]
 [0.121]]
maxi score, test score, baseline:  0.1621 0.0 0.1621
probs:  [1.0]
maxi score, test score, baseline:  0.1621 0.0 0.1621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1644 33533
maxi score, test score, baseline:  0.1641 0.0 0.1641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.0 0.1661
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.0 0.1661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.088]
 [0.208]
 [0.07 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.07 ]
 [0.088]
 [0.208]
 [0.07 ]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.483]
 [0.211]
 [0.122]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.122]
 [0.483]
 [0.211]
 [0.122]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.0 0.17209999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.0 0.17609999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.0 0.17609999999999998
probs:  [1.0]
1644 33603
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.0 0.17609999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.0 0.17609999999999998
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.0 0.1681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.0 0.1661
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.134]
 [0.393]
 [0.111]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.111]
 [0.134]
 [0.393]
 [0.111]]
maxi score, test score, baseline:  0.1661 0.0 0.1661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.0 0.1661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.0 0.1621
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.019]
 [0.048]
 [0.048]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.048]
 [0.019]
 [0.048]
 [0.048]]
UNIT TEST: sample policy line 217 mcts : [0.167 0.333 0.458 0.042]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.853]
 [0.614]
 [0.526]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.335]
 [0.853]
 [0.614]
 [0.526]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.0 0.1621
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.468]
 [0.468]
 [0.468]
 [0.468]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.0 0.1581
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.0 0.1581
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1644 33746
maxi score, test score, baseline:  0.1601 0.0 0.1601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.0 0.1621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.0 0.1661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1645 33760
1645 33762
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.0 0.1681
probs:  [1.0]
maxi score, test score, baseline:  0.1681 0.0 0.1681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.063]
 [0.364]
 [0.324]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.324]
 [0.063]
 [0.364]
 [0.324]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.115]
 [0.609]
 [0.266]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.266]
 [0.115]
 [0.609]
 [0.266]]
maxi score, test score, baseline:  0.18009999999999998 0.0 0.18009999999999998
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.531]
 [0.813]
 [0.661]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.661]
 [0.531]
 [0.813]
 [0.661]]
maxi score, test score, baseline:  0.17409999999999998 0.0 0.17409999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.218]
 [0.257]
 [0.191]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.106]
 [0.218]
 [0.257]
 [0.191]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.0 0.17609999999999998
probs:  [1.0]
1646 33836
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.0 0.17809999999999998
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1646 33851
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1901 0.0 0.1901
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.246]
 [0.303]
 [0.189]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.189]
 [0.246]
 [0.303]
 [0.189]]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.275]
 [0.275]
 [0.275]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.275]
 [0.275]
 [0.275]
 [0.275]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.0 0.1921
probs:  [1.0]
1646 33864
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.0 0.1921
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.0 0.1921
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.0 0.1941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.0 0.1941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.275]
 [0.269]
 [0.157]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.07 ]
 [0.275]
 [0.269]
 [0.157]]
maxi score, test score, baseline:  0.1981 0.0 0.1981
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.656]
 [0.174]
 [0.142]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.075]
 [0.656]
 [0.174]
 [0.142]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2021 0.0 0.2021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.649]
 [0.092]
 [0.167]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.167]
 [0.649]
 [0.092]
 [0.167]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20809999999999998 0.0 0.20809999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.025]
 [0.178]
 [0.079]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.113]
 [0.025]
 [0.178]
 [0.079]]
maxi score, test score, baseline:  0.2001 0.0 0.2001
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.51]
 [0.51]
 [0.51]
 [0.51]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.51]
 [0.51]
 [0.51]
 [0.51]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
siam score:  0.0
maxi score, test score, baseline:  0.2021 0.0 0.2021
1646 33982
maxi score, test score, baseline:  0.2001 0.0 0.2001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.0 0.1961
probs:  [1.0]
maxi score, test score, baseline:  0.1961 0.0 0.1961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.0 0.1941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2161 0.45 0.45
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2161 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2181 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2201 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2181 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2181 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2241 0.45 0.45
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2221 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.19]
 [0.19]
 [0.19]
 [0.19]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.19]
 [0.19]
 [0.19]
 [0.19]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2281 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.45 0.45
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.2281 0.45 0.45
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2301 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.45 0.45
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2321 0.45 0.45
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23609999999999998 0.45 0.45
maxi score, test score, baseline:  0.23609999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.612]
 [0.218]
 [0.507]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.507]
 [0.612]
 [0.218]
 [0.507]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.074]
 [0.49 ]
 [0.161]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.161]
 [0.074]
 [0.49 ]
 [0.161]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2501 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1646 34119
maxi score, test score, baseline:  0.2521 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2521 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.2581 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1646 34137
maxi score, test score, baseline:  0.2641 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2661 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2661 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.2661 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2621 0.45 0.45
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.512]
 [0.512]
 [0.512]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.512]
 [0.512]
 [0.512]
 [0.512]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2661 0.45 0.45
maxi score, test score, baseline:  0.2661 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.875 0.042]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2741 0.45 0.45
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2721 0.45 0.45
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2781 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2761 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.594]
 [0.087]
 [0.052]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.109]
 [0.594]
 [0.087]
 [0.052]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2781 0.45 0.45
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
1646 34218
maxi score, test score, baseline:  0.28409999999999996 0.45 0.45
maxi score, test score, baseline:  0.28409999999999996 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28609999999999997 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28609999999999997 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.28609999999999997 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [1.036]
 [0.509]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.509]
 [0.509]
 [1.036]
 [0.509]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28809999999999997 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28609999999999997 0.45 0.45
probs:  [1.0]
1647 34252
1647 34259
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.347]
 [0.702]
 [0.347]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.347]
 [0.347]
 [0.702]
 [0.347]]
maxi score, test score, baseline:  0.28209999999999996 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2801 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.451]
 [0.121]
 [0.18 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.119]
 [0.451]
 [0.121]
 [0.18 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.286]
 [0.318]
 [0.179]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.179]
 [0.286]
 [0.318]
 [0.179]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2941 0.45 0.45
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.461]
 [0.929]
 [0.256]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.256]
 [0.461]
 [0.929]
 [0.256]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.45 0.45
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.45 0.45
maxi score, test score, baseline:  0.3021 0.45 0.45
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.143]
 [0.394]
 [0.387]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.161]
 [0.143]
 [0.394]
 [0.387]]
maxi score, test score, baseline:  0.3001 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.3021 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.239]
 [0.48 ]
 [0.038]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.038]
 [0.239]
 [0.48 ]
 [0.038]]
maxi score, test score, baseline:  0.29209999999999997 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28809999999999997 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28809999999999997 0.45 0.45
probs:  [1.0]
siam score:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.204]
 [0.859]
 [0.43 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.43 ]
 [0.204]
 [0.859]
 [0.43 ]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28209999999999996 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28209999999999996 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28409999999999996 0.45 0.45
maxi score, test score, baseline:  0.28409999999999996 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.45 0.45
1648 34395
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.28809999999999997 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.28809999999999997 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2941 0.45 0.45
maxi score, test score, baseline:  0.2941 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1648 34424
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.264]
 [0.335]
 [0.271]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.293]
 [0.264]
 [0.335]
 [0.271]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1648 34429
maxi score, test score, baseline:  0.3041 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.613]
 [0.198]
 [0.105]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.118]
 [0.613]
 [0.198]
 [0.105]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.45 0.45
maxi score, test score, baseline:  0.3121 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.228]
 [0.228]
 [0.228]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.336]
 [0.228]
 [0.228]
 [0.228]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.45 0.45
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.45 0.45
1648 34457
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.348]
 [0.348]
 [0.348]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.348]
 [0.348]
 [0.348]
 [0.348]]
maxi score, test score, baseline:  0.3121 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.45 0.45
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.3201 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1649 34481
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.083 0.583 0.333 0.   ]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.45 0.45
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.208 0.333 0.292]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.45 0.45
maxi score, test score, baseline:  0.3121 0.45 0.45
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2961 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2941 0.45 0.45
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1649 34581
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.258]
 [0.258]
 [0.258]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.258]
 [0.258]
 [0.258]
 [0.258]]
1649 34591
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.28609999999999997 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28609999999999997 0.45 0.45
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2781 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2781 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2721 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2681 0.45 0.45
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.2561 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2561 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2621 0.45 0.45
1649 34672
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2561 0.45 0.45
maxi score, test score, baseline:  0.2561 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.45 0.45
maxi score, test score, baseline:  0.24609999999999999 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.24609999999999999 0.45 0.45
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
1649 34702
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1649 34715
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.524]
 [0.411]
 [0.411]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.411]
 [0.524]
 [0.411]
 [0.411]]
maxi score, test score, baseline:  0.24209999999999998 0.45 0.45
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.45 0.45
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.24209999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.24009999999999998 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.24009999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.171]
 [0.694]
 [0.262]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.262]
 [0.171]
 [0.694]
 [0.262]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.45 0.45
1649 34785
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2341 0.45 0.45
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[1.052]
 [0.474]
 [1.052]
 [1.052]] [[0.]
 [0.]
 [0.]
 [0.]] [[1.052]
 [0.474]
 [1.052]
 [1.052]]
maxi score, test score, baseline:  0.2301 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2321 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2321 0.45 0.45
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2341 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23609999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23609999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.45 0.45
maxi score, test score, baseline:  0.24009999999999998 0.45 0.45
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1650 34837
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.45 0.45
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.45 0.45
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.936]
 [0.277]
 [0.277]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.277]
 [0.936]
 [0.277]
 [0.277]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.02 ]
 [0.278]
 [0.24 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.174]
 [0.02 ]
 [0.278]
 [0.24 ]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2501 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2521 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2501 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.45 0.45
maxi score, test score, baseline:  0.2521 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.208 0.417 0.25  0.125]
maxi score, test score, baseline:  0.2541 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2561 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.45 0.45
maxi score, test score, baseline:  0.2501 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2501 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2501 0.45 0.45
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.45 0.45
maxi score, test score, baseline:  0.24209999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.24209999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1653 34987
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.329]
 [0.358]
 [0.329]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.329]
 [0.329]
 [0.358]
 [0.329]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
1653 35012
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1653 35022
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2341 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.45 0.45
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.511]
 [0.246]
 [0.205]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.161]
 [0.511]
 [0.246]
 [0.205]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.423]
 [0.423]
 [0.423]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.423]
 [0.423]
 [0.423]
 [0.423]]
maxi score, test score, baseline:  0.24209999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1654 35059
maxi score, test score, baseline:  0.24609999999999999 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.24409999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1655 35072
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.615]
 [0.334]
 [0.334]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.334]
 [0.615]
 [0.334]
 [0.334]]
maxi score, test score, baseline:  0.2481 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2501 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.4  ]
 [0.779]
 [0.779]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.779]
 [0.4  ]
 [0.779]
 [0.779]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.45 0.45
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.338]
 [0.42 ]
 [0.437]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.437]
 [0.338]
 [0.42 ]
 [0.437]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2601 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2601 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.45 0.45
maxi score, test score, baseline:  0.2581 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.207]
 [0.63 ]
 [0.207]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.207]
 [0.207]
 [0.63 ]
 [0.207]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1656 35106
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1656 35114
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2641 0.45 0.45
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2641 0.45 0.45
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.262]
 [0.262]
 [0.262]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.262]
 [0.262]
 [0.262]
 [0.262]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1656 35125
Printing some Q and Qe and total Qs values:  [[0.173]
 [0.247]
 [0.244]
 [0.265]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.173]
 [0.247]
 [0.244]
 [0.265]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2701 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2721 0.45 0.45
maxi score, test score, baseline:  0.2721 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2701 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2681 0.45 0.45
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.45 0.45
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.362]
 [0.184]
 [0.113]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.166]
 [0.362]
 [0.184]
 [0.113]]
siam score:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.23809999999999998 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.23809999999999998 0.45 0.45
1656 35218
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.45 0.45
maxi score, test score, baseline:  0.23809999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1656 35234
maxi score, test score, baseline:  0.23609999999999998 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.23609999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2341 0.45 0.45
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.396]
 [0.418]
 [0.396]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.396]
 [0.396]
 [0.418]
 [0.396]]
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.723]
 [0.723]
 [0.723]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.723]
 [0.723]
 [0.723]
 [0.723]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.2341 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2261 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.2281 0.45 0.45
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.24209999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.45 0.45
probs:  [1.0]
1657 35370
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.45 0.45
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23609999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2341 0.45 0.45
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1657 35450
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.455]
 [0.197]
 [0.253]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.253]
 [0.455]
 [0.197]
 [0.253]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2241 0.45 0.45
probs:  [1.0]
1658 35471
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2261 0.45 0.45
1658 35481
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
1658 35490
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2261 0.45 0.45
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.208]
 [0.204]
 [0.204]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.204]
 [0.208]
 [0.204]
 [0.204]]
maxi score, test score, baseline:  0.2261 0.45 0.45
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2261 0.45 0.45
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.136]
 [0.425]
 [0.106]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.106]
 [0.136]
 [0.425]
 [0.106]]
maxi score, test score, baseline:  0.2261 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2241 0.45 0.45
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2181 0.45 0.45
probs:  [1.0]
maxi score, test score, baseline:  0.2181 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2161 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.45 0.45
maxi score, test score, baseline:  0.1941 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.201]
 [0.234]
 [0.137]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.141]
 [0.201]
 [0.234]
 [0.137]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.485]
 [0.701]
 [0.799]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.491]
 [0.485]
 [0.701]
 [0.799]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.45 0.45
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2001 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1660 35613
maxi score, test score, baseline:  0.1981 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2001 0.45 0.45
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2021 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20409999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20609999999999998 0.45 0.45
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.234]
 [0.393]
 [0.052]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.052]
 [0.234]
 [0.393]
 [0.052]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.482]
 [0.482]
 [0.482]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.482]
 [0.482]
 [0.482]
 [0.482]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.272]
 [0.731]
 [0.485]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.485]
 [0.272]
 [0.731]
 [0.485]]
rdn probs:  [1.0]
maxi score, test score, baseline:  0.2481 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1661 35669
maxi score, test score, baseline:  0.2681 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.2681 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2701 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.2701 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28209999999999996 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28409999999999996 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.2  ]
 [0.213]
 [0.178]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.188]
 [0.2  ]
 [0.213]
 [0.178]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28609999999999997 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.28609999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.313]
 [0.385]
 [0.403]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.403]
 [0.313]
 [0.385]
 [0.403]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.357]
 [0.357]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.357]
 [0.357]
 [0.357]
 [0.357]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3001 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1661 35783
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.33 ]
 [0.387]
 [0.705]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.705]
 [0.33 ]
 [0.387]
 [0.705]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.393]
 [0.779]
 [0.779]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.779]
 [0.393]
 [0.779]
 [0.779]]
maxi score, test score, baseline:  0.2981 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.402]
 [0.705]
 [0.724]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.415]
 [0.402]
 [0.705]
 [0.724]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.85 0.85
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.85 0.85
probs:  [1.0]
1661 35806
maxi score, test score, baseline:  0.2981 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.979]
 [0.735]
 [0.735]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.735]
 [0.979]
 [0.735]
 [0.735]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.424]
 [0.592]
 [0.699]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.413]
 [0.424]
 [0.592]
 [0.699]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2941 0.85 0.85
maxi score, test score, baseline:  0.2941 0.85 0.85
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1662 35824
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.85 0.85
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28809999999999997 0.85 0.85
maxi score, test score, baseline:  0.28809999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2941 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.85 0.85
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1662 35887
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1662 35919
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1662 35932
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.458 0.417 0.083]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1663 35951
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3221 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1664
maxi score, test score, baseline:  0.3281 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.356]
 [0.583]
 [0.381]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.407]
 [0.356]
 [0.583]
 [0.381]]
maxi score, test score, baseline:  0.3341 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.85 0.85
maxi score, test score, baseline:  0.3321 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3341 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.   ]
 [0.535]
 [0.176]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.144]
 [0.   ]
 [0.535]
 [0.176]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.85 0.85
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1667 35987
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.85 0.85
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.85 0.85
maxi score, test score, baseline:  0.35609999999999997 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.35609999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.85 0.85
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1668 36021
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1668 36042
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1668 36045
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3921 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.408]
 [0.764]
 [0.304]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.304]
 [0.408]
 [0.764]
 [0.304]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.85 0.85
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.85 0.85
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.85 0.85
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
1668 36104
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.254]
 [0.225]
 [0.107]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.235]
 [0.254]
 [0.225]
 [0.107]]
maxi score, test score, baseline:  0.35409999999999997 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.061]
 [0.275]
 [0.187]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.094]
 [0.061]
 [0.275]
 [0.187]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.35209999999999997 0.85 0.85
maxi score, test score, baseline:  0.35209999999999997 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.249]
 [0.644]
 [0.232]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.251]
 [0.249]
 [0.644]
 [0.232]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1669 36189
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.447]
 [0.414]
 [0.264]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.264]
 [0.447]
 [0.414]
 [0.264]]
maxi score, test score, baseline:  0.3581 0.85 0.85
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.291]
 [0.267]
 [0.218]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.242]
 [0.291]
 [0.267]
 [0.218]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.   ]
 [0.443]
 [0.286]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.192]
 [0.   ]
 [0.443]
 [0.286]]
maxi score, test score, baseline:  0.35209999999999997 0.85 0.85
siam score:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3641 0.85 0.85
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.458]
 [0.458]
 [0.458]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.458]
 [0.458]
 [0.458]
 [0.458]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.85 0.85
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3701 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.85 0.85
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.85 0.85
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1671 36327
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.85 0.85
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.85 0.85
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.574]
 [0.394]
 [0.732]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.732]
 [0.574]
 [0.394]
 [0.732]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.182]
 [0.508]
 [0.472]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.312]
 [0.182]
 [0.508]
 [0.472]]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3681 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.85 0.85
maxi score, test score, baseline:  0.3701 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1676 36418
maxi score, test score, baseline:  0.3661 0.85 0.85
maxi score, test score, baseline:  0.3661 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.85 0.85
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.34]
 [0.34]
 [0.34]
 [0.34]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.34]
 [0.34]
 [0.34]
 [0.34]]
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.959]
 [0.844]
 [0.844]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.844]
 [0.959]
 [0.844]
 [0.844]]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.328]
 [0.874]
 [0.328]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.328]
 [0.328]
 [0.874]
 [0.328]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1681
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3701 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1681 36464
maxi score, test score, baseline:  0.35609999999999997 0.85 0.85
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.85 0.85
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.85 0.85
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.85 0.85
maxi score, test score, baseline:  0.35609999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.85 0.85
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1684 36486
maxi score, test score, baseline:  0.35209999999999997 0.85 0.85
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.34609999999999996 0.85 0.85
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.85 0.85
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.85 0.85
maxi score, test score, baseline:  0.34609999999999996 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.528]
 [0.687]
 [0.528]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.528]
 [0.528]
 [0.687]
 [0.528]]
maxi score, test score, baseline:  0.3401 0.85 0.85
probs:  [1.0]
1687 36518
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1688 36532
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.85 0.85
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.85 0.85
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.85 0.85
maxi score, test score, baseline:  0.34809999999999997 0.85 0.85
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.201]
 [0.323]
 [0.323]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.323]
 [0.201]
 [0.323]
 [0.323]]
maxi score, test score, baseline:  0.35209999999999997 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.35009999999999997 0.85 0.85
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.85 0.85
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.34409999999999996 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3341 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.85 0.85
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1689 36684
maxi score, test score, baseline:  0.3241 0.85 0.85
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1689 36694
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1690 36718
maxi score, test score, baseline:  0.3261 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3261 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3021 0.85 0.85
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.308]
 [0.283]
 [0.217]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.276]
 [0.308]
 [0.283]
 [0.217]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.85 0.85
maxi score, test score, baseline:  0.29009999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.85 0.85
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.867]
 [0.316]
 [0.32 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.32 ]
 [0.867]
 [0.316]
 [0.32 ]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.85 0.85
maxi score, test score, baseline:  0.29009999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.85 0.85
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.394]
 [0.42 ]
 [0.276]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.289]
 [0.394]
 [0.42 ]
 [0.276]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1693 36867
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1693 36880
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.3001 0.85 0.85
maxi score, test score, baseline:  0.3001 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1693 36889
maxi score, test score, baseline:  0.3001 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.319]
 [0.301]
 [0.231]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.283]
 [0.319]
 [0.301]
 [0.231]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.85 0.85
probs:  [1.0]
Starting evaluation
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1695 36940
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.3041 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.8 0.8
maxi score, test score, baseline:  0.2961 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.8 0.8
maxi score, test score, baseline:  0.2981 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3001 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.307]
 [0.609]
 [0.307]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.307]
 [0.307]
 [0.609]
 [0.307]]
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1695 37024
maxi score, test score, baseline:  0.2981 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.2981 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.8 0.8
probs:  [1.0]
1695 37056
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1695 37072
maxi score, test score, baseline:  0.3021 0.8 0.8
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3021 0.8 0.8
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3041 0.8 0.8
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.8 0.8
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1695 37131
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1695 37142
maxi score, test score, baseline:  0.3061 0.8 0.8
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.082]
 [0.045]
 [0.045]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.045]
 [0.082]
 [0.045]
 [0.045]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.534]
 [0.534]
 [0.534]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.534]
 [0.534]
 [0.534]
 [0.534]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3161 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.831]
 [0.44 ]
 [0.388]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.388]
 [0.831]
 [0.44 ]
 [0.388]]
maxi score, test score, baseline:  0.3141 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3141 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.443]
 [0.443]
 [0.443]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.443]
 [0.443]
 [0.443]
 [0.443]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.8 0.8
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.108]
 [0.713]
 [0.473]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.473]
 [0.108]
 [0.713]
 [0.473]]
maxi score, test score, baseline:  0.3221 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.8 0.8
maxi score, test score, baseline:  0.3241 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.8 0.8
maxi score, test score, baseline:  0.3281 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1697 37240
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1697 37243
maxi score, test score, baseline:  0.3261 0.8 0.8
maxi score, test score, baseline:  0.3261 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1698 37248
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3281 0.8 0.8
maxi score, test score, baseline:  0.3281 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.8 0.8
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.454]
 [0.282]
 [0.292]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.292]
 [0.454]
 [0.282]
 [0.292]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.8 0.8
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.34609999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1700 37314
maxi score, test score, baseline:  0.34609999999999996 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.34609999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.8 0.8
siam score:  0.0
1701 37348
maxi score, test score, baseline:  0.3141 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3081 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.2981 0.8 0.8
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.147]
 [0.238]
 [0.767]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.767]
 [0.147]
 [0.238]
 [0.767]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1701 37380
maxi score, test score, baseline:  0.29009999999999997 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2601 0.8 0.8
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1702 37413
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1702 37427
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2641 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1702 37441
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2621 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2681 0.8 0.8
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.28209999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1702 37469
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.8 0.8
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.8 0.8
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.8 0.8
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.8 0.8
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.361]
 [0.361]
 [0.361]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.361]
 [0.361]
 [0.361]
 [0.361]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.8 0.8
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3181 0.8 0.8
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.647]
 [0.689]
 [0.34 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.34 ]
 [0.647]
 [0.689]
 [0.34 ]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1702 37538
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3081 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.936]
 [0.765]
 [0.765]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.765]
 [0.936]
 [0.765]
 [0.765]]
maxi score, test score, baseline:  0.3121 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3121 0.8 0.8
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1703 37561
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.311]
 [0.311]
 [0.311]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.311]
 [0.311]
 [0.311]
 [0.311]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3181 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1703 37578
maxi score, test score, baseline:  0.3201 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3201 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3221 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.8 0.8
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.8 0.8
maxi score, test score, baseline:  0.3361 0.8 0.8
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.167]
 [0.298]
 [0.167]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.167]
 [0.167]
 [0.298]
 [0.167]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.34409999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1703 37623
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.8 0.8
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1704 37638
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1705 37648
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.8 0.8
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.8 0.8
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1705 37672
maxi score, test score, baseline:  0.3581 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.35409999999999997 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.35409999999999997 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.46 ]
 [0.465]
 [0.46 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.46 ]
 [0.46 ]
 [0.465]
 [0.46 ]]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.46 ]
 [0.218]
 [0.41 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.41 ]
 [0.46 ]
 [0.218]
 [0.41 ]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.8 0.8
maxi score, test score, baseline:  0.35009999999999997 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.34409999999999996 0.8 0.8
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.467]
 [0.536]
 [0.476]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.476]
 [0.467]
 [0.536]
 [0.476]]
maxi score, test score, baseline:  0.34409999999999996 0.8 0.8
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.288]
 [0.275]
 [0.288]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.288]
 [0.288]
 [0.275]
 [0.288]]
1706 37752
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.8 0.8
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3221 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.8 0.8
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1709 37815
maxi score, test score, baseline:  0.3241 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.8 0.8
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.353]
 [0.378]
 [0.289]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.313]
 [0.353]
 [0.378]
 [0.289]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1709 37835
maxi score, test score, baseline:  0.3341 0.8 0.8
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.8 0.8
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1710 37881
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.554]
 [0.652]
 [0.303]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.303]
 [0.554]
 [0.652]
 [0.303]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.647]
 [0.197]
 [0.204]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.246]
 [0.647]
 [0.197]
 [0.204]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3581 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3621 0.8 0.8
maxi score, test score, baseline:  0.3621 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1715 37905
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1715 37906
maxi score, test score, baseline:  0.3661 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.513]
 [0.134]
 [0.22 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.19 ]
 [0.513]
 [0.134]
 [0.22 ]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.8 0.8
probs:  [1.0]
1716 37928
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.8 0.8
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.503]
 [0.113]
 [0.113]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.113]
 [0.503]
 [0.113]
 [0.113]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.8 0.8
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.083 0.75  0.125 0.042]
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.8 0.8
probs:  [1.0]
1717 37962
maxi score, test score, baseline:  0.3301 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1718 37983
maxi score, test score, baseline:  0.3221 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.278]
 [0.278]
 [0.278]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.278]
 [0.278]
 [0.278]
 [0.278]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1718 38021
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1718 38031
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.431]
 [0.48 ]
 [0.505]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.285]
 [0.431]
 [0.48 ]
 [0.505]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1719 38044
maxi score, test score, baseline:  0.3041 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.054]
 [0.853]
 [0.292]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.136]
 [0.054]
 [0.853]
 [0.292]]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.8 0.8
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.3181 0.8 0.8
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.8 0.8
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1723
maxi score, test score, baseline:  0.3241 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.8 0.8
maxi score, test score, baseline:  0.3221 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.8 0.8
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.8 0.8
probs:  [1.0]
1727 38123
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1727 38127
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.739]
 [0.549]
 [0.272]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.272]
 [0.739]
 [0.549]
 [0.272]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3161 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.542]
 [0.474]
 [0.649]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.355]
 [0.542]
 [0.474]
 [0.649]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.8 0.8
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.508]
 [0.508]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.508]
 [0.508]
 [0.508]
 [0.508]]
maxi score, test score, baseline:  0.3201 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.083 0.417 0.208 0.292]
1733 38181
maxi score, test score, baseline:  0.3381 0.8 0.8
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.8 0.8
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1734 38188
maxi score, test score, baseline:  0.3381 0.8 0.8
maxi score, test score, baseline:  0.3381 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3381 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.671]
 [0.671]
 [0.671]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.671]
 [0.671]
 [0.671]
 [0.671]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3401 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
maxi score, test score, baseline:  0.3421 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.636]
 [0.229]
 [0.281]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.297]
 [0.636]
 [0.229]
 [0.281]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.445]
 [0.414]
 [0.374]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.35 ]
 [0.445]
 [0.414]
 [0.374]]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.525]
 [0.402]
 [0.402]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.402]
 [0.525]
 [0.402]
 [0.402]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.412]
 [0.541]
 [0.371]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.347]
 [0.412]
 [0.541]
 [0.371]]
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.355]
 [0.468]
 [0.319]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.298]
 [0.355]
 [0.468]
 [0.319]]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.533]
 [0.543]
 [0.592]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.469]
 [0.533]
 [0.543]
 [0.592]]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.8 0.8
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.3621 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3621 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3601 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.35209999999999997 0.55 0.55
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.35609999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.388]
 [0.379]
 [0.352]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.352]
 [0.388]
 [0.379]
 [0.352]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.618]
 [0.618]
 [0.618]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.618]
 [0.618]
 [0.618]
 [0.618]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.55 0.55
maxi score, test score, baseline:  0.3781 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1744 38252
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.54 ]
 [0.634]
 [0.54 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.54 ]
 [0.54 ]
 [0.634]
 [0.54 ]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.66]
 [0.66]
 [0.66]
 [0.66]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.66]
 [0.66]
 [0.66]
 [0.66]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1745 38270
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1746 38276
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.55 0.55
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.55 0.55
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.833 0.083]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1747 38291
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.55 0.55
probs:  [1.0]
1749 38299
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.41809999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.41409999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1756
maxi score, test score, baseline:  0.41209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.40809999999999996 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.251]
 [0.475]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.475]
 [0.475]
 [0.251]
 [0.475]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4341 0.55 0.55
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4421 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4461 0.55 0.55
maxi score, test score, baseline:  0.4461 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4501 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4501 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4521 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4561 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4561 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4581 0.55 0.55
maxi score, test score, baseline:  0.4581 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4561 0.55 0.55
maxi score, test score, baseline:  0.4561 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4641 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4661 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4661 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4661 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4641 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.386]
 [0.19 ]
 [0.19 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.19 ]
 [0.386]
 [0.19 ]
 [0.19 ]]
maxi score, test score, baseline:  0.4641 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.231]
 [0.27 ]
 [0.211]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.185]
 [0.231]
 [0.27 ]
 [0.211]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1773 38451
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47409999999999997 0.55 0.55
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47009999999999996 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.47009999999999996 0.55 0.55
probs:  [1.0]
1773 38476
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.303]
 [0.116]
 [0.103]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.187]
 [0.303]
 [0.116]
 [0.103]]
1774 38479
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4621 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4621 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1774 38486
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1774 38495
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1775 38508
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4521 0.55 0.55
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4461 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.246]
 [0.11 ]
 [0.225]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.198]
 [0.246]
 [0.11 ]
 [0.225]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.266]
 [0.624]
 [0.505]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.404]
 [0.266]
 [0.624]
 [0.505]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4421 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4401 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4421 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4421 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4401 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1777 38564
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1777 38566
maxi score, test score, baseline:  0.4421 0.55 0.55
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4461 0.55 0.55
maxi score, test score, baseline:  0.4461 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.774]
 [0.317]
 [0.702]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.702]
 [0.774]
 [0.317]
 [0.702]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4441 0.55 0.55
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4461 0.55 0.55
maxi score, test score, baseline:  0.4461 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.811]
 [0.396]
 [0.404]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.404]
 [0.811]
 [0.396]
 [0.404]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4501 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4561 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4581 0.55 0.55
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4661 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4661 0.55 0.55
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4661 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1780 38625
maxi score, test score, baseline:  0.47009999999999996 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.355]
 [0.683]
 [0.355]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.355]
 [0.355]
 [0.683]
 [0.355]]
UNIT TEST: sample policy line 217 mcts : [0.083 0.708 0.167 0.042]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4461 0.55 0.55
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.403]
 [0.575]
 [0.56 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.564]
 [0.403]
 [0.575]
 [0.56 ]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4461 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4501 0.55 0.55
1783 38658
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4521 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
1784 38672
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4661 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1784 38687
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1785 38689
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4581 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4561 0.55 0.55
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4541 0.55 0.55
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [1.0]
1785 38709
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.803]
 [0.279]
 [0.239]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.269]
 [0.803]
 [0.279]
 [0.239]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4561 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.485]
 [0.482]
 [0.405]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.405]
 [0.485]
 [0.482]
 [0.405]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.325]
 [0.314]
 [0.314]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.314]
 [0.325]
 [0.314]
 [0.314]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4481 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4481 0.55 0.55
probs:  [1.0]
1786 38741
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.396]
 [0.643]
 [0.447]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.273]
 [0.396]
 [0.643]
 [0.447]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.415]
 [0.451]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.451]
 [0.451]
 [0.415]
 [0.451]]
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4381 0.55 0.55
maxi score, test score, baseline:  0.4321 0.55 0.55
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.419]
 [0.372]
 [0.372]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.372]
 [0.419]
 [0.372]
 [0.372]]
maxi score, test score, baseline:  0.4281 0.55 0.55
maxi score, test score, baseline:  0.4281 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.55 0.55
maxi score, test score, baseline:  0.4301 0.55 0.55
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.806]
 [0.669]
 [0.355]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.355]
 [0.806]
 [0.669]
 [0.355]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.4341 0.55 0.55
maxi score, test score, baseline:  0.4341 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1789 38808
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4361 0.55 0.55
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4381 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.55 0.55
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.541]
 [1.052]
 [0.541]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.541]
 [0.541]
 [1.052]
 [0.541]]
maxi score, test score, baseline:  0.4261 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.271]
 [0.354]
 [0.763]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.763]
 [0.271]
 [0.354]
 [0.763]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.55 0.55
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.4281 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4221 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.41809999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.78]
 [0.78]
 [0.78]
 [0.78]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.78]
 [0.78]
 [0.78]
 [0.78]]
maxi score, test score, baseline:  0.41609999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.55 0.55
maxi score, test score, baseline:  0.41209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.383]
 [0.552]
 [0.273]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.273]
 [0.383]
 [0.552]
 [0.273]]
maxi score, test score, baseline:  0.41209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1790 38933
maxi score, test score, baseline:  0.4021 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4001 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3681 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.55 0.55
probs:  [1.0]
1792 39010
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.322]
 [0.685]
 [0.685]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.685]
 [0.322]
 [0.685]
 [0.685]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1792 39044
maxi score, test score, baseline:  0.3601 0.55 0.55
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.55 0.55
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.289]
 [0.289]
 [0.289]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.289]
 [0.289]
 [0.289]
 [0.289]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.35209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.55 0.55
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.167 0.667 0.125]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1797 39088
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1797 39108
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3381 0.55 0.55
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.3381 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.473]
 [0.444]
 [0.446]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.446]
 [0.473]
 [0.444]
 [0.446]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3361 0.55 0.55
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.55 0.55
1799 39168
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.414]
 [0.591]
 [0.43 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.43 ]
 [0.414]
 [0.591]
 [0.43 ]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.659]
 [0.371]
 [0.371]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.371]
 [0.659]
 [0.371]
 [0.371]]
maxi score, test score, baseline:  0.3161 0.55 0.55
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.569]
 [0.569]
 [0.569]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.569]
 [0.569]
 [0.569]
 [0.569]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.446]
 [0.578]
 [0.607]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.615]
 [0.446]
 [0.578]
 [0.607]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.852]
 [0.852]
 [0.852]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.852]
 [0.852]
 [0.852]
 [0.852]]
maxi score, test score, baseline:  0.3361 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3361 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.55 0.55
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3401 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.55 0.55
probs:  [1.0]
Starting evaluation
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.75  0.125]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.457]
 [0.75 ]
 [0.764]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.753]
 [0.457]
 [0.75 ]
 [0.764]]
maxi score, test score, baseline:  0.3421 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.315]
 [0.594]
 [0.264]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.268]
 [0.315]
 [0.594]
 [0.264]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3421 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.55 0.55
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.55 0.55
probs:  [1.0]
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.337]
 [0.303]
 [0.303]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.303]
 [0.337]
 [0.303]
 [0.303]]
maxi score, test score, baseline:  0.3341 0.0 0.3341
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1805 39227
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.478]
 [0.473]
 [0.392]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.392]
 [0.478]
 [0.473]
 [0.392]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3421 0.0 0.3421
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.0 0.34809999999999997
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.279]
 [0.293]
 [0.373]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.296]
 [0.279]
 [0.293]
 [0.373]]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.408]
 [0.267]
 [0.253]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.268]
 [0.408]
 [0.267]
 [0.253]]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.0 0.35009999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.35009999999999997 0.0 0.35009999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.35009999999999997 0.0 0.35009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.0 0.35209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.0 0.35209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.0 0.34809999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.0 0.35009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.0 0.3601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1809 39303
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.0 0.3621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.0 0.3761
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.51 ]
 [0.449]
 [0.449]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.449]
 [0.51 ]
 [0.449]
 [0.449]]
maxi score, test score, baseline:  0.3801 0.0 0.3801
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.0 0.3841
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.0 0.3861
probs:  [1.0]
maxi score, test score, baseline:  0.3861 0.0 0.3861
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.867]
 [0.867]
 [0.867]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.867]
 [0.867]
 [0.867]
 [0.867]]
maxi score, test score, baseline:  0.3881 0.0 0.3881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3901 0.0 0.3901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.0 0.3901
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.738]
 [0.186]
 [0.183]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.237]
 [0.738]
 [0.186]
 [0.183]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.719]
 [0.378]
 [0.408]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.408]
 [0.719]
 [0.378]
 [0.408]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.0 0.3941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1809 39366
siam score:  0.0
maxi score, test score, baseline:  0.3921 0.0 0.3921
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.806]
 [0.224]
 [0.693]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.693]
 [0.806]
 [0.224]
 [0.693]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.0 0.3941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.0 0.3981
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.423]
 [0.525]
 [0.605]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.605]
 [0.423]
 [0.525]
 [0.605]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.0 0.40809999999999996
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.693]
 [0.512]
 [0.512]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.512]
 [0.693]
 [0.512]
 [0.512]]
maxi score, test score, baseline:  0.41009999999999996 0.0 0.41009999999999996
maxi score, test score, baseline:  0.41009999999999996 0.0 0.41009999999999996
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.0 0.41209999999999997
maxi score, test score, baseline:  0.41209999999999997 0.0 0.41209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 0.0 0.41609999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.41609999999999997 0.0 0.41609999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.0 0.4201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1813
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1813 39407
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.796]
 [0.478]
 [0.474]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.474]
 [0.796]
 [0.478]
 [0.474]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4221 0.0 0.4221
probs:  [1.0]
maxi score, test score, baseline:  0.4221 0.0 0.4221
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1815 39418
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.0 0.4241
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.926]
 [0.349]
 [0.349]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.349]
 [0.926]
 [0.349]
 [0.349]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.263]
 [0.648]
 [0.263]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.263]
 [0.263]
 [0.648]
 [0.263]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.0 0.4241
probs:  [1.0]
maxi score, test score, baseline:  0.4241 0.0 0.4241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.   ]
 [0.279]
 [0.259]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.   ]
 [0.279]
 [0.259]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.0 0.4301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4401 0.0 0.4401
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4421 0.0 0.4421
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.356]
 [0.421]
 [0.295]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.342]
 [0.356]
 [0.421]
 [0.295]]
maxi score, test score, baseline:  0.4441 0.0 0.4441
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.0 0.4441
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.578]
 [0.228]
 [0.221]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.25 ]
 [0.578]
 [0.228]
 [0.221]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4521 0.0 0.4521
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4581 0.0 0.4581
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4601 0.0 0.4601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.145]
 [0.508]
 [0.557]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.567]
 [0.145]
 [0.508]
 [0.557]]
maxi score, test score, baseline:  0.4621 0.0 0.4621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.429]
 [0.416]
 [0.307]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.4  ]
 [0.429]
 [0.416]
 [0.307]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47409999999999997 0.0 0.47409999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.47209999999999996 0.0 0.47209999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47209999999999996 0.0 0.47209999999999996
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.301]
 [0.136]
 [0.225]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.225]
 [0.301]
 [0.136]
 [0.225]]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [1.042]
 [0.414]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.414]
 [0.414]
 [1.042]
 [0.414]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47409999999999997 0.0 0.47409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47609999999999997 0.0 0.47609999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.622]
 [0.238]
 [0.256]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.256]
 [0.622]
 [0.238]
 [0.256]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 0.0 0.48009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1821 39523
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.429]
 [0.206]
 [0.278]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.278]
 [0.429]
 [0.206]
 [0.278]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47409999999999997 0.0 0.47409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.47409999999999997 0.0 0.47409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47409999999999997 0.0 0.47409999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.47409999999999997 0.0 0.47409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.268]
 [0.213]
 [0.238]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.238]
 [0.268]
 [0.213]
 [0.238]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 0.0 0.47809999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4841 0.0 0.4841
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4881 0.0 0.4881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.502]
 [0.964]
 [0.502]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.502]
 [0.502]
 [0.964]
 [0.502]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4901 0.0 0.4901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4941 0.0 0.4941
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[1.053]
 [1.053]
 [1.053]
 [1.053]] [[0.]
 [0.]
 [0.]
 [0.]] [[1.053]
 [1.053]
 [1.053]
 [1.053]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.701]
 [0.529]
 [0.701]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.701]
 [0.701]
 [0.529]
 [0.701]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4981 0.0 0.4981
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Sims:  25 1 epoch:  283923 pick best:  False frame count:  283923
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.59 ]
 [0.967]
 [0.291]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.291]
 [0.59 ]
 [0.967]
 [0.291]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5081 0.0 0.5081
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5081 0.0 0.5081
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5101 0.0 0.5101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.918]
 [0.669]
 [0.334]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.334]
 [0.918]
 [0.669]
 [0.334]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5161 0.0 0.5161
maxi score, test score, baseline:  0.5141 0.0 0.5141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5141 0.0 0.5141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.94 ]
 [0.571]
 [0.571]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.571]
 [0.94 ]
 [0.571]
 [0.571]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5181 0.0 0.5181
maxi score, test score, baseline:  0.5181 0.0 0.5181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.449]
 [0.237]
 [0.251]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.251]
 [0.449]
 [0.237]
 [0.251]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.533]
 [1.031]
 [0.533]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.533]
 [0.533]
 [1.031]
 [0.533]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5201 0.0 0.5201
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5141 0.0 0.5141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5161 0.0 0.5161
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.5101 0.0 0.5101
probs:  [1.0]
maxi score, test score, baseline:  0.5101 0.0 0.5101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1827 39684
maxi score, test score, baseline:  0.5101 0.0 0.5101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5161 0.0 0.5161
probs:  [1.0]
maxi score, test score, baseline:  0.5161 0.0 0.5161
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5201 0.0 0.5201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5261 0.0 0.5261
probs:  [1.0]
maxi score, test score, baseline:  0.5261 0.0 0.5261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5201 0.0 0.5201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5221 0.0 0.5221
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5241 0.0 0.5241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.459]
 [0.314]
 [0.314]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.314]
 [0.459]
 [0.314]
 [0.314]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5241 0.0 0.5241
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5201 0.0 0.5201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5221 0.0 0.5221
probs:  [1.0]
maxi score, test score, baseline:  0.5221 0.0 0.5221
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5201 0.0 0.5201
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5201 0.0 0.5201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1831 39763
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5181 0.0 0.5181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.458 0.417 0.083]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.797]
 [0.859]
 [0.329]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.329]
 [0.797]
 [0.859]
 [0.329]]
siam score:  0.0
maxi score, test score, baseline:  0.5201 0.0 0.5201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5181 0.0 0.5181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.793]
 [0.513]
 [0.473]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.568]
 [0.793]
 [0.513]
 [0.473]]
maxi score, test score, baseline:  0.5181 0.0 0.5181
probs:  [1.0]
maxi score, test score, baseline:  0.5161 0.0 0.5161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5161 0.0 0.5161
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5081 0.0 0.5081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5081 0.0 0.5081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.502]
 [0.48 ]
 [0.45 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.583]
 [0.502]
 [0.48 ]
 [0.45 ]]
maxi score, test score, baseline:  0.5041 0.0 0.5041
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5001 0.0 0.5001
maxi score, test score, baseline:  0.5001 0.0 0.5001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5021 0.0 0.5021
probs:  [1.0]
maxi score, test score, baseline:  0.5021 0.0 0.5021
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5001 0.0 0.5001
probs:  [1.0]
maxi score, test score, baseline:  0.5001 0.0 0.5001
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.505]
 [0.192]
 [0.61 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.61 ]
 [0.505]
 [0.192]
 [0.61 ]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4961 0.0 0.4961
maxi score, test score, baseline:  0.4941 0.0 0.4941
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.347]
 [0.826]
 [0.347]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.347]
 [0.347]
 [0.826]
 [0.347]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4961 0.0 0.4961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5001 0.0 0.5001
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.358]
 [0.398]
 [0.344]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.344]
 [0.358]
 [0.398]
 [0.344]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4961 0.0 0.4961
probs:  [1.0]
maxi score, test score, baseline:  0.4961 0.0 0.4961
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4961 0.0 0.4961
probs:  [1.0]
maxi score, test score, baseline:  0.4961 0.0 0.4961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4941 0.0 0.4941
probs:  [1.0]
maxi score, test score, baseline:  0.4921 0.0 0.4921
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4901 0.0 0.4901
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4921 0.0 0.4921
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4881 0.0 0.4881
probs:  [1.0]
maxi score, test score, baseline:  0.4881 0.0 0.4881
maxi score, test score, baseline:  0.4861 0.0 0.4861
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4881 0.0 0.4881
probs:  [1.0]
maxi score, test score, baseline:  0.4881 0.0 0.4881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4921 0.0 0.4921
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4841 0.0 0.4841
probs:  [1.0]
maxi score, test score, baseline:  0.4841 0.0 0.4841
maxi score, test score, baseline:  0.4841 0.0 0.4841
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 0.0 0.48009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4861 0.0 0.4861
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4881 0.0 0.4881
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.898]
 [0.898]
 [0.898]
 [0.898]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.898]
 [0.898]
 [0.898]
 [0.898]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4841 0.0 0.4841
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4841 0.0 0.4841
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.021]
 [0.021]
 [0.021]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1845 39960
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.929]
 [0.479]
 [0.492]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.492]
 [0.929]
 [0.479]
 [0.492]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 0.0 0.48009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 0.0 0.48009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47209999999999996 0.0 0.47209999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.377]
 [0.377]
 [0.377]
 [0.377]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47209999999999996 0.0 0.47209999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4661 0.0 0.4661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.0 0.4681
maxi score, test score, baseline:  0.4681 0.0 0.4681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.0 0.4681
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47009999999999996 0.0 0.47009999999999996
probs:  [1.0]
maxi score, test score, baseline:  0.4681 0.0 0.4681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47009999999999996 0.0 0.47009999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47009999999999996 0.0 0.47009999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47209999999999996 0.0 0.47209999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47409999999999997 0.0 0.47409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47409999999999997 0.0 0.47409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1849 40032
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47209999999999996 0.0 0.47209999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47409999999999997 0.0 0.47409999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.47009999999999996 0.0 0.47009999999999996
maxi score, test score, baseline:  0.47009999999999996 0.0 0.47009999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.0 0.4681
probs:  [1.0]
maxi score, test score, baseline:  0.4681 0.0 0.4681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.0 0.4681
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4661 0.0 0.4661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47609999999999997 0.0 0.47609999999999997
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.282]
 [0.44 ]
 [0.429]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.057]
 [0.282]
 [0.44 ]
 [0.429]]
maxi score, test score, baseline:  0.4821 0.0 0.4821
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47609999999999997 0.0 0.47609999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.0 0.4681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47009999999999996 0.0 0.47009999999999996
maxi score, test score, baseline:  0.47009999999999996 0.0 0.47009999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.0 0.4681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1851 40103
maxi score, test score, baseline:  0.4661 0.0 0.4661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.0 0.4681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4661 0.0 0.4661
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.4661 0.0 0.4661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4641 0.0 0.4641
maxi score, test score, baseline:  0.4641 0.0 0.4641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4641 0.0 0.4641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4621 0.0 0.4621
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4621 0.0 0.4621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4621 0.0 0.4621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.0 0.4681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47009999999999996 0.0 0.47009999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47009999999999996 0.0 0.47009999999999996
probs:  [1.0]
maxi score, test score, baseline:  0.47009999999999996 0.0 0.47009999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47209999999999996 0.0 0.47209999999999996
maxi score, test score, baseline:  0.47209999999999996 0.0 0.47209999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1856 40169
maxi score, test score, baseline:  0.4681 0.0 0.4681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.746]
 [0.425]
 [0.432]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.432]
 [0.746]
 [0.425]
 [0.432]]
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.425]
 [0.918]
 [0.918]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.918]
 [0.425]
 [0.918]
 [0.918]]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.319]
 [0.101]
 [0.315]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.315]
 [0.319]
 [0.101]
 [0.315]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.577]
 [0.346]
 [0.335]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.335]
 [0.577]
 [0.346]
 [0.335]]
maxi score, test score, baseline:  0.4581 0.4 0.4581
probs:  [1.0]
maxi score, test score, baseline:  0.4461 0.4 0.4461
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1856 40198
maxi score, test score, baseline:  0.4441 0.4 0.4441
probs:  [1.0]
maxi score, test score, baseline:  0.4421 0.4 0.4421
maxi score, test score, baseline:  0.4421 0.4 0.4421
probs:  [1.0]
maxi score, test score, baseline:  0.4381 0.4 0.4381
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.4 0.4241
probs:  [1.0]
maxi score, test score, baseline:  0.4221 0.4 0.4221
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4221 0.4 0.4221
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.698]
 [0.633]
 [0.638]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.621]
 [0.698]
 [0.633]
 [0.638]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1856 40238
maxi score, test score, baseline:  0.41409999999999997 0.4 0.41409999999999997
maxi score, test score, baseline:  0.41409999999999997 0.4 0.41409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.4 0.41209999999999997
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.367]
 [0.741]
 [0.367]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.367]
 [0.367]
 [0.741]
 [0.367]]
maxi score, test score, baseline:  0.40809999999999996 0.4 0.40809999999999996
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.4 0.4061
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1857 40256
maxi score, test score, baseline:  0.4001 0.4 0.4001
probs:  [1.0]
maxi score, test score, baseline:  0.4001 0.4 0.4001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.4 0.4061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.4 0.4061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.4 0.41209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 0.4 0.41609999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.282]
 [0.162]
 [0.283]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.283]
 [0.282]
 [0.162]
 [0.283]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.4 0.4041
probs:  [1.0]
maxi score, test score, baseline:  0.4021 0.4 0.4021
probs:  [1.0]
maxi score, test score, baseline:  0.4001 0.4 0.4001
maxi score, test score, baseline:  0.4001 0.4 0.4001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.4 0.4021
probs:  [1.0]
maxi score, test score, baseline:  0.4021 0.4 0.4021
maxi score, test score, baseline:  0.4001 0.4 0.4001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.4 0.4021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1864 40326
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.4 0.4
probs:  [1.0]
maxi score, test score, baseline:  0.3961 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.4 0.4
probs:  [1.0]
maxi score, test score, baseline:  0.3921 0.4 0.4
probs:  [1.0]
maxi score, test score, baseline:  0.3921 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1867 40343
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1867 40353
maxi score, test score, baseline:  0.3861 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3821 0.4 0.4
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.001]
 [0.481]
 [0.424]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.424]
 [0.001]
 [0.481]
 [0.424]]
1871 40376
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1872 40377
maxi score, test score, baseline:  0.3781 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.4 0.4
probs:  [1.0]
maxi score, test score, baseline:  0.3841 0.4 0.4
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.512]
 [0.512]
 [0.512]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.512]
 [0.512]
 [0.512]
 [0.512]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1876 40393
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.353]
 [0.181]
 [0.154]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.156]
 [0.353]
 [0.181]
 [0.154]]
maxi score, test score, baseline:  0.3821 0.4 0.4
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.437]
 [0.631]
 [0.437]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.437]
 [0.437]
 [0.631]
 [0.437]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.327]
 [0.558]
 [0.504]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.367]
 [0.327]
 [0.558]
 [0.504]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.4 0.4021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.397]
 [0.374]
 [0.374]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.374]
 [0.397]
 [0.374]
 [0.374]]
maxi score, test score, baseline:  0.4041 0.4 0.4041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1879 40438
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.4 0.40809999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.4 0.41009999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.4 0.41209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.4 0.41209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.4 0.41409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.4 0.4201
maxi score, test score, baseline:  0.41609999999999997 0.4 0.41609999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.4 0.4061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.4 0.4021
maxi score, test score, baseline:  0.4021 0.4 0.4021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.47 ]
 [0.743]
 [0.411]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.411]
 [0.47 ]
 [0.743]
 [0.411]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.548]
 [0.548]
 [0.548]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.548]
 [0.548]
 [0.548]
 [0.548]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.3941 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
1880 40498
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.4 0.4
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.4 0.4
probs:  [1.0]
maxi score, test score, baseline:  0.3841 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1880 40517
maxi score, test score, baseline:  0.3881 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1880 40522
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.299]
 [0.299]
 [0.299]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.299]
 [0.299]
 [0.299]
 [0.299]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.4 0.4
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3981 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.4 0.4
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4021 0.4 0.4021
probs:  [1.0]
maxi score, test score, baseline:  0.4001 0.4 0.4001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4001 0.4 0.4001
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.4 0.4
probs:  [1.0]
maxi score, test score, baseline:  0.3981 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.208 0.417 0.333 0.042]
maxi score, test score, baseline:  0.40809999999999996 0.4 0.40809999999999996
probs:  [1.0]
maxi score, test score, baseline:  0.40809999999999996 0.4 0.40809999999999996
probs:  [1.0]
1883 40580
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.4 0.4061
probs:  [1.0]
maxi score, test score, baseline:  0.4061 0.4 0.4061
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.767]
 [0.391]
 [0.391]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.391]
 [0.767]
 [0.391]
 [0.391]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.4 0.41009999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4061 0.4 0.4061
probs:  [1.0]
maxi score, test score, baseline:  0.4061 0.4 0.4061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.4 0.4041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.4 0.4021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.4 0.4021
probs:  [1.0]
maxi score, test score, baseline:  0.4021 0.4 0.4021
probs:  [1.0]
maxi score, test score, baseline:  0.4021 0.4 0.4021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.4 0.4021
siam score:  0.0
maxi score, test score, baseline:  0.4001 0.4 0.4001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.4 0.4041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.4 0.4
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3981 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.4 0.40809999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.4 0.41009999999999996
probs:  [1.0]
1888 40667
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.4 0.41409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 0.4 0.41609999999999997
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.651]
 [0.609]
 [0.571]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.571]
 [0.651]
 [0.609]
 [0.571]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.4 0.4201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.4 0.4241
probs:  [1.0]
maxi score, test score, baseline:  0.4241 0.4 0.4241
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.4 0.4301
UNIT TEST: sample policy line 217 mcts : [0.083 0.458 0.292 0.167]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1890 40687
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4361 0.4 0.4361
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4401 0.4 0.4401
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.4 0.4441
probs:  [1.0]
maxi score, test score, baseline:  0.4441 0.4 0.4441
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.652]
 [0.663]
 [0.663]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.663]
 [0.652]
 [0.663]
 [0.663]]
maxi score, test score, baseline:  0.4461 0.4 0.4461
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.904]
 [0.273]
 [0.328]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.328]
 [0.904]
 [0.273]
 [0.328]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4481 0.4 0.4481
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4501 0.4 0.4501
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.01 ]
 [0.544]
 [0.518]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.371]
 [0.01 ]
 [0.544]
 [0.518]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4581 0.4 0.4581
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.755]
 [0.336]
 [0.328]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.328]
 [0.755]
 [0.336]
 [0.328]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4621 0.4 0.4621
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4641 0.4 0.4641
siam score:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4581 0.4 0.4581
probs:  [1.0]
maxi score, test score, baseline:  0.4541 0.4 0.4541
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4601 0.4 0.4601
probs:  [1.0]
maxi score, test score, baseline:  0.4601 0.4 0.4601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4601 0.4 0.4601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.4 0.4681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47009999999999996 0.4 0.47009999999999996
maxi score, test score, baseline:  0.47009999999999996 0.4 0.47009999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47009999999999996 0.4 0.47009999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.364]
 [0.212]
 [0.212]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.212]
 [0.364]
 [0.212]
 [0.212]]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.594]
 [0.594]
 [0.594]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.594]
 [0.594]
 [0.594]
 [0.594]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1898
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.559]
 [0.463]
 [0.978]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.804]
 [0.559]
 [0.463]
 [0.978]]
maxi score, test score, baseline:  0.4681 0.4 0.4681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47409999999999997 0.4 0.47409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1904
maxi score, test score, baseline:  0.48009999999999997 0.4 0.48009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.   ]
 [0.571]
 [0.571]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.482]
 [0.   ]
 [0.571]
 [0.571]]
1904 40803
maxi score, test score, baseline:  0.4861 0.4 0.4861
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4881 0.4 0.4881
maxi score, test score, baseline:  0.4881 0.4 0.4881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4901 0.4 0.4901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4981 0.4 0.4981
maxi score, test score, baseline:  0.4981 0.4 0.4981
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1909 40821
1909 40826
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5001 0.4 0.5001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5021 0.4 0.5021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 0.4 0.47809999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 0.4 0.47809999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 0.4 0.48009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.682]
 [0.605]
 [0.604]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.604]
 [0.682]
 [0.605]
 [0.604]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [1.039]
 [0.482]
 [0.37 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.37 ]
 [1.039]
 [0.482]
 [0.37 ]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 0.4 0.47809999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.4681 0.4 0.4681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.4 0.4681
maxi score, test score, baseline:  0.4681 0.4 0.4681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4641 0.4 0.4641
probs:  [1.0]
maxi score, test score, baseline:  0.4641 0.4 0.4641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1910 40892
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.4501 0.4 0.4501
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4481 0.4 0.4481
maxi score, test score, baseline:  0.4481 0.4 0.4481
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4501 0.4 0.4501
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4501 0.4 0.4501
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4481 0.4 0.4481
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.042 0.583 0.292 0.083]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4481 0.4 0.4481
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.4 0.4441
probs:  [1.0]
maxi score, test score, baseline:  0.4441 0.4 0.4441
maxi score, test score, baseline:  0.4441 0.4 0.4441
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1914 40921
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4401 0.4 0.4401
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.002]
 [0.786]
 [0.646]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.002]
 [0.786]
 [0.646]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4401 0.4 0.4401
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.783]
 [0.795]
 [0.783]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.783]
 [0.783]
 [0.795]
 [0.783]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.4 0.4441
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.4 0.4441
probs:  [1.0]
maxi score, test score, baseline:  0.4421 0.4 0.4421
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.694]
 [0.777]
 [0.818]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.818]
 [0.694]
 [0.777]
 [0.818]]
maxi score, test score, baseline:  0.4421 0.4 0.4421
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4461 0.4 0.4461
probs:  [1.0]
maxi score, test score, baseline:  0.4461 0.4 0.4461
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4501 0.4 0.4501
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.4 0.4441
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.694]
 [0.395]
 [0.386]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.384]
 [0.694]
 [0.395]
 [0.386]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1919 40989
maxi score, test score, baseline:  0.4401 0.4 0.4401
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.792 0.125 0.042]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4401 0.4 0.4401
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4421 0.4 0.4421
probs:  [1.0]
maxi score, test score, baseline:  0.4401 0.4 0.4401
probs:  [1.0]
maxi score, test score, baseline:  0.4401 0.4 0.4401
probs:  [1.0]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.622]
 [0.355]
 [0.131]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.366]
 [0.622]
 [0.355]
 [0.131]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4361 0.4 0.4361
probs:  [1.0]
maxi score, test score, baseline:  0.4361 0.4 0.4361
probs:  [1.0]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.698]
 [0.416]
 [0.698]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.698]
 [0.698]
 [0.416]
 [0.698]]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.4 0.4301
probs:  [1.0]
maxi score, test score, baseline:  0.4301 0.4 0.4301
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.4 0.4301
maxi score, test score, baseline:  0.4301 0.4 0.4301
maxi score, test score, baseline:  0.4301 0.4 0.4301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.4 0.4301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.4 0.4281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4261 0.4 0.4261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4261 0.4 0.4261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4261 0.4 0.4261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.4 0.4281
probs:  [1.0]
maxi score, test score, baseline:  0.4281 0.4 0.4281
probs:  [1.0]
maxi score, test score, baseline:  0.4281 0.4 0.4281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.4 0.4301
probs:  [1.0]
maxi score, test score, baseline:  0.4301 0.4 0.4301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4261 0.4 0.4261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.4 0.4201
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.4 0.4201
probs:  [1.0]
maxi score, test score, baseline:  0.4201 0.4 0.4201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.4 0.4201
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.279]
 [0.259]
 [0.237]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.264]
 [0.279]
 [0.259]
 [0.237]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.4 0.41809999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.41809999999999997 0.4 0.41809999999999997
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.4 0.4201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.4 0.41409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.3  ]
 [0.426]
 [0.3  ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.3  ]
 [0.3  ]
 [0.426]
 [0.3  ]]
maxi score, test score, baseline:  0.41409999999999997 0.4 0.41409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.4 0.41409999999999997
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 0.4 0.41609999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.4 0.41809999999999997
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.4 0.41409999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.40809999999999996 0.4 0.40809999999999996
maxi score, test score, baseline:  0.40809999999999996 0.4 0.40809999999999996
probs:  [1.0]
1939 41107
maxi score, test score, baseline:  0.4041 0.4 0.4041
probs:  [1.0]
maxi score, test score, baseline:  0.3961 0.4 0.4
probs:  [1.0]
Starting evaluation
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.4 0.4
maxi score, test score, baseline:  0.3781 0.4 0.4
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.433]
 [0.884]
 [0.884]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.884]
 [0.433]
 [0.884]
 [0.884]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.678]
 [0.653]
 [0.653]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.653]
 [0.678]
 [0.653]
 [0.653]]
siam score:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.15 0.35409999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.35009999999999997 0.15 0.35009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.15 0.3401
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.   ]
 [0.327]
 [0.222]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.054]
 [0.   ]
 [0.327]
 [0.222]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.15 0.3321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.324]
 [0.324]
 [0.324]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.324]
 [0.324]
 [0.324]
 [0.324]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.412]
 [0.349]
 [0.286]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.273]
 [0.412]
 [0.349]
 [0.286]]
maxi score, test score, baseline:  0.3281 0.15 0.3281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.15 0.3281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.15 0.3301
probs:  [1.0]
1948 41197
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.387]
 [0.346]
 [0.544]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.51 ]
 [0.387]
 [0.346]
 [0.544]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.15 0.3281
probs:  [1.0]
maxi score, test score, baseline:  0.3281 0.15 0.3281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.375 0.292 0.25  0.083]
UNIT TEST: sample policy line 217 mcts : [0.208 0.25  0.458 0.083]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.15 0.3341
probs:  [1.0]
maxi score, test score, baseline:  0.3341 0.15 0.3341
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.15 0.3361
maxi score, test score, baseline:  0.3361 0.15 0.3361
probs:  [1.0]
maxi score, test score, baseline:  0.3361 0.15 0.3361
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1949 41227
maxi score, test score, baseline:  0.3341 0.15 0.3341
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.427]
 [0.367]
 [0.425]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.425]
 [0.427]
 [0.367]
 [0.425]]
maxi score, test score, baseline:  0.3281 0.15 0.3281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3261 0.15 0.3261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.15 0.3261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1951 41240
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.15 0.3261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.15 0.3241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
1953 41248
maxi score, test score, baseline:  0.3221 0.15 0.3221
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1953 41253
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.15 0.3161
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.15 0.3181
maxi score, test score, baseline:  0.3181 0.15 0.3181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1953 41274
maxi score, test score, baseline:  0.3181 0.15 0.3181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.15 0.3321
maxi score, test score, baseline:  0.3321 0.15 0.3321
probs:  [1.0]
maxi score, test score, baseline:  0.3321 0.15 0.3321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.15 0.3361
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.424]
 [0.308]
 [0.302]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.494]
 [0.424]
 [0.308]
 [0.302]]
maxi score, test score, baseline:  0.3421 0.15 0.3421
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.237]
 [0.191]
 [0.149]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.203]
 [0.237]
 [0.191]
 [0.149]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.528]
 [0.449]
 [0.183]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.183]
 [0.528]
 [0.449]
 [0.183]]
1954 41313
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.15 0.3361
probs:  [1.0]
maxi score, test score, baseline:  0.3321 0.15 0.3321
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.15 0.3301
probs:  [1.0]
1954 41354
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.15 0.3201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.63 ]
 [0.706]
 [0.63 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.63 ]
 [0.63 ]
 [0.706]
 [0.63 ]]
maxi score, test score, baseline:  0.3181 0.15 0.3181
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.15 0.3181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.15 0.3141
probs:  [1.0]
maxi score, test score, baseline:  0.3121 0.15 0.3121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.15 0.3161
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.493]
 [0.386]
 [0.35 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.448]
 [0.493]
 [0.386]
 [0.35 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.15 0.3061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.659]
 [0.63 ]
 [0.57 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.575]
 [0.659]
 [0.63 ]
 [0.57 ]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.168]
 [0.428]
 [0.323]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.283]
 [0.168]
 [0.428]
 [0.323]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.15 0.3101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.15 0.3141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.15 0.3141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.15 0.3141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.15 0.3161
maxi score, test score, baseline:  0.3161 0.15 0.3161
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.186]
 [0.186]
 [0.186]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.186]
 [0.186]
 [0.186]
 [0.186]]
maxi score, test score, baseline:  0.3141 0.15 0.3141
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.15 0.3161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.15 0.3161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.15 0.3101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.15 0.3061
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.15 0.3021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.15 0.3081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.15 0.3081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.453]
 [0.277]
 [0.266]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.266]
 [0.453]
 [0.277]
 [0.266]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.15 0.3061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.15 0.3081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.15 0.3081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1965 41506
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.15 0.3081
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.15 0.3121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.15 0.3181
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.3181 0.15 0.3181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.15 0.3221
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.15 0.3301
probs:  [1.0]
maxi score, test score, baseline:  0.3301 0.15 0.3301
probs:  [1.0]
maxi score, test score, baseline:  0.3301 0.15 0.3301
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3301 0.15 0.3301
maxi score, test score, baseline:  0.3301 0.15 0.3301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.15 0.3321
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1970 41543
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.001]
 [0.567]
 [0.56 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.383]
 [0.001]
 [0.567]
 [0.56 ]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1972 41561
maxi score, test score, baseline:  0.34809999999999997 0.15 0.34809999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.15 0.3621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.15 0.3661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.15 0.3701
probs:  [1.0]
maxi score, test score, baseline:  0.3701 0.15 0.3701
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.15 0.3701
probs:  [1.0]
maxi score, test score, baseline:  0.3701 0.15 0.3701
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.15 0.3721
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.15 0.3741
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.296]
 [0.418]
 [0.298]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.317]
 [0.296]
 [0.418]
 [0.298]]
maxi score, test score, baseline:  0.3741 0.15 0.3741
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.15 0.3761
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.15 0.3921
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.15 0.3981
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.15 0.4021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.15 0.4041
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.686]
 [0.686]
 [0.686]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.686]
 [0.686]
 [0.686]
 [0.686]]
maxi score, test score, baseline:  0.4041 0.15 0.4041
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.687]
 [0.582]
 [0.685]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.685]
 [0.687]
 [0.582]
 [0.685]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
