dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[9, 10]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[5, 5]
observable_size:[5, 5]
game_modes:1
env_map:[['S' 'H' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'H']
 ['F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'H']
 ['F' 'H' 'F' 'F' 'G']]
max_steps:40
actions_size:4
optimal_score:1
total_frames:305000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:5
VK_ceiling:False
VK:False
use_two_heads:False
use_siam:False
exploration_type:none
rdn_beta:[0, 0.0, 1]
explorer_percentage:0.0
follow_better_policy:0.0
reward_exploration:False
train_dones:False
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 25)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'H' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'H']
 ['F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'H']
 ['F' 'H' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Starting evaluation
rdn probs:  [1.0]
0 7
0 20
0 26
main train batch thing paused
add a thread
Adding thread: now have 4 threads
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.01844862385321101 0.0 0.01844862385321101
probs:  [1.0]
maxi score, test score, baseline:  0.018118018018018017 0.0 0.018118018018018017
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.02618695652173913 0.0 0.02618695652173913
probs:  [1.0]
maxi score, test score, baseline:  0.02618695652173913 0.0 0.02618695652173913
maxi score, test score, baseline:  0.02618695652173913 0.0 0.02618695652173913
probs:  [1.0]
maxi score, test score, baseline:  0.02618695652173913 0.0 0.02618695652173913
probs:  [1.0]
deleting a thread, now have 3 threads
Frames:  746 train batches done:  16 episodes:  94
maxi score, test score, baseline:  0.02618695652173913 0.0 0.02618695652173913
maxi score, test score, baseline:  0.02618695652173913 0.0 0.02618695652173913
probs:  [1.0]
maxi score, test score, baseline:  0.02618695652173913 0.0 0.02618695652173913
probs:  [1.0]
maxi score, test score, baseline:  0.02618695652173913 0.0 0.02618695652173913
probs:  [1.0]
maxi score, test score, baseline:  0.02618695652173913 0.0 0.02618695652173913
probs:  [1.0]
maxi score, test score, baseline:  0.02618695652173913 0.0 0.02618695652173913
probs:  [1.0]
maxi score, test score, baseline:  0.02618695652173913 0.0 0.02618695652173913
maxi score, test score, baseline:  0.02618695652173913 0.0 0.02618695652173913
probs:  [1.0]
maxi score, test score, baseline:  0.02574102564102564 0.0 0.02574102564102564
probs:  [1.0]
10 101
UNIT TEST: sample policy line 217 mcts : [0.2 0.4 0.2 0.2]
10 103
maxi score, test score, baseline:  0.021839130434782607 0.0 0.021839130434782607
probs:  [1.0]
maxi score, test score, baseline:  0.021528571428571428 0.0 0.021528571428571428
probs:  [1.0]
maxi score, test score, baseline:  0.02107902097902098 0.0 0.02107902097902098
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.01958051948051948 0.0 0.01958051948051948
probs:  [1.0]
12 122
maxi score, test score, baseline:  0.018392682926829267 0.0 0.018392682926829267
probs:  [1.0]
maxi score, test score, baseline:  0.015328426395939086 0.0 0.015328426395939086
probs:  [1.0]
14 168
maxi score, test score, baseline:  0.014250943396226415 0.0 0.014250943396226415
probs:  [1.0]
maxi score, test score, baseline:  0.014053488372093022 0.0 0.014053488372093022
probs:  [1.0]
maxi score, test score, baseline:  0.013988888888888888 0.0 0.013988888888888888
probs:  [1.0]
maxi score, test score, baseline:  0.013315859030837004 0.0 0.013315859030837004
probs:  [1.0]
maxi score, test score, baseline:  0.013315859030837004 0.0 0.013315859030837004
maxi score, test score, baseline:  0.013315859030837004 0.0 0.013315859030837004
probs:  [1.0]
maxi score, test score, baseline:  0.013257894736842104 0.0 0.013257894736842104
probs:  [1.0]
maxi score, test score, baseline:  0.013200436681222706 0.0 0.013200436681222706
maxi score, test score, baseline:  0.01303103448275862 0.0 0.01303103448275862
probs:  [1.0]
maxi score, test score, baseline:  0.0126 0.0 0.0126
probs:  [1.0]
Sims:  6 1 epoch:  1662 pick best:  False frame count:  1662
maxi score, test score, baseline:  0.011594252873563218 0.0 0.011594252873563218
probs:  [1.0]
maxi score, test score, baseline:  0.011252416356877322 0.0 0.011252416356877322
probs:  [1.0]
19 233
maxi score, test score, baseline:  0.010930324909747291 0.0 0.010930324909747291
probs:  [1.0]
maxi score, test score, baseline:  0.010814285714285714 0.0 0.010814285714285714
probs:  [1.0]
20 261
maxi score, test score, baseline:  0.009968421052631578 0.0 0.009968421052631578
probs:  [1.0]
maxi score, test score, baseline:  0.009623809523809524 0.0 0.009623809523809524
probs:  [1.0]
20 275
maxi score, test score, baseline:  0.009533962264150942 0.0 0.009533962264150942
probs:  [1.0]
maxi score, test score, baseline:  0.009474999999999999 0.0 0.009474999999999999
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.009445794392523363 0.0 0.009445794392523363
maxi score, test score, baseline:  0.009445794392523363 0.0 0.009445794392523363
22 292
maxi score, test score, baseline:  0.0085985835694051 0.0 0.0085985835694051
probs:  [1.0]
maxi score, test score, baseline:  0.008341758241758241 0.0 0.008341758241758241
probs:  [1.0]
maxi score, test score, baseline:  0.008164516129032257 0.0 0.008164516129032257
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.010766666666666666 0.0 0.010766666666666666
probs:  [1.0]
maxi score, test score, baseline:  0.010654089709762532 0.0 0.010654089709762532
probs:  [1.0]
maxi score, test score, baseline:  0.010435917312661499 0.0 0.010435917312661499
probs:  [1.0]
29 351
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.006]
 [0.006]]
30 358
maxi score, test score, baseline:  0.00976183574879227 0.0 0.00976183574879227
probs:  [1.0]
maxi score, test score, baseline:  0.00955626477541371 0.0 0.00955626477541371
probs:  [1.0]
31 375
maxi score, test score, baseline:  0.009424009324009323 0.0 0.009424009324009323
probs:  [1.0]
maxi score, test score, baseline:  0.009129345372460495 0.0 0.009129345372460495
probs:  [1.0]
maxi score, test score, baseline:  0.008871929824561402 0.0 0.008871929824561402
probs:  [1.0]
maxi score, test score, baseline:  0.008702150537634408 0.0 0.008702150537634408
34 411
34 415
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.010648523206751054 0.0 0.010648523206751054
probs:  [1.0]
maxi score, test score, baseline:  0.010516666666666665 0.0 0.010516666666666665
probs:  [1.0]
maxi score, test score, baseline:  0.010388065843621399 0.0 0.010388065843621399
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
36 458
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
37 527
37 530
37 543
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.008]
 [0.006]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.008]
 [0.006]
 [0.008]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [1.0]
39 611
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
39 630
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
49 865
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.001]
 [0.001]]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.009]
 [0.009]
 [0.009]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
main train batch thing paused
add a thread
Adding thread: now have 6 threads
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
main train batch thing paused
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
55 962
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
main train batch thing paused
57 979
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
61 1002
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
main train batch thing paused
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.007]
 [0.005]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.007]
 [0.005]
 [0.004]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.003]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.003]
 [0.005]]
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
main train batch thing paused
65 1082
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
main train batch thing paused
65 1112
65 1117
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.094]
 [0.116]
 [0.083]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.097]
 [0.094]
 [0.116]
 [0.083]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
66 1129
line 256 mcts: sample exp_bonus 0.0
main train batch thing paused
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.092]
 [0.05 ]
 [0.05 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.05 ]
 [0.092]
 [0.05 ]
 [0.05 ]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
main train batch thing paused
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: False
Self play flag: True
resampling flag: False
add more workers flag:  True
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
main train batch thing paused
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
71 1246
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
main train batch thing paused
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.037]
 [0.037]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.028]
 [0.037]
 [0.037]
 [0.021]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
73 1306
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.013]
 [0.013]
 [0.013]]
main train batch thing paused
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.158]
 [0.099]
 [0.158]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.158]
 [0.158]
 [0.099]
 [0.158]]
74 1359
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
main train batch thing paused
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.042]
 [0.031]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.042]
 [0.031]
 [0.018]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0201 0.0 0.0201
76 1413
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.013]
 [0.011]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.013]
 [0.011]
 [0.009]]
rdn probs:  [1.0]
78 1430
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
siam score:  0.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0086],
        [0.1368],
        [0.0000],
        [0.0164],
        [0.0000],
        [0.0000],
        [0.0092],
        [0.0000],
        [0.0000],
        [0.0087]], dtype=torch.float64)
0.0 0.008627056759196285
0.0 0.1367701901530374
0.0 0.0
0.0 0.01639606633362569
0.970299 0.970299
0.970299 0.970299
0.0 0.009162393039574217
0.0 0.0
0.0 0.0
0.0 0.008708938203493902
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0201 0.0 0.0201
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.003]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.004]
 [0.003]
 [0.002]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
87 1518
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]]
88 1555
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
88 1587
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
90 1606
90 1611
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
91 1622
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
92 1645
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.25  0.292 0.333 0.125]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.208 0.167 0.5   0.125]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
102 1770
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Sims:  25 1 epoch:  12902 pick best:  False frame count:  12902
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.005]
 [0.005]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.006]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.006]
 [0.005]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.004]
 [0.007]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.004]
 [0.007]
 [0.011]]
104 1806
UNIT TEST: sample policy line 217 mcts : [0.167 0.167 0.208 0.458]
104 1815
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
110 1897
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
111 1922
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.003]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.003]
 [0.003]
 [0.006]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.008]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.008]
 [0.01 ]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
114 1990
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
114 2012
maxi score, test score, baseline:  0.0101 0.0 0.0101
114 2013
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
117 2085
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
117 2089
117 2094
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
117 2123
118 2129
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
120 2149
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
121 2167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.003]
 [0.007]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.003]
 [0.007]
 [0.005]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.004]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.004]
 [0.003]]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.04 ]
 [0.04 ]
 [0.04 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.035]
 [0.04 ]
 [0.04 ]
 [0.04 ]]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
129 2305
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
133 2354
133 2355
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.061]
 [0.023]
 [0.023]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.061]
 [0.023]
 [0.023]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.017]
 [0.011]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.017]
 [0.011]
 [0.011]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.013]
 [0.014]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.013]
 [0.014]
 [0.011]]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.011]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.01 ]
 [0.011]
 [0.009]]
maxi score, test score, baseline:  0.0161 0.0 0.0161
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01]
 [0.01]
 [0.01]
 [0.01]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
135 2454
135 2465
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.099]
 [0.099]
 [0.099]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.099]
 [0.099]
 [0.099]
 [0.099]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
136 2498
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
139 2535
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
142 2578
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.009]
 [0.008]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.009]
 [0.008]
 [0.005]]
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
142 2591
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.292 0.417 0.167 0.125]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.25  0.417 0.167]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
144 2621
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
153 2757
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
STARTED EXPV TRAINING ON FRAME NO.  20022
Starting evaluation
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  154
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.15 0.15
155 2832
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.004]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.001]
 [0.004]
 [0.003]]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.049]
 [0.008]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.049]
 [0.008]
 [0.005]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.03 ]
 [0.086]
 [0.03 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.03 ]
 [0.03 ]
 [0.086]
 [0.03 ]]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
156 2890
156 2896
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.167 0.208 0.167 0.458]
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.15 0.15
maxi score, test score, baseline:  0.0461 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.016]
 [0.016]
 [0.016]]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.032]
 [0.073]
 [0.073]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.026]
 [0.032]
 [0.073]
 [0.073]]
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.15 0.15
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.042]
 [0.036]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.036]
 [0.036]
 [0.042]
 [0.036]]
maxi score, test score, baseline:  0.0521 0.15 0.15
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0541 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.026]
 [0.032]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.026]
 [0.026]
 [0.026]
 [0.032]]
Sims:  25 1 epoch:  22336 pick best:  False frame count:  22336
in main func line 156:  169
170 3095
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.168]
 [0.06 ]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.168]
 [0.06 ]
 [0.011]]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
170 3110
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.909]
 [0.454]
 [0.454]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.454]
 [0.909]
 [0.454]
 [0.454]]
maxi score, test score, baseline:  0.048100000000000004 0.15 0.15
170 3131
maxi score, test score, baseline:  0.048100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.15 0.15
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
171 3199
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.15 0.15
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
175 3279
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
177 3294
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.15 0.15
maxi score, test score, baseline:  0.0461 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
179 3364
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.01 ]
 [0.006]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.01 ]
 [0.006]
 [0.012]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0541 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.167 0.167 0.292 0.375]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.019]
 [0.007]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.019]
 [0.007]
 [0.008]]
187 3460
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.605]
 [0.474]
 [0.086]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.086]
 [0.605]
 [0.474]
 [0.086]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.021]
 [0.013]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.021]
 [0.013]
 [0.003]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.355]
 [0.031]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.355]
 [0.031]
 [0.009]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
188 3517
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
189 3642
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.15 0.15
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
siam score:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.15 0.15
193 3727
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
193 3745
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0441 0.15 0.15
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
194 3768
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.031]
 [0.175]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.175]
 [0.031]
 [0.175]
 [0.012]]
siam score:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.15 0.15
maxi score, test score, baseline:  0.048100000000000004 0.15 0.15
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.048100000000000004 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.049]
 [0.474]
 [0.039]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.049]
 [0.474]
 [0.039]]
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
194 3835
maxi score, test score, baseline:  0.0381 0.15 0.15
maxi score, test score, baseline:  0.0361 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.107]
 [0.057]
 [0.031]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.052]
 [0.107]
 [0.057]
 [0.031]]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
195 3899
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
siam score:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
198 3919
200 3923
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
201 3934
201 3935
maxi score, test score, baseline:  0.0301 0.15 0.15
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.01 ]
 [0.013]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.01 ]
 [0.013]
 [0.012]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
207 4023
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
207 4047
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.088]
 [0.074]
 [0.049]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.088]
 [0.074]
 [0.049]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
210 4117
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
deleting a thread, now have 5 threads
Frames:  29327 train batches done:  2060 episodes:  4329
maxi score, test score, baseline:  0.0381 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
in main func line 156:  211
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.043]
 [0.041]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.043]
 [0.041]
 [0.003]]
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
211 4164
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.15 0.15
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0461 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.002]
 [0.002]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.013]
 [0.036]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.013]
 [0.036]
 [0.005]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
216 4216
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0541 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.012]
 [0.087]
 [0.051]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.012]
 [0.087]
 [0.051]]
218 4228
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
deleting a thread, now have 4 threads
Frames:  30444 train batches done:  2140 episodes:  4462
maxi score, test score, baseline:  0.048100000000000004 0.15 0.15
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.048100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
219 4266
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.15 0.15
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
222 4284
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0601 0.15 0.15
maxi score, test score, baseline:  0.0601 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.15 0.15
probs:  [1.0]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0014],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0013649067605288104
0.0 0.0
0.0 0.0
0.0 0.0
0.99 0.99
0.970299 0.970299
0.0 0.0
0.0 0.0
0.9801 0.9801
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0621 0.15 0.15
maxi score, test score, baseline:  0.0621 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.105]
 [0.439]
 [0.037]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.105]
 [0.439]
 [0.037]]
maxi score, test score, baseline:  0.0621 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
225 4303
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
229 4401
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.155]
 [0.155]
 [0.155]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.155]
 [0.155]
 [0.155]
 [0.155]]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0721 0.15 0.15
probs:  [1.0]
230 4431
maxi score, test score, baseline:  0.0721 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.15 0.15
maxi score, test score, baseline:  0.0741 0.15 0.15
maxi score, test score, baseline:  0.0721 0.15 0.15
siam score:  0.0
maxi score, test score, baseline:  0.0721 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.88 ]
 [0.567]
 [0.53 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.53 ]
 [0.88 ]
 [0.567]
 [0.53 ]]
maxi score, test score, baseline:  0.0721 0.15 0.15
probs:  [1.0]
231 4466
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
232 4489
siam score:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.125]
 [0.074]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.125]
 [0.074]
 [0.007]]
maxi score, test score, baseline:  0.0661 0.15 0.15
probs:  [1.0]
236 4558
maxi score, test score, baseline:  0.0641 0.15 0.15
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.15 0.15
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.095]
 [0.018]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.095]
 [0.018]
 [0.008]]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
237 4591
maxi score, test score, baseline:  0.0661 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.15 0.15
probs:  [1.0]
239 4689
maxi score, test score, baseline:  0.0541 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.15 0.15
maxi score, test score, baseline:  0.0521 0.15 0.15
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
240 4710
240 4718
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
Sims:  25 1 epoch:  34117 pick best:  False frame count:  34117
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.005]
 [0.003]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.005]
 [0.003]
 [0.   ]]
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.15 0.15
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.15 0.15
maxi score, test score, baseline:  0.0461 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
244 4828
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.15 0.15
maxi score, test score, baseline:  0.0381 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.826]
 [0.006]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.826]
 [0.006]
 [0.002]]
maxi score, test score, baseline:  0.040100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.15 0.15
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.15 0.15
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0361 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.014]
 [0.014]
 [0.014]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [1.0]
253 5004
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.002]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.002]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0301 0.15 0.15
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.15 0.15
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.15 0.15
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.041]
 [0.004]
 [0.023]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.041]
 [0.004]
 [0.023]]
siam score:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.018]
 [0.007]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.018]
 [0.007]
 [0.001]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.003]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.003]
 [0.001]]
maxi score, test score, baseline:  0.0241 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.085]
 [0.005]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.085]
 [0.005]
 [0.002]]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
259 5133
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
262 5161
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
266 5237
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.023]
 [0.057]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.023]
 [0.057]
 [0.011]]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0241 0.15 0.15
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
273 5335
maxi score, test score, baseline:  0.0201 0.15 0.15
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
273 5366
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.045]
 [0.069]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.045]
 [0.069]
 [0.015]]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0201 0.15 0.15
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.014]
 [0.015]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.014]
 [0.015]
 [0.004]]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.079]
 [0.048]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.031]
 [0.079]
 [0.048]
 [0.006]]
siam score:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.15 0.15
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.15 0.15
maxi score, test score, baseline:  0.0361 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.004]
 [0.005]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.004]
 [0.005]
 [0.003]]
maxi score, test score, baseline:  0.0361 0.15 0.15
maxi score, test score, baseline:  0.0361 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.15 0.15
probs:  [1.0]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.009]
 [0.011]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.009]
 [0.011]
 [0.001]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.003]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.003]
 [0.002]
 [0.001]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.182]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.01 ]
 [0.182]
 [0.01 ]]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
278 5594
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.104]
 [0.012]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.104]
 [0.012]
 [0.   ]]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.022]
 [0.008]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.022]
 [0.008]
 [0.005]]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.05 0.05
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.05 0.05
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
284 5693
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.05 0.05
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
284 5729
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.008]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.008]
 [0.003]]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
287 5763
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
289 5794
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.417 0.208 0.208 0.167]
maxi score, test score, baseline:  0.0461 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.002]
 [0.001]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.05 0.0541
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.628]
 [0.169]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.025]
 [0.628]
 [0.169]
 [0.021]]
maxi score, test score, baseline:  0.056100000000000004 0.05 0.056100000000000004
probs:  [1.0]
292 5875
maxi score, test score, baseline:  0.056100000000000004 0.05 0.056100000000000004
maxi score, test score, baseline:  0.056100000000000004 0.05 0.056100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
294 5894
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [1.0]
294 5903
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [1.0]
295 5917
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
296 5930
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.007]
 [0.023]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.007]
 [0.023]
 [0.003]]
maxi score, test score, baseline:  0.0601 0.05 0.0601
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.05 0.0601
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.05 0.0601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
297 5962
maxi score, test score, baseline:  0.0601 0.05 0.0601
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
probs:  [1.0]
297 5987
maxi score, test score, baseline:  0.048100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.359]
 [0.058]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.359]
 [0.058]
 [0.01 ]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.404]
 [0.018]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.404]
 [0.018]
 [0.003]]
line 256 mcts: sample exp_bonus 0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.05 0.0521
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.174]
 [0.017]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.174]
 [0.017]
 [0.003]]
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.375 0.167 0.292]
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.05 0.050100000000000006
probs:  [1.0]
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
in main func line 156:  310
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.002]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.002]
 [0.001]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.015]
 [0.045]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.015]
 [0.045]
 [0.002]]
310 6153
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
310 6176
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
311 6196
311 6201
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
311 6219
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.002]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.002]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
314 6287
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.007]
 [0.002]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.007]
 [0.002]
 [0.   ]]
315 6299
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
316 6389
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.403]
 [0.122]
 [0.051]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.051]
 [0.403]
 [0.122]
 [0.051]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.03 ]
 [0.005]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.031]
 [0.03 ]
 [0.005]
 [0.002]]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0031],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0017]], dtype=torch.float64)
0.99 0.99
0.0 0.0
0.0 0.0030826939434574273
0.99 0.99
0.0 0.0
0.99 0.99
0.9801 0.9801
0.0 0.0
0.9801 0.9801
0.0 0.0017003528435191393
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
319 6479
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.072]
 [0.01 ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.072]
 [0.01 ]
 [0.   ]]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
321 6527
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
321 6537
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.017]
 [0.004]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.017]
 [0.004]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.003]
 [0.003]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.003]
 [0.003]
 [0.001]]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.003]]
323 6603
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.013]
 [0.009]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.013]
 [0.009]
 [0.004]]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
324 6618
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.05 0.05
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.003]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
330 6741
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0361 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.021]
 [0.055]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.021]
 [0.055]
 [0.012]]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
maxi score, test score, baseline:  0.034100000000000005 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  332
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
332 6791
332 6794
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
332 6804
maxi score, test score, baseline:  0.032100000000000004 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
333 6828
maxi score, test score, baseline:  0.0301 0.05 0.05
probs:  [1.0]
333 6832
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
333 6835
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
333 6868
maxi score, test score, baseline:  0.0281 0.05 0.05
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.05 0.05
maxi score, test score, baseline:  0.0281 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.012]
 [0.014]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.012]
 [0.014]
 [0.003]]
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
333 6921
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0241 0.05 0.05
maxi score, test score, baseline:  0.0241 0.05 0.05
maxi score, test score, baseline:  0.0241 0.05 0.05
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.125 0.167 0.542 0.167]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
335 7019
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0201 0.05 0.05
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.05 0.05
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.001]]
339 7069
339 7077
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
Starting evaluation
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
siam score:  0.0
339 7124
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
341 7158
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.125 0.333 0.375 0.167]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.003]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.003]
 [0.001]
 [0.001]]
341 7184
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
341 7187
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
341 7214
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.148]
 [0.169]
 [0.019]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.019]
 [0.148]
 [0.169]
 [0.019]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.198]
 [0.021]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.198]
 [0.021]
 [0.021]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
341 7265
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.08 ]
 [0.005]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.08 ]
 [0.005]
 [0.004]]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
346 7362
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.075]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.075]
 [0.004]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
351 7515
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.021]
 [0.021]
 [0.021]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.178]
 [0.084]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.178]
 [0.084]
 [0.008]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
352 7543
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
356 7567
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.033]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.033]
 [0.002]
 [0.002]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.157]
 [0.007]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.157]
 [0.007]
 [0.002]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.007]
 [0.021]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.007]
 [0.021]
 [0.002]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
362 7747
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [1.016]
 [0.546]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.546]
 [0.546]
 [1.016]
 [0.546]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
365 7808
deleting a thread, now have 3 threads
Frames:  54511 train batches done:  3832 episodes:  8180
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
365 7823
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
366 7880
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
368 7965
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.825]
 [0.77 ]
 [0.825]
 [0.825]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.825]
 [0.77 ]
 [0.825]
 [0.825]]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
371 8022
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.038]
 [0.005]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.038]
 [0.005]
 [0.001]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
371 8069
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.007]
 [0.015]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.007]
 [0.015]
 [0.013]]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
373 8129
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
siam score:  0.0
374 8144
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
siam score:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
UNIT TEST: sample policy line 217 mcts : [0.375 0.208 0.208 0.208]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
376 8209
377 8215
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.012]
 [0.006]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.012]
 [0.006]
 [0.004]]
380 8320
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
380 8333
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.01 ]
 [0.008]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.01 ]
 [0.008]
 [0.005]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
380 8358
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.012]
 [0.01 ]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.012]
 [0.01 ]
 [0.01 ]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
380 8393
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
384 8419
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
385 8435
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.259]
 [0.149]
 [0.058]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.058]
 [0.259]
 [0.149]
 [0.058]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.25  0.208 0.417 0.125]
maxi score, test score, baseline:  0.0121 0.0 0.0121
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
388 8460
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.012]
 [0.005]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.012]
 [0.005]
 [0.004]]
siam score:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.039]
 [0.038]
 [0.053]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.039]
 [0.038]
 [0.053]]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.102]
 [0.105]
 [0.088]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.034]
 [0.102]
 [0.105]
 [0.088]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
396 8549
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
399 8596
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
400 8625
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Starting evaluation
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
401 8701
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.031]
 [0.02 ]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.031]
 [0.02 ]
 [0.007]]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
407 8845
siam score:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
410 8875
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [1.016]
 [0.65 ]
 [0.65 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.65 ]
 [1.016]
 [0.65 ]
 [0.65 ]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
412 8918
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
413 9004
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.806]
 [0.15 ]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.806]
 [0.15 ]
 [0.015]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
413 9064
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
siam score:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.088]
 [0.025]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.034]
 [0.088]
 [0.025]
 [0.003]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.759]
 [0.759]
 [0.759]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.759]
 [0.759]
 [0.759]
 [0.759]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.226]
 [1.052]
 [0.548]
 [0.226]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.226]
 [1.052]
 [0.548]
 [0.226]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.01 ]
 [0.009]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.01 ]
 [0.009]
 [0.007]]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.0 0.0241
417 9221
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.069]
 [0.068]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.069]
 [0.068]
 [0.003]]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
417 9237
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
417 9249
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.0 0.0241
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.179]
 [0.204]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.179]
 [0.204]
 [0.012]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.014]
 [0.005]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.014]
 [0.005]
 [0.004]]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
siam score:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
419 9286
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.038]
 [0.015]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.038]
 [0.015]
 [0.01 ]]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.0 0.022099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.015]
 [0.017]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.043]
 [0.015]
 [0.017]
 [0.013]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
424 9463
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[1.052]
 [1.052]
 [1.052]
 [1.052]] [[0.]
 [0.]
 [0.]
 [0.]] [[1.052]
 [1.052]
 [1.052]
 [1.052]]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.004]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.003]
 [0.004]
 [0.002]]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
429 9541
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
429 9550
maxi score, test score, baseline:  0.0281 0.0 0.0281
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
429 9552
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
433 9660
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
433 9671
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.026]
 [0.035]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.026]
 [0.035]
 [0.006]]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.086]
 [0.017]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.086]
 [0.017]
 [0.006]]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
435 9733
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
435 9756
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
436 9768
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
437 9833
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
437 9841
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0361 0.0 0.0361
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.006]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.006]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
442 9915
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.016]
 [0.312]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.039]
 [0.016]
 [0.312]
 [0.001]]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.755]
 [0.545]
 [0.545]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.545]
 [0.755]
 [0.545]
 [0.545]]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.164]
 [0.026]
 [0.026]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.026]
 [0.164]
 [0.026]
 [0.026]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
445 10068
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
445 10076
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.02]
 [0.05]
 [0.03]
 [0.01]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.02]
 [0.05]
 [0.03]
 [0.01]]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.261]
 [0.002]
 [0.261]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.261]
 [0.002]
 [0.261]]
maxi score, test score, baseline:  0.0301 0.0 0.0301
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
446 10127
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.125 0.167 0.25  0.458]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
451 10187
maxi score, test score, baseline:  0.0281 0.0 0.0281
siam score:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.369]
 [0.076]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.096]
 [0.369]
 [0.076]
 [0.   ]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
siam score:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.908]
 [0.941]
 [0.768]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.768]
 [0.908]
 [0.941]
 [0.768]]
Printing some Q and Qe and total Qs values:  [[0.07]
 [0.95]
 [0.07]
 [0.07]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.07]
 [0.95]
 [0.07]
 [0.07]]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
455 10319
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.19]
 [0.19]
 [0.19]
 [0.19]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.19]
 [0.19]
 [0.19]
 [0.19]]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.1 0.1
456 10491
maxi score, test score, baseline:  0.056100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.004]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.009]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.009]
 [0.001]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.9801 0.9801
0.0 0.0
0.0 0.0
0.99 0.99
0.99 0.99
0.0 6.356662810320047e-06
0.0 0.0
0.0 0.0
0.99 0.99
0.0 0.0
459 10516
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
461 10557
461 10560
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0875],
        [0.0396],
        [0.0000],
        [0.0719],
        [0.0000],
        [0.0000],
        [0.0639],
        [0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 0.08753125272385306
0.0 0.039552149997283174
0.0 0.0
0.0 0.07186974272816382
0.99 0.99
0.99 0.99
0.0 0.06386612813221304
0.0 0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.051]
 [0.041]
 [0.03 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.025]
 [0.051]
 [0.041]
 [0.03 ]]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.051]
 [0.051]
 [0.051]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.051]
 [0.051]
 [0.051]
 [0.051]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.075]
 [0.04 ]
 [0.037]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.037]
 [0.075]
 [0.04 ]
 [0.037]]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.091]
 [0.026]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.037]
 [0.091]
 [0.026]
 [0.021]]
467 10628
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.009]
 [0.016]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.009]
 [0.016]
 [0.015]]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.016]
 [0.024]
 [0.02 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.016]
 [0.024]
 [0.02 ]]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.036]
 [0.183]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.036]
 [0.183]
 [0.016]]
471 10728
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.007]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.007]
 [0.005]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.1 0.1
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.062]
 [0.099]
 [0.078]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.101]
 [0.062]
 [0.099]
 [0.078]]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.189]
 [0.189]
 [0.189]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.189]
 [0.189]
 [0.189]
 [0.189]]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
478 10853
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.118]
 [0.219]
 [0.19 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.099]
 [0.118]
 [0.219]
 [0.19 ]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
Printing some Q and Qe and total Qs values:  [[0.09 ]
 [0.208]
 [0.193]
 [0.18 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.09 ]
 [0.208]
 [0.193]
 [0.18 ]]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
482 10889
siam score:  0.0
482 10902
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.102]
 [0.115]
 [0.067]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.067]
 [0.102]
 [0.115]
 [0.067]]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
484 10927
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
UNIT TEST: sample policy line 217 mcts : [0.167 0.25  0.458 0.125]
maxi score, test score, baseline:  0.0381 0.1 0.1
487 10976
siam score:  0.0
487 10984
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.021]
 [0.019]
 [0.019]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.021]
 [0.019]
 [0.019]]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
siam score:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
496 11162
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
497 11183
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.1 0.1
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.005]
 [0.022]
 [0.019]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.005]
 [0.022]
 [0.019]]
siam score:  0.0
498 11197
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.1 0.1
498 11218
maxi score, test score, baseline:  0.0521 0.1 0.1
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.146]
 [0.146]
 [0.146]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.146]
 [0.146]
 [0.146]
 [0.146]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.024]
 [0.046]
 [0.026]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.025]
 [0.024]
 [0.046]
 [0.026]]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
499 11265
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.025]
 [0.026]
 [0.027]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.02 ]
 [0.025]
 [0.026]
 [0.027]]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.01 ]
 [0.025]
 [0.026]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.01 ]
 [0.025]
 [0.026]]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
siam score:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.022]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.022]
 [0.005]]
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
504 11354
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.191]
 [0.11 ]
 [0.019]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.19 ]
 [0.191]
 [0.11 ]
 [0.019]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
504 11369
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
504 11403
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.114]
 [0.108]
 [0.127]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.107]
 [0.114]
 [0.108]
 [0.127]]
504 11422
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.103]
 [0.094]
 [0.127]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.099]
 [0.103]
 [0.094]
 [0.127]]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.131]
 [0.227]
 [0.119]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.119]
 [0.131]
 [0.227]
 [0.119]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
504 11495
siam score:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.943]
 [0.811]
 [0.811]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.811]
 [0.943]
 [0.811]
 [0.811]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.398]
 [0.575]
 [0.33 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.184]
 [0.398]
 [0.575]
 [0.33 ]]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
509 11546
509 11559
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.627]
 [0.443]
 [0.443]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.443]
 [0.627]
 [0.443]
 [0.443]]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.314]
 [0.477]
 [0.314]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.314]
 [0.314]
 [0.477]
 [0.314]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
siam score:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
515 11635
515 11637
515 11639
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.022]
 [0.025]
 [0.025]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.025]
 [0.022]
 [0.025]
 [0.025]]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
Starting evaluation
maxi score, test score, baseline:  0.0301 0.1 0.1
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0594],
        [0.0000],
        [0.0000],
        [0.0587],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0858],
        [0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.05942301142437116
0.99 0.99
0.970299 0.970299
0.0 0.05868064596233519
0.0 0.0
0.99 0.99
0.99 0.99
0.0 0.085819070289115
0.0 0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
515 11681
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
515 11726
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
515 11763
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
515 11820
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
515 11851
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
515 11872
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
515 11944
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
516 11956
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
siam score:  0.0
516 11967
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.018]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.018]
 [0.018]
 [0.018]]
maxi score, test score, baseline:  0.0361 0.1 0.1
518 12015
maxi score, test score, baseline:  0.0361 0.1 0.1
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.125 0.333 0.208 0.333]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
519 12083
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [1.0]
522 12242
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.082]
 [0.034]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.082]
 [0.034]
 [0.009]]
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
522 12286
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
522 12291
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.044]
 [0.065]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.044]
 [0.065]
 [0.006]]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
524 12339
524 12358
maxi score, test score, baseline:  0.0241 0.1 0.1
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.033]
 [0.051]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.033]
 [0.051]
 [0.009]]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0241 0.1 0.1
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0201 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
527 12447
line 256 mcts: sample exp_bonus 0.0
527 12463
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.087]
 [0.06 ]
 [0.03 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.087]
 [0.06 ]
 [0.03 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
528 12564
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
528 12586
528 12587
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
529 12672
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.08 ]
 [0.08 ]
 [0.061]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.065]
 [0.08 ]
 [0.08 ]
 [0.061]]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
531 12750
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.1 0.1
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.012]
 [0.018]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.012]
 [0.018]
 [0.016]]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
533 12812
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.025]
 [0.048]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.025]
 [0.048]
 [0.016]]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.1 0.1
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [1.0]
533 12843
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
534 12850
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
siam score:  0.0
534 12890
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
534 12902
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.101]
 [0.209]
 [0.073]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.035]
 [0.101]
 [0.209]
 [0.073]]
534 12911
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.047]
 [0.047]
 [0.047]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.047]
 [0.047]
 [0.047]
 [0.047]]
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.064]
 [0.064]
 [0.064]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.064]
 [0.064]
 [0.064]
 [0.064]]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
538 13004
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
538 13040
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.033]
 [0.033]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.033]
 [0.033]
 [0.033]
 [0.033]]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
540 13164
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
540 13194
540 13222
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
540 13225
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.   ]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.   ]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
540 13256
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
541 13282
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
542 13298
siam score:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
542 13309
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.014]
 [0.014]
 [0.014]]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.054]
 [0.007]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.054]
 [0.007]
 [0.007]]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.018]
 [0.017]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.018]
 [0.017]
 [0.007]]
Starting evaluation
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
543 13347
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.126]
 [0.072]
 [0.02 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.126]
 [0.072]
 [0.02 ]]
maxi score, test score, baseline:  0.0461 0.1 0.1
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.02]
 [0.02]
 [0.02]
 [0.02]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.02]
 [0.02]
 [0.02]
 [0.02]]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.013]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.012]
 [0.013]
 [0.012]]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.25  0.375 0.208]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
547 13532
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.258]
 [0.164]
 [0.089]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.089]
 [0.258]
 [0.164]
 [0.089]]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
547 13541
547 13556
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.039]
 [0.027]
 [0.019]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.039]
 [0.027]
 [0.019]]
548 13592
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01]
 [0.01]
 [0.01]
 [0.01]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.025]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.015]
 [0.025]
 [0.009]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
550 13689
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
551 13700
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
553 13732
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.083 0.625 0.125 0.167]
maxi score, test score, baseline:  0.0461 0.1 0.1
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
555 13791
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.1 0.1
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.119]
 [0.084]
 [0.084]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.084]
 [0.119]
 [0.084]
 [0.084]]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.075]
 [0.059]
 [0.043]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.043]
 [0.075]
 [0.059]
 [0.043]]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
559 13890
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.027]
 [0.047]
 [0.047]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.047]
 [0.027]
 [0.047]
 [0.047]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.050100000000000006 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.022]
 [0.02 ]
 [0.017]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.031]
 [0.022]
 [0.02 ]
 [0.017]]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.101]
 [0.022]
 [0.022]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.101]
 [0.022]
 [0.022]]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
562 14023
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.016]
 [0.022]
 [0.02 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.02 ]
 [0.016]
 [0.022]
 [0.02 ]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.016]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.008]
 [0.016]
 [0.008]]
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.015]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.009]
 [0.015]
 [0.009]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.   ]
 [0.013]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.   ]
 [0.013]
 [0.013]]
564 14117
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.1 0.1
564 14141
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
564 14158
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.034]
 [0.007]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.034]
 [0.007]
 [0.005]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.022]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.016]
 [0.022]
 [0.016]]
567 14218
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.042]
 [0.049]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.042]
 [0.049]
 [0.015]]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.042]
 [0.042]
 [0.042]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.042]
 [0.042]
 [0.042]
 [0.042]]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.011]
 [0.019]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.011]
 [0.019]
 [0.015]]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.214]
 [0.046]
 [0.027]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.025]
 [0.214]
 [0.046]
 [0.027]]
570 14306
570 14307
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.347]
 [0.078]
 [0.078]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.078]
 [0.347]
 [0.078]
 [0.078]]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.662]
 [0.554]
 [0.471]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.471]
 [0.662]
 [0.554]
 [0.471]]
maxi score, test score, baseline:  0.0301 0.1 0.1
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.012]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.012]
 [0.006]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.356]
 [0.198]
 [0.198]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.198]
 [0.356]
 [0.198]
 [0.198]]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
571 14398
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.026]
 [0.011]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.026]
 [0.011]
 [0.011]]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
573 14480
573 14483
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.1 0.1
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
573 14567
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.027]
 [0.029]
 [0.033]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.032]
 [0.027]
 [0.029]
 [0.033]]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
573 14601
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
575 14617
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
575 14650
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.1 0.1
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.121]
 [0.175]
 [0.178]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.178]
 [0.121]
 [0.175]
 [0.178]]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.023]
 [0.008]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.023]
 [0.008]
 [0.01 ]]
maxi score, test score, baseline:  0.0441 0.1 0.1
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
583 14760
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
583 14781
maxi score, test score, baseline:  0.0441 0.1 0.1
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.1 0.1
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0601 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.918]
 [0.188]
 [0.188]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.188]
 [0.918]
 [0.188]
 [0.188]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.049]
 [0.025]
 [0.023]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.049]
 [0.025]
 [0.023]]
maxi score, test score, baseline:  0.0641 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.1 0.1
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.126]
 [0.097]
 [0.11 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.11 ]
 [0.126]
 [0.097]
 [0.11 ]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
maxi score, test score, baseline:  0.06810000000000001 0.1 0.1
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.1 0.1
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.1 0.1
probs:  [1.0]
Starting evaluation
rdn probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
590 14946
maxi score, test score, baseline:  0.0741 0.0 0.0741
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.042]
 [0.019]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.042]
 [0.019]
 [0.015]]
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0781 0.0 0.0781
probs:  [1.0]
maxi score, test score, baseline:  0.0781 0.0 0.0781
maxi score, test score, baseline:  0.0781 0.0 0.0781
probs:  [1.0]
maxi score, test score, baseline:  0.0761 0.0 0.0761
maxi score, test score, baseline:  0.0761 0.0 0.0761
maxi score, test score, baseline:  0.0741 0.0 0.0741
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.0 0.0741
maxi score, test score, baseline:  0.0741 0.0 0.0741
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.0 0.0741
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.0 0.0741
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.0 0.0741
594 15113
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.0 0.0721
595 15132
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.003]
 [0.025]
 [0.02 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.003]
 [0.025]
 [0.02 ]]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.017]
 [0.018]
 [0.017]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.017]
 [0.018]
 [0.017]]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.011]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.013]
 [0.011]
 [0.013]]
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.167 0.5   0.167 0.167]
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.0 0.0661
maxi score, test score, baseline:  0.0641 0.0 0.0641
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.0 0.0641
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.019]
 [0.014]
 [0.014]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.019]
 [0.014]
 [0.014]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.138]
 [0.163]
 [0.044]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.024]
 [0.138]
 [0.163]
 [0.044]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.016]
 [0.016]
 [0.016]]
maxi score, test score, baseline:  0.0661 0.0 0.0661
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.0 0.0641
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.769]
 [0.244]
 [0.068]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.769]
 [0.244]
 [0.068]]
maxi score, test score, baseline:  0.0641 0.0 0.0641
probs:  [1.0]
597 15253
maxi score, test score, baseline:  0.0641 0.0 0.0641
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.0 0.0641
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.0 0.0641
probs:  [1.0]
598 15271
UNIT TEST: sample policy line 217 mcts : [0.083 0.708 0.125 0.083]
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.0 0.0521
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.069]
 [0.036]
 [0.038]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.038]
 [0.069]
 [0.036]
 [0.038]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.163]
 [0.037]
 [0.041]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.034]
 [0.163]
 [0.037]
 [0.041]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.0 0.0541
probs:  [1.0]
maxi score, test score, baseline:  0.0521 0.0 0.0521
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.211]
 [0.345]
 [0.102]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.102]
 [0.211]
 [0.345]
 [0.102]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.012]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.004]
 [0.012]
 [0.003]]
siam score:  0.0
601 15404
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [1.0]
601 15416
601 15419
maxi score, test score, baseline:  0.0441 0.0 0.0441
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
601 15440
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
602 15491
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
maxi score, test score, baseline:  0.0381 0.0 0.0381
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [1.0]
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.005]
 [0.027]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.005]
 [0.027]
 [0.013]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.085]
 [0.042]
 [0.053]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.03 ]
 [0.085]
 [0.042]
 [0.053]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.021]
 [0.047]
 [0.03 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.021]
 [0.047]
 [0.03 ]]
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
604 15667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.0 0.0541
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
606 15680
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0601 0.0 0.0601
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.448]
 [0.307]
 [0.373]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.373]
 [0.448]
 [0.307]
 [0.373]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0641 0.0 0.0641
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.04 ]
 [0.398]
 [0.051]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.04 ]
 [0.398]
 [0.051]]
maxi score, test score, baseline:  0.0641 0.0 0.0641
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.0 0.0641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.0 0.0661
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.145]
 [0.145]
 [0.145]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.145]
 [0.145]
 [0.145]
 [0.145]]
maxi score, test score, baseline:  0.0641 0.0 0.0641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.0 0.0741
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.034]
 [0.057]
 [0.058]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.183]
 [0.034]
 [0.057]
 [0.058]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.0 0.0741
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.32 ]
 [0.132]
 [0.132]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.132]
 [0.32 ]
 [0.132]
 [0.132]]
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
609 15893
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
609 15927
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.12 ]
 [0.094]
 [0.134]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.047]
 [0.12 ]
 [0.094]
 [0.134]]
maxi score, test score, baseline:  0.0601 0.0 0.0601
maxi score, test score, baseline:  0.0601 0.0 0.0601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0641 0.0 0.0641
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
maxi score, test score, baseline:  0.0641 0.0 0.0641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
612 16038
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.085]
 [0.101]
 [0.085]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.085]
 [0.085]
 [0.101]
 [0.085]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.106]
 [0.175]
 [0.14 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.112]
 [0.106]
 [0.175]
 [0.14 ]]
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.192]
 [0.136]
 [0.131]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.087]
 [0.192]
 [0.136]
 [0.131]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
614 16110
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
614 16114
maxi score, test score, baseline:  0.0741 0.0 0.0741
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
614 16119
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.101]
 [0.101]
 [0.101]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.101]
 [0.101]
 [0.101]
 [0.101]]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0781 0.0 0.0781
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0721 0.0 0.0721
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
616 16190
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
probs:  [1.0]
617 16214
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0601 0.0 0.0601
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.197]
 [0.24 ]
 [0.239]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.187]
 [0.197]
 [0.24 ]
 [0.239]]
maxi score, test score, baseline:  0.0601 0.0 0.0601
siam score:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
probs:  [1.0]
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.0541 0.0 0.0541
probs:  [1.0]
maxi score, test score, baseline:  0.0541 0.0 0.0541
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.196]
 [0.146]
 [0.146]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.092]
 [0.196]
 [0.146]
 [0.146]]
618 16285
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
618 16289
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
probs:  [1.0]
618 16297
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0601 0.0 0.0601
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.0 0.0601
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.079]
 [0.114]
 [0.08 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.065]
 [0.079]
 [0.114]
 [0.08 ]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0601 0.0 0.0601
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0621 0.0 0.0621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
619 16365
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
619 16398
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
620 16410
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0821 0.0 0.0821
probs:  [1.0]
maxi score, test score, baseline:  0.0821 0.0 0.0821
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.08410000000000001 0.0 0.08410000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0861 0.0 0.0861
maxi score, test score, baseline:  0.0861 0.0 0.0861
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.227]
 [0.241]
 [0.232]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.193]
 [0.227]
 [0.241]
 [0.232]]
maxi score, test score, baseline:  0.0821 0.0 0.0821
probs:  [1.0]
621 16463
621 16467
maxi score, test score, baseline:  0.0801 0.0 0.0801
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.073]
 [0.052]
 [0.046]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.049]
 [0.073]
 [0.052]
 [0.046]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.0 0.0801
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
622 16487
maxi score, test score, baseline:  0.0801 0.0 0.0801
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
maxi score, test score, baseline:  0.0821 0.0 0.0821
probs:  [1.0]
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
siam score:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.0 0.0741
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.082]
 [0.168]
 [0.108]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.046]
 [0.082]
 [0.168]
 [0.108]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
622 16572
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
622 16585
maxi score, test score, baseline:  0.0741 0.0 0.0741
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.0 0.0741
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.0 0.0741
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
625 16606
maxi score, test score, baseline:  0.0781 0.0 0.0781
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.0 0.0801
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.368]
 [0.446]
 [0.407]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.407]
 [0.368]
 [0.446]
 [0.407]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.034]
 [0.153]
 [0.051]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.093]
 [0.034]
 [0.153]
 [0.051]]
maxi score, test score, baseline:  0.08410000000000001 0.0 0.08410000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.08410000000000001 0.0 0.08410000000000001
maxi score, test score, baseline:  0.08410000000000001 0.0 0.08410000000000001
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.663]
 [0.45 ]
 [0.546]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.546]
 [0.663]
 [0.45 ]
 [0.546]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
627 16629
maxi score, test score, baseline:  0.0861 0.0 0.0861
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.121]
 [0.149]
 [0.092]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.082]
 [0.121]
 [0.149]
 [0.092]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.10010000000000001 0.0 0.10010000000000001
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.287]
 [0.287]
 [0.287]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.287]
 [0.287]
 [0.287]
 [0.287]]
maxi score, test score, baseline:  0.10010000000000001 0.0 0.10010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
627 16662
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.0 0.1081
probs:  [1.0]
627 16672
maxi score, test score, baseline:  0.1081 0.0 0.1081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.0 0.1121
maxi score, test score, baseline:  0.1121 0.0 0.1121
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1101 0.0 0.1101
631 16708
maxi score, test score, baseline:  0.1101 0.0 0.1101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1101 0.0 0.1101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.245]
 [0.184]
 [0.273]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.262]
 [0.245]
 [0.184]
 [0.273]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
635 16771
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1201 0.0 0.1201
probs:  [1.0]
maxi score, test score, baseline:  0.1201 0.0 0.1201
probs:  [1.0]
maxi score, test score, baseline:  0.1201 0.0 0.1201
probs:  [1.0]
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
637 16816
maxi score, test score, baseline:  0.1201 0.0 0.1201
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.599]
 [0.613]
 [0.492]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.472]
 [0.599]
 [0.613]
 [0.492]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
637 16820
maxi score, test score, baseline:  0.1241 0.0 0.1241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.0 0.1301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.0 0.1321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.0 0.1341
maxi score, test score, baseline:  0.1341 0.0 0.1341
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.0 0.1321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.0 0.1301
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.308]
 [0.312]
 [0.306]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.162]
 [0.308]
 [0.312]
 [0.306]]
maxi score, test score, baseline:  0.1241 0.0 0.1241
probs:  [1.0]
maxi score, test score, baseline:  0.1241 0.0 0.1241
probs:  [1.0]
maxi score, test score, baseline:  0.1241 0.0 0.1241
probs:  [1.0]
maxi score, test score, baseline:  0.1241 0.0 0.1241
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.0 0.1261
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
maxi score, test score, baseline:  0.1261 0.0 0.1261
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.0 0.1261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.0 0.1341
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.0 0.1381
maxi score, test score, baseline:  0.1381 0.0 0.1381
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.926]
 [0.926]
 [0.076]
 [0.926]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.926]
 [0.926]
 [0.076]
 [0.926]]
maxi score, test score, baseline:  0.1381 0.0 0.1381
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.0 0.14209999999999998
probs:  [1.0]
639 16979
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.174]
 [0.308]
 [0.22 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.091]
 [0.174]
 [0.308]
 [0.22 ]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.0 0.15009999999999998
639 17021
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1541 0.0 0.1541
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
641 17042
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.208]
 [0.161]
 [0.168]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.177]
 [0.208]
 [0.161]
 [0.168]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.0 0.15209999999999999
probs:  [1.0]
maxi score, test score, baseline:  0.14609999999999998 0.0 0.14609999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.0 0.15009999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.15009999999999998 0.0 0.15009999999999998
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.0 0.15009999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.0 0.15009999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.038]
 [0.117]
 [0.1  ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.077]
 [0.038]
 [0.117]
 [0.1  ]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.0 0.15209999999999999
probs:  [1.0]
maxi score, test score, baseline:  0.15209999999999999 0.0 0.15209999999999999
probs:  [1.0]
maxi score, test score, baseline:  0.15209999999999999 0.0 0.15209999999999999
probs:  [1.0]
maxi score, test score, baseline:  0.15209999999999999 0.0 0.15209999999999999
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.023]
 [0.122]
 [0.077]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.077]
 [0.023]
 [0.122]
 [0.077]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.0 0.14809999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.14609999999999998 0.0 0.14609999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.099]
 [0.128]
 [0.108]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.076]
 [0.099]
 [0.128]
 [0.108]]
644 17105
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.0 0.1401
maxi score, test score, baseline:  0.1401 0.0 0.1401
probs:  [1.0]
maxi score, test score, baseline:  0.1381 0.0 0.1381
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
646 17127
maxi score, test score, baseline:  0.1361 0.0 0.1361
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.0 0.1321
probs:  [1.0]
siam score:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.0 0.1261
maxi score, test score, baseline:  0.1241 0.0 0.1241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1241 0.0 0.1241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.0 0.1341
probs:  [1.0]
maxi score, test score, baseline:  0.1341 0.0 0.1341
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.0 0.1321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.0 0.1401
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.0 0.1341
probs:  [1.0]
maxi score, test score, baseline:  0.1341 0.0 0.1341
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.0 0.1321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.0 0.1321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1341 0.0 0.1341
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.132]
 [0.132]
 [0.132]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.132]
 [0.132]
 [0.132]
 [0.132]]
maxi score, test score, baseline:  0.1321 0.0 0.1321
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.0 0.1321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1301 0.0 0.1301
probs:  [1.0]
maxi score, test score, baseline:  0.1301 0.0 0.1301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.0 0.1341
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.0 0.1301
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.093]
 [0.093]
 [0.093]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.093]
 [0.093]
 [0.093]
 [0.093]]
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1241 0.0 0.1241
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.024]
 [0.137]
 [0.133]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.024]
 [0.137]
 [0.133]]
maxi score, test score, baseline:  0.1221 0.0 0.1221
maxi score, test score, baseline:  0.1201 0.0 0.1201
probs:  [1.0]
maxi score, test score, baseline:  0.11610000000000001 0.0 0.11610000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
658 17347
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.0 0.1181
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.088]
 [0.237]
 [0.199]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.157]
 [0.088]
 [0.237]
 [0.199]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.0 0.1321
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.144]
 [0.144]
 [0.144]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.144]
 [0.144]
 [0.144]
 [0.144]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.0 0.1321
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.0 0.1321
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.0 0.1341
probs:  [1.0]
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.254]
 [0.254]
 [0.254]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.254]
 [0.254]
 [0.254]
 [0.254]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.0 0.1281
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.169]
 [0.159]
 [0.169]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.169]
 [0.169]
 [0.159]
 [0.169]]
maxi score, test score, baseline:  0.1181 0.0 0.1181
660 17455
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
660 17464
maxi score, test score, baseline:  0.1101 0.0 0.1101
probs:  [1.0]
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.0 0.1081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.0 0.1181
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.098]
 [0.129]
 [0.123]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.098]
 [0.129]
 [0.123]]
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
661 17505
maxi score, test score, baseline:  0.1181 0.0 0.1181
probs:  [1.0]
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.175]
 [0.175]
 [0.175]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.175]
 [0.175]
 [0.175]
 [0.175]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.161]
 [0.186]
 [0.185]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.091]
 [0.161]
 [0.186]
 [0.185]]
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
662 17529
maxi score, test score, baseline:  0.1101 0.0 0.1101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.156]
 [0.16 ]
 [0.155]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.133]
 [0.156]
 [0.16 ]
 [0.155]]
maxi score, test score, baseline:  0.1081 0.0 0.1081
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
maxi score, test score, baseline:  0.1041 0.0 0.1041
probs:  [1.0]
maxi score, test score, baseline:  0.1041 0.0 0.1041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
maxi score, test score, baseline:  0.1061 0.0 0.1061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.0 0.1081
maxi score, test score, baseline:  0.1081 0.0 0.1081
probs:  [1.0]
663 17588
663 17596
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.036]
 [0.053]
 [0.047]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.044]
 [0.036]
 [0.053]
 [0.047]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.0 0.1081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.0 0.1121
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.228]
 [0.153]
 [0.153]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.153]
 [0.228]
 [0.153]
 [0.153]]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.801]
 [0.297]
 [0.335]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.335]
 [0.801]
 [0.297]
 [0.335]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
664 17628
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.205]
 [0.25 ]
 [0.234]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.234]
 [0.205]
 [0.25 ]
 [0.234]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
665 17639
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.0 0.11410000000000001
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1121 0.0 0.1121
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.37]
 [0.37]
 [0.37]
 [0.37]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.37]
 [0.37]
 [0.37]
 [0.37]]
maxi score, test score, baseline:  0.1041 0.0 0.1041
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.292 0.25  0.292 0.167]
maxi score, test score, baseline:  0.1021 0.0 0.1021
probs:  [1.0]
maxi score, test score, baseline:  0.0981 0.0 0.0981
probs:  [1.0]
maxi score, test score, baseline:  0.0961 0.0 0.0961
probs:  [1.0]
maxi score, test score, baseline:  0.0961 0.0 0.0961
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0921 0.0 0.0921
probs:  [1.0]
maxi score, test score, baseline:  0.0921 0.0 0.0921
probs:  [1.0]
maxi score, test score, baseline:  0.0921 0.0 0.0921
probs:  [1.0]
maxi score, test score, baseline:  0.0901 0.0 0.0901
maxi score, test score, baseline:  0.0901 0.0 0.0901
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.208 0.458 0.125 0.208]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0901 0.0 0.0901
maxi score, test score, baseline:  0.0901 0.0 0.0901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0901 0.0 0.0901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0921 0.0 0.0921
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0941 0.0 0.0941
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.549]
 [0.135]
 [0.135]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.135]
 [0.549]
 [0.135]
 [0.135]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
670 17787
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0881 0.0 0.0881
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.208 0.375]
maxi score, test score, baseline:  0.0881 0.0 0.0881
maxi score, test score, baseline:  0.0881 0.0 0.0881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0821 0.0 0.0821
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0781 0.0 0.0781
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.0 0.0801
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.335]
 [0.521]
 [0.494]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.494]
 [0.335]
 [0.521]
 [0.494]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0821 0.0 0.0821
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.003]
 [0.021]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.003]
 [0.021]
 [0.021]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0821 0.0 0.0821
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.094]
 [0.056]
 [0.036]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.026]
 [0.094]
 [0.056]
 [0.036]]
maxi score, test score, baseline:  0.08410000000000001 0.0 0.08410000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0861 0.0 0.0861
probs:  [1.0]
maxi score, test score, baseline:  0.0861 0.0 0.0861
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0881 0.0 0.0881
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0781 0.0 0.0781
probs:  [1.0]
maxi score, test score, baseline:  0.0781 0.0 0.0781
Starting evaluation
maxi score, test score, baseline:  0.0781 0.0 0.0781
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.08410000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.08410000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.08410000000000001 0.15 0.15
probs:  [1.0]
671 18013
671 18015
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0861 0.15 0.15
maxi score, test score, baseline:  0.0861 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.337]
 [0.254]
 [0.125]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.125]
 [0.337]
 [0.254]
 [0.125]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.072]
 [0.102]
 [0.044]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.072]
 [0.102]
 [0.044]]
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.095]
 [0.131]
 [0.08 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.062]
 [0.095]
 [0.131]
 [0.08 ]]
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0961 0.15 0.15
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0941 0.15 0.15
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0961 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.1583],
        [0.8439],
        [0.0043],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.1583074958409325
0.0 0.8438640670938315
0.0 0.004269140995427379
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.15 0.15
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.064]
 [0.118]
 [0.091]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.097]
 [0.064]
 [0.118]
 [0.091]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.15 0.15
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
673 18186
maxi score, test score, baseline:  0.11610000000000001 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.351]
 [0.056]
 [0.026]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.044]
 [0.351]
 [0.056]
 [0.026]]
maxi score, test score, baseline:  0.11410000000000001 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.037]
 [0.666]
 [0.04 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.04 ]
 [0.037]
 [0.666]
 [0.04 ]]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.003]
 [0.357]
 [0.065]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.065]
 [0.003]
 [0.357]
 [0.065]]
maxi score, test score, baseline:  0.1101 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1081 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
673 18229
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1081 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1081 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.1061 0.15 0.15
674 18269
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.072]
 [0.295]
 [0.288]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.041]
 [0.072]
 [0.295]
 [0.288]]
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
674 18319
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
674 18321
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
675 18343
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.15 0.15
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.1121 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.15 0.15
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
677 18384
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.946]
 [0.434]
 [0.434]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.434]
 [0.946]
 [0.434]
 [0.434]]
maxi score, test score, baseline:  0.1201 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.15 0.15
maxi score, test score, baseline:  0.1221 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
677 18404
maxi score, test score, baseline:  0.1221 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.887]
 [0.496]
 [0.496]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.496]
 [0.887]
 [0.496]
 [0.496]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.15 0.15
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1201 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1221 0.15 0.15
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1281 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.563]
 [0.14 ]
 [0.219]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.219]
 [0.563]
 [0.14 ]
 [0.219]]
maxi score, test score, baseline:  0.1281 0.15 0.15
probs:  [1.0]
677 18503
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
677 18522
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.174]
 [0.728]
 [0.299]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.299]
 [0.174]
 [0.728]
 [0.299]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.264]
 [0.179]
 [0.155]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.144]
 [0.264]
 [0.179]
 [0.155]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
677 18538
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.15 0.15009999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.14409999999999998 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.228]
 [0.187]
 [0.194]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.133]
 [0.228]
 [0.187]
 [0.194]]
maxi score, test score, baseline:  0.14409999999999998 0.15 0.15
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
677 18575
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.15 0.15
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  678
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.15 0.15009999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.15 0.15209999999999999
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.15 0.15209999999999999
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
679 18612
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.15 0.15009999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.15 0.15209999999999999
probs:  [1.0]
maxi score, test score, baseline:  0.15009999999999998 0.15 0.15009999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.15 0.15009999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.415]
 [0.753]
 [0.753]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.753]
 [0.415]
 [0.753]
 [0.753]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.04 ]
 [0.508]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.508]
 [0.508]
 [0.04 ]
 [0.508]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
679 18645
maxi score, test score, baseline:  0.15209999999999999 0.15 0.15209999999999999
maxi score, test score, baseline:  0.15209999999999999 0.15 0.15209999999999999
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.15 0.15209999999999999
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.15 0.1541
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.15 0.1561
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.15 0.1561
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.15 0.1581
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.15 0.1621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
680 18677
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1641 0.15 0.1641
maxi score, test score, baseline:  0.1641 0.15 0.1641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [1.0]
680 18691
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.15 0.17809999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.049]
 [0.08 ]
 [0.049]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.055]
 [0.049]
 [0.08 ]
 [0.049]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.15 0.17809999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.15 0.17809999999999998
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.15 0.17809999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
probs:  [1.0]
681 18774
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.15 0.18409999999999999
probs:  [1.0]
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [1.0]
681 18825
maxi score, test score, baseline:  0.1661 0.15 0.1661
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [1.0]
maxi score, test score, baseline:  0.1641 0.15 0.1641
probs:  [1.0]
681 18867
maxi score, test score, baseline:  0.1601 0.15 0.1601
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.15 0.1581
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
682 18881
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.381]
 [0.367]
 [0.209]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.072]
 [0.381]
 [0.367]
 [0.209]]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.118]
 [0.174]
 [0.135]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.118]
 [0.174]
 [0.135]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.   ]
 [0.309]
 [0.27 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.266]
 [0.   ]
 [0.309]
 [0.27 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1301 0.15 0.15
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.15 0.15
maxi score, test score, baseline:  0.1121 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1101 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1021 0.15 0.15
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
maxi score, test score, baseline:  0.0981 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0961 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0881 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0881 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0801 0.15 0.15
probs:  [1.0]
683 19103
maxi score, test score, baseline:  0.0801 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0781 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0781 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0741 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.056]
 [0.056]
 [0.056]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.056]
 [0.056]
 [0.056]
 [0.056]]
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
maxi score, test score, baseline:  0.0601 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.211]
 [0.118]
 [0.083]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.043]
 [0.211]
 [0.118]
 [0.083]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.056100000000000004 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.199]
 [0.176]
 [0.199]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.199]
 [0.199]
 [0.176]
 [0.199]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0521 0.15 0.15
maxi score, test score, baseline:  0.0521 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0461 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.002]
 [0.102]
 [0.048]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.03 ]
 [0.002]
 [0.102]
 [0.048]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.933]
 [0.064]
 [0.063]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.102]
 [0.933]
 [0.064]
 [0.063]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.058100000000000006 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0621 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
688 19270
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.15 0.15
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.15 0.15
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.08410000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0861 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0901 0.15 0.15
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.089]
 [0.089]
 [0.089]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.081]
 [0.089]
 [0.089]
 [0.089]]
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0961 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1101 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.15 0.15
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1241 0.15 0.15
probs:  [1.0]
689 19380
maxi score, test score, baseline:  0.1221 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1221 0.15 0.15
probs:  [1.0]
689 19390
maxi score, test score, baseline:  0.1221 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
689 19392
maxi score, test score, baseline:  0.1241 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.87 ]
 [0.484]
 [0.484]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.484]
 [0.87 ]
 [0.484]
 [0.484]]
maxi score, test score, baseline:  0.1361 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.14209999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.206]
 [0.066]
 [0.066]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.066]
 [0.206]
 [0.066]
 [0.066]]
689 19443
maxi score, test score, baseline:  0.14609999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.14609999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.15 0.15
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
689 19453
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
689 19460
maxi score, test score, baseline:  0.15209999999999999 0.15 0.15209999999999999
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.845]
 [0.331]
 [0.331]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.331]
 [0.845]
 [0.331]
 [0.331]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.15 0.1581
maxi score, test score, baseline:  0.1581 0.15 0.1581
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.15 0.1581
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
690 19474
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17409999999999998 0.15 0.17409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [1.0]
maxi score, test score, baseline:  0.1901 0.15 0.1901
690 19528
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [1.0]
maxi score, test score, baseline:  0.1881 0.15 0.1881
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [1.0]
maxi score, test score, baseline:  0.1881 0.15 0.1881
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.04 ]
 [0.144]
 [0.04 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.04 ]
 [0.04 ]
 [0.144]
 [0.04 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.15 0.1941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.15 0.1961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
691 19590
maxi score, test score, baseline:  0.20609999999999998 0.15 0.20609999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
691 19597
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2181 0.15 0.2181
probs:  [1.0]
692 19608
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2201 0.15 0.2201
probs:  [1.0]
maxi score, test score, baseline:  0.2201 0.15 0.2201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.923]
 [0.186]
 [0.096]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.096]
 [0.923]
 [0.186]
 [0.096]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
692 19621
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2341 0.15 0.2341
probs:  [1.0]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.013]
 [0.168]
 [0.1  ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.085]
 [0.013]
 [0.168]
 [0.1  ]]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.082]
 [0.065]
 [0.083]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.082]
 [0.065]
 [0.083]]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.096]
 [0.032]
 [0.077]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.084]
 [0.096]
 [0.032]
 [0.077]]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.152]
 [0.183]
 [0.445]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.445]
 [0.152]
 [0.183]
 [0.445]]
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.962]
 [0.467]
 [0.467]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.467]
 [0.962]
 [0.467]
 [0.467]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2501 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2541 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2521 0.35 0.35
probs:  [1.0]
694 19678
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.35 0.35
maxi score, test score, baseline:  0.24409999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.24009999999999998 0.35 0.35
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.24009999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.24009999999999998 0.35 0.35
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23609999999999998 0.35 0.35
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.23809999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.35 0.35
maxi score, test score, baseline:  0.24009999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.395]
 [0.2  ]
 [0.164]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.515]
 [0.395]
 [0.2  ]
 [0.164]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
696 19768
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.24209999999999998 0.35 0.35
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2321 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
696 19801
maxi score, test score, baseline:  0.2321 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2261 0.35 0.35
probs:  [1.0]
696 19812
maxi score, test score, baseline:  0.2261 0.35 0.35
maxi score, test score, baseline:  0.2241 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2261 0.35 0.35
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2241 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2261 0.35 0.35
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
696 19837
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.216]
 [0.216]
 [0.216]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.216]
 [0.216]
 [0.216]
 [0.216]]
maxi score, test score, baseline:  0.2261 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2241 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
696 19847
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.35 0.35
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2221 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2201 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2221 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.424]
 [0.424]
 [0.424]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.424]
 [0.424]
 [0.424]
 [0.424]]
maxi score, test score, baseline:  0.2201 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.251]
 [0.132]
 [0.073]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.073]
 [0.251]
 [0.132]
 [0.073]]
maxi score, test score, baseline:  0.20809999999999998 0.35 0.35
maxi score, test score, baseline:  0.20409999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1981 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2021 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21009999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.21009999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20809999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21009999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.20609999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2021 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.35 0.35
probs:  [1.0]
699 20075
maxi score, test score, baseline:  0.1961 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1901 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1901 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
699 20110
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.021]
 [0.452]
 [0.148]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.069]
 [0.021]
 [0.452]
 [0.148]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
699 20126
maxi score, test score, baseline:  0.1961 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1941 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.281]
 [0.281]
 [0.281]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.281]
 [0.281]
 [0.281]
 [0.281]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
699 20139
maxi score, test score, baseline:  0.1921 0.35 0.35
probs:  [1.0]
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.096]
 [0.294]
 [0.174]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.204]
 [0.096]
 [0.294]
 [0.174]]
line 256 mcts: sample exp_bonus 0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18209999999999998 0.35 0.35
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.497]
 [0.179]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.001]
 [0.497]
 [0.179]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
699 20200
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
700 20206
700 20216
siam score:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.35 0.35
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.35 0.35
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.35 0.35
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.003]
 [0.226]
 [0.127]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.003]
 [0.226]
 [0.127]]
maxi score, test score, baseline:  0.17209999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.35 0.35
maxi score, test score, baseline:  0.1641 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.206]
 [0.19 ]
 [0.141]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.157]
 [0.206]
 [0.19 ]
 [0.141]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17409999999999998 0.35 0.35
maxi score, test score, baseline:  0.17409999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
700 20315
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.407]
 [0.154]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.407]
 [0.154]]
maxi score, test score, baseline:  0.1701 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1701 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.17209999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.35 0.35
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17409999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.554]
 [0.554]
 [0.554]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.554]
 [0.554]
 [0.554]
 [0.554]]
maxi score, test score, baseline:  0.17409999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.17209999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.17209999999999998 0.35 0.35
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.924]
 [0.3  ]
 [0.915]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.3  ]
 [0.924]
 [0.3  ]
 [0.915]]
700 20400
maxi score, test score, baseline:  0.1701 0.35 0.35
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1701 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1621 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.35 0.35
maxi score, test score, baseline:  0.1581 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.   ]
 [0.317]
 [0.272]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.272]
 [0.   ]
 [0.317]
 [0.272]]
maxi score, test score, baseline:  0.1621 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1701 0.35 0.35
maxi score, test score, baseline:  0.1701 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1701 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.35 0.35
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.726]
 [0.726]
 [0.726]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.726]
 [0.726]
 [0.726]
 [0.726]]
maxi score, test score, baseline:  0.17209999999999998 0.35 0.35
probs:  [1.0]
700 20512
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
700 20516
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1641 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.121]
 [0.121]
 [0.121]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.121]
 [0.121]
 [0.121]
 [0.121]]
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.796]
 [0.796]
 [0.796]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.796]
 [0.796]
 [0.796]
 [0.796]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
700 20556
maxi score, test score, baseline:  0.1601 0.35 0.35
700 20560
siam score:  0.0
maxi score, test score, baseline:  0.1581 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.35 0.35
maxi score, test score, baseline:  0.1561 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1541 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
700 20599
maxi score, test score, baseline:  0.15209999999999999 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.35 0.35
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.007]
 [0.215]
 [0.102]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.007]
 [0.215]
 [0.102]]
700 20613
700 20614
maxi score, test score, baseline:  0.14209999999999998 0.35 0.35
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.35 0.35
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.35 0.35
maxi score, test score, baseline:  0.15009999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
700 20659
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1601 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.35 0.35
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.586]
 [0.417]
 [0.395]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.24 ]
 [0.586]
 [0.417]
 [0.395]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.077]
 [0.47 ]
 [0.276]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.14 ]
 [0.077]
 [0.47 ]
 [0.276]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
700 20693
maxi score, test score, baseline:  0.1701 0.35 0.35
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.093]
 [0.093]
 [0.093]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.093]
 [0.093]
 [0.093]
 [0.093]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.333 0.458 0.167]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1641 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1701 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
700 20731
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
700 20756
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.717]
 [0.687]
 [0.687]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.687]
 [0.717]
 [0.687]
 [0.687]]
maxi score, test score, baseline:  0.1621 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
700 20763
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.077]
 [0.204]
 [0.117]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.043]
 [0.077]
 [0.204]
 [0.117]]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.467]
 [0.427]
 [0.427]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.427]
 [0.467]
 [0.427]
 [0.427]]
maxi score, test score, baseline:  0.1641 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
700 20769
maxi score, test score, baseline:  0.1661 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1701 0.35 0.35
probs:  [1.0]
700 20791
siam score:  0.0
maxi score, test score, baseline:  0.1661 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.35 0.35
maxi score, test score, baseline:  0.1661 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1701 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.17409999999999998 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.169]
 [0.291]
 [0.158]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.08 ]
 [0.169]
 [0.291]
 [0.158]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
700 20852
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17409999999999998 0.35 0.35
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1701 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.333 0.333 0.208 0.125]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18209999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.35 0.35
maxi score, test score, baseline:  0.18409999999999999 0.35 0.35
probs:  [1.0]
700 20901
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1901 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1901 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1901 0.35 0.35
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1921 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1881 0.35 0.35
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.18209999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.35 0.35
maxi score, test score, baseline:  0.18009999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.17809999999999998 0.35 0.35
maxi score, test score, baseline:  0.17809999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.17809999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.17609999999999998 0.35 0.35
maxi score, test score, baseline:  0.17609999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1681 0.35 0.35
probs:  [1.0]
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.1621 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1621 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1621 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1621 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.324]
 [0.324]
 [0.324]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.324]
 [0.324]
 [0.324]
 [0.324]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.495]
 [0.495]
 [0.339]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.339]
 [0.495]
 [0.495]
 [0.339]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
701 21065
maxi score, test score, baseline:  0.18009999999999998 0.35 0.35
probs:  [1.0]
701 21069
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.18009999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
701 21079
UNIT TEST: sample policy line 217 mcts : [0.083 0.5   0.25  0.167]
701 21082
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
701 21085
maxi score, test score, baseline:  0.18409999999999999 0.35 0.35
maxi score, test score, baseline:  0.18409999999999999 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.18409999999999999 0.35 0.35
maxi score, test score, baseline:  0.18409999999999999 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.18409999999999999 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.18209999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.18009999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.17809999999999998 0.35 0.35
probs:  [1.0]
701 21103
701 21105
703 21122
maxi score, test score, baseline:  0.17409999999999998 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.003]
 [0.515]
 [0.03 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.03 ]
 [0.003]
 [0.515]
 [0.03 ]]
maxi score, test score, baseline:  0.17209999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1621 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1621 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.414]
 [0.799]
 [0.799]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.799]
 [0.414]
 [0.799]
 [0.799]]
maxi score, test score, baseline:  0.1601 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.35 0.35
maxi score, test score, baseline:  0.1581 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.1541 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.14609999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.14609999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
704 21195
maxi score, test score, baseline:  0.14809999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.35 0.35
probs:  [1.0]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.209]
 [0.263]
 [0.11 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.046]
 [0.209]
 [0.263]
 [0.11 ]]
maxi score, test score, baseline:  0.1361 0.35 0.35
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.35 0.35
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.14209999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
704 21240
maxi score, test score, baseline:  0.14809999999999998 0.25 0.25
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
704 21266
maxi score, test score, baseline:  0.1541 0.25 0.25
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.25 0.25
704 21292
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.413]
 [0.608]
 [0.351]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.351]
 [0.413]
 [0.608]
 [0.351]]
maxi score, test score, baseline:  0.1401 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.939]
 [0.052]
 [0.062]
 [0.939]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.939]
 [0.052]
 [0.062]
 [0.939]]
704 21360
maxi score, test score, baseline:  0.1281 0.25 0.25
probs:  [1.0]
704 21369
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.25 0.25
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1181 0.25 0.25
probs:  [1.0]
704 21402
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.105]
 [0.124]
 [0.055]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.069]
 [0.105]
 [0.124]
 [0.055]]
704 21409
maxi score, test score, baseline:  0.11410000000000001 0.25 0.25
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1201 0.25 0.25
maxi score, test score, baseline:  0.1201 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.098]
 [0.217]
 [0.113]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.109]
 [0.098]
 [0.217]
 [0.113]]
704 21440
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.063]
 [0.188]
 [0.109]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.109]
 [0.063]
 [0.188]
 [0.109]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.402]
 [0.286]
 [0.117]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.402]
 [0.286]
 [0.117]]
maxi score, test score, baseline:  0.1321 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1341 0.25 0.25
maxi score, test score, baseline:  0.1341 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1341 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
704 21487
maxi score, test score, baseline:  0.1321 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.215]
 [0.212]
 [0.191]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.212]
 [0.215]
 [0.212]
 [0.191]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.095]
 [0.177]
 [0.095]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.095]
 [0.095]
 [0.177]
 [0.095]]
maxi score, test score, baseline:  0.1401 0.25 0.25
probs:  [1.0]
704 21524
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
704 21562
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.061]
 [0.191]
 [0.137]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.099]
 [0.061]
 [0.191]
 [0.137]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1621 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1701 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1701 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.17209999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
704 21638
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.25 0.25
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18209999999999998 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.722]
 [0.807]
 [0.722]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.722]
 [0.722]
 [0.807]
 [0.722]]
maxi score, test score, baseline:  0.17609999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
704 21710
maxi score, test score, baseline:  0.1961 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2001 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2001 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1961 0.25 0.25
maxi score, test score, baseline:  0.1961 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.337]
 [0.367]
 [0.302]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.302]
 [0.337]
 [0.367]
 [0.302]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.   ]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.   ]
 [0.003]
 [0.003]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.25 0.25
maxi score, test score, baseline:  0.1981 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2021 0.25 0.25
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.2001 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.2001 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1981 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.25 0.25
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1921 0.25 0.25
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1941 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.25 0.25
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.25 0.25
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.731]
 [1.039]
 [0.475]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.475]
 [0.731]
 [1.039]
 [0.475]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20409999999999998 0.25 0.25
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.20409999999999998 0.25 0.25
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.20409999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.202]
 [0.619]
 [0.323]
 [0.202]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.202]
 [0.619]
 [0.323]
 [0.202]]
maxi score, test score, baseline:  0.20609999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 21873
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21009999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21209999999999998 0.25 0.25
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2201 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2241 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.481]
 [0.671]
 [0.373]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.373]
 [0.481]
 [0.671]
 [0.373]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2201 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2201 0.25 0.25
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2181 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2181 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2221 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.2221 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2261 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.2281 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.556]
 [0.255]
 [0.272]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.157]
 [0.556]
 [0.255]
 [0.272]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 21989
maxi score, test score, baseline:  0.2301 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 21996
706 21997
maxi score, test score, baseline:  0.2321 0.25 0.25
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 22008
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2321 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.285]
 [0.259]
 [0.173]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.285]
 [0.259]
 [0.173]]
706 22015
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.2261 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 22032
706 22037
maxi score, test score, baseline:  0.2161 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21409999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20809999999999998 0.25 0.25
maxi score, test score, baseline:  0.20809999999999998 0.25 0.25
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.2001 0.25 0.25
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 22140
706 22141
maxi score, test score, baseline:  0.1901 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1901 0.25 0.25
maxi score, test score, baseline:  0.1901 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1901 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.25 0.25
706 22162
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.769]
 [0.776]
 [0.726]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.726]
 [0.769]
 [0.776]
 [0.726]]
maxi score, test score, baseline:  0.1881 0.25 0.25
maxi score, test score, baseline:  0.18009999999999998 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.17809999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.25 0.25
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
706 22240
UNIT TEST: sample policy line 217 mcts : [0.042 0.833 0.042 0.083]
maxi score, test score, baseline:  0.14409999999999998 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.363]
 [0.512]
 [0.184]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.363]
 [0.512]
 [0.184]]
maxi score, test score, baseline:  0.14209999999999998 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1381 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1341 0.25 0.25
probs:  [1.0]
706 22286
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.412]
 [0.284]
 [0.284]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.284]
 [0.412]
 [0.284]
 [0.284]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.542]
 [0.476]
 [0.403]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.236]
 [0.542]
 [0.476]
 [0.403]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.25 0.25
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.205]
 [0.207]
 [0.117]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.137]
 [0.205]
 [0.207]
 [0.117]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.25 0.25
maxi score, test score, baseline:  0.1381 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.122]
 [0.204]
 [0.121]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.14 ]
 [0.122]
 [0.204]
 [0.121]]
maxi score, test score, baseline:  0.1361 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1341 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1341 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.116]
 [0.261]
 [0.245]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.064]
 [0.116]
 [0.261]
 [0.245]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 22378
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1281 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1301 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 22400
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.733]
 [0.424]
 [0.428]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.428]
 [0.733]
 [0.424]
 [0.428]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.25 0.25
probs:  [1.0]
706 22425
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.25 0.25
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.372]
 [0.384]
 [0.317]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.317]
 [0.372]
 [0.384]
 [0.317]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.25 0.25
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.427]
 [0.296]
 [0.224]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.137]
 [0.427]
 [0.296]
 [0.224]]
maxi score, test score, baseline:  0.1361 0.25 0.25
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.004]
 [0.328]
 [0.099]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.086]
 [0.004]
 [0.328]
 [0.099]]
maxi score, test score, baseline:  0.1361 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.25 0.25
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.159]
 [0.855]
 [0.139]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.071]
 [0.159]
 [0.855]
 [0.139]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.454]
 [0.454]
 [0.454]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.454]
 [0.454]
 [0.454]
 [0.454]]
maxi score, test score, baseline:  0.14809999999999998 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.25 0.25
maxi score, test score, baseline:  0.1581 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 22596
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1541 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.25 0.25
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1641 0.25 0.25
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 22643
maxi score, test score, baseline:  0.1661 0.25 0.25
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1641 0.25 0.25
maxi score, test score, baseline:  0.1641 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18209999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1861 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.1861 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.25 0.25
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 22717
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20409999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 22729
maxi score, test score, baseline:  0.21209999999999998 0.25 0.25
maxi score, test score, baseline:  0.21209999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.008]
 [0.034]
 [0.072]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.468]
 [0.008]
 [0.034]
 [0.072]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2241 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.25 0.25
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.08 ]
 [0.344]
 [0.057]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.071]
 [0.08 ]
 [0.344]
 [0.057]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 22753
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.172]
 [0.156]
 [0.125]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.1  ]
 [0.172]
 [0.156]
 [0.125]]
maxi score, test score, baseline:  0.24009999999999998 0.25 0.25
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.25 0.25
probs:  [1.0]
maxi score, test score, baseline:  0.24209999999999998 0.25 0.25
probs:  [1.0]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.4196],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 0.0
0.970299 0.970299
0.970299 0.970299
0.0 0.41958648960835887
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
706 22777
maxi score, test score, baseline:  0.24209999999999998 0.25 0.25
maxi score, test score, baseline:  0.24209999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.25 0.25
probs:  [1.0]
706 22791
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.25 0.2521
probs:  [1.0]
maxi score, test score, baseline:  0.2521 0.25 0.2521
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.25 0.2521
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.25 0.2541
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 22821
maxi score, test score, baseline:  0.2521 0.25 0.2521
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.25 0.2521
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.2501 0.25 0.2501
maxi score, test score, baseline:  0.2501 0.25 0.2501
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.58 ]
 [0.216]
 [0.347]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.298]
 [0.58 ]
 [0.216]
 [0.347]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2501 0.25 0.2501
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.14 ]
 [0.301]
 [0.193]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.161]
 [0.14 ]
 [0.301]
 [0.193]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2501 0.25 0.2501
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.114]
 [0.559]
 [0.296]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.203]
 [0.114]
 [0.559]
 [0.296]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2561 0.25 0.2561
probs:  [1.0]
706 22908
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2621 0.25 0.2621
maxi score, test score, baseline:  0.2621 0.25 0.2621
probs:  [1.0]
maxi score, test score, baseline:  0.2621 0.25 0.2621
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2621 0.25 0.2621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2621 0.25 0.2621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.011]
 [0.28 ]
 [0.145]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.206]
 [0.011]
 [0.28 ]
 [0.145]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.25 0.25
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.35 0.35
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2621 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2601 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2601 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
706 23031
maxi score, test score, baseline:  0.2481 0.35 0.35
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2341 0.35 0.35
probs:  [1.0]
707 23061
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.35 0.35
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
707 23094
maxi score, test score, baseline:  0.24209999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.795]
 [0.798]
 [0.798]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.798]
 [0.795]
 [0.798]
 [0.798]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.47 ]
 [0.46 ]
 [0.243]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.253]
 [0.47 ]
 [0.46 ]
 [0.243]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.35 0.35
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2561 0.35 0.35
maxi score, test score, baseline:  0.2541 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.2521 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.35 0.35
maxi score, test score, baseline:  0.2521 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.481]
 [0.386]
 [0.367]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.283]
 [0.481]
 [0.386]
 [0.367]]
maxi score, test score, baseline:  0.2561 0.35 0.35
maxi score, test score, baseline:  0.2561 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2521 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.35 0.35
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.028]
 [0.13 ]
 [0.13 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.13 ]
 [0.028]
 [0.13 ]
 [0.13 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
707 23204
707 23205
maxi score, test score, baseline:  0.2321 0.35 0.35
707 23221
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21009999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  708
maxi score, test score, baseline:  0.21009999999999998 0.35 0.35
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21009999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
708 23231
maxi score, test score, baseline:  0.21009999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2161 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2161 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2201 0.35 0.35
probs:  [1.0]
711 23257
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2221 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
712 23268
maxi score, test score, baseline:  0.2221 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2201 0.35 0.35
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2221 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.432]
 [0.85 ]
 [0.39 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.39 ]
 [0.432]
 [0.85 ]
 [0.39 ]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
713 23361
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.35 0.35
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.286]
 [0.801]
 [0.399]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.399]
 [0.286]
 [0.801]
 [0.399]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
713 23390
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.135]
 [0.11 ]
 [0.11 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.11 ]
 [0.135]
 [0.11 ]
 [0.11 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
713 23401
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23609999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2341 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2341 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
713 23427
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.23609999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.478]
 [0.478]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.478]
 [0.478]
 [0.478]
 [0.478]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.24409999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.   ]
 [0.419]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.   ]
 [0.419]
 [0.012]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.35 0.35
probs:  [1.0]
713 23467
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23609999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.568]
 [0.756]
 [0.413]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.413]
 [0.568]
 [0.756]
 [0.413]]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.471]
 [0.468]
 [0.468]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.468]
 [0.471]
 [0.468]
 [0.468]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.35 0.35
maxi score, test score, baseline:  0.2481 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.2501 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
714 23499
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.303]
 [0.478]
 [0.336]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.243]
 [0.303]
 [0.478]
 [0.336]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2561 0.35 0.35
probs:  [1.0]
714 23511
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.513]
 [0.386]
 [0.406]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.406]
 [0.513]
 [0.386]
 [0.406]]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.717]
 [0.662]
 [0.579]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.579]
 [0.717]
 [0.662]
 [0.579]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
714 23526
maxi score, test score, baseline:  0.2661 0.35 0.35
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2641 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2641 0.35 0.35
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2661 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2701 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2721 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2681 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2681 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2681 0.35 0.35
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2681 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2641 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
714 23599
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2681 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2701 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2721 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.062]
 [0.185]
 [0.096]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.484]
 [0.062]
 [0.185]
 [0.096]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
714 23635
maxi score, test score, baseline:  0.2741 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2741 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28409999999999996 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.35 0.35
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.492]
 [0.645]
 [0.608]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.515]
 [0.492]
 [0.645]
 [0.608]]
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.271]
 [0.271]
 [0.271]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.271]
 [0.271]
 [0.271]
 [0.271]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2941 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.35 0.35
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.342]
 [0.361]
 [0.209]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.18 ]
 [0.342]
 [0.361]
 [0.209]]
714 23696
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.35 0.35
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.3201 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.3201 0.35 0.35
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.248]
 [0.248]
 [0.248]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.248]
 [0.248]
 [0.248]
 [0.248]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.741]
 [0.612]
 [0.612]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.612]
 [0.741]
 [0.612]
 [0.612]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.35 0.35
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.35 0.35
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.3201 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.3201 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.3221 0.35 0.35
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.35 0.35
maxi score, test score, baseline:  0.3181 0.35 0.35
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
716 23854
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3161 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
716 23907
maxi score, test score, baseline:  0.3141 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.586]
 [0.668]
 [0.668]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.668]
 [0.586]
 [0.668]
 [0.668]]
maxi score, test score, baseline:  0.3161 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.27 ]
 [0.216]
 [0.041]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.041]
 [0.27 ]
 [0.216]
 [0.041]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.39 ]
 [0.278]
 [0.278]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.278]
 [0.39 ]
 [0.278]
 [0.278]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.35 0.35
maxi score, test score, baseline:  0.3261 0.35 0.35
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.3121 0.35 0.35
maxi score, test score, baseline:  0.3041 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.3021 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2981 0.35 0.35
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.3001 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.29009999999999997 0.35 0.35
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.222]
 [0.073]
 [0.222]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.222]
 [0.222]
 [0.073]
 [0.222]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.28609999999999997 0.35 0.35
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.554]
 [0.436]
 [0.436]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.436]
 [0.554]
 [0.436]
 [0.436]]
maxi score, test score, baseline:  0.28409999999999996 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
719 24035
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28809999999999997 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
719 24063
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.29209999999999997 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
719 24090
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28609999999999997 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2801 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2561 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.588]
 [0.433]
 [0.273]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.366]
 [0.588]
 [0.433]
 [0.273]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
722 24150
maxi score, test score, baseline:  0.2521 0.35 0.35
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.35 0.35
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.023]
 [0.466]
 [0.336]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.222]
 [0.023]
 [0.466]
 [0.336]]
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.35 0.35
maxi score, test score, baseline:  0.24409999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.24409999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.299]
 [0.412]
 [0.315]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.676]
 [0.299]
 [0.412]
 [0.315]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
722 24203
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.35 0.35
maxi score, test score, baseline:  0.2481 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.24209999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2501 0.35 0.35
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.834]
 [0.587]
 [0.587]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.587]
 [0.834]
 [0.587]
 [0.587]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
726 24255
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.705]
 [0.529]
 [0.529]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.529]
 [0.705]
 [0.529]
 [0.529]]
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.314]
 [0.426]
 [0.315]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.344]
 [0.314]
 [0.426]
 [0.315]]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.013]
 [0.534]
 [0.534]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.534]
 [0.013]
 [0.534]
 [0.534]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
726 24283
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
726 24299
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2481 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.35 0.35
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.935]
 [0.935]
 [0.935]
 [0.935]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.935]
 [0.935]
 [0.935]
 [0.935]]
maxi score, test score, baseline:  0.2521 0.35 0.35
maxi score, test score, baseline:  0.2521 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.296]
 [0.605]
 [0.449]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.449]
 [0.296]
 [0.605]
 [0.449]]
maxi score, test score, baseline:  0.2521 0.35 0.35
probs:  [1.0]
726 24315
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.125 0.833 0.   ]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2621 0.35 0.35
probs:  [1.0]
726 24329
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
726 24339
726 24340
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.516]
 [0.508]
 [0.508]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.508]
 [0.516]
 [0.508]
 [0.508]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2801 0.35 0.35
probs:  [1.0]
maxi score, test score, baseline:  0.2781 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28209999999999996 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
maxi score, test score, baseline:  0.28209999999999996 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.631]
 [0.631]
 [0.631]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.631]
 [0.631]
 [0.631]
 [0.631]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2801 0.35 0.35
maxi score, test score, baseline:  0.2801 0.35 0.35
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.2961 0.7 0.7
probs:  [1.0]
728 24371
maxi score, test score, baseline:  0.2941 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.29209999999999997 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28809999999999997 0.7 0.7
maxi score, test score, baseline:  0.28809999999999997 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28409999999999996 0.7 0.7
maxi score, test score, baseline:  0.28409999999999996 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28409999999999996 0.7 0.7
maxi score, test score, baseline:  0.2761 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.472]
 [0.447]
 [0.447]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.447]
 [0.472]
 [0.447]
 [0.447]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.77 ]
 [0.454]
 [0.287]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.287]
 [0.77 ]
 [0.454]
 [0.287]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2801 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28409999999999996 0.7 0.7
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28809999999999997 0.7 0.7
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.28809999999999997 0.7 0.7
probs:  [1.0]
728 24452
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2941 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.29209999999999997 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2941 0.7 0.7
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.7 0.7
maxi score, test score, baseline:  0.28609999999999997 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28409999999999996 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.29009999999999997 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
730 24543
maxi score, test score, baseline:  0.3001 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.7 0.7
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.2941 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.13]
 [0.13]
 [0.13]
 [0.13]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.13]
 [0.13]
 [0.13]
 [0.13]]
maxi score, test score, baseline:  0.2941 0.7 0.7
maxi score, test score, baseline:  0.29209999999999997 0.7 0.7
maxi score, test score, baseline:  0.29209999999999997 0.7 0.7
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.465]
 [0.7  ]
 [0.502]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.502]
 [0.465]
 [0.7  ]
 [0.502]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2961 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2961 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28809999999999997 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.28409999999999996 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2801 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2721 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2721 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2661 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2661 0.7 0.7
maxi score, test score, baseline:  0.2661 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.313]
 [0.332]
 [0.332]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.332]
 [0.313]
 [0.332]
 [0.332]]
maxi score, test score, baseline:  0.2621 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.381]
 [0.381]
 [0.381]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.381]
 [0.381]
 [0.381]
 [0.381]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.42 ]
 [0.363]
 [0.342]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.42 ]
 [0.363]
 [0.342]]
maxi score, test score, baseline:  0.2581 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2561 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2601 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2621 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2601 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2521 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.94 ]
 [0.321]
 [0.321]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.321]
 [0.94 ]
 [0.321]
 [0.321]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.7 0.7
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
733 24792
maxi score, test score, baseline:  0.2181 0.7 0.7
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2181 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2181 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.619]
 [0.598]
 [0.598]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.598]
 [0.619]
 [0.598]
 [0.598]]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.114]
 [0.329]
 [0.096]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.096]
 [0.114]
 [0.329]
 [0.096]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21009999999999998 0.7 0.7
maxi score, test score, baseline:  0.21009999999999998 0.7 0.7
maxi score, test score, baseline:  0.21009999999999998 0.7 0.7
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.18009999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1881 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1901 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.7 0.7
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.327]
 [0.352]
 [0.264]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.264]
 [0.327]
 [0.352]
 [0.264]]
734 24872
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1921 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1901 0.7 0.7
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1881 0.7 0.7
probs:  [1.0]
734 24892
maxi score, test score, baseline:  0.1881 0.7 0.7
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1881 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
734 24910
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
734 24916
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.005]
 [0.318]
 [0.273]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.273]
 [0.005]
 [0.318]
 [0.273]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.7 0.7
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1921 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.7 0.7
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.7 0.7
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2001 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2001 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2001 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.659]
 [0.61 ]
 [0.643]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.643]
 [0.659]
 [0.61 ]
 [0.643]]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.95 ]
 [0.453]
 [0.453]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.453]
 [0.95 ]
 [0.453]
 [0.453]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20809999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21009999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21409999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.21009999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
736 25017
736 25027
siam score:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.18009999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.639]
 [1.052]
 [0.639]
 [0.639]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.639]
 [1.052]
 [0.639]
 [0.639]]
737 25061
maxi score, test score, baseline:  0.17609999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.002]
 [0.335]
 [0.234]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.234]
 [0.002]
 [0.335]
 [0.234]]
maxi score, test score, baseline:  0.17809999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18209999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.7 0.7
siam score:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.17809999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.614]
 [0.614]
 [0.614]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.614]
 [0.614]
 [0.614]
 [0.614]]
maxi score, test score, baseline:  0.18209999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.18009999999999998 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.141]
 [0.07 ]
 [0.07 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.07 ]
 [0.141]
 [0.07 ]
 [0.07 ]]
maxi score, test score, baseline:  0.17809999999999998 0.7 0.7
maxi score, test score, baseline:  0.17809999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.17809999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.17409999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1701 0.7 0.7
probs:  [1.0]
739 25189
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1681 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.7 0.7
probs:  [1.0]
739 25206
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.7 0.7
probs:  [1.0]
siam score:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.1561 0.7 0.7
probs:  [1.0]
739 25236
maxi score, test score, baseline:  0.1541 0.7 0.7
probs:  [1.0]
739 25240
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1541 0.7 0.7
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.042 0.292 0.542 0.125]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.7 0.7
probs:  [1.0]
740 25313
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.7 0.7
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.276]
 [0.231]
 [0.231]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.35 ]
 [0.276]
 [0.231]
 [0.231]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.7 0.7
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1541 0.7 0.7
maxi score, test score, baseline:  0.15009999999999998 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [0.509]
 [0.509]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.509]
 [0.509]
 [0.509]
 [0.509]]
maxi score, test score, baseline:  0.14609999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.7 0.7
741 25366
maxi score, test score, baseline:  0.1361 0.7 0.7
maxi score, test score, baseline:  0.1361 0.7 0.7
probs:  [1.0]
741 25370
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1281 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.11610000000000001 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
741 25466
maxi score, test score, baseline:  0.11610000000000001 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1201 0.7 0.7
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
742 25490
maxi score, test score, baseline:  0.1221 0.7 0.7
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1241 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1241 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.435]
 [0.613]
 [0.543]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.547]
 [0.435]
 [0.613]
 [0.543]]
maxi score, test score, baseline:  0.1281 0.7 0.7
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1281 0.7 0.7
probs:  [1.0]
743 25526
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
743 25533
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
743 25539
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.7 0.7
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.042 0.417 0.458 0.083]
maxi score, test score, baseline:  0.1341 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.001]
 [0.485]
 [0.37 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.29 ]
 [0.001]
 [0.485]
 [0.37 ]]
maxi score, test score, baseline:  0.1321 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.7 0.7
maxi score, test score, baseline:  0.1341 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.7 0.7
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1401 0.7 0.7
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.7 0.7
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.061]
 [0.93 ]
 [0.306]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.198]
 [0.061]
 [0.93 ]
 [0.306]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.7 0.7
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.7 0.7
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.402]
 [0.565]
 [0.574]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.574]
 [0.402]
 [0.565]
 [0.574]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1641 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.7 0.7
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.1661 0.7 0.7
siam score:  0.0
maxi score, test score, baseline:  0.1661 0.7 0.7
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.17209999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.17209999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.17209999999999998 0.7 0.7
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.672]
 [0.377]
 [0.262]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.262]
 [0.672]
 [0.377]
 [0.262]]
743 25705
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.17209999999999998 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.696]
 [0.696]
 [0.696]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.696]
 [0.696]
 [0.696]
 [0.696]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17409999999999998 0.7 0.7
maxi score, test score, baseline:  0.1701 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1701 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.7 0.7
maxi score, test score, baseline:  0.1681 0.7 0.7
probs:  [1.0]
745 25727
maxi score, test score, baseline:  0.1681 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1701 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
747 25733
maxi score, test score, baseline:  0.17209999999999998 0.7 0.7
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17409999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.038]
 [0.45 ]
 [0.251]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.448]
 [0.038]
 [0.45 ]
 [0.251]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.992]
 [0.992]
 [0.992]
 [0.992]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.992]
 [0.992]
 [0.992]
 [0.992]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.225]
 [0.583]
 [0.508]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.277]
 [0.225]
 [0.583]
 [0.508]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18209999999999998 0.7 0.7
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1861 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1881 0.7 0.7
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.7 0.7
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.7 0.7
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.7 0.7
maxi score, test score, baseline:  0.1961 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
748 25803
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1921 0.7 0.7
probs:  [1.0]
748 25818
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2021 0.7 0.7
probs:  [1.0]
Starting evaluation
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.129]
 [0.475]
 [0.339]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.426]
 [0.129]
 [0.475]
 [0.339]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.592]
 [0.459]
 [0.459]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.459]
 [0.592]
 [0.459]
 [0.459]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2561 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
750 25879
maxi score, test score, baseline:  0.2561 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
750 25888
maxi score, test score, baseline:  0.2561 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.2561 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.659]
 [0.65 ]
 [0.65 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.65 ]
 [0.659]
 [0.65 ]
 [0.65 ]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
750 25926
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.973]
 [0.   ]
 [0.365]
 [0.204]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.973]
 [0.   ]
 [0.365]
 [0.204]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
750 25935
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
750 25939
maxi score, test score, baseline:  0.2601 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.2581 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.2581 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
750 25974
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
750 25980
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2601 0.9 0.9
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2641 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2601 0.9 0.9
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2621 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2641 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.2701 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2721 0.9 0.9
siam score:  0.0
maxi score, test score, baseline:  0.2701 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.2681 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
750 26063
750 26066
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2661 0.9 0.9
probs:  [1.0]
750 26081
maxi score, test score, baseline:  0.2601 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.2541 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.2481 0.9 0.9
probs:  [1.0]
750 26114
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.85 ]
 [0.319]
 [0.319]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.319]
 [0.85 ]
 [0.319]
 [0.319]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.25  0.667 0.042]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.637]
 [0.246]
 [0.283]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.464]
 [0.637]
 [0.246]
 [0.283]]
maxi score, test score, baseline:  0.24609999999999999 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2501 0.9 0.9
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.961]
 [0.963]
 [0.961]
 [0.961]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.961]
 [0.963]
 [0.961]
 [0.961]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2641 0.9 0.9
maxi score, test score, baseline:  0.2641 0.9 0.9
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.596]
 [0.596]
 [0.596]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.596]
 [0.596]
 [0.596]
 [0.596]]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.515]
 [0.674]
 [0.526]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.305]
 [0.515]
 [0.674]
 [0.526]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.006]
 [0.006]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2661 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.2661 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2681 0.9 0.9
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.97 ]
 [0.493]
 [0.493]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.493]
 [0.97 ]
 [0.493]
 [0.493]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2721 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
750 26197
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2741 0.9 0.9
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2681 0.9 0.9
maxi score, test score, baseline:  0.2681 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.279]
 [0.279]
 [0.279]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.279]
 [0.279]
 [0.279]
 [0.279]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.2721 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2761 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.2761 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2761 0.9 0.9
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2781 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
750 26244
maxi score, test score, baseline:  0.2781 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
750 26250
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2761 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
750 26265
maxi score, test score, baseline:  0.2681 0.9 0.9
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.2601 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23609999999999998 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.2301 0.9 0.9
maxi score, test score, baseline:  0.2301 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2221 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2181 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.21409999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
751 26344
maxi score, test score, baseline:  0.21209999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.667]
 [0.516]
 [0.728]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.728]
 [0.667]
 [0.516]
 [0.728]]
maxi score, test score, baseline:  0.20609999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20809999999999998 0.9 0.9
probs:  [1.0]
751 26395
maxi score, test score, baseline:  0.1961 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1901 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1901 0.9 0.9
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.583]
 [0.59 ]
 [0.59 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.59 ]
 [0.583]
 [0.59 ]
 [0.59 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
753 26424
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.9 0.9
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
753 26487
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18209999999999998 0.9 0.9
probs:  [1.0]
753 26516
maxi score, test score, baseline:  0.17809999999999998 0.9 0.9
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.042]
 [0.441]
 [0.401]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.401]
 [0.042]
 [0.441]
 [0.401]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1701 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1681 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
753 26547
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1701 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17409999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17409999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.17809999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
755 26590
maxi score, test score, baseline:  0.17809999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.18009999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.006]
 [0.643]
 [0.466]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.466]
 [0.006]
 [0.643]
 [0.466]]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.378]
 [0.302]
 [0.259]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.058]
 [0.378]
 [0.302]
 [0.259]]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.403]
 [0.488]
 [0.403]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.403]
 [0.403]
 [0.488]
 [0.403]]
maxi score, test score, baseline:  0.17409999999999998 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.17209999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.792 0.125 0.042]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.9 0.9
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
755 26646
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.422]
 [0.422]
 [0.422]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.422]
 [0.422]
 [0.422]
 [0.422]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
755 26707
siam score:  0.0
maxi score, test score, baseline:  0.1541 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.9 0.9
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.9 0.9
probs:  [1.0]
755 26730
maxi score, test score, baseline:  0.14409999999999998 0.9 0.9
probs:  [1.0]
755 26745
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.518]
 [0.615]
 [0.442]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.442]
 [0.518]
 [0.615]
 [0.442]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1301 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.9 0.9
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.9 0.9
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.594]
 [0.546]
 [0.594]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.594]
 [0.594]
 [0.546]
 [0.594]]
maxi score, test score, baseline:  0.1281 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1281 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1281 0.9 0.9
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
756 26863
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1201 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.417 0.25  0.292]
maxi score, test score, baseline:  0.1221 0.9 0.9
UNIT TEST: sample policy line 217 mcts : [0.083 0.833 0.083 0.   ]
maxi score, test score, baseline:  0.1221 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1221 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1221 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1201 0.9 0.9
maxi score, test score, baseline:  0.1201 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.464]
 [0.523]
 [0.425]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.272]
 [0.464]
 [0.523]
 [0.425]]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.381]
 [0.332]
 [0.381]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.381]
 [0.381]
 [0.332]
 [0.381]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1321 0.9 0.9
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
756 26969
maxi score, test score, baseline:  0.1341 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.9 0.9
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.9 0.9
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.3  ]
 [0.033]
 [0.033]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.033]
 [0.3  ]
 [0.033]
 [0.033]]
maxi score, test score, baseline:  0.14209999999999998 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.14209999999999998 0.9 0.9
probs:  [1.0]
757 27019
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.429]
 [0.442]
 [0.423]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.423]
 [0.429]
 [0.442]
 [0.423]]
maxi score, test score, baseline:  0.1401 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.9 0.9
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.9 0.9
maxi score, test score, baseline:  0.15009999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.9 0.9
maxi score, test score, baseline:  0.14809999999999998 0.9 0.9
probs:  [1.0]
757 27098
siam score:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.9 0.9
maxi score, test score, baseline:  0.14409999999999998 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.14409999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.9 0.9
maxi score, test score, baseline:  0.14409999999999998 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.14409999999999998 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1401 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.9 0.9
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.472]
 [0.439]
 [0.439]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.439]
 [0.472]
 [0.439]
 [0.439]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
757 27139
maxi score, test score, baseline:  0.14409999999999998 0.9 0.9
757 27140
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.14409999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.9 0.9
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.128]
 [0.23 ]
 [0.118]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.156]
 [0.128]
 [0.23 ]
 [0.118]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.9 0.9
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.9 0.9
probs:  [1.0]
757 27235
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1541 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
757 27256
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1641 0.9 0.9
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1681 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1641 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1641 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1641 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1621 0.9 0.9
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.1621 0.9 0.9
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.608]
 [0.03 ]
 [0.13 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.514]
 [0.608]
 [0.03 ]
 [0.13 ]]
757 27336
maxi score, test score, baseline:  0.1601 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1601 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.9 0.9
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.042 0.542 0.333 0.083]
maxi score, test score, baseline:  0.1541 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.15009999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.012]
 [0.135]
 [0.277]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.277]
 [0.012]
 [0.135]
 [0.277]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.9 0.9
maxi score, test score, baseline:  0.15209999999999999 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.15209999999999999 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1601 0.9 0.9
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.511]
 [0.425]
 [0.425]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.425]
 [0.511]
 [0.425]
 [0.425]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
757 27432
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.9 0.9
probs:  [1.0]
757 27436
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.9 0.9
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.1561 0.9 0.9
maxi score, test score, baseline:  0.1561 0.9 0.9
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.9 0.9
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.9 0.9
probs:  [1.0]
757 27473
maxi score, test score, baseline:  0.14409999999999998 0.9 0.9
probs:  [1.0]
maxi score, test score, baseline:  0.14209999999999998 0.9 0.9
probs:  [1.0]
757 27486
Starting evaluation
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.107]
 [0.139]
 [0.103]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.136]
 [0.107]
 [0.139]
 [0.103]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.106]
 [0.135]
 [0.122]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.118]
 [0.106]
 [0.135]
 [0.122]]
maxi score, test score, baseline:  0.1361 0.9 0.9
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.385]
 [0.385]
 [0.385]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.385]
 [0.385]
 [0.385]
 [0.385]]
758 27505
maxi score, test score, baseline:  0.1381 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
758 27543
maxi score, test score, baseline:  0.1241 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1181 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.11610000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1181 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.022]
 [0.594]
 [0.54 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.375]
 [0.022]
 [0.594]
 [0.54 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1221 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1221 0.15 0.15
maxi score, test score, baseline:  0.1221 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.11410000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.11410000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1021 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.10010000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
759 27692
maxi score, test score, baseline:  0.0821 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0801 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0801 0.15 0.15
siam score:  0.0
maxi score, test score, baseline:  0.0801 0.15 0.15
probs:  [1.0]
759 27712
759 27728
Printing some Q and Qe and total Qs values:  [[0.45]
 [0.45]
 [0.45]
 [0.45]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.45]
 [0.45]
 [0.45]
 [0.45]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.9449],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.9365],
        [0.0000],
        [0.0000],
        [0.8952],
        [0.9081],
        [0.0000]], dtype=torch.float64)
0.0 0.9449269031454135
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.9365376902395643
0.9509900498999999 0.9509900498999999
0.0 0.0
0.0 0.8952377305382155
0.0 0.9080611748689327
0.970299 0.970299
siam score:  0.0
maxi score, test score, baseline:  0.0601 0.15 0.15
probs:  [1.0]
759 27789
maxi score, test score, baseline:  0.0601 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0601 0.15 0.15
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0621 0.15 0.15
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.4  ]
 [0.416]
 [0.375]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.448]
 [0.4  ]
 [0.416]
 [0.375]]
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.179]
 [0.249]
 [0.237]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.236]
 [0.179]
 [0.249]
 [0.237]]
maxi score, test score, baseline:  0.0641 0.15 0.15
maxi score, test score, baseline:  0.0641 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0781 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0861 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0821 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.08410000000000001 0.15 0.15
maxi score, test score, baseline:  0.08410000000000001 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.659]
 [0.532]
 [0.532]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.532]
 [0.659]
 [0.532]
 [0.532]]
maxi score, test score, baseline:  0.0761 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0781 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0781 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.445]
 [0.445]
 [0.445]
 [0.445]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
760 27934
maxi score, test score, baseline:  0.0721 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.073]
 [0.182]
 [0.166]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.073]
 [0.182]
 [0.166]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0741 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0801 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0801 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
761 27972
761 27974
maxi score, test score, baseline:  0.0821 0.15 0.15
probs:  [1.0]
761 27982
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.652]
 [0.652]
 [0.652]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.652]
 [0.652]
 [0.652]
 [0.652]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.204]
 [0.204]
 [0.204]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.204]
 [0.204]
 [0.204]
 [0.204]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0821 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0801 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0801 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.246]
 [0.406]
 [0.274]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.251]
 [0.246]
 [0.406]
 [0.274]]
761 28009
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0781 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.416]
 [0.274]
 [0.396]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.329]
 [0.416]
 [0.274]
 [0.396]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0801 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.196]
 [0.4  ]
 [0.345]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.35 ]
 [0.196]
 [0.4  ]
 [0.345]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.0861 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0861 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.259]
 [0.464]
 [0.541]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.51 ]
 [0.259]
 [0.464]
 [0.541]]
maxi score, test score, baseline:  0.0901 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0901 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0881 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0901 0.15 0.15
probs:  [1.0]
767 28067
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0901 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0901 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.0941 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.615]
 [0.615]
 [0.615]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.615]
 [0.615]
 [0.615]
 [0.615]]
maxi score, test score, baseline:  0.0961 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1041 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1061 0.15 0.15
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.1061 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1081 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1061 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1081 0.15 0.15
maxi score, test score, baseline:  0.1081 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.001]
 [0.15 ]
 [0.182]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.41 ]
 [0.001]
 [0.15 ]
 [0.182]]
maxi score, test score, baseline:  0.1101 0.15 0.15
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
768 28167
768 28168
maxi score, test score, baseline:  0.1121 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1121 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11610000000000001 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1181 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.857]
 [0.72 ]
 [0.72 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.72 ]
 [0.857]
 [0.72 ]
 [0.72 ]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1261 0.15 0.15
maxi score, test score, baseline:  0.1261 0.15 0.15
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1301 0.15 0.15
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1341 0.15 0.15
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.164]
 [0.259]
 [0.28 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.077]
 [0.164]
 [0.259]
 [0.28 ]]
769 28239
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.295]
 [0.481]
 [0.289]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.289]
 [0.295]
 [0.481]
 [0.289]]
maxi score, test score, baseline:  0.1341 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1361 0.15 0.15
maxi score, test score, baseline:  0.1361 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1361 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1381 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1381 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.15 0.15
maxi score, test score, baseline:  0.1401 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1381 0.15 0.15
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.15 0.15
maxi score, test score, baseline:  0.1381 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1341 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
769 28340
maxi score, test score, baseline:  0.1321 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.604]
 [0.637]
 [0.632]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.578]
 [0.604]
 [0.637]
 [0.632]]
maxi score, test score, baseline:  0.1341 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.15 0.15
maxi score, test score, baseline:  0.1321 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1321 0.15 0.15
maxi score, test score, baseline:  0.1321 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1401 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1381 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1381 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1381 0.15 0.15
probs:  [1.0]
769 28388
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
769 28397
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.57]
 [0.57]
 [0.57]
 [0.57]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.57]
 [0.57]
 [0.57]
 [0.57]]
maxi score, test score, baseline:  0.14209999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
769 28411
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.15 0.15009999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.15 0.15009999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.333]
 [0.884]
 [0.275]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.275]
 [0.333]
 [0.884]
 [0.275]]
maxi score, test score, baseline:  0.15209999999999999 0.15 0.15209999999999999
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
770 28421
maxi score, test score, baseline:  0.1541 0.15 0.1541
probs:  [1.0]
maxi score, test score, baseline:  0.1541 0.15 0.1541
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.15 0.1561
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
771 28436
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.15 0.1601
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.352]
 [0.74 ]
 [0.409]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.409]
 [0.352]
 [0.74 ]
 [0.409]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.15 0.1601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
771 28444
maxi score, test score, baseline:  0.1621 0.15 0.1621
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [1.0]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1701 0.15 0.1701
probs:  [1.0]
maxi score, test score, baseline:  0.1701 0.15 0.1701
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.15 0.1661
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [1.0]
772 28499
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.473]
 [0.547]
 [0.547]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.547]
 [0.473]
 [0.547]
 [0.547]]
maxi score, test score, baseline:  0.1661 0.15 0.1661
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.207]
 [0.271]
 [0.181]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.197]
 [0.207]
 [0.271]
 [0.181]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
772 28541
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.355]
 [0.423]
 [0.319]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.225]
 [0.355]
 [0.423]
 [0.319]]
maxi score, test score, baseline:  0.15209999999999999 0.15 0.15209999999999999
probs:  [1.0]
maxi score, test score, baseline:  0.15009999999999998 0.15 0.15009999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.14609999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.14409999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
772 28592
maxi score, test score, baseline:  0.14609999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.583]
 [0.46 ]
 [0.46 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.46 ]
 [0.583]
 [0.46 ]
 [0.46 ]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15009999999999998 0.15 0.15009999999999998
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.197]
 [0.362]
 [0.366]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.208]
 [0.197]
 [0.362]
 [0.366]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
772 28615
772 28617
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
772 28636
maxi score, test score, baseline:  0.14409999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.459]
 [0.459]
 [0.459]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.459]
 [0.459]
 [0.459]
 [0.459]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.14409999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.14209999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.15 0.15
probs:  [1.0]
maxi score, test score, baseline:  0.1401 0.15 0.15
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14209999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14409999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.15 0.1541
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.139]
 [0.199]
 [0.143]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.106]
 [0.139]
 [0.199]
 [0.143]]
maxi score, test score, baseline:  0.1561 0.15 0.1561
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
774 28728
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.249]
 [0.791]
 [0.791]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.791]
 [0.249]
 [0.791]
 [0.791]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.15 0.1561
probs:  [1.0]
774 28750
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.15 0.1601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1701 0.15 0.1701
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.15 0.18409999999999999
probs:  [1.0]
maxi score, test score, baseline:  0.18409999999999999 0.15 0.18409999999999999
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.15 0.1941
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.184]
 [0.184]
 [0.184]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.184]
 [0.184]
 [0.184]
 [0.184]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.15 0.1941
probs:  [1.0]
maxi score, test score, baseline:  0.1941 0.15 0.1941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.15 0.1961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.15 0.1981
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.15 0.1981
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.1941 0.15 0.1941
probs:  [1.0]
775 28882
siam score:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.15 0.18409999999999999
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.45 ]
 [0.252]
 [0.252]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.252]
 [0.45 ]
 [0.252]
 [0.252]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17409999999999998 0.15 0.17409999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.422]
 [0.325]
 [0.325]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.102]
 [0.422]
 [0.325]
 [0.325]]
maxi score, test score, baseline:  0.1701 0.15 0.1701
probs:  [1.0]
maxi score, test score, baseline:  0.1701 0.15 0.1701
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1641 0.15 0.1641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.15 0.1541
probs:  [1.0]
maxi score, test score, baseline:  0.15209999999999999 0.15 0.15209999999999999
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.009]
 [0.39 ]
 [0.286]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.286]
 [0.009]
 [0.39 ]
 [0.286]]
maxi score, test score, baseline:  0.14809999999999998 0.15 0.15
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.396]
 [0.441]
 [0.441]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.441]
 [0.396]
 [0.441]
 [0.441]]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.305]
 [0.59 ]
 [0.534]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.577]
 [0.305]
 [0.59 ]
 [0.534]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.15 0.15
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.15 0.15
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.15209999999999999 0.15 0.15209999999999999
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
775 29000
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.15 0.1581
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.15 0.1561
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
775 29062
maxi score, test score, baseline:  0.18409999999999999 0.15 0.18409999999999999
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1881 0.15 0.1881
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.206]
 [0.434]
 [0.17 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.247]
 [0.206]
 [0.434]
 [0.17 ]]
maxi score, test score, baseline:  0.1881 0.15 0.1881
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.476]
 [0.401]
 [0.401]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.401]
 [0.476]
 [0.401]
 [0.401]]
Starting evaluation
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1881 0.15 0.1881
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.735]
 [0.625]
 [0.56 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.56 ]
 [0.735]
 [0.625]
 [0.56 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21209999999999998 0.15 0.21209999999999998
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21409999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2221 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2221 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.018]
 [0.458]
 [0.33 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.048]
 [0.018]
 [0.458]
 [0.33 ]]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.42 ]
 [0.348]
 [0.355]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.358]
 [0.42 ]
 [0.348]
 [0.355]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.7 0.7
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.042 0.75  0.042 0.167]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
780 29125
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2281 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2261 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.014]
 [0.382]
 [0.124]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.155]
 [0.014]
 [0.382]
 [0.124]]
maxi score, test score, baseline:  0.2301 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2341 0.7 0.7
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.974]
 [0.396]
 [0.396]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.396]
 [0.974]
 [0.396]
 [0.396]]
maxi score, test score, baseline:  0.24009999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
781 29183
maxi score, test score, baseline:  0.2481 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2481 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.7 0.7
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.7 0.7
probs:  [1.0]
781 29206
maxi score, test score, baseline:  0.24609999999999999 0.7 0.7
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.24409999999999998 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.338]
 [0.429]
 [0.429]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.429]
 [0.338]
 [0.429]
 [0.429]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
782 29237
782 29238
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.23809999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.7 0.7
maxi score, test score, baseline:  0.24009999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.125 0.333 0.208 0.333]
maxi score, test score, baseline:  0.2501 0.7 0.7
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
782 29300
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.239]
 [0.354]
 [0.223]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.408]
 [0.239]
 [0.354]
 [0.223]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2541 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2541 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.2541 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2561 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2561 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2621 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.167]
 [0.234]
 [0.506]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.506]
 [0.167]
 [0.234]
 [0.506]]
782 29337
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2681 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2681 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2701 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2701 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2701 0.7 0.7
maxi score, test score, baseline:  0.2681 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.557]
 [0.647]
 [0.629]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.444]
 [0.557]
 [0.647]
 [0.629]]
maxi score, test score, baseline:  0.2641 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2661 0.7 0.7
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2681 0.7 0.7
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2701 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.924]
 [0.924]
 [0.924]
 [0.924]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.924]
 [0.924]
 [0.924]
 [0.924]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
782 29428
maxi score, test score, baseline:  0.2721 0.7 0.7
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
siam score:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
782 29437
maxi score, test score, baseline:  0.2781 0.7 0.7
probs:  [1.0]
782 29455
maxi score, test score, baseline:  0.2741 0.7 0.7
maxi score, test score, baseline:  0.2741 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.505]
 [0.433]
 [0.285]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.269]
 [0.505]
 [0.433]
 [0.285]]
siam score:  0.0
782 29463
maxi score, test score, baseline:  0.2701 0.7 0.7
maxi score, test score, baseline:  0.2681 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
782 29473
maxi score, test score, baseline:  0.2661 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2621 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2621 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2601 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2521 0.7 0.7
probs:  [1.0]
783 29515
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.581]
 [0.581]
 [0.581]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.581]
 [0.581]
 [0.581]
 [0.581]]
maxi score, test score, baseline:  0.24209999999999998 0.7 0.7
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.7 0.7
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.23609999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23609999999999998 0.7 0.7
maxi score, test score, baseline:  0.23609999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.21009999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.20409999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20609999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1881 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1861 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.003]
 [0.314]
 [0.348]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.307]
 [0.003]
 [0.314]
 [0.348]]
maxi score, test score, baseline:  0.1881 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.18409999999999999 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18209999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.18209999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.7 0.7
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1861 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
785 29664
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18209999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.44 ]
 [0.496]
 [0.44 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.44 ]
 [0.44 ]
 [0.496]
 [0.44 ]]
UNIT TEST: sample policy line 217 mcts : [0.208 0.583 0.083 0.125]
maxi score, test score, baseline:  0.17609999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.17209999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17209999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17809999999999998 0.7 0.7
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
785 29710
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.663]
 [0.214]
 [0.214]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.214]
 [0.663]
 [0.214]
 [0.214]]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.678]
 [0.527]
 [0.461]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.403]
 [0.678]
 [0.527]
 [0.461]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.18409999999999999 0.7 0.7
maxi score, test score, baseline:  0.18009999999999998 0.7 0.7
maxi score, test score, baseline:  0.18009999999999998 0.7 0.7
probs:  [1.0]
786 29746
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.638]
 [0.638]
 [0.638]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.638]
 [0.638]
 [0.638]
 [0.638]]
maxi score, test score, baseline:  0.1701 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1701 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1601 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1541 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.14209999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1381 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.924]
 [0.593]
 [0.593]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.593]
 [0.924]
 [0.593]
 [0.593]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1401 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
788 29793
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
788 29803
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.14609999999999998 0.7 0.7
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.042 0.875 0.042 0.042]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14809999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.14809999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.14609999999999998 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.211]
 [0.288]
 [0.276]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.346]
 [0.211]
 [0.288]
 [0.276]]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1541 0.7 0.7
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1621 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.435]
 [0.394]
 [0.525]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.369]
 [0.435]
 [0.394]
 [0.525]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.17609999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1861 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.7 0.7
maxi score, test score, baseline:  0.1941 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1941 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1961 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.1901 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1941 0.7 0.7
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1961 0.7 0.7
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1981 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
794 29949
maxi score, test score, baseline:  0.21009999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.21409999999999998 0.7 0.7
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
794 29955
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2241 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2301 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2321 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2321 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.23809999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23809999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.23809999999999998 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.23809999999999998 0.7 0.7
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.943]
 [0.57 ]
 [0.653]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.653]
 [0.943]
 [0.57 ]
 [0.653]]
maxi score, test score, baseline:  0.24209999999999998 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
799 30030
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2641 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2641 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2681 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2741 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2761 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2761 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.2801 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
800 30057
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.7 0.7
801 30081
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.516]
 [0.581]
 [0.683]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.56 ]
 [0.516]
 [0.581]
 [0.683]]
802 30088
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.7 0.7
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.023]
 [0.648]
 [0.591]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.028]
 [0.023]
 [0.648]
 [0.591]]
803 30104
maxi score, test score, baseline:  0.3121 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.3181 0.7 0.7
probs:  [1.0]
805 30131
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
805 30155
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.7 0.7
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.7 0.7
probs:  [1.0]
805 30166
maxi score, test score, baseline:  0.2981 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2961 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2961 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2941 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
805 30225
maxi score, test score, baseline:  0.3061 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.141]
 [0.768]
 [0.438]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.376]
 [0.141]
 [0.768]
 [0.438]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.25  0.417 0.292]
maxi score, test score, baseline:  0.3201 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.7 0.7
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.3261 0.7 0.7
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.7 0.7
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.   ]
 [0.612]
 [0.994]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.994]
 [0.   ]
 [0.612]
 [0.994]]
maxi score, test score, baseline:  0.3221 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.326]
 [0.403]
 [0.366]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.305]
 [0.326]
 [0.403]
 [0.366]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3241 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.3241 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.479]
 [0.685]
 [0.57 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.612]
 [0.479]
 [0.685]
 [0.57 ]]
maxi score, test score, baseline:  0.3201 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.3141 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3061 0.7 0.7
maxi score, test score, baseline:  0.3021 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.3021 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.3041 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.553]
 [0.485]
 [0.485]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.435]
 [0.553]
 [0.485]
 [0.485]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.7 0.7
probs:  [1.0]
maxi score, test score, baseline:  0.2981 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.569]
 [0.699]
 [0.699]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.64 ]
 [0.569]
 [0.699]
 [0.699]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.7 0.7
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.915]
 [0.49 ]
 [0.49 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.49 ]
 [0.915]
 [0.49 ]
 [0.49 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.567]
 [1.01 ]
 [0.742]
 [0.742]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.567]
 [1.01 ]
 [0.742]
 [0.742]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.183]
 [0.941]
 [0.315]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.244]
 [0.183]
 [0.941]
 [0.315]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
809 30397
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.7 0.7
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.65 0.65
maxi score, test score, baseline:  0.3081 0.65 0.65
probs:  [1.0]
810 30433
810 30435
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.65 0.65
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.2  ]
 [0.214]
 [0.229]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.208]
 [0.2  ]
 [0.214]
 [0.229]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.65 0.65
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
813 30449
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.407]
 [0.326]
 [0.377]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.326]
 [0.407]
 [0.326]
 [0.377]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.65 0.65
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.65 0.65
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3121 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3161 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.631]
 [0.631]
 [0.631]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.631]
 [0.631]
 [0.631]
 [0.631]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.174]
 [0.516]
 [0.56 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.174]
 [0.516]
 [0.56 ]]
maxi score, test score, baseline:  0.3041 0.65 0.65
probs:  [1.0]
Sims:  25 1 epoch:  200990 pick best:  False frame count:  200990
814 30531
maxi score, test score, baseline:  0.2981 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.2961 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.2961 0.65 0.65
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.449]
 [0.449]
 [0.449]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.449]
 [0.449]
 [0.449]
 [0.449]]
maxi score, test score, baseline:  0.2961 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28409999999999996 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
818 30588
maxi score, test score, baseline:  0.28609999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.606]
 [0.55 ]
 [0.709]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.715]
 [0.606]
 [0.55 ]
 [0.709]]
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.031]
 [0.15 ]
 [0.15 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.15 ]
 [0.031]
 [0.15 ]
 [0.15 ]]
maxi score, test score, baseline:  0.28809999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.65 0.65
maxi score, test score, baseline:  0.29009999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2941 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.65 0.65
maxi score, test score, baseline:  0.2981 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.65 0.65
maxi score, test score, baseline:  0.3021 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
819 30612
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.526]
 [0.471]
 [0.379]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.471]
 [0.526]
 [0.471]
 [0.379]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.64]
 [0.64]
 [0.64]
 [0.64]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.64]
 [0.64]
 [0.64]
 [0.64]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
820 30677
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.65 0.65
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.578]
 [0.546]
 [0.546]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.546]
 [0.578]
 [0.546]
 [0.546]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.65 0.65
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.06 ]
 [0.556]
 [0.556]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.556]
 [0.06 ]
 [0.556]
 [0.556]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.482]
 [0.726]
 [0.615]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.844]
 [0.482]
 [0.726]
 [0.615]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.43]
 [0.43]
 [0.43]
 [0.43]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.43]
 [0.43]
 [0.43]
 [0.43]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.65 0.65
maxi score, test score, baseline:  0.3101 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.65 0.65
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.65 0.65
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.65 0.65
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.65 0.65
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.65 0.65
maxi score, test score, baseline:  0.3181 0.65 0.65
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.261]
 [0.628]
 [0.425]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.389]
 [0.261]
 [0.628]
 [0.425]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
822 30786
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
822 30788
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.65 0.65
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3261 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.65 0.65
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.162]
 [0.626]
 [0.626]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.626]
 [0.162]
 [0.626]
 [0.626]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.721]
 [0.735]
 [0.76 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.629]
 [0.721]
 [0.735]
 [0.76 ]]
maxi score, test score, baseline:  0.3281 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3281 0.65 0.65
maxi score, test score, baseline:  0.3261 0.65 0.65
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.1  ]
 [0.601]
 [0.629]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.35 ]
 [0.1  ]
 [0.601]
 [0.629]]
maxi score, test score, baseline:  0.3221 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.65 0.65
maxi score, test score, baseline:  0.3301 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  828
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.372]
 [0.637]
 [0.689]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.593]
 [0.372]
 [0.637]
 [0.689]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.65 0.65
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.559]
 [0.559]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.559]
 [0.559]
 [0.559]
 [0.559]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3401 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
830 30885
maxi score, test score, baseline:  0.34809999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.572]
 [0.452]
 [0.465]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.251]
 [0.572]
 [0.452]
 [0.465]]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.65 ]
 [0.758]
 [0.758]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.758]
 [0.65 ]
 [0.758]
 [0.758]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
830 30896
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.713]
 [0.626]
 [0.634]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.634]
 [0.713]
 [0.626]
 [0.634]]
maxi score, test score, baseline:  0.3381 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3341 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3381 0.65 0.65
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.308]
 [0.027]
 [0.103]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.304]
 [0.308]
 [0.027]
 [0.103]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.65 0.65
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
831 30929
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3401 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.65 0.65
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3601 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
834 31022
maxi score, test score, baseline:  0.3621 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3601 0.65 0.65
maxi score, test score, baseline:  0.3601 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3601 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.677]
 [0.447]
 [0.447]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.472]
 [0.677]
 [0.447]
 [0.447]]
maxi score, test score, baseline:  0.3621 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.063]
 [0.121]
 [0.147]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.064]
 [0.063]
 [0.121]
 [0.147]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.167 0.125 0.333 0.375]
in main func line 156:  837
maxi score, test score, baseline:  0.3821 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3781 0.65 0.65
maxi score, test score, baseline:  0.3781 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3761 0.65 0.65
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 0.65 0.65
maxi score, test score, baseline:  0.3801 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
839 31137
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3801 0.65 0.65
maxi score, test score, baseline:  0.3761 0.65 0.65
maxi score, test score, baseline:  0.3741 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3741 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3741 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3721 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3681 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.65 0.65
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.65 0.65
maxi score, test score, baseline:  0.3701 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.65 0.65
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.042 0.125 0.792 0.042]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.65 0.65
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.272]
 [0.422]
 [0.422]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.422]
 [0.272]
 [0.422]
 [0.422]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.583 0.292 0.083]
maxi score, test score, baseline:  0.35609999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
841 31219
maxi score, test score, baseline:  0.35609999999999997 0.65 0.65
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.65 0.65
probs:  [1.0]
siam score:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.25  0.417 0.292]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.65 0.65
maxi score, test score, baseline:  0.35009999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
843 31258
maxi score, test score, baseline:  0.35009999999999997 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.35009999999999997 0.65 0.65
maxi score, test score, baseline:  0.35009999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
844 31259
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3401 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3401 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3381 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.65 0.65
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.689]
 [0.638]
 [0.754]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.754]
 [0.689]
 [0.638]
 [0.754]]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.341]
 [0.46 ]
 [0.49 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.49 ]
 [0.341]
 [0.46 ]
 [0.49 ]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3401 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3401 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.65 0.65
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.625]
 [0.669]
 [0.669]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.669]
 [0.625]
 [0.669]
 [0.669]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.65 0.65
maxi score, test score, baseline:  0.3661 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3641 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3641 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
851 31334
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.035]
 [0.532]
 [0.396]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.396]
 [0.035]
 [0.532]
 [0.396]]
maxi score, test score, baseline:  0.3641 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.65 0.65
maxi score, test score, baseline:  0.3601 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.65 0.65
maxi score, test score, baseline:  0.35409999999999997 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.35209999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
852 31362
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.178]
 [0.282]
 [0.024]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.178]
 [0.282]
 [0.024]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.65 0.65
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.65 0.65
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
853 31406
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.65 0.65
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
853 31477
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3181 0.65 0.65
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.651]
 [0.356]
 [0.651]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.651]
 [0.651]
 [0.356]
 [0.651]]
maxi score, test score, baseline:  0.3181 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.354]
 [0.306]
 [0.306]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.306]
 [0.354]
 [0.306]
 [0.306]]
maxi score, test score, baseline:  0.3201 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3201 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3161 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.65 0.65
probs:  [1.0]
maxi score, test score, baseline:  0.3101 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.653]
 [0.619]
 [0.695]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.72 ]
 [0.653]
 [0.619]
 [0.695]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.65 0.65
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
853 31568
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.65 0.65
853 31574
maxi score, test score, baseline:  0.28809999999999997 0.65 0.65
probs:  [1.0]
Starting evaluation
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.906]
 [0.384]
 [0.384]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.384]
 [0.906]
 [0.384]
 [0.384]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.208]
 [0.293]
 [0.574]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.166]
 [0.208]
 [0.293]
 [0.574]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3261 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
853 31646
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
853 31662
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
853 31677
maxi score, test score, baseline:  0.3221 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.55 0.55
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.55 0.55
maxi score, test score, baseline:  0.3341 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.809]
 [1.052]
 [0.809]
 [0.809]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.809]
 [1.052]
 [0.809]
 [0.809]]
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3261 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.55 0.55
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.659]
 [0.663]
 [0.663]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.663]
 [0.659]
 [0.663]
 [0.663]]
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3381 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.292 0.375 0.292]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.55 0.55
maxi score, test score, baseline:  0.3041 0.55 0.55
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.2981 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
856 31828
maxi score, test score, baseline:  0.3161 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.55 0.55
maxi score, test score, baseline:  0.3181 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3181 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3181 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3121 0.55 0.55
maxi score, test score, baseline:  0.3121 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.55 0.55
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.55 0.55
maxi score, test score, baseline:  0.3121 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.243]
 [0.059]
 [0.059]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.059]
 [0.243]
 [0.059]
 [0.059]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3061 0.55 0.55
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3001 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.218]
 [0.234]
 [0.25 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.246]
 [0.218]
 [0.234]
 [0.25 ]]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.55 0.55
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3041 0.55 0.55
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3081 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
858 31942
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.55 0.55
maxi score, test score, baseline:  0.3121 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
860 31966
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.526]
 [0.609]
 [0.609]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.609]
 [0.526]
 [0.609]
 [0.609]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
860 31976
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.484]
 [0.486]
 [0.483]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.428]
 [0.484]
 [0.486]
 [0.483]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.55 0.55
maxi score, test score, baseline:  0.34409999999999996 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.083 0.708 0.167 0.042]
maxi score, test score, baseline:  0.34409999999999996 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.34409999999999996 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.55 0.55
maxi score, test score, baseline:  0.3421 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3381 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3361 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.55 0.55
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.365]
 [0.669]
 [0.669]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.669]
 [0.365]
 [0.669]
 [0.669]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.55 0.55
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.55 0.55
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.55 0.55
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.55 0.55
maxi score, test score, baseline:  0.35409999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.329]
 [0.426]
 [0.349]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.349]
 [0.329]
 [0.426]
 [0.349]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.875 0.042 0.042]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.55 0.55
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.   ]
 [0.678]
 [0.603]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.489]
 [0.   ]
 [0.678]
 [0.603]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.963]
 [0.462]
 [0.963]
 [0.963]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.963]
 [0.462]
 [0.963]
 [0.963]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.639]
 [0.639]
 [0.639]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.639]
 [0.639]
 [0.639]
 [0.639]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.663]
 [0.678]
 [0.714]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.714]
 [0.663]
 [0.678]
 [0.714]]
maxi score, test score, baseline:  0.41009999999999996 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.544]
 [0.655]
 [0.576]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.576]
 [0.544]
 [0.655]
 [0.576]]
maxi score, test score, baseline:  0.4041 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.873]
 [0.451]
 [0.451]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.451]
 [0.873]
 [0.451]
 [0.451]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
879 32195
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.643]
 [0.725]
 [0.739]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.643]
 [0.643]
 [0.725]
 [0.739]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.185]
 [0.92 ]
 [0.3  ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.3  ]
 [0.185]
 [0.92 ]
 [0.3  ]]
maxi score, test score, baseline:  0.3961 0.55 0.55
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3921 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.55 0.55
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3981 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.899]
 [0.899]
 [0.485]
 [0.899]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.899]
 [0.899]
 [0.485]
 [0.899]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.55 0.55
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.41409999999999997 0.55 0.55
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.77 ]
 [0.448]
 [0.441]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.77 ]
 [0.448]
 [0.441]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4241 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
881 32271
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.066]
 [0.074]
 [0.074]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.074]
 [0.066]
 [0.074]
 [0.074]]
siam score:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.55 0.55
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.41409999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.55 0.55
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.4281 0.55 0.55
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.167 0.5   0.208 0.125]
maxi score, test score, baseline:  0.4321 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4361 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.741]
 [0.741]
 [0.741]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.741]
 [0.741]
 [0.741]
 [0.741]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.4481 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4541 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4561 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
884 32359
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4641 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4641 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4661 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4641 0.55 0.55
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4621 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4601 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4621 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
886 32425
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.875 0.042 0.042]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.629]
 [0.775]
 [0.721]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.503]
 [0.629]
 [0.775]
 [0.721]]
maxi score, test score, baseline:  0.4041 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.55 0.55
maxi score, test score, baseline:  0.4061 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4061 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.41009999999999996 0.55 0.55
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.41209999999999997 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3901 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.303]
 [0.58 ]
 [0.58 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.58 ]
 [0.303]
 [0.58 ]
 [0.58 ]]
maxi score, test score, baseline:  0.3921 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.55 0.55
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3941 0.55 0.55
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.55 0.55
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.672]
 [0.761]
 [0.728]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.646]
 [0.672]
 [0.761]
 [0.728]]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.271]
 [0.525]
 [0.533]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.533]
 [0.271]
 [0.525]
 [0.533]]
maxi score, test score, baseline:  0.4021 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.201]
 [0.342]
 [0.28 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.28 ]
 [0.201]
 [0.342]
 [0.28 ]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.162]
 [0.297]
 [0.237]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.354]
 [0.162]
 [0.297]
 [0.237]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.598]
 [0.709]
 [0.663]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.519]
 [0.598]
 [0.709]
 [0.663]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.55 0.55
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.4001 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.4001 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
891 32614
siam score:  0.0
maxi score, test score, baseline:  0.4021 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.326]
 [0.406]
 [0.326]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.326]
 [0.326]
 [0.406]
 [0.326]]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.341]
 [0.454]
 [0.581]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.341]
 [0.454]
 [0.581]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.792 0.042 0.125]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.55 0.55
maxi score, test score, baseline:  0.3721 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
895 32657
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.14 ]
 [0.583]
 [0.274]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.274]
 [0.14 ]
 [0.583]
 [0.274]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3861 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3841 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3801 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.55 0.55
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.55 0.55
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.896]
 [0.994]
 [0.896]
 [0.896]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.896]
 [0.994]
 [0.896]
 [0.896]]
maxi score, test score, baseline:  0.3841 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.55 0.55
probs:  [1.0]
Starting evaluation
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.3801 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3741 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.6 0.6
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3721 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3721 0.6 0.6
899 32743
maxi score, test score, baseline:  0.3641 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.6 0.6
siam score:  0.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.706]
 [0.706]
 [0.706]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.706]
 [0.706]
 [0.706]
 [0.706]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.745]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.745]
 [0.004]
 [0.004]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
UNIT TEST: sample policy line 217 mcts : [0.125 0.625 0.167 0.083]
maxi score, test score, baseline:  0.35209999999999997 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.34609999999999996 0.6 0.6
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.397]
 [0.327]
 [0.448]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.448]
 [0.397]
 [0.327]
 [0.448]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.386]
 [0.73 ]
 [0.717]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.602]
 [0.386]
 [0.73 ]
 [0.717]]
maxi score, test score, baseline:  0.3381 0.6 0.6
maxi score, test score, baseline:  0.3381 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3361 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3341 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3341 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.617]
 [0.617]
 [0.617]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.617]
 [0.617]
 [0.617]
 [0.617]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.058]
 [0.698]
 [0.569]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.569]
 [0.058]
 [0.698]
 [0.569]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.271]
 [0.532]
 [0.532]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.532]
 [0.271]
 [0.532]
 [0.532]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.712]
 [0.807]
 [0.975]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.661]
 [0.712]
 [0.807]
 [0.975]]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.013]
 [0.303]
 [0.199]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.013]
 [0.303]
 [0.199]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3101 0.6 0.6
906 32858
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
907 32863
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.6 0.6
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.    0.917 0.    0.083]
maxi score, test score, baseline:  0.3021 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3041 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.328]
 [0.843]
 [0.843]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.843]
 [0.328]
 [0.843]
 [0.843]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.6 0.6
probs:  [1.0]
912 32917
maxi score, test score, baseline:  0.3241 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3241 0.6 0.6
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.667 0.167 0.125]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.125 0.5   0.167 0.208]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
916 32928
maxi score, test score, baseline:  0.3341 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3341 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
918 32931
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3421 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3401 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.407]
 [0.545]
 [0.407]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.407]
 [0.407]
 [0.545]
 [0.407]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.6 0.6
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.084]
 [0.367]
 [0.367]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.367]
 [0.084]
 [0.367]
 [0.367]]
maxi score, test score, baseline:  0.35409999999999997 0.6 0.6
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.6 0.6
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.685]
 [0.685]
 [0.685]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.685]
 [0.685]
 [0.685]
 [0.685]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.35409999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.522]
 [0.632]
 [0.688]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.406]
 [0.522]
 [0.632]
 [0.688]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
923 33034
maxi score, test score, baseline:  0.3321 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3281 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3281 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.6 0.6
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
924 33056
maxi score, test score, baseline:  0.3261 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3261 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3201 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.6 0.6
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3281 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.546]
 [0.616]
 [0.583]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.464]
 [0.546]
 [0.616]
 [0.583]]
maxi score, test score, baseline:  0.3281 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
927 33124
maxi score, test score, baseline:  0.3321 0.6 0.6
maxi score, test score, baseline:  0.3321 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3401 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.56 ]
 [0.652]
 [0.669]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.445]
 [0.56 ]
 [0.652]
 [0.669]]
maxi score, test score, baseline:  0.3421 0.6 0.6
siam score:  0.0
maxi score, test score, baseline:  0.3421 0.6 0.6
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
929 33147
siam score:  0.0
maxi score, test score, baseline:  0.3421 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.616]
 [0.485]
 [0.485]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.485]
 [0.616]
 [0.485]
 [0.485]]
maxi score, test score, baseline:  0.34409999999999996 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.34409999999999996 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
929 33166
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.6 0.6
929 33176
maxi score, test score, baseline:  0.3421 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.596]
 [0.255]
 [0.255]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.255]
 [0.596]
 [0.255]
 [0.255]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
929 33210
maxi score, test score, baseline:  0.3261 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.748]
 [0.748]
 [0.748]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.748]
 [0.748]
 [0.748]
 [0.748]]
siam score:  0.0
maxi score, test score, baseline:  0.3361 0.6 0.6
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
930 33235
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.6 0.6
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.6 0.6
UNIT TEST: sample policy line 217 mcts : [0.042 0.833 0.042 0.083]
maxi score, test score, baseline:  0.34609999999999996 0.6 0.6
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.6 0.6
maxi score, test score, baseline:  0.34809999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.6 0.6
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.6 0.6
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.289]
 [0.855]
 [0.257]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.257]
 [0.289]
 [0.855]
 [0.257]]
maxi score, test score, baseline:  0.3621 0.6 0.6
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.6 0.6
maxi score, test score, baseline:  0.3581 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.969]
 [0.361]
 [0.361]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.361]
 [0.969]
 [0.361]
 [0.361]]
934 33284
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3581 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3581 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.99 ]
 [0.741]
 [0.354]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.354]
 [0.99 ]
 [0.741]
 [0.354]]
934 33299
maxi score, test score, baseline:  0.3581 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.602]
 [0.602]
 [0.602]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.602]
 [0.602]
 [0.602]
 [0.602]]
maxi score, test score, baseline:  0.3581 0.6 0.6
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.817]
 [0.471]
 [0.471]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.471]
 [0.817]
 [0.471]
 [0.471]]
maxi score, test score, baseline:  0.35609999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
936 33319
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3681 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3641 0.6 0.6
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
936 33347
maxi score, test score, baseline:  0.3661 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3661 0.6 0.6
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.3621 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3621 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3621 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3621 0.6 0.6
maxi score, test score, baseline:  0.3621 0.6 0.6
maxi score, test score, baseline:  0.3621 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.35209999999999997 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3341 0.6 0.6
probs:  [1.0]
936 33403
Printing some Q and Qe and total Qs values:  [[1.021]
 [0.227]
 [1.021]
 [1.021]] [[0.]
 [0.]
 [0.]
 [0.]] [[1.021]
 [0.227]
 [1.021]
 [1.021]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.3181 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.449]
 [0.47 ]
 [0.323]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.292]
 [0.449]
 [0.47 ]
 [0.323]]
maxi score, test score, baseline:  0.3041 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
938 33458
maxi score, test score, baseline:  0.2981 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.2981 0.6 0.6
maxi score, test score, baseline:  0.2981 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.648]
 [0.648]
 [0.648]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.648]
 [0.648]
 [0.648]
 [0.648]]
maxi score, test score, baseline:  0.3101 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.6 0.6
maxi score, test score, baseline:  0.3101 0.6 0.6
maxi score, test score, baseline:  0.3081 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.6 0.6
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.864]
 [0.494]
 [0.386]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.802]
 [0.864]
 [0.494]
 [0.386]]
maxi score, test score, baseline:  0.3061 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3001 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.234]
 [0.687]
 [0.665]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.594]
 [0.234]
 [0.687]
 [0.665]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.6 0.6
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.235]
 [0.229]
 [0.244]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.234]
 [0.235]
 [0.229]
 [0.244]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3021 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.656]
 [0.358]
 [0.358]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.358]
 [0.656]
 [0.358]
 [0.358]]
maxi score, test score, baseline:  0.3001 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.6 0.6
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28609999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.376]
 [0.439]
 [0.396]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.511]
 [0.376]
 [0.439]
 [0.396]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28409999999999996 0.6 0.6
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.28209999999999996 0.6 0.6
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2801 0.6 0.6
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2761 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.2761 0.6 0.6
probs:  [1.0]
942 33612
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
942 33632
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2721 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2741 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.2661 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.2661 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.2621 0.6 0.6
maxi score, test score, baseline:  0.2621 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.2561 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2601 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.134]
 [0.6  ]
 [0.464]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.331]
 [0.134]
 [0.6  ]
 [0.464]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2701 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2721 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.2681 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
944 33705
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2661 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.2661 0.6 0.6
maxi score, test score, baseline:  0.2661 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2701 0.6 0.6
probs:  [1.0]
945 33715
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.2481 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
945 33740
945 33746
maxi score, test score, baseline:  0.23609999999999998 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.23609999999999998 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.23609999999999998 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.23609999999999998 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24009999999999998 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.2341 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24209999999999998 0.6 0.6
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.6 0.6
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24409999999999998 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.24609999999999999 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2501 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2581 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2641 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.85 0.85
probs:  [1.0]
951 33865
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.28 ]
 [0.234]
 [0.258]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.246]
 [0.28 ]
 [0.234]
 [0.258]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
953 33876
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
954 33891
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.958]
 [0.34 ]
 [0.34 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.34 ]
 [0.958]
 [0.34 ]
 [0.34 ]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.287]
 [0.517]
 [0.604]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.604]
 [0.287]
 [0.517]
 [0.604]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3281 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
955 33961
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3321 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.012]
 [0.628]
 [0.402]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.48 ]
 [0.012]
 [0.628]
 [0.402]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.505]
 [0.692]
 [0.505]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.505]
 [0.505]
 [0.692]
 [0.505]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.85 0.85
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
957 33998
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.35009999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.35209999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.85 0.85
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
957 34008
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.85 0.85
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.85 0.85
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.85 0.85
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.3701 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.85 0.85
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
961 34052
maxi score, test score, baseline:  0.3741 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.389]
 [0.438]
 [0.447]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.358]
 [0.389]
 [0.438]
 [0.447]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
961 34059
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3781 0.85 0.85
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.773]
 [0.016]
 [0.317]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.345]
 [0.773]
 [0.016]
 [0.317]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3761 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.85 0.85
maxi score, test score, baseline:  0.3761 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3901 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.85 0.85
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3961 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3961 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.911]
 [0.911]
 [0.782]
 [0.911]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.911]
 [0.911]
 [0.782]
 [0.911]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.993]
 [0.596]
 [0.596]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.596]
 [0.993]
 [0.596]
 [0.596]]
maxi score, test score, baseline:  0.4001 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.002]
 [0.524]
 [0.498]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.763]
 [0.002]
 [0.524]
 [0.498]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.123]
 [0.687]
 [0.255]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.149]
 [0.123]
 [0.687]
 [0.255]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.085]
 [0.303]
 [0.169]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.169]
 [0.085]
 [0.303]
 [0.169]]
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.534]
 [0.613]
 [0.769]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.769]
 [0.534]
 [0.613]
 [0.769]]
Printing some Q and Qe and total Qs values:  [[0.3 ]
 [0.86]
 [0.3 ]
 [0.3 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.3 ]
 [0.86]
 [0.3 ]
 [0.3 ]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.634]
 [0.686]
 [0.718]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.542]
 [0.634]
 [0.686]
 [0.718]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.356]
 [0.488]
 [0.466]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.473]
 [0.356]
 [0.488]
 [0.466]]
maxi score, test score, baseline:  0.41209999999999997 0.85 0.85
maxi score, test score, baseline:  0.41209999999999997 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.41209999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.85 0.85
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
962 34260
maxi score, test score, baseline:  0.4061 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.395]
 [0.258]
 [0.314]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.336]
 [0.395]
 [0.258]
 [0.314]]
maxi score, test score, baseline:  0.3981 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
962 34290
maxi score, test score, baseline:  0.3821 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3821 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
962 34309
962 34315
maxi score, test score, baseline:  0.35209999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.85 0.85
maxi score, test score, baseline:  0.34809999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.85 0.85
maxi score, test score, baseline:  0.34809999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3641 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.434]
 [0.612]
 [0.434]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.434]
 [0.434]
 [0.612]
 [0.434]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.   ]
 [0.635]
 [0.303]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.207]
 [0.   ]
 [0.635]
 [0.303]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.85 0.85
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.808]
 [0.808]
 [0.787]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.709]
 [0.808]
 [0.808]
 [0.787]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.814]
 [0.404]
 [0.404]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.404]
 [0.814]
 [0.404]
 [0.404]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.85 0.85
maxi score, test score, baseline:  0.41009999999999996 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
969 34426
maxi score, test score, baseline:  0.41209999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
969 34434
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.85 0.85
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.259]
 [0.323]
 [0.331]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.293]
 [0.259]
 [0.323]
 [0.331]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.715]
 [0.71 ]
 [0.71 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.71 ]
 [0.715]
 [0.71 ]
 [0.71 ]]
maxi score, test score, baseline:  0.41609999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.4201 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.487]
 [0.732]
 [0.436]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.456]
 [0.487]
 [0.732]
 [0.436]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.41809999999999997 0.85 0.85
maxi score, test score, baseline:  0.41809999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.41209999999999997 0.85 0.85
maxi score, test score, baseline:  0.41209999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.85 0.85
maxi score, test score, baseline:  0.41209999999999997 0.85 0.85
probs:  [1.0]
973 34482
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.567]
 [0.43 ]
 [0.396]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.523]
 [0.567]
 [0.43 ]
 [0.396]]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.757]
 [0.54 ]
 [0.786]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.695]
 [0.757]
 [0.54 ]
 [0.786]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.85 0.85
probs:  [1.0]
973 34497
973 34502
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.423]
 [0.406]
 [0.406]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.406]
 [0.423]
 [0.406]
 [0.406]]
973 34508
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.85 0.85
probs:  [1.0]
974 34512
maxi score, test score, baseline:  0.3861 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3821 0.85 0.85
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.85 0.85
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.85 0.85
maxi score, test score, baseline:  0.3861 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3821 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.292]
 [0.309]
 [0.309]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.309]
 [0.292]
 [0.309]
 [0.309]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.85 0.85
probs:  [1.0]
976 34570
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3861 0.85 0.85
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
978 34578
maxi score, test score, baseline:  0.3901 0.85 0.85
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.85 0.85
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.325]
 [0.352]
 [0.158]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.458]
 [0.325]
 [0.352]
 [0.158]]
maxi score, test score, baseline:  0.3741 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3741 0.85 0.85
maxi score, test score, baseline:  0.3741 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3761 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3761 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.85 0.85
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.85 0.85
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.85 0.85
probs:  [1.0]
980 34640
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3621 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3621 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.85 0.85
maxi score, test score, baseline:  0.3621 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
981 34654
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.496]
 [0.521]
 [0.444]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.444]
 [0.496]
 [0.521]
 [0.444]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3601 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.85 0.85
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
982 34672
maxi score, test score, baseline:  0.3721 0.85 0.85
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.85 0.85
maxi score, test score, baseline:  0.3781 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3781 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.85 0.85
maxi score, test score, baseline:  0.3781 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3821 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.564]
 [0.564]
 [0.564]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.564]
 [0.564]
 [0.564]
 [0.564]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.85 0.85
maxi score, test score, baseline:  0.3961 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.3861 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.583 0.208 0.167]
maxi score, test score, baseline:  0.3881 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.85 0.85
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.85 0.85
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.85 0.85
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
992 34762
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.895]
 [0.413]
 [0.413]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.413]
 [0.895]
 [0.413]
 [0.413]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.571]
 [1.025]
 [0.487]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.487]
 [0.571]
 [1.025]
 [0.487]]
maxi score, test score, baseline:  0.4201 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.385]
 [0.487]
 [0.348]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.29 ]
 [0.385]
 [0.487]
 [0.348]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.85 0.85
maxi score, test score, baseline:  0.4301 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4341 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4341 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4361 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.402]
 [0.379]
 [0.345]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.445]
 [0.402]
 [0.379]
 [0.345]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.85 0.85
maxi score, test score, baseline:  0.4301 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.4281 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4261 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4261 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4261 0.85 0.85
maxi score, test score, baseline:  0.4261 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.4221 0.85 0.85
probs:  [1.0]
992 34832
maxi score, test score, baseline:  0.4221 0.85 0.85
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.443]
 [0.443]
 [0.586]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.443]
 [0.443]
 [0.443]
 [0.586]]
maxi score, test score, baseline:  0.4221 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4261 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.4261 0.85 0.85
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
992 34855
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.252]
 [0.258]
 [0.201]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.223]
 [0.252]
 [0.258]
 [0.201]]
maxi score, test score, baseline:  0.41809999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.85 0.85
maxi score, test score, baseline:  0.41009999999999996 0.85 0.85
probs:  [1.0]
maxi score, test score, baseline:  0.40809999999999996 0.85 0.85
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.85 0.85
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
maxi score, test score, baseline:  0.3901 0.85 0.85
maxi score, test score, baseline:  0.3881 0.85 0.85
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
992 34926
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.6 0.6
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3921 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
992 34948
maxi score, test score, baseline:  0.3941 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
994 34967
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 0.6 0.6
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.392]
 [0.821]
 [0.392]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.392]
 [0.392]
 [0.821]
 [0.392]]
maxi score, test score, baseline:  0.41809999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.796]
 [0.678]
 [0.627]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.627]
 [0.796]
 [0.678]
 [0.627]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.7  ]
 [0.646]
 [0.7  ]
 [0.7  ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.7  ]
 [0.646]
 [0.7  ]
 [0.7  ]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.41809999999999997 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.41409999999999997 0.6 0.6
maxi score, test score, baseline:  0.41209999999999997 0.6 0.6
maxi score, test score, baseline:  0.41209999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.881]
 [0.857]
 [0.826]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.826]
 [0.881]
 [0.857]
 [0.826]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.6 0.6
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1001 35074
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.806]
 [0.003]
 [0.084]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.215]
 [0.806]
 [0.003]
 [0.084]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.41609999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.79 ]
 [0.578]
 [0.544]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.544]
 [0.79 ]
 [0.578]
 [0.544]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 0.6 0.6
maxi score, test score, baseline:  0.41609999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4221 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.592]
 [0.69 ]
 [0.385]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.385]
 [0.592]
 [0.69 ]
 [0.385]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.4301 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.429]
 [0.665]
 [0.666]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.637]
 [0.429]
 [0.665]
 [0.666]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.6 0.6
maxi score, test score, baseline:  0.4301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.256]
 [0.086]
 [0.241]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.226]
 [0.256]
 [0.086]
 [0.241]]
maxi score, test score, baseline:  0.4301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.6 0.6
maxi score, test score, baseline:  0.4281 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.6 0.6
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.51 ]
 [0.659]
 [0.357]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.357]
 [0.51 ]
 [0.659]
 [0.357]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4221 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.4201 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1003 35148
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.6 0.6
maxi score, test score, baseline:  0.41809999999999997 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.41609999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.708 0.125 0.125]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.6 0.6
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.6 0.6
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.6 0.6
probs:  [1.0]
siam score:  0.0
maxi score, test score, baseline:  0.4301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4321 0.6 0.6
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4401 0.6 0.6
probs:  [1.0]
1004 35202
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4401 0.6 0.6
probs:  [1.0]
siam score:  0.0
1005 35209
maxi score, test score, baseline:  0.4341 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.629]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.629]
 [0.002]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4261 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4221 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1006 35258
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4201 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1006 35273
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4221 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.4201 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1007 35289
maxi score, test score, baseline:  0.41409999999999997 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.41409999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.41609999999999997 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.41409999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1007 35302
maxi score, test score, baseline:  0.41209999999999997 0.6 0.6
probs:  [1.0]
1007 35305
1007 35306
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.291]
 [0.257]
 [0.486]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.238]
 [0.291]
 [0.257]
 [0.486]]
maxi score, test score, baseline:  0.40809999999999996 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.4001 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3941 0.6 0.6
maxi score, test score, baseline:  0.3921 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.6 0.6
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.6 0.6
maxi score, test score, baseline:  0.3921 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3961 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1007 35368
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.6 0.6
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.4061 0.6 0.6
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.867]
 [0.277]
 [0.265]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.265]
 [0.867]
 [0.277]
 [0.265]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.584]
 [0.635]
 [0.601]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.611]
 [0.584]
 [0.635]
 [0.601]]
maxi score, test score, baseline:  0.3841 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.3881 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.977]
 [0.522]
 [0.545]
 [0.54 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.977]
 [0.522]
 [0.545]
 [0.54 ]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.6 0.6
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.6 0.6
maxi score, test score, baseline:  0.3961 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.6 0.6
maxi score, test score, baseline:  0.3921 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.6 0.6
probs:  [1.0]
1011 35512
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.6 0.6
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.724]
 [0.899]
 [0.487]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.487]
 [0.724]
 [0.899]
 [0.487]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.877]
 [0.877]
 [0.877]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.877]
 [0.877]
 [0.877]
 [0.877]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.73 ]
 [0.795]
 [0.789]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.674]
 [0.73 ]
 [0.795]
 [0.789]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.6 0.6
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.751]
 [0.8  ]
 [0.805]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.814]
 [0.751]
 [0.8  ]
 [0.805]]
maxi score, test score, baseline:  0.4001 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.4001 0.6 0.6
maxi score, test score, baseline:  0.4001 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.6 0.6
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1012 35573
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.6 0.6
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1014 35593
maxi score, test score, baseline:  0.4001 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.6 0.6
maxi score, test score, baseline:  0.41409999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.41609999999999997 0.6 0.6
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.4201 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.41409999999999997 0.6 0.6
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.6 0.6
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1021 35656
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.6 0.6
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.6 0.6
maxi score, test score, baseline:  0.41809999999999997 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1022 35695
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4261 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.6 0.6
maxi score, test score, baseline:  0.4301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4321 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1023 35707
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.815]
 [0.722]
 [0.722]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.722]
 [0.815]
 [0.722]
 [0.722]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4361 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.4361 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4401 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4421 0.6 0.6
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.319]
 [0.18 ]
 [0.18 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.18 ]
 [0.319]
 [0.18 ]
 [0.18 ]]
maxi score, test score, baseline:  0.4401 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4421 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.368]
 [0.255]
 [0.255]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.255]
 [0.368]
 [0.255]
 [0.255]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4461 0.6 0.6
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.392]
 [0.419]
 [0.389]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.32 ]
 [0.392]
 [0.419]
 [0.389]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4421 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4461 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4501 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.231]
 [0.552]
 [0.552]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.552]
 [0.231]
 [0.552]
 [0.552]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4581 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.594]
 [0.647]
 [0.647]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.647]
 [0.594]
 [0.647]
 [0.647]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.645]
 [0.645]
 [0.645]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.645]
 [0.645]
 [0.645]
 [0.645]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4661 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.71 ]
 [0.735]
 [0.685]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.699]
 [0.71 ]
 [0.735]
 [0.685]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47409999999999997 0.6 0.6
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 0.6 0.6
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1025 35807
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.577]
 [0.716]
 [0.694]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.508]
 [0.577]
 [0.716]
 [0.694]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4941 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4961 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.4961 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.4961 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5021 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5061 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5061 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.5061 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.5061 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5081 0.6 0.6
probs:  [1.0]
maxi score, test score, baseline:  0.4981 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5001 0.6 0.6
maxi score, test score, baseline:  0.5001 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5061 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.5041 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5041 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5081 0.6 0.6
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.039]
 [0.623]
 [0.628]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.044]
 [0.039]
 [0.623]
 [0.628]]
maxi score, test score, baseline:  0.5101 0.6 0.6
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5121 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5121 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.583 0.292]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.799]
 [0.715]
 [0.715]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.715]
 [0.799]
 [0.715]
 [0.715]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.864]
 [0.338]
 [0.307]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.307]
 [0.864]
 [0.338]
 [0.307]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.468]
 [0.371]
 [0.257]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.595]
 [0.468]
 [0.371]
 [0.257]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5041 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5041 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5061 0.6 0.6
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5121 0.6 0.6
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [0.318]
 [0.765]
 [0.691]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.2  ]
 [0.318]
 [0.765]
 [0.691]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5461 0.6 0.6
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.659]
 [0.715]
 [0.649]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.623]
 [0.659]
 [0.715]
 [0.649]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [1.0]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5521 1.0 1.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5461 1.0 1.0
1029 35932
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.125 0.042 0.458 0.375]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5481 1.0 1.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5481 1.0 1.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.131]
 [0.695]
 [0.243]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.243]
 [0.131]
 [0.695]
 [0.243]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1033 35956
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5601 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1033 35968
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.732]
 [0.732]
 [0.732]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.732]
 [0.732]
 [0.732]
 [0.732]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.432]
 [0.492]
 [0.407]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.492]
 [0.432]
 [0.492]
 [0.407]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.93 ]
 [0.522]
 [0.522]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.316]
 [0.93 ]
 [0.522]
 [0.522]]
maxi score, test score, baseline:  0.5621 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.689]
 [0.289]
 [0.297]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.121]
 [0.689]
 [0.289]
 [0.297]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5680999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5680999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1036 36031
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.783]
 [0.248]
 [0.248]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.248]
 [0.783]
 [0.248]
 [0.248]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.   ]
 [0.58 ]
 [0.585]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.585]
 [0.   ]
 [0.58 ]
 [0.585]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.386]
 [0.569]
 [0.538]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.584]
 [0.386]
 [0.569]
 [0.538]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.422]
 [1.03 ]
 [0.422]
 [0.422]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.422]
 [1.03 ]
 [0.422]
 [0.422]]
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [1.0]
1037 36055
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [1.0]
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5680999999999999 1.0 1.0
maxi score, test score, baseline:  0.5680999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.613]
 [0.798]
 [0.607]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.637]
 [0.613]
 [0.798]
 [0.607]]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.601]
 [0.601]
 [0.601]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.601]
 [0.601]
 [0.601]
 [0.601]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5680999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5680999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1042 36189
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5621 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.372]
 [0.399]
 [0.399]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.399]
 [0.372]
 [0.399]
 [0.399]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1044 36229
maxi score, test score, baseline:  0.5521 1.0 1.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.495]
 [0.014]
 [0.014]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.495]
 [0.014]
 [0.014]]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.484]
 [1.021]
 [0.539]
 [0.554]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.484]
 [1.021]
 [0.539]
 [0.554]]
maxi score, test score, baseline:  0.5501 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5261 1.0 1.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [1.0]
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [1.0]
maxi score, test score, baseline:  0.5061 1.0 1.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.491]
 [0.491]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.491]
 [0.491]
 [0.491]
 [0.491]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1048 36304
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5081 1.0 1.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.008]
 [0.864]
 [0.331]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.331]
 [0.008]
 [0.864]
 [0.331]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5081 1.0 1.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.459]
 [0.648]
 [0.667]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.459]
 [0.648]
 [0.667]]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5061 1.0 1.0
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5041 1.0 1.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.861]
 [0.62 ]
 [0.62 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.62 ]
 [0.861]
 [0.62 ]
 [0.62 ]]
maxi score, test score, baseline:  0.4981 1.0 1.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1056 36365
maxi score, test score, baseline:  0.4921 1.0 1.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1056 36377
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[1]
 [1]
 [1]
 [1]] [[0]
 [0]
 [0]
 [0]] [[1]
 [1]
 [1]
 [1]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.283]
 [1.02 ]
 [0.283]
 [0.283]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.283]
 [1.02 ]
 [0.283]
 [0.283]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [1.0]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.306]
 [0.506]
 [0.306]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.306]
 [0.306]
 [0.506]
 [0.306]]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [1.0]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.478]
 [0.351]
 [0.472]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.488]
 [0.478]
 [0.351]
 [0.472]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1059 36511
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.337]
 [0.46 ]
 [0.368]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.492]
 [0.337]
 [0.46 ]
 [0.368]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.29 ]
 [0.294]
 [0.304]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.3  ]
 [0.29 ]
 [0.294]
 [0.304]]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1059 36546
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.323]
 [0.256]
 [0.323]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.323]
 [0.323]
 [0.256]
 [0.323]]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.676]
 [0.267]
 [0.267]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.267]
 [0.676]
 [0.267]
 [0.267]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.    0.583 0.333 0.083]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.577]
 [0.577]
 [0.577]
 [0.577]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.519]
 [0.397]
 [0.397]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.397]
 [0.519]
 [0.397]
 [0.397]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.631]
 [0.742]
 [0.784]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.617]
 [0.631]
 [0.742]
 [0.784]]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.235]
 [0.209]
 [0.209]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.209]
 [0.235]
 [0.209]
 [0.209]]
siam score:  0.0
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.927]
 [0.341]
 [0.32 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.32 ]
 [0.927]
 [0.341]
 [0.32 ]]
maxi score, test score, baseline:  0.4541 1.0 1.0
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [1.0]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.396]
 [0.435]
 [0.396]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.396]
 [0.396]
 [0.435]
 [0.396]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1064 36669
siam score:  0.0
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [1.0]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [1.0]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.097]
 [0.566]
 [0.558]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.392]
 [0.097]
 [0.566]
 [0.558]]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.375]
 [0.661]
 [0.665]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.605]
 [0.375]
 [0.661]
 [0.665]]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [1.0]
1065 36736
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1065 36753
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1065 36763
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.336]
 [0.341]
 [0.363]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.428]
 [0.336]
 [0.341]
 [0.363]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [1.0]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [1.0]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [1.0]
1065 36820
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.583]
 [0.44 ]
 [0.447]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.447]
 [0.583]
 [0.44 ]
 [0.447]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.585]
 [0.599]
 [0.544]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.529]
 [0.585]
 [0.599]
 [0.544]]
1066 36846
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1066 36849
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 1.0 1.0
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1067 36875
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1067 36881
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 1.0 1.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.58]
 [0.58]
 [0.58]
 [0.58]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.58]
 [0.58]
 [0.58]
 [0.58]]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 1.0 1.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [1.0]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.56 ]
 [0.274]
 [0.244]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.348]
 [0.56 ]
 [0.274]
 [0.244]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1068 36939
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 1.0 1.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.355]
 [0.631]
 [0.588]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.666]
 [0.355]
 [0.631]
 [0.588]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.947]
 [0.341]
 [0.341]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.341]
 [0.947]
 [0.341]
 [0.341]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.255]
 [0.736]
 [0.707]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.508]
 [0.255]
 [0.736]
 [0.707]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.   ]
 [0.549]
 [0.483]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.423]
 [0.   ]
 [0.549]
 [0.483]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.701]
 [0.701]
 [0.701]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.701]
 [0.701]
 [0.701]
 [0.701]]
1069 36982
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.3421 0.75 0.75
maxi score, test score, baseline:  0.3421 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.75 0.75
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.445]
 [0.681]
 [0.73 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.445]
 [0.681]
 [0.73 ]]
maxi score, test score, baseline:  0.34609999999999996 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1069 36993
maxi score, test score, baseline:  0.34809999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.75 0.75
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1069 36998
maxi score, test score, baseline:  0.34809999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1069 37015
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.578]
 [0.73 ]
 [0.578]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.578]
 [0.578]
 [0.73 ]
 [0.578]]
maxi score, test score, baseline:  0.34609999999999996 0.75 0.75
probs:  [1.0]
1069 37024
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.75 0.75
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.702]
 [0.599]
 [0.659]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.599]
 [0.702]
 [0.599]
 [0.659]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.54 ]
 [0.058]
 [0.058]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.058]
 [0.54 ]
 [0.058]
 [0.058]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1069 37040
maxi score, test score, baseline:  0.3381 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1070
siam score:  0.0
maxi score, test score, baseline:  0.3141 0.75 0.75
probs:  [1.0]
1070 37094
maxi score, test score, baseline:  0.3121 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.8742],
        [0.0000],
        [0.0000],
        [0.9227],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000]], dtype=torch.float64)
0.0 0.8742128172920004
0.0 0.0
0.0 0.0
0.0 0.9227213034171652
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1070 37130
maxi score, test score, baseline:  0.3201 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.768]
 [0.354]
 [0.354]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.354]
 [0.768]
 [0.354]
 [0.354]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1070 37142
maxi score, test score, baseline:  0.3301 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.75 0.75
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3341 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.584]
 [0.766]
 [0.803]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.676]
 [0.584]
 [0.766]
 [0.803]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.75 0.75
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1071 37167
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1072 37173
maxi score, test score, baseline:  0.3401 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.233]
 [0.402]
 [0.335]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.335]
 [0.233]
 [0.402]
 [0.335]]
maxi score, test score, baseline:  0.3421 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.75 0.75
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.35209999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.75 0.75
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.01 ]
 [0.615]
 [0.179]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.194]
 [0.01 ]
 [0.615]
 [0.179]]
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.3641 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.521]
 [0.66 ]
 [0.686]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.582]
 [0.521]
 [0.66 ]
 [0.686]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1076 37241
maxi score, test score, baseline:  0.3741 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3841 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.75 0.75
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.691]
 [0.703]
 [0.641]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.078]
 [0.691]
 [0.703]
 [0.641]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1076 37271
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4001 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1076 37282
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.75 0.75
maxi score, test score, baseline:  0.4021 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.75 0.75
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.201]
 [0.227]
 [0.221]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.227]
 [0.201]
 [0.227]
 [0.221]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.75 0.75
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.724]
 [0.763]
 [0.763]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.763]
 [0.724]
 [0.763]
 [0.763]]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.885]
 [0.306]
 [0.306]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.306]
 [0.885]
 [0.306]
 [0.306]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.057]
 [0.057]
 [0.057]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.057]
 [0.057]
 [0.057]
 [0.057]]
maxi score, test score, baseline:  0.4341 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4341 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.4321 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4381 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4401 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4381 0.75 0.75
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.59 ]
 [0.646]
 [0.82 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.489]
 [0.59 ]
 [0.646]
 [0.82 ]]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.059]
 [0.445]
 [0.208]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.524]
 [0.059]
 [0.445]
 [0.208]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.257]
 [0.406]
 [0.486]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.486]
 [0.257]
 [0.406]
 [0.486]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4521 0.75 0.75
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.636]
 [0.478]
 [0.487]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.531]
 [0.636]
 [0.478]
 [0.487]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4521 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.4521 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4561 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.4541 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4541 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.4541 0.75 0.75
maxi score, test score, baseline:  0.4541 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1085 37421
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4561 0.75 0.75
maxi score, test score, baseline:  0.4561 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4581 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.5   0.25  0.208]
maxi score, test score, baseline:  0.4581 0.75 0.75
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4581 0.75 0.75
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4581 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.4581 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4581 0.75 0.75
probs:  [1.0]
siam score:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4441 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4421 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1088 37474
maxi score, test score, baseline:  0.4261 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1088 37475
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.279]
 [0.279]
 [0.279]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.279]
 [0.279]
 [0.279]
 [0.279]]
maxi score, test score, baseline:  0.4241 0.75 0.75
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.356]
 [0.523]
 [0.49 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.497]
 [0.356]
 [0.523]
 [0.49 ]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4261 0.75 0.75
maxi score, test score, baseline:  0.4261 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4281 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.741]
 [0.741]
 [0.741]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.741]
 [0.741]
 [0.741]
 [0.741]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.729]
 [0.74 ]
 [0.74 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.74 ]
 [0.729]
 [0.74 ]
 [0.74 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4441 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4461 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1090 37551
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4501 0.75 0.75
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4501 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.743]
 [0.513]
 [0.475]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.502]
 [0.743]
 [0.513]
 [0.475]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4581 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4601 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4621 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4641 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1091 37596
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4661 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.4661 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.75 0.75
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.75 0.75
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1091 37622
maxi score, test score, baseline:  0.4661 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.184]
 [0.617]
 [0.606]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.534]
 [0.184]
 [0.617]
 [0.606]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47009999999999996 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47209999999999996 0.75 0.75
1091 37647
maxi score, test score, baseline:  0.47009999999999996 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47209999999999996 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47209999999999996 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.47009999999999996 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47009999999999996 0.75 0.75
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47009999999999996 0.75 0.75
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.706]
 [0.345]
 [0.34 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.34 ]
 [0.706]
 [0.345]
 [0.34 ]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4681 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47809999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 0.75 0.75
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.722]
 [0.76 ]
 [0.73 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.639]
 [0.722]
 [0.76 ]
 [0.73 ]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47609999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4821 0.75 0.75
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4821 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4841 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4901 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.4861 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.25  0.5   0.208]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.781]
 [0.781]
 [0.781]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.781]
 [0.781]
 [0.781]
 [0.781]]
maxi score, test score, baseline:  0.4861 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4861 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4861 0.75 0.75
maxi score, test score, baseline:  0.4861 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.4841 0.75 0.75
maxi score, test score, baseline:  0.4841 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4841 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4841 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4841 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.646]
 [0.711]
 [0.676]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.694]
 [0.646]
 [0.711]
 [0.676]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4861 0.75 0.75
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4881 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47609999999999997 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1098 37790
maxi score, test score, baseline:  0.4661 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1099
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1099 37808
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4481 0.75 0.75
maxi score, test score, baseline:  0.4481 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1099 37817
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4361 0.75 0.75
maxi score, test score, baseline:  0.4361 0.75 0.75
maxi score, test score, baseline:  0.4361 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4341 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4341 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.4321 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1099 37846
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.386]
 [0.448]
 [0.631]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.513]
 [0.386]
 [0.448]
 [0.631]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4201 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.75 0.75
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.546]
 [0.197]
 [0.197]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.197]
 [0.546]
 [0.197]
 [0.197]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41209999999999997 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.41009999999999996 0.75 0.75
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.75 0.75
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.41009999999999996 0.75 0.75
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1104 37909
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.75 0.75
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.4021 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4061 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.75 0.75
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.40809999999999996 0.75 0.75
probs:  [1.0]
maxi score, test score, baseline:  0.4061 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.582]
 [0.44 ]
 [0.44 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.44 ]
 [0.582]
 [0.44 ]
 [0.44 ]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.75 0.75
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.75 0.75
rdn probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.15 0.3621
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.15 0.3621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.15 0.3581
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.491]
 [0.375]
 [0.36 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.279]
 [0.491]
 [0.375]
 [0.36 ]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.15 0.35409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.15 0.35409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.15 0.3581
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.15 0.3581
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.052]
 [0.626]
 [0.249]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.486]
 [0.052]
 [0.626]
 [0.249]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.15 0.3581
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.15 0.35609999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35609999999999997 0.15 0.35609999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1114 38103
maxi score, test score, baseline:  0.35609999999999997 0.15 0.35609999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.811]
 [0.348]
 [0.337]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.288]
 [0.811]
 [0.348]
 [0.337]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.15 0.35009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.15 0.34809999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.15 0.34809999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.15 0.34809999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1117 38157
maxi score, test score, baseline:  0.35009999999999997 0.15 0.35009999999999997
maxi score, test score, baseline:  0.35009999999999997 0.15 0.35009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.15 0.35009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.15 0.34809999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.796]
 [0.077]
 [0.043]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.177]
 [0.796]
 [0.077]
 [0.043]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.567]
 [0.662]
 [0.66 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.576]
 [0.567]
 [0.662]
 [0.66 ]]
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.554]
 [0.554]
 [0.554]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.554]
 [0.554]
 [0.554]
 [0.554]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.15 0.3301
probs:  [1.0]
maxi score, test score, baseline:  0.3301 0.15 0.3301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.15 0.3321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.15 0.3341
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.15 0.3361
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.15 0.3401
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.15 0.3361
probs:  [1.0]
1117 38233
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1117 38267
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.15 0.34809999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.15 0.35009999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.35009999999999997 0.15 0.35009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.15 0.35409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.15 0.35409999999999997
maxi score, test score, baseline:  0.35409999999999997 0.15 0.35409999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35409999999999997 0.15 0.35409999999999997
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.618]
 [0.558]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.558]
 [0.558]
 [0.618]
 [0.558]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.15 0.3601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.15 0.3581
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.15 0.3601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.15 0.3621
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.15 0.3641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3601 0.15 0.3601
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1119 38330
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.58 ]
 [0.496]
 [0.243]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.496]
 [0.58 ]
 [0.496]
 [0.243]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.15 0.3621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.15 0.3661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.15 0.3681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.367]
 [0.347]
 [0.347]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.347]
 [0.367]
 [0.347]
 [0.347]]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.635]
 [0.635]
 [0.635]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.635]
 [0.635]
 [0.635]
 [0.635]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.15 0.3681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.15 0.3701
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.15 0.3681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.15 0.3701
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.15 0.3701
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.15 0.3721
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.15 0.3781
maxi score, test score, baseline:  0.3781 0.15 0.3781
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.736]
 [0.75 ]
 [0.729]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.725]
 [0.736]
 [0.75 ]
 [0.729]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.16 ]
 [0.511]
 [0.143]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.511]
 [0.16 ]
 [0.511]
 [0.143]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.15 0.3781
maxi score, test score, baseline:  0.3781 0.15 0.3781
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 0.15 0.3801
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.15 0.3781
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 0.15 0.3801
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
maxi score, test score, baseline:  0.3781 0.15 0.3781
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.225]
 [0.312]
 [0.225]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.225]
 [0.225]
 [0.312]
 [0.225]]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.777]
 [0.187]
 [0.24 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.24 ]
 [0.777]
 [0.187]
 [0.24 ]]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.463]
 [0.49 ]
 [0.49 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.49 ]
 [0.463]
 [0.49 ]
 [0.49 ]]
maxi score, test score, baseline:  0.3741 0.15 0.3741
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.15 0.3761
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.272]
 [0.255]
 [0.22 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.231]
 [0.272]
 [0.255]
 [0.22 ]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.15 0.3761
probs:  [1.0]
maxi score, test score, baseline:  0.3761 0.15 0.3761
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.15 0.3781
probs:  [1.0]
maxi score, test score, baseline:  0.3741 0.15 0.3741
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.482]
 [0.475]
 [0.447]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.393]
 [0.482]
 [0.475]
 [0.447]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.15 0.3741
Printing some Q and Qe and total Qs values:  [[0.24]
 [0.24]
 [0.24]
 [0.24]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.24]
 [0.24]
 [0.24]
 [0.24]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.15 0.3741
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.15 0.3701
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.15 0.3721
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.15 0.3781
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 0.15 0.3801
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.15 0.3881
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.15 0.3881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.15 0.3861
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.817]
 [0.684]
 [0.873]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.684]
 [0.817]
 [0.684]
 [0.873]]
1127 38536
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.788]
 [0.788]
 [0.788]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.788]
 [0.788]
 [0.788]
 [0.788]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.162]
 [0.612]
 [0.26 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.142]
 [0.162]
 [0.612]
 [0.26 ]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.54 ]
 [0.616]
 [0.611]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.511]
 [0.54 ]
 [0.616]
 [0.611]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.15 0.3901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.15 0.3901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.313]
 [0.518]
 [0.366]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.366]
 [0.313]
 [0.518]
 [0.366]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.15 0.3941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.15 0.3961
probs:  [1.0]
1131 38559
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.15 0.3961
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1131 38567
maxi score, test score, baseline:  0.3961 0.15 0.3961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1132 38571
maxi score, test score, baseline:  0.3961 0.15 0.3961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1132 38572
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.15 0.3961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.15 0.3961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.15 0.3981
probs:  [1.0]
maxi score, test score, baseline:  0.3981 0.15 0.3981
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.15 0.4021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.15 0.4041
maxi score, test score, baseline:  0.4041 0.15 0.4041
maxi score, test score, baseline:  0.4041 0.15 0.4041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.15 0.4061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.15 0.4061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.15 0.4041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.15 0.4021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.15 0.4041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.594]
 [0.032]
 [0.276]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.276]
 [0.594]
 [0.032]
 [0.276]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.15 0.3921
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.15 0.3941
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.345]
 [0.789]
 [0.789]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.789]
 [0.345]
 [0.789]
 [0.789]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3901 0.15 0.3901
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1134 38649
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.15 0.3921
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.15 0.3941
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.15 0.3961
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.15 0.4001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.15 0.4001
probs:  [1.0]
maxi score, test score, baseline:  0.4001 0.15 0.4001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.15 0.4021
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.15 0.4021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.341]
 [0.45 ]
 [0.45 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.45 ]
 [0.341]
 [0.45 ]
 [0.45 ]]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.152]
 [0.608]
 [0.152]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.152]
 [0.152]
 [0.608]
 [0.152]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4001 0.15 0.4001
probs:  [1.0]
maxi score, test score, baseline:  0.3961 0.15 0.3961
probs:  [1.0]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.15 0.3961
probs:  [1.0]
maxi score, test score, baseline:  0.3921 0.15 0.3921
probs:  [1.0]
maxi score, test score, baseline:  0.3921 0.15 0.3921
maxi score, test score, baseline:  0.3841 0.15 0.3841
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.15 0.3881
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1142 38713
maxi score, test score, baseline:  0.3881 0.15 0.3881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.15 0.3881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.15 0.3881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.828]
 [0.998]
 [1.01 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.468]
 [0.828]
 [0.998]
 [1.01 ]]
maxi score, test score, baseline:  0.3881 0.15 0.3881
probs:  [1.0]
maxi score, test score, baseline:  0.3881 0.15 0.3881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.15 0.3861
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.15 0.3861
probs:  [1.0]
1142 38739
maxi score, test score, baseline:  0.3841 0.15 0.3841
probs:  [1.0]
maxi score, test score, baseline:  0.3821 0.15 0.3821
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3801 0.15 0.3801
probs:  [1.0]
maxi score, test score, baseline:  0.3801 0.15 0.3801
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 0.15 0.3801
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 0.15 0.3801
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.271]
 [0.221]
 [0.236]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.236]
 [0.271]
 [0.221]
 [0.236]]
maxi score, test score, baseline:  0.3801 0.15 0.3801
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1143 38756
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3821 0.15 0.3821
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 0.15 0.3801
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1143 38765
maxi score, test score, baseline:  0.3821 0.15 0.3821
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.748]
 [0.644]
 [0.683]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.748]
 [0.748]
 [0.644]
 [0.683]]
siam score:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3761 0.15 0.3761
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.15 0.3781
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.15 0.3721
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.15 0.3701
probs:  [1.0]
maxi score, test score, baseline:  0.3701 0.15 0.3701
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.727]
 [0.856]
 [0.423]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.423]
 [0.727]
 [0.856]
 [0.423]]
maxi score, test score, baseline:  0.3681 0.15 0.3681
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.15 0.3621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1149 38803
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3621 0.15 0.3621
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35009999999999997 0.15 0.35009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.35209999999999997 0.15 0.35209999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.34809999999999997 0.15 0.34809999999999997
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.15 0.34809999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.34609999999999996 0.15 0.34609999999999996
maxi score, test score, baseline:  0.34409999999999996 0.15 0.34409999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.15 0.3361
maxi score, test score, baseline:  0.3361 0.15 0.3361
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.15 0.3301
maxi score, test score, baseline:  0.3261 0.15 0.3261
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.567]
 [0.122]
 [0.132]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.135]
 [0.567]
 [0.122]
 [0.132]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.15 0.3201
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.15 0.3221
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1151 38905
maxi score, test score, baseline:  0.3221 0.15 0.3221
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.15 0.3221
maxi score, test score, baseline:  0.3221 0.15 0.3221
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.709]
 [0.627]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.627]
 [0.627]
 [0.709]
 [0.627]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.15 0.3261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.15 0.3281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.15 0.3261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.15 0.3341
probs:  [1.0]
maxi score, test score, baseline:  0.3341 0.15 0.3341
maxi score, test score, baseline:  0.3341 0.15 0.3341
probs:  [1.0]
maxi score, test score, baseline:  0.3321 0.15 0.3321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.645]
 [0.542]
 [0.708]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.607]
 [0.645]
 [0.542]
 [0.708]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.15 0.3381
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.15 0.3361
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.15 0.3401
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.15 0.34409999999999996
probs:  [1.0]
maxi score, test score, baseline:  0.34409999999999996 0.15 0.34409999999999996
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.15 0.34409999999999996
probs:  [1.0]
maxi score, test score, baseline:  0.34409999999999996 0.15 0.34409999999999996
probs:  [1.0]
maxi score, test score, baseline:  0.3421 0.15 0.3421
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.15 0.3381
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.15 0.3341
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.284]
 [0.23 ]
 [0.23 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.23 ]
 [0.284]
 [0.23 ]
 [0.23 ]]
maxi score, test score, baseline:  0.3321 0.15 0.3321
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.149]
 [0.602]
 [0.141]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.602]
 [0.149]
 [0.602]
 [0.141]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1155 39003
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.417]
 [0.268]
 [0.268]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.268]
 [0.417]
 [0.268]
 [0.268]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.15 0.3241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.15 0.3081
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.15 0.2981
probs:  [1.0]
maxi score, test score, baseline:  0.29209999999999997 0.15 0.29209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.15 0.29009999999999997
probs:  [1.0]
maxi score, test score, baseline:  0.28809999999999997 0.15 0.28809999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.667]
 [0.599]
 [0.574]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.581]
 [0.667]
 [0.599]
 [0.574]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.15 0.29209999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.294]
 [0.297]
 [0.373]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.319]
 [0.294]
 [0.297]
 [0.373]]
maxi score, test score, baseline:  0.3021 0.15 0.3021
probs:  [1.0]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.3001 0.8 0.8
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2981 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.8 0.8
maxi score, test score, baseline:  0.3001 0.8 0.8
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3041 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3041 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1160 39100
siam score:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1160 39129
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1160 39132
maxi score, test score, baseline:  0.3081 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3081 0.8 0.8
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.8 0.8
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.794]
 [0.794]
 [0.794]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.794]
 [0.794]
 [0.794]
 [0.794]]
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1160 39185
maxi score, test score, baseline:  0.3121 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1161 39192
maxi score, test score, baseline:  0.3121 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.8 0.8
maxi score, test score, baseline:  0.3081 0.8 0.8
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.325]
 [0.624]
 [0.392]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.422]
 [0.325]
 [0.624]
 [0.392]]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.556]
 [0.572]
 [0.562]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.524]
 [0.556]
 [0.572]
 [0.562]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1165 39247
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1165 39250
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.403]
 [0.525]
 [0.476]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.481]
 [0.403]
 [0.525]
 [0.476]]
maxi score, test score, baseline:  0.3101 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.8 0.8
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3201 0.8 0.8
maxi score, test score, baseline:  0.3201 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3201 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.8 0.8
maxi score, test score, baseline:  0.3181 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3181 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3181 0.8 0.8
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1165 39287
maxi score, test score, baseline:  0.3221 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.8 0.8
siam score:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3301 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.823]
 [0.624]
 [0.624]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.624]
 [0.823]
 [0.624]
 [0.624]]
maxi score, test score, baseline:  0.3321 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3301 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3421 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.641]
 [0.698]
 [0.672]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.596]
 [0.641]
 [0.698]
 [0.672]]
maxi score, test score, baseline:  0.34409999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1168 39330
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3581 0.8 0.8
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.701]
 [0.701]
 [0.705]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.701]
 [0.701]
 [0.701]
 [0.705]]
Printing some Q and Qe and total Qs values:  [[0.348]
 [1.052]
 [0.348]
 [0.348]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.348]
 [1.052]
 [0.348]
 [0.348]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3641 0.8 0.8
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1168 39366
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3661 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.105]
 [0.547]
 [0.211]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.214]
 [0.105]
 [0.547]
 [0.211]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.351]
 [0.87 ]
 [0.493]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.493]
 [0.351]
 [0.87 ]
 [0.493]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3721 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.864]
 [0.189]
 [0.178]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.178]
 [0.864]
 [0.189]
 [0.178]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.258]
 [0.246]
 [0.261]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.258]
 [0.258]
 [0.246]
 [0.261]]
maxi score, test score, baseline:  0.3681 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3801 0.8 0.8
maxi score, test score, baseline:  0.3801 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3801 0.8 0.8
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1172 39456
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.253]
 [0.874]
 [0.253]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.253]
 [0.253]
 [0.874]
 [0.253]]
maxi score, test score, baseline:  0.3821 0.8 0.8
siam score:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.779]
 [0.302]
 [0.302]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.302]
 [0.779]
 [0.302]
 [0.302]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.3901 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3901 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.61 ]
 [0.489]
 [0.435]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.488]
 [0.61 ]
 [0.489]
 [0.435]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.8 0.8
maxi score, test score, baseline:  0.3961 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.742]
 [0.298]
 [0.293]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.293]
 [0.742]
 [0.298]
 [0.293]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41009999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.435]
 [0.774]
 [0.463]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.477]
 [0.435]
 [0.774]
 [0.463]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.8 0.8
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.321]
 [0.281]
 [0.321]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.321]
 [0.321]
 [0.281]
 [0.321]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41409999999999997 0.8 0.8
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.794]
 [0.794]
 [0.794]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.794]
 [0.794]
 [0.794]
 [0.794]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41809999999999997 0.8 0.8
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1176 39546
maxi score, test score, baseline:  0.41409999999999997 0.8 0.8
maxi score, test score, baseline:  0.4061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1177 39580
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
maxi score, test score, baseline:  0.4041 0.8 0.8
probs:  [1.0]
1178 39583
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.4061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.246]
 [0.265]
 [0.246]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.246]
 [0.246]
 [0.265]
 [0.246]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4061 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.8 0.8
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1179 39624
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.8 0.8
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.435]
 [0.759]
 [0.822]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.696]
 [0.435]
 [0.759]
 [0.822]]
maxi score, test score, baseline:  0.3941 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.173]
 [0.234]
 [0.218]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.523]
 [0.173]
 [0.234]
 [0.218]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3981 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3941 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3941 0.8 0.8
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3961 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.8 0.8
maxi score, test score, baseline:  0.3881 0.8 0.8
maxi score, test score, baseline:  0.3881 0.8 0.8
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.8 0.8
maxi score, test score, baseline:  0.3841 0.8 0.8
maxi score, test score, baseline:  0.3841 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.8 0.8
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.65 ]
 [0.638]
 [0.633]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.533]
 [0.65 ]
 [0.638]
 [0.633]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.711]
 [0.272]
 [0.272]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.272]
 [0.711]
 [0.272]
 [0.272]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3941 0.8 0.8
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1184
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1184 39731
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4021 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3921 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.178]
 [0.336]
 [0.657]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.657]
 [0.178]
 [0.336]
 [0.657]]
maxi score, test score, baseline:  0.3981 0.8 0.8
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1186 39784
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3861 0.8 0.8
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.702]
 [0.702]
 [0.702]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.702]
 [0.702]
 [0.702]
 [0.702]]
maxi score, test score, baseline:  0.3801 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.183]
 [0.183]
 [0.183]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.183]
 [0.183]
 [0.183]
 [0.183]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1186 39826
maxi score, test score, baseline:  0.3701 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3681 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3721 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[1.001]
 [0.519]
 [0.534]
 [1.001]] [[0.]
 [0.]
 [0.]
 [0.]] [[1.001]
 [0.519]
 [0.534]
 [1.001]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3681 0.8 0.8
maxi score, test score, baseline:  0.3681 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3701 0.8 0.8
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
1188 39887
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1188 39924
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.07 ]
 [0.357]
 [0.293]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.779]
 [0.07 ]
 [0.357]
 [0.293]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.8 0.8
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1189 39947
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34809999999999997 0.8 0.8
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.723]
 [0.65 ]
 [0.65 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.65 ]
 [0.723]
 [0.65 ]
 [0.65 ]]
maxi score, test score, baseline:  0.34409999999999996 0.8 0.8
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.686]
 [0.553]
 [0.553]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.553]
 [0.686]
 [0.553]
 [0.553]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.34609999999999996 0.8 0.8
maxi score, test score, baseline:  0.34609999999999996 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.34609999999999996 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.8 0.8
maxi score, test score, baseline:  0.3401 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.8 0.8
1189 39978
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.463]
 [0.549]
 [0.516]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.501]
 [0.463]
 [0.549]
 [0.516]]
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.649]
 [0.097]
 [0.237]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.223]
 [0.649]
 [0.097]
 [0.237]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.744]
 [0.744]
 [0.744]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.744]
 [0.744]
 [0.744]
 [0.744]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1190 40004
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.238]
 [0.232]
 [0.239]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.241]
 [0.238]
 [0.232]
 [0.239]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3181 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1191 40032
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.613]
 [0.351]
 [0.351]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.351]
 [0.613]
 [0.351]
 [0.351]]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.435]
 [0.435]
 [0.435]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.435]
 [0.435]
 [0.435]
 [0.435]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3041 0.8 0.8
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3001 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1191 40065
maxi score, test score, baseline:  0.2961 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2961 0.8 0.8
maxi score, test score, baseline:  0.2961 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.2961 0.8 0.8
maxi score, test score, baseline:  0.2961 0.8 0.8
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.647]
 [0.569]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.524]
 [0.524]
 [0.647]
 [0.569]]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.8 0.8
maxi score, test score, baseline:  0.29009999999999997 0.8 0.8
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28609999999999997 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28809999999999997 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29009999999999997 0.8 0.8
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28809999999999997 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2941 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.989]
 [0.293]
 [0.293]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.293]
 [0.989]
 [0.293]
 [0.293]]
maxi score, test score, baseline:  0.3021 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3041 0.8 0.8
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
probs:  [1.0]
maxi score, test score, baseline:  0.3041 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.754]
 [0.632]
 [0.632]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.632]
 [0.754]
 [0.632]
 [0.632]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.8 0.8
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.778]
 [0.3  ]
 [0.283]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.52 ]
 [0.778]
 [0.3  ]
 [0.283]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.8 0.8
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.382]
 [0.382]
 [0.382]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.382]
 [0.382]
 [0.382]
 [0.382]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.8 0.8
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.589]
 [0.632]
 [0.539]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.515]
 [0.589]
 [0.632]
 [0.539]]
siam score:  0.0
maxi score, test score, baseline:  0.3181 0.8 0.8
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.3241 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.361]
 [0.22 ]
 [0.183]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.183]
 [0.361]
 [0.22 ]
 [0.183]]
maxi score, test score, baseline:  0.3141 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.931]
 [0.251]
 [0.251]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.251]
 [0.931]
 [0.251]
 [0.251]]
maxi score, test score, baseline:  0.3141 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1195 40221
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.55 0.55
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1196
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [0.742]
 [0.18 ]
 [0.199]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.2  ]
 [0.742]
 [0.18 ]
 [0.199]]
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.55 0.55
probs:  [1.0]
1199 40273
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1199 40276
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3241 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.55 0.55
maxi score, test score, baseline:  0.3281 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.705]
 [0.695]
 [0.695]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.695]
 [0.705]
 [0.695]
 [0.695]]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.55 0.55
probs:  [1.0]
siam score:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.034]
 [0.769]
 [0.684]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.671]
 [0.034]
 [0.769]
 [0.684]]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.621]
 [0.641]
 [0.611]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.621]
 [0.621]
 [0.641]
 [0.611]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.9486],
        [0.0000],
        [0.8769],
        [0.8671],
        [0.0000],
        [0.8666],
        [0.0000],
        [0.7766],
        [0.0000],
        [0.0000]], dtype=torch.float64)
0.0 0.9485729755682957
0.0 0.0
0.0 0.8769355382895703
0.0 0.8671255840748637
0.0 0.0
0.0 0.8665512955389291
0.0 0.0
0.0 0.7765852602100866
0.0 0.0
0.0 0.0
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
1202 40340
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1202 40356
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [0.612]
 [0.547]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.547]
 [0.547]
 [0.612]
 [0.547]]
1202 40358
maxi score, test score, baseline:  0.3221 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3261 0.55 0.55
maxi score, test score, baseline:  0.3261 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.693]
 [0.693]
 [0.725]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.623]
 [0.693]
 [0.693]
 [0.725]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.55 0.55
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.083 0.667 0.167 0.083]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.55 0.55
maxi score, test score, baseline:  0.3341 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1206 40404
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3281 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
in main func line 156:  1208
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3341 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3321 0.55 0.55
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3361 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.939]
 [0.063]
 [0.625]
 [0.939]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.939]
 [0.063]
 [0.625]
 [0.939]]
maxi score, test score, baseline:  0.3341 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3381 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1209 40463
maxi score, test score, baseline:  0.3341 0.55 0.55
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.001]
 [0.497]
 [0.506]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.459]
 [0.001]
 [0.497]
 [0.506]]
maxi score, test score, baseline:  0.3281 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.55 0.55
maxi score, test score, baseline:  0.3281 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.55 0.55
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3281 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3321 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.501]
 [0.511]
 [0.392]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.392]
 [0.501]
 [0.511]
 [0.392]]
maxi score, test score, baseline:  0.3381 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.093]
 [0.093]
 [0.093]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.093]
 [0.093]
 [0.093]
 [0.093]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34409999999999996 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1211 40542
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.55 0.55
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.608]
 [0.674]
 [0.608]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.608]
 [0.608]
 [0.674]
 [0.608]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3401 0.55 0.55
siam score:  0.0
maxi score, test score, baseline:  0.3281 0.55 0.55
maxi score, test score, baseline:  0.3241 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.55 0.55
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.723]
 [0.723]
 [0.723]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.723]
 [0.723]
 [0.723]
 [0.723]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3241 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.55 0.55
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
1214 40622
maxi score, test score, baseline:  0.3241 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3321 0.55 0.55
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20022
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  0.0
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.308]
 [0.261]
 [0.261]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.261]
 [0.308]
 [0.261]
 [0.261]]
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.159]
 [0.164]
 [0.402]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.402]
 [0.159]
 [0.164]
 [0.402]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3361 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3301 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3201 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3181 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3221 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.55 0.55
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.55 0.55
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3161 0.55 0.55
probs:  [1.0]
maxi score, test score, baseline:  0.3161 0.55 0.55
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.859]
 [0.859]
 [0.859]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.859]
 [0.859]
 [0.859]
 [0.859]]
maxi score, test score, baseline:  0.3161 0.55 0.55
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.55 0.55
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.772]
 [0.808]
 [0.753]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.762]
 [0.772]
 [0.808]
 [0.753]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.55 0.55
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.623]
 [0.739]
 [0.73 ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.675]
 [0.623]
 [0.739]
 [0.73 ]]
