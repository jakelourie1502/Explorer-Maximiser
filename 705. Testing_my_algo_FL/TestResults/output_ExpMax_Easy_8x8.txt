[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
Adding thread: now have 13 threads
Starting evaluation
siam score:  -0.0033997938
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 218, frames: 1365, time: 24.039011001586914
training steps:  2
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
218 : 0.0
LR:  1e-06
replay buffer size:  1587
Time Taken :  0.0  mins 24.038841009140015  seconds
[[[486. 152.   6.   0.   0.   0.   0.   0.]
  [251.  64.  23.   4.   0.   0.   0.   0.]
  [ 51.  20.  13.   3.   0.   0.   0.   0.]
  [  3.  17.  13.   1.   0.   0.   0.   0.]
  [  3.   7.  11.   2.   0.   0.   0.   0.]
  [  1.   3.   3.   0.   0.   0.   0.   0.]
  [  0.   1.   2.   1.   1.   0.   0.   0.]
  [  0.   1.   2.   1.   1.   0.   0.   0.]]]
siam score:  -0.014535997
siam score:  -0.06287762
siam score:  -0.090936236
deleting a thread, now have 12 threads
Frames:  1455 train batches done:  52 episodes:  232
siam score:  -0.30633563
siam score:  -0.3345317
deleting a thread, now have 11 threads
Frames:  1663 train batches done:  96 episodes:  257
deleting a thread, now have 10 threads
Frames:  1780 train batches done:  122 episodes:  280
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.2 0.2 0.2]
siam score:  -0.42769784
siam score:  -0.42080843
deleting a thread, now have 9 threads
Frames:  1923 train batches done:  146 episodes:  301
deleting a thread, now have 8 threads
Frames:  2016 train batches done:  173 episodes:  320
first move QE:  -0.30591968848077705
siam score:  -0.31974307
first move QE:  -0.31077043754621836
siam score:  -0.3262616
siam score:  -0.40681118
siam score:  -0.042087663
first move QE:  -0.3154991643888252
siam score:  -0.22227333
siam score:  -0.24465168
siam score:  -0.2750487
siam score:  -0.35998353
siam score:  -0.36699829
siam score:  -0.3683756
step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.3333333333333333
siam score:  -0.33641174
first move QE:  -0.1974964292334458
siam score:  -0.19119549
siam score:  -0.30883813
siam score:  -0.37169856
siam score:  -0.38211998
first move QE:  -0.1852379694595505
siam score:  -0.40146768
first move QE:  -0.16456289693923695
siam score:  -0.3862059
siam score:  -0.32690528
siam score:  -0.348127
siam score:  -0.38301018
siam score:  -0.40654528
siam score:  -0.39309403
siam score:  -0.37158737
siam score:  -0.38542268
first move QE:  -0.12296782236063641
siam score:  -0.5022103
siam score:  -0.47815892
siam score:  -0.44950554
siam score:  -0.4674774
siam score:  -0.48358643
Starting evaluation
siam score:  -0.48512334
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 1.0, '0.5': 0}
Episodes: 1540, frames: 10075, time: 2876.939487218857
training steps:  1179
RDN obj mus: [-0.9474747619450092, -0.9482935378313064, -0.9324404575407506, -0.9356598637873885, -0.9424893940707773]
RDN obj sigmas: [0.02721606051701698, 0.025300296242275446, 0.029869062528598063, 0.029428739206719012, 0.030880675853774717]
1540 : 0.0
LR:  0.001
replay buffer size:  10463
Time Taken :  47.0  mins 56.939321994781494  seconds
[[[3417.  986.   33.   14.    1.    1.    0.    0.]
  [2125.  608.  189.   45.    0.    0.    0.    0.]
  [ 443.  208.  101.   20.    0.    0.    0.    0.]
  [  25.   94.   54.   13.    2.    0.    0.    0.]
  [  19.   29.   31.   10.    0.    0.    0.    0.]
  [   4.    7.   11.   10.    1.    0.    2.    1.]
  [   0.    3.    3.    6.    3.    1.    1.    6.]
  [   0.    2.    2.    2.    1.    0.    0.    1.]]]
first move QE:  -0.1150392663658787
siam score:  -0.41328877
siam score:  -0.4096423
siam score:  -0.40884164
first move QE:  -0.11180585573603237
first move QE:  -0.11189748551875736
UNIT TEST: sample policy line 217 mcts : [0.292 0.083 0.25  0.208 0.167]
siam score:  -0.5325805
line 193 mcts: sample exp_bonus 0.2872138023376465
siam score:  -0.52610755
siam score:  -0.54148453
line 193 mcts: sample exp_bonus 0.524003463089466
siam score:  -0.5187003
siam score:  -0.53286445
siam score:  -0.54654384
first move QE:  -0.0959831300957335
siam score:  -0.51225746
siam score:  -0.503074
siam score:  -0.5050415
siam score:  -0.5085582
first move QE:  -0.09974162126275984
siam score:  -0.57085633
siam score:  -0.57184964
siam score:  -0.52620745
siam score:  -0.57696915
siam score:  -0.60743743
siam score:  -0.5975115
siam score:  -0.599557
Starting evaluation
siam score:  -0.5876139
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 1.0, '0.5': 0}
Episodes: 3007, frames: 20448, time: 7026.9000680446625
training steps:  2375
RDN obj mus: [-0.9400614173769951, -0.9249593237519265, -0.9305260188400746, -0.9350425128809707, -0.9242029506705127]
RDN obj sigmas: [0.03100574142918697, 0.04008168856576875, 0.034257693595985905, 0.03454397920214371, 0.04049672320880407]
3007 : 0.0
LR:  0.001
replay buffer size:  23239
Time Taken :  117.0  mins 6.899953842163086  seconds
[[[7054. 1938.  134.   51.    9.    3.    0.    0.]
  [4097. 1260.  390.   74.    1.    0.    0.    0.]
  [ 850.  408.  226.   42.    1.    0.    0.    0.]
  [  48.  196.  151.   33.    4.    0.    0.    0.]
  [  37.   77.   73.   20.    1.    0.    0.    0.]
  [  13.   33.   45.   18.    7.    0.    2.    1.]
  [   7.   29.   32.   16.    6.    1.    1.    6.]
  [   3.   10.   22.    7.    3.    0.    0.    1.]]]
siam score:  -0.5001867
siam score:  -0.4950108
line 193 mcts: sample exp_bonus 0.3834119141101837
siam score:  -0.5530983
line 193 mcts: sample exp_bonus 0.42463165521621704
UNIT TEST: sample policy line 217 mcts : [0.083 0.458 0.042 0.25  0.167]
first move QE:  -0.09181775086727365
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0022],
        [    0.0003],
        [    0.0000],
        [    0.0000],
        [    0.8302],
        [    0.0000],
        [    0.0028],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0021875428833756745
0.0 0.00033059802334613035
0.0 0.0
0.0 0.0
0.0 0.8302387803382026
0.0 0.0
0.0 0.002848381165837531
0.0 0.0
0.0 0.0
0.0 0.0
siam score:  -0.62702125
siam score:  -0.63156545
siam score:  -0.68198687
siam score:  -0.6758273
siam score:  -0.6817384
siam score:  -0.6927777
siam score:  -0.7547986
siam score:  -0.75293136
siam score:  -0.6734898
siam score:  -0.6737291
siam score:  -0.65761703
first move QE:  -0.0899649594881602
UNIT TEST: sample policy line 217 mcts : [0.125 0.042 0.417 0.292 0.125]
siam score:  -0.7480286
line 193 mcts: sample exp_bonus 0.006027968600392342
STARTED EXPV TRAINING ON FRAME NO.  28603
UNIT TEST: sample policy line 217 mcts : [0.083 0.417 0.208 0.125 0.167]
first move QE:  -0.09211647688852143
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.80770755
siam score:  -0.81100243
first move QE:  -0.09445226131859799
siam score:  -0.8324814
siam score:  -0.8520181
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  56
Q value #non_end_state_>threshold:  458
Scores:  {'0.0': 1.0, '0.17': 0, '0.33': 1.0, '0.5': 0}
Episodes: 4109, frames: 30400, time: 10965.733101129532
training steps:  3541
RDN obj mus: [-0.8081948586285115, -0.8230987146139145, -0.8332755842864513, -0.894285780441761, -0.89267975518765]
RDN obj sigmas: [0.08028263350899359, 0.06628380881766344, 0.07028034665258974, 0.07350068428814222, 0.05782808108600992]
4109 : 0.0
LR:  0.001
replay buffer size:  35083
Time Taken :  182.0  mins 45.732949018478394  seconds
[[[10646.  2595.   237.    96.    12.     4.     0.     0.]
  [ 5780.  1875.   605.   107.     1.     0.     0.     0.]
  [ 1139.   667.   380.    62.     1.     0.     0.     0.]
  [   76.   356.   289.    65.     9.     1.     0.     0.]
  [   64.   202.   218.    46.     5.     3.     1.     0.]
  [   25.   104.   163.    59.    25.    12.     4.     1.]
  [   17.    54.    71.    51.    29.    14.     2.     6.]
  [    9.    20.    38.    20.    16.     3.     4.     2.]]]
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.82718873
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.4289],
        [0.4149],
        [0.0000],
        [0.5520],
        [0.3583],
        [0.0064],
        [0.3950],
        [0.2256],
        [0.4248]], dtype=torch.float64)
0.0 0.0
0.0 0.4288913606218833
0.0 0.414911275177066
0.0 0.0
0.0 0.5519944120696535
0.0 0.3582518124333745
0.0 0.006361562129761327
0.0 0.39496614291976856
0.0 0.22558374910080392
0.0 0.42478304467658695
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus -1.3783869269452773
siam score:  -0.83562624
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8187768
siam score:  -0.8107683
siam score:  -0.81433713
siam score:  -0.8152951
start point for exploration sampling:  28603
first move QE:  -0.13993766268569213
step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8245314
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.81611234
siam score:  -0.8233882
step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  28603
UNIT TEST: sample policy line 217 mcts : [0.167 0.5   0.167 0.083 0.083]
start point for exploration sampling:  28603
step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8265805
line 193 mcts: sample exp_bonus 4.456081073849202
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus -0.3062154033064842
step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  28603
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus -0.9598689516556924
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
first move QE:  -0.146964170416507
step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
first move QE:  -0.14860848169205557
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8126079
siam score:  -0.8132104
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.81719315
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
first move QE:  -0.15290055936452476
siam score:  -0.8138338
step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  28603
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8118602
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  28603
start point for exploration sampling:  28603
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.85125786
first move QE:  -0.16412213906801446
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus 0.3184406446207777
step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8899281
step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
first move QE:  -0.16845651811612777
first move QE:  -0.16864074870376938
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus -0.1576862720489502
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  28603
first move QE:  -0.16490914257691464
step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus 6.801845004367828
step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0
Test reward:  0.72
Q value #end_state_>_threshold:  479
Q value #non_end_state_>threshold:  911
Scores:  {'0.0': 53.0, '0.17': 0, '0.33': 1.0, '0.5': 0}
Episodes: 5122, frames: 40453, time: 14872.562978982925
training steps:  4625
RDN obj mus: [-0.9192927986025811, -0.9173284997701645, -0.9164310001313687, -0.9164519833564758, -0.911853497749567]
RDN obj sigmas: [0.026980413539585857, 0.03532895895813571, 0.04328170138598455, 0.049561151049576396, 0.05241173745577046]
5122 : 0.041666666666666664
LR:  0.001
replay buffer size:  47321
Time Taken :  247.0  mins 52.56286907196045  seconds
[[[12477.  3111.   290.   199.   153.    11.     0.     0.]
  [ 7218.  2430.   744.   139.    22.     3.     0.     0.]
  [ 1403.   964.   494.    74.     5.     0.     0.     1.]
  [  102.   625.   394.    72.    13.     4.     0.     2.]
  [  176.   471.   390.    68.    17.    20.    14.     5.]
  [   49.   291.   519.   267.   269.   189.   141.    40.]
  [   40.   105.   161.   155.   151.   203.   169.    95.]
  [   17.    32.    44.    40.    28.    49.   112.    54.]]]
step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  28603
step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8880053
step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8859884
step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.88742477
siam score:  -0.88713217
step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus -0.2392762467265129
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
first move QE:  -0.1394111759948803
start point for exploration sampling:  28603
step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8656025
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  28603
start point for exploration sampling:  28603
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
first move QE:  -0.10527156510800217
siam score:  -0.8548131
siam score:  -0.85394824
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
first move QE:  -0.09856685795267763
step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  28603
step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8790327
siam score:  -0.8813242
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8886983
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  28603
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
first move QE:  -0.09918090137248424
line 193 mcts: sample exp_bonus 0.5713757874011993
siam score:  -0.8756844
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8634068
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0599],
        [0.0260],
        [0.0000],
        [0.0929],
        [0.7863],
        [0.0000],
        [0.3662],
        [0.2757],
        [0.7670],
        [0.7670]], dtype=torch.float64)
0.0 0.05990761196001314
0.0 0.025967576457317496
0.0 0.0
0.0 0.09285550943638617
0.0 0.7863193604932381
0.0 0.0
0.0 0.3662073088914347
0.0 0.27570694839557397
0.0 0.766987325624147
0.0 0.766987325624147
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  28603
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.85170335
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.84709394
siam score:  -0.84659576
line 193 mcts: sample exp_bonus -0.02268029329404235
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.84768033
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8499091
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8469078
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
first move QE:  -0.08631819893820057
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  28603
Starting evaluation
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0
Test reward:  1.0
Q value #end_state_>_threshold:  911
Q value #non_end_state_>threshold:  1115
Scores:  {'0.0': 131.0, '0.17': 0, '0.33': 1.0, '0.5': 0}
Episodes: 5816, frames: 50162, time: 17760.60801410675
training steps:  5412
RDN obj mus: [-0.899415988177061, -0.9174351745665074, -0.9185935454815626, -0.9169556158602238, -0.9017133989810944]
RDN obj sigmas: [0.041992943252353095, 0.035642790634932274, 0.04743316428479717, 0.05458772292508491, 0.06623220511268288]
5816 : 0.15
LR:  0.001
replay buffer size:  57472
Time Taken :  296.0  mins 0.6078550815582275  seconds
[[[14321.  3376.   618.   897.   709.    37.     0.     0.]
  [ 8302.  2942.   984.   228.   101.     6.     0.     0.]
  [ 1571.  1204.   575.    82.    16.    13.     0.     1.]
  [  112.   802.   494.    82.    16.    30.     3.     2.]
  [  238.   673.   543.    85.    20.    23.    15.     5.]
  [   55.   443.   803.   436.   403.   277.   203.    52.]
  [   44.   139.   284.   285.   266.   362.   282.   173.]
  [   17.    37.    86.    92.    40.   114.   195.   132.]]]
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
