[['S' 'F' 'H' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'H' 'F']
 ['F' 'F' 'F' 'F' 'H' 'G']]
Adding thread: now have 13 threads
Starting evaluation
siam score:  0.0108471755
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.  0.  0.6]
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 121, frames: 1563, time: 45.69578409194946
training steps:  5
RDN obj mus: [0.043815752902961094, 0.0015810154501620748, 0.00016443344065919518, 0, 0]
RDN obj sigmas: [0.009701084072318827, 0.0036904440004621388, 4.0273283957503736e-05, 1, 1]
121 : 0.0
LR:  4e-06
replay buffer size:  2414
Time Taken :  0.0  mins 45.695655822753906  seconds
[[[419. 237.  54.   0.   0.   0.]
  [258. 151.  33.   0.   0.   0.]
  [106.  66.  12.   0.   0.   0.]
  [ 46.  17.   0.   0.   0.   0.]
  [ 17.   5.   0.   0.   0.   0.]
  [  9.  11.   1.   0.   0.   0.]]]
first move QE:  -0.10415463356697974
deleting a thread, now have 12 threads
Frames:  1567 train batches done:  16 episodes:  122
deleting a thread, now have 11 threads
Frames:  1888 train batches done:  41 episodes:  147
siam score:  -0.3406823
siam score:  -0.35319787
deleting a thread, now have 10 threads
Frames:  2167 train batches done:  68 episodes:  170
siam score:  -0.4708011
siam score:  -0.5165277
siam score:  -0.544697
deleting a thread, now have 9 threads
Frames:  2448 train batches done:  95 episodes:  191
siam score:  -0.5553475
first move QE:  -0.4752257313073403
deleting a thread, now have 8 threads
Frames:  2677 train batches done:  122 episodes:  210
siam score:  -0.37645054
siam score:  -0.36744443
siam score:  -0.5830226
siam score:  -0.51424795
siam score:  -0.5126137
siam score:  -0.50562423
siam score:  -0.57426614
siam score:  -0.601672
first move QE:  -0.31430859064101346
siam score:  -0.61136866
line 193 mcts: sample exp_bonus -0.178343266248703
first move QE:  -0.3025926782030844
siam score:  -0.63708854
siam score:  -0.63939226
line 193 mcts: sample exp_bonus 0.6220341920852661
siam score:  -0.6008794
siam score:  -0.5745596
siam score:  -0.638967
siam score:  -0.64309907
first move QE:  -0.23305120682476763
siam score:  -0.7276218
siam score:  -0.71581435
line 193 mcts: sample exp_bonus -0.4703514245152473
siam score:  -0.7528526
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 710, frames: 10174, time: 2407.5067172050476
training steps:  974
RDN obj mus: [-0.9669140186309815, -0.9589223262965679, -0.952887629109621, -0.949443133284984, -0.9640066739051573]
RDN obj sigmas: [0.01629270875246101, 0.020913550275884114, 0.022421236404605676, 0.026007639769216235, 0.009100932107208175]
710 : 0.0
LR:  0.000973
replay buffer size:  11465
Time Taken :  40.0  mins 7.506593704223633  seconds
[[[2688. 1427.  277.    0.    0.    0.]
  [1751.  861.  163.    0.    0.    0.]
  [ 889.  410.   76.    0.    0.    0.]
  [ 358.  157.    0.    0.    0.    0.]
  [ 189.   35.    0.    2.    0.    0.]
  [ 127.   40.    9.    5.    0.    0.]]]
siam score:  -0.74249405
siam score:  -0.7433783
siam score:  -0.7370737
line 193 mcts: sample exp_bonus -0.013315883724135346
line 193 mcts: sample exp_bonus -0.08316793262958526
siam score:  -0.7374657
first move QE:  -0.14412604279423374
siam score:  -0.75938445
siam score:  -0.76565653
siam score:  -0.7600049
UNIT TEST: sample policy line 217 mcts : [0.125 0.125 0.125 0.458 0.167]
siam score:  -0.75804216
first move QE:  -0.13102266457688344
siam score:  -0.7528714
siam score:  -0.7538712
siam score:  -0.7522455
siam score:  -0.7522438
siam score:  -0.75251305
siam score:  -0.7489941
UNIT TEST: sample policy line 217 mcts : [0.125 0.25  0.083 0.167 0.375]
siam score:  -0.7361728
siam score:  -0.7398532
siam score:  -0.75054
siam score:  -0.761098
siam score:  -0.76301867
siam score:  -0.7553541
first move QE:  -0.12509418589591711
siam score:  -0.7570621
siam score:  -0.7518841
siam score:  -0.75692916
siam score:  -0.75014526
siam score:  -0.75864136
siam score:  -0.7566258
siam score:  -0.75258577
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 1421, frames: 20054, time: 5525.673534154892
training steps:  2079
RDN obj mus: [-0.9565966174006462, -0.9732005461037159, -0.9602464953601361, -0.9261375411091564, -0.9621953633096483]
RDN obj sigmas: [0.03211982387720545, 0.01828417169399941, 0.03482156279869007, 0.043599028402600964, 0.009574338859859194]
1421 : 0.0
LR:  0.001
replay buffer size:  21595
Time Taken :  92.0  mins 5.673408031463623  seconds
[[[5409. 2949.  565.    0.    0.    0.]
  [3393. 1682.  321.    0.    0.    0.]
  [1676.  825.  160.    0.    0.    0.]
  [ 633.  298.    0.    0.    0.    0.]
  [ 331.   73.    2.    4.    0.    0.]
  [ 222.   71.   13.    6.    0.    0.]]]
first move QE:  -0.11604283139390115
siam score:  -0.7536856
siam score:  -0.7463341
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.125 0.25  0.208]
siam score:  -0.7552376
siam score:  -0.757097
first move QE:  -0.11369223137176465
siam score:  -0.7624689
siam score:  -0.7422178
first move QE:  -0.11056731272189725
siam score:  -0.7487836
UNIT TEST: sample policy line 217 mcts : [0.125 0.083 0.542 0.125 0.125]
siam score:  -0.74517304
first move QE:  -0.11037161545102032
siam score:  -0.75074625
siam score:  -0.74496704
line 193 mcts: sample exp_bonus -0.10476649090647695
siam score:  -0.7622479
line 193 mcts: sample exp_bonus -0.06569597609341145
siam score:  -0.7569989
siam score:  -0.757182
UNIT TEST: sample policy line 217 mcts : [0.083 0.125 0.417 0.125 0.25 ]
siam score:  -0.7510768
first move QE:  -0.1060257481151451
siam score:  -0.75185305
siam score:  -0.7521709
siam score:  -0.761459
siam score:  -0.7560465
siam score:  -0.755428
siam score:  -0.77076066
siam score:  -0.7543543
siam score:  -0.75612277
first move QE:  -0.10251373461655931
siam score:  -0.75011617
siam score:  -0.74997693
first move QE:  -0.10049415854311235
siam score:  -0.7658203
siam score:  -0.75348073
line 193 mcts: sample exp_bonus -0.14078481455147263
siam score:  -0.7545765
first move QE:  -0.10062879403031777
siam score:  -0.75330603
siam score:  -0.7386373
siam score:  -0.7325336
first move QE:  -0.09764502987270478
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 0.0
0.0 1.530487819074991e-20
0.0 0.0
0.0 1.3693838381197288e-20
0.0 8.457959000151266e-21
0.0 1.530487819074991e-20
0.0 -3.4596816471301863e-11
0.0 9.666238857315734e-21
0.0 0.0
0.0 0.0
siam score:  -0.75746965
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 2155, frames: 30053, time: 8414.50620007515
training steps:  3515
RDN obj mus: [-0.9738869169056416, -0.9760054149210453, -0.968567308717966, -0.9133845254801405, -0.9298157817787595]
RDN obj sigmas: [0.027264109245484596, 0.03301100896394137, 0.04561304166012593, 0.05663443447914736, 0.07425219766258406]
2155 : 0.0
LR:  0.001
replay buffer size:  31916
Time Taken :  140.0  mins 14.50607180595398  seconds
[[[8017. 4493.  893.    0.    0.    0.]
  [5034. 2543.  469.    0.    0.    0.]
  [2482. 1253.  239.    0.    0.    0.]
  [ 943.  439.    2.    0.    0.    0.]
  [ 452.  104.   12.   10.    0.    0.]
  [ 330.  123.   41.   18.    1.    0.]]]
siam score:  -0.7615469
siam score:  -0.75855494
first move QE:  -0.09715218885408779
line 193 mcts: sample exp_bonus -0.06786902042627335
siam score:  -0.7560784
UNIT TEST: sample policy line 217 mcts : [0.292 0.208 0.167 0.125 0.208]
first move QE:  -0.0965515038847192
first move QE:  -0.0972641955549135
first move QE:  -0.0923115013866784
line 193 mcts: sample exp_bonus 0.37418368792533874
siam score:  -0.76237226
siam score:  -0.770349
first move QE:  -0.08163936451625711
siam score:  -0.7625403
first move QE:  -0.07852868785411235
first move QE:  -0.07570677473682332
siam score:  -0.754361
siam score:  -0.77047455
siam score:  -0.7699564
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 1.297380625386673e-12
0.0 1.6110398095526221e-21
0.0 1.2082798571644666e-20
0.0 9.666238857315734e-21
0.0 0.0
0.0 1.9460709274068707e-12
0.0 1.2888318476420977e-20
0.0 0.0
0.0 8.56271208864543e-12
0.0 5.638639333434178e-21
first move QE:  -0.057938974658222966
siam score:  -0.75467867
siam score:  -0.76668763
siam score:  -0.7664939
siam score:  -0.7675781
start point for exploration sampling:  30000
siam score:  -0.76862717
start point for exploration sampling:  30000
siam score:  -0.7666727
siam score:  -0.76656187
first move QE:  -0.011012486185020572
siam score:  -0.7610239
siam score:  -0.76674604
start point for exploration sampling:  30000
siam score:  -0.7590356
siam score:  -0.7610645
siam score:  -0.7737994
line 193 mcts: sample exp_bonus -0.09619970660209656
siam score:  -0.7708891
step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Starting evaluation
first move QE:  0.014936509938541032
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.625 0.25  0.   ]
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 1.0}
Episodes: 2665, frames: 40162, time: 10440.573884010315
training steps:  4686
RDN obj mus: [-0.9643310335680843, -0.9765015110939741, -0.9612482542932034, -0.9429156911462545, -0.9378211166637382]
RDN obj sigmas: [0.062381342649850756, 0.02895948244735908, 0.05510832061654182, 0.06059963457815056, 0.07659935375913297]
2665 : 0.008333333333333333
LR:  0.001
replay buffer size:  42319
Time Taken :  174.0  mins 0.5737557411193848  seconds
[[[9937. 5340. 1057.    1.    9.   27.]
  [6035. 2997.  546.    0.    2.   12.]
  [3189. 1475.  281.    1.    6.   15.]
  [1436.  526.   66.   13.    8.   11.]
  [1296.  186.  170.   41.    1.    8.]
  [1479.  696.  468.  144.   17.    1.]]]
first move QE:  0.0302239117206944
siam score:  -0.7557547
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.167 0.292 0.375]
line 193 mcts: sample exp_bonus -0.5491692512341141
start point for exploration sampling:  30000
line 193 mcts: sample exp_bonus 1.9062682789555492
siam score:  -0.7543872
line 193 mcts: sample exp_bonus -0.15716914398813098
line 193 mcts: sample exp_bonus 1.373283266735077
siam score:  -0.75339484
siam score:  -0.74596757
siam score:  -0.7519284
step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7524991
first move QE:  0.07545171335779724
siam score:  -0.7532194
siam score:  -0.7566536
UNIT TEST: sample policy line 217 mcts : [0.125 0.083 0.708 0.042 0.042]
siam score:  -0.7600787
step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.25  0.625 0.042]
first move QE:  0.07871950235132867
siam score:  -0.75334924
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  30000
step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8395691
siam score:  -0.86313814
siam score:  -0.8953888
first move QE:  0.07106951794298243
siam score:  -0.9091265
siam score:  -0.91469455
Starting evaluation
siam score:  -0.92541945
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  4
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 6.0}
Episodes: 3123, frames: 50503, time: 12466.087543964386
training steps:  5764
RDN obj mus: [-0.9829424292445182, -0.9861180094003678, -0.980152705770731, -0.946504329842329, -0.9285721201896667]
RDN obj sigmas: [0.011562046470910567, 0.009785262153118176, 0.028115777458888384, 0.045783523000955194, 0.05231323659982964]
3123 : 0.025
LR:  0.001
replay buffer size:  54660
Time Taken :  207.0  mins 46.08741569519043  seconds
[[[12282.  6205.  1180.     1.    22.    99.]
  [ 7411.  3437.   600.     1.    24.   112.]
  [ 3977.  1713.   337.    16.    82.   172.]
  [ 1909.   614.   153.    79.   122.   235.]
  [ 1731.   231.   284.    54.    20.   119.]
  [ 2281.   969.   677.   200.    25.     6.]]]
first move QE:  0.07694031177135834
start point for exploration sampling:  30000
line 193 mcts: sample exp_bonus -0.8004452881098687
siam score:  -0.9089334
siam score:  -0.90882933
UNIT TEST: sample policy line 217 mcts : [0.458 0.125 0.208 0.083 0.125]
siam score:  -0.93004197
siam score:  -0.9294556
line 193 mcts: sample exp_bonus 6.423854596564174
siam score:  -0.9231499
siam score:  -0.9231085
siam score:  -0.9151409
siam score:  -0.91111314
siam score:  -0.91412115
start point for exploration sampling:  30000
siam score:  -0.9129044
siam score:  -0.9127372
siam score:  -0.90688896
siam score:  -0.90552276
siam score:  -0.9011132
start point for exploration sampling:  30000
siam score:  -0.90126574
first move QE:  0.06478032191858653
first move QE:  0.06370486751178853
siam score:  -0.8905572
start point for exploration sampling:  30000
start point for exploration sampling:  30000
step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.88725233
line 193 mcts: sample exp_bonus 1.161939453639984
start point for exploration sampling:  30000
line 193 mcts: sample exp_bonus 5.8938360358543695
UNIT TEST: sample policy line 217 mcts : [0.75 0.   0.   0.   0.25]
siam score:  -0.88364387
siam score:  -0.8849686
siam score:  -0.88955224
Starting evaluation
siam score:  -0.89016706
start point for exploration sampling:  30000
Test reward:  0.0
Q value #end_state_>_threshold:  6
Q value #non_end_state_>threshold:  5
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 7.0}
Episodes: 3472, frames: 60202, time: 14183.206623077393
training steps:  6635
RDN obj mus: [-0.986886203110218, -0.9885182101130485, -0.9906773574590683, -0.9642105369389057, -0.9533452496647835]
RDN obj sigmas: [0.007779955995654121, 0.005738447252132545, 0.00791625732077672, 0.03003088949344716, 0.03847158561469846]
3472 : 0.008333333333333333
LR:  0.001
replay buffer size:  66359
Time Taken :  236.0  mins 23.206493854522705  seconds
[[[13677.  6906.  1272.     4.    90.   355.]
  [ 8003.  3743.   673.     1.    32.   380.]
  [ 4308.  1788.   363.    30.   215.   508.]
  [ 2241.   652.   206.   157.   281.   554.]
  [ 2251.   268.   362.    60.    26.   209.]
  [ 4392.  1580.   884.   226.    26.     7.]]]
siam score:  -0.8889309
UNIT TEST: sample policy line 217 mcts : [0.167 0.083 0.292 0.208 0.25 ]
siam score:  -0.8898326
siam score:  -0.89127535
line 193 mcts: sample exp_bonus 0.5521655654029979
start point for exploration sampling:  30000
siam score:  -0.88993543
start point for exploration sampling:  30000
siam score:  -0.88593763
line 193 mcts: sample exp_bonus -0.16678183355458975
siam score:  -0.87489974
siam score:  -0.8740332
siam score:  -0.8761642
siam score:  -0.8726152
start point for exploration sampling:  30000
line 193 mcts: sample exp_bonus -0.2668733923678994
siam score:  -0.86867887
first move QE:  0.06067123737928705
line 193 mcts: sample exp_bonus -1.0364217021763324
siam score:  -0.88464814
siam score:  -0.8813968
step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8733409
first move QE:  0.05964441023836998
siam score:  -0.8786171
line 193 mcts: sample exp_bonus 0.27870985878676174
first move QE:  0.05938622534067397
siam score:  -0.8726727
siam score:  -0.87034994
siam score:  -0.8757291
siam score:  -0.875949
siam score:  -0.878894
start point for exploration sampling:  30000
start point for exploration sampling:  30000
siam score:  -0.88026917
start point for exploration sampling:  30000
siam score:  -0.86906266
siam score:  -0.8683009
first move QE:  0.061290865806317665
start point for exploration sampling:  30000
siam score:  -0.8713743
siam score:  -0.8623801
UNIT TEST: sample policy line 217 mcts : [0.375 0.125 0.042 0.417 0.042]
siam score:  -0.87917763
start point for exploration sampling:  30000
siam score:  -0.87661445
siam score:  -0.8744727
Starting evaluation
Sims:  25 1 epoch:  70182 pick best:  False frame count:  70182
line 193 mcts: sample exp_bonus -1.0772018060392248
Test reward:  0.0
Q value #end_state_>_threshold:  12
Q value #non_end_state_>threshold:  5
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 8.0}
Episodes: 3868, frames: 70310, time: 16019.45758008957
training steps:  7880
RDN obj mus: [-0.9810376677393913, -0.9869165008604527, -0.9879076223790646, -0.9699449545264244, -0.9531881255209446]
RDN obj sigmas: [0.011931160926156866, 0.0074218881717803414, 0.006683576513864259, 0.03437256354576418, 0.0770677826340816]
3868 : 0.0
LR:  0.001
replay buffer size:  78467
Time Taken :  266.0  mins 59.45744585990906  seconds
[[[14990.  7740.  1405.     5.   112.   607.]
  [ 8618.  4064.   751.     2.    40.   624.]
  [ 4704.  1875.   391.    60.   307.   783.]
  [ 2638.   682.   261.   234.   367.   640.]
  [ 2902.   319.   427.    68.    32.   347.]
  [ 6693.  2350.  1117.   251.    28.     8.]]]
siam score:  -0.8786983
siam score:  -0.87766695
siam score:  -0.8731015
line 193 mcts: sample exp_bonus -0.5700372846139669
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8729482
siam score:  -0.86448896
line 193 mcts: sample exp_bonus 5.945966780286312
first move QE:  0.07464911611738168
siam score:  -0.8722757
siam score:  -0.86859536
UNIT TEST: sample policy line 217 mcts : [0.167 0.125 0.542 0.083 0.083]
first move QE:  0.08481492616526437
siam score:  -0.8675186
siam score:  -0.8696886
siam score:  -0.8794725
start point for exploration sampling:  30000
siam score:  -0.891461
siam score:  -0.87955904
start point for exploration sampling:  30000
first move QE:  0.09446196667823252
siam score:  -0.87635374
siam score:  -0.87426406
line 193 mcts: sample exp_bonus -0.8624251132011413
first move QE:  0.09570818361785688
siam score:  -0.88725275
UNIT TEST: sample policy line 217 mcts : [0.125 0.125 0.375 0.167 0.208]
siam score:  -0.8786959
siam score:  -0.86609477
siam score:  -0.8692336
start point for exploration sampling:  30000
siam score:  -0.8569753
first move QE:  0.09695677085643932
line 193 mcts: sample exp_bonus -0.9391475234353541
siam score:  -0.84804744
line 193 mcts: sample exp_bonus 3.7526792917039247
start point for exploration sampling:  30000
first move QE:  0.1016633416216249
siam score:  -0.85114896
siam score:  -0.85073274
start point for exploration sampling:  30000
siam score:  -0.8473944
siam score:  -0.8477783
start point for exploration sampling:  30000
siam score:  -0.8450623
siam score:  -0.8468451
first move QE:  0.10035389986985244
siam score:  -0.85578966
siam score:  -0.859626
siam score:  -0.86011034
first move QE:  0.10623136652174353
siam score:  -0.86485535
siam score:  -0.8655164
start point for exploration sampling:  30000
start point for exploration sampling:  30000
start point for exploration sampling:  30000
start point for exploration sampling:  30000
siam score:  -0.85289854
siam score:  -0.8446764
first move QE:  0.1081877215772065
Starting evaluation
line 193 mcts: sample exp_bonus -0.6982384241426829
start point for exploration sampling:  30000
Test reward:  0.0
Q value #end_state_>_threshold:  21
Q value #non_end_state_>threshold:  6
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 9.0}
Episodes: 4284, frames: 80411, time: 17797.49206495285
training steps:  9418
RDN obj mus: [-0.9876207698464393, -0.9901476032793521, -0.9920737220644951, -0.9769035406947136, -0.9671598272264004]
RDN obj sigmas: [0.0076337418320123925, 0.006313231140304657, 0.008913719729651392, 0.026348398497955488, 0.027227020939467366]
4284 : 0.0
LR:  0.001
replay buffer size:  90568
Time Taken :  296.0  mins 37.4919319152832  seconds
[[[16315.  8616.  1526.     5.   119.   653.]
  [ 9245.  4476.   835.     5.    50.   721.]
  [ 5110.  2013.   444.   108.   383.   873.]
  [ 3085.   711.   348.   322.   422.   690.]
  [ 3729.   375.   515.    79.    36.   387.]
  [ 9114.  3143.  1344.   290.    31.     9.]]]
siam score:  -0.85135865
siam score:  -0.8472922
siam score:  -0.84059405
siam score:  -0.8426181
siam score:  -0.8467094
siam score:  -0.84818345
line 193 mcts: sample exp_bonus 0.16041256313215196
siam score:  -0.86096054
siam score:  -0.83339083
siam score:  -0.8321664
first move QE:  0.1255351353573397
siam score:  -0.8296811
siam score:  -0.83167785
siam score:  -0.83236724
siam score:  -0.8316437
siam score:  -0.8297664
siam score:  -0.83958685
siam score:  -0.8373018
start point for exploration sampling:  30000
siam score:  -0.83629274
first move QE:  0.13091906549716273
siam score:  -0.83287656
first move QE:  0.1336514949249522
start point for exploration sampling:  30000
start point for exploration sampling:  30000
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0226],
        [0.0062],
        [0.0062],
        [0.0286],
        [0.0061],
        [0.0059],
        [0.0205],
        [0.0246],
        [0.0043]], dtype=torch.float64)
0.0 0.0
0.0 0.02263982397507893
0.0 0.00615512208017294
0.0 0.006208921299582365
0.0 0.028617092447940234
0.0 0.0061410578421820525
0.0 0.005911498952900101
0.0 0.020472485629928677
0.0 0.024646876205743285
0.0 0.004256859086143453
siam score:  -0.82224745
siam score:  -0.83098376
line 193 mcts: sample exp_bonus -1.0466094037441627
first move QE:  0.14377822799221984
siam score:  -0.81461203
siam score:  -0.81185067
siam score:  -0.8117784
start point for exploration sampling:  30000
siam score:  -0.8192093
Starting evaluation
siam score:  -0.8214732
Test reward:  0.0
Q value #end_state_>_threshold:  22
Q value #non_end_state_>threshold:  6
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 9.0}
Episodes: 4705, frames: 90405, time: 19076.687006235123
training steps:  10582
RDN obj mus: [-0.9846972292900086, -0.9879761202692986, -0.9850208823680877, -0.9720004006564618, -0.9525783662617207]
RDN obj sigmas: [0.007408503912560418, 0.007150165961893568, 0.00764364370016202, 0.031108935088003727, 0.042184362127522836]
4705 : 0.0
LR:  0.001
replay buffer size:  102562
Time Taken :  317.0  mins 56.68687081336975  seconds
[[[17756.  9478.  1639.     5.   119.   653.]
  [ 9968.  4991.   932.     7.    54.   776.]
  [ 5594.  2442.   505.   170.   415.   891.]
  [ 3607.   767.   402.   408.   438.   712.]
  [ 4481.   412.   574.    83.    36.   390.]
  [10973.  4017.  1625.   335.    36.     9.]]]
start point for exploration sampling:  30000
siam score:  -0.8410043
siam score:  -0.8412219
siam score:  -0.8423287
line 193 mcts: sample exp_bonus -0.596567926120758
first move QE:  0.1632473470600383
siam score:  -0.86041427
first move QE:  0.16318102441774335
start point for exploration sampling:  30000
line 193 mcts: sample exp_bonus 1.585127168946862
siam score:  -0.8492462
first move QE:  0.1623191539718904
start point for exploration sampling:  30000
first move QE:  0.1623423117406975
siam score:  -0.8362741
siam score:  -0.8398845
first move QE:  0.1632835141529324
start point for exploration sampling:  30000
siam score:  -0.83603615
siam score:  -0.8392024
siam score:  -0.8355544
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0033],
        [0.0000],
        [0.0207],
        [0.0215],
        [0.0030],
        [0.0229],
        [0.0033],
        [0.0000],
        [0.0000],
        [0.0019]], dtype=torch.float64)
0.0 0.0032639622049343247
0.0 0.0
0.0 0.020704985011060432
0.0 0.02154183350307417
0.0 0.003008874655798271
0.0 0.022893407845002385
0.0 0.003289633183987919
0.0 0.0
0.0 0.0
0.0 0.0018832619434962041
start point for exploration sampling:  30000
siam score:  -0.84274703
siam score:  -0.84304184
first move QE:  0.181084294560512
siam score:  -0.8359051
first move QE:  0.18193869643867536
first move QE:  0.1838101538069828
siam score:  -0.8140719
first move QE:  0.1833313975690604
siam score:  -0.8160638
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  23
Q value #non_end_state_>threshold:  12
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 9.0}
Episodes: 5182, frames: 100265, time: 20377.57594013214
training steps:  11752
RDN obj mus: [-0.9836068112313747, -0.9896264114737511, -0.9846245371043683, -0.9665658398747444, -0.9432813562333584]
RDN obj sigmas: [0.013545218566309176, 0.008246365613720486, 0.01853932757022078, 0.03885132671258145, 0.04996038186846554]
5182 : 0.0
LR:  0.001
replay buffer size:  114209
Time Taken :  339.0  mins 37.57581686973572  seconds
[[[19484. 10396.  1778.     6.   119.   653.]
  [10881.  5514.  1031.    13.    57.   777.]
  [ 6141.  2946.   598.   209.   465.   899.]
  [ 4101.   820.   428.   491.   475.   723.]
  [ 5163.   451.   654.    92.    38.   395.]
  [12337.  4637.  1886.   377.    39.     9.]]]
start point for exploration sampling:  30000
siam score:  -0.8110735
start point for exploration sampling:  30000
siam score:  -0.81258386
siam score:  -0.8138945
siam score:  -0.81830275
siam score:  -0.8307779
start point for exploration sampling:  30000
siam score:  -0.8091088
UNIT TEST: sample policy line 217 mcts : [0.125 0.042 0.5   0.125 0.208]
first move QE:  0.19009446053523052
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0038],
        [0.0000],
        [0.0000],
        [0.0035],
        [0.0001],
        [0.0035],
        [0.0171],
        [0.0035],
        [0.0035]], dtype=torch.float64)
0.0 0.0
0.0 0.0037818964669752617
0.0 0.0
0.0 0.0
0.0 0.003476837953061098
0.0 0.00012589584417510718
0.0 0.0035048981616207617
0.0 0.017128379264610277
0.0 0.003476837953061098
0.0 0.0035157923857890403
siam score:  -0.81116956
siam score:  -0.8111082
first move QE:  0.19055798418940906
start point for exploration sampling:  30000
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  30000
first move QE:  0.19103425190209647
step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8160398
siam score:  -0.8153833
first move QE:  0.1899868554321706
siam score:  -0.82907486
siam score:  -0.8375084
start point for exploration sampling:  30000
siam score:  -0.8363974
siam score:  -0.83721805
step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  30000
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
first move QE:  0.19262667645713852
siam score:  -0.8105159
step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  30000
siam score:  -0.8058
start point for exploration sampling:  30000
step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus 3.346149535746365
step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.80527574
siam score:  -0.8207076
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8230239
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.87036777
UNIT TEST: sample policy line 217 mcts : [0.667 0.042 0.083 0.167 0.042]
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.87630403
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
first move QE:  0.190787044829204
siam score:  -0.8860347
siam score:  -0.88777155
start point for exploration sampling:  30000
step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
first move QE:  0.19373979257161283
siam score:  -0.9232316
start point for exploration sampling:  30000
UNIT TEST: sample policy line 217 mcts : [0.042 0.583 0.042 0.042 0.292]
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  30000
siam score:  -0.93808305
step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
start point for exploration sampling:  30000
Starting evaluation
siam score:  -0.94072825
step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0
Test reward:  1.0
Q value #end_state_>_threshold:  172
Q value #non_end_state_>threshold:  100
Scores:  {'0.0': 31.0, '0.17': 0, '0.33': 0, '0.5': 9.0}
Episodes: 5679, frames: 110367, time: 21600.535333156586
training steps:  12905
RDN obj mus: [-0.9781271919846535, -0.9813732551038266, -0.9791853086650372, -0.965000300514698, -0.9663466802597046]
RDN obj sigmas: [0.022411666161223247, 0.023536441081199903, 0.023609957491912668, 0.038362011460026074, 0.03919183278350062]
5679 : 0.10833333333333334
LR:  0.001
replay buffer size:  125289
Time Taken :  360.0  mins 0.5351998805999756  seconds
[[[21158. 11373.  1897.     6.   121.   661.]
  [11950.  6231.  1133.    18.    57.   781.]
  [ 6787.  3427.   712.   310.   526.   922.]
  [ 4566.   871.   522.   684.   571.   798.]
  [ 5779.   483.   735.    99.    43.   438.]
  [13455.  5075.  2032.   387.    40.    40.]]]
