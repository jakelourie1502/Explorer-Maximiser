[['S' 'F' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'H']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'H']
 ['F' 'F' 'F' 'F' 'H' 'F' 'H' 'H']
 ['F' 'F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'H' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'F' 'H' 'H' 'F']
 ['F' 'F' 'F' 'F' 'F' 'H' 'H' 'G']]
Starting evaluation
Adding thread: now have 13 threads
siam score:  -0.011932169
siam score:  -0.021897992
deleting a thread, now have 12 threads
Frames:  1527 train batches done:  13 episodes:  160
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 172, frames: 1677, time: 87.38551807403564
training steps:  17
RDN obj mus: [-0.029290179690718652, -0.03246909203703688, 0, 0, 0]
RDN obj sigmas: [0.0019315214039105184, 0.0018384534476690363, 1, 1, 1]
172 : 0.0
LR:  1.6e-05
replay buffer size:  4177
Time Taken :  1.0  mins 27.38536500930786  seconds
[[[486. 242.  50.   0.   0.   0.   0.   0.]
  [278.  88.   1.   0.   0.   0.   0.   0.]
  [102.  24.   2.   3.   1.   0.   0.   0.]
  [ 51.  22.   7.   4.   1.   0.   0.   0.]
  [ 30.  19.   6.   2.   0.   0.   0.   0.]
  [ 24.   8.   2.   0.   3.   1.   0.   0.]
  [ 17.   6.   2.   2.   4.   0.   0.   0.]
  [  5.   3.   2.   4.   2.   1.   0.   0.]]]
deleting a thread, now have 11 threads
Frames:  1837 train batches done:  34 episodes:  185
siam score:  -0.26293802
siam score:  -0.2716419
deleting a thread, now have 10 threads
Frames:  2060 train batches done:  62 episodes:  208
siam score:  -0.5809841
siam score:  -0.58658165
deleting a thread, now have 9 threads
Frames:  2231 train batches done:  90 episodes:  229
deleting a thread, now have 8 threads
Frames:  2389 train batches done:  120 episodes:  248
siam score:  -0.49248672
siam score:  -0.32259265
siam score:  -0.37934464
siam score:  -0.17253265
siam score:  -0.16458648
first move QE:  -0.3950468243429333
siam score:  -0.37842625
siam score:  -0.5154861
siam score:  -0.6017406
siam score:  -0.61044014
siam score:  -0.61162454
first move QE:  -0.3624242485727913
siam score:  -0.59113973
siam score:  -0.6642382
siam score:  -0.64733243
siam score:  -0.64322263
siam score:  -0.64031214
siam score:  -0.63817513
siam score:  -0.5532915
first move QE:  -0.2897185284147271
siam score:  -0.5445548
siam score:  -0.605339
line 193 mcts: sample exp_bonus -0.1882968686148524
siam score:  -0.6968432
siam score:  -0.70152533
siam score:  -0.69798803
siam score:  -0.6909239
first move QE:  -0.26216098009887434
Sims:  25 1 epoch:  7079 pick best:  False frame count:  7079
siam score:  -0.70091355
line 193 mcts: sample exp_bonus -0.04435454998165369
first move QE:  -0.22777961879051195
siam score:  -0.69340223
first move QE:  -0.20086744572985657
line 193 mcts: sample exp_bonus -0.2717643678188324
line 193 mcts: sample exp_bonus -0.017279652878642082
UNIT TEST: sample policy line 217 mcts : [0.292 0.167 0.25  0.167 0.125]
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 1053, frames: 10153, time: 3036.34961104393
training steps:  1177
RDN obj mus: [-0.9378033878445625, -0.9199615177512169, -0.9139294426083565, -0.897087074816227, -0.9161887168884277]
RDN obj sigmas: [0.029534663302035594, 0.03448330708597687, 0.03699282570548437, 0.04034418925722001, 0.0]
1053 : 0.0
LR:  0.001
replay buffer size:  12931
Time Taken :  50.0  mins 36.349510192871094  seconds
[[[3138. 1500.  269.    0.    0.    0.    0.    0.]
  [1551.  587.    5.    0.    0.    0.    0.    0.]
  [ 604.  149.   23.   13.    3.    0.    0.    0.]
  [ 283.  119.   42.   11.    2.    0.    0.    0.]
  [ 148.  100.   31.   11.    0.    0.    0.    0.]
  [ 110.   48.   18.    0.    3.    1.    0.    0.]
  [  81.   48.   12.    2.    4.    0.    0.    0.]
  [  94.   66.   16.    5.    2.    1.    0.    0.]]]
UNIT TEST: sample policy line 217 mcts : [0.208 0.292 0.208 0.125 0.167]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 -3.7646239340110494e-08
0.0 -6.539218880930007e-08
0.0 -6.937056322756246e-08
0.0 -3.7646239340110494e-08
0.0 -3.7646239340110494e-08
siam score:  -0.7065511
siam score:  -0.7053873
siam score:  -0.69951034
siam score:  -0.6991147
siam score:  -0.70536155
siam score:  -0.69586015
siam score:  -0.7298
siam score:  -0.72096324
UNIT TEST: sample policy line 217 mcts : [0.167 0.042 0.167 0.417 0.208]
siam score:  -0.71746904
siam score:  -0.7208744
first move QE:  -0.17167429747483612
line 193 mcts: sample exp_bonus -0.0036510746576823294
first move QE:  -0.1689128180544525
siam score:  -0.713268
siam score:  -0.7124798
first move QE:  -0.15860017313561975
siam score:  -0.70814705
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.292 0.083 0.208]
line 193 mcts: sample exp_bonus -0.059648670256137855
line 193 mcts: sample exp_bonus -0.000968892398814205
siam score:  -0.7090196
siam score:  -0.71314865
line 193 mcts: sample exp_bonus 0.00023091310868039727
Starting evaluation
line 193 mcts: sample exp_bonus -0.0695519670844078
siam score:  -0.70520175
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 2149, frames: 20036, time: 6944.870241165161
training steps:  2342
RDN obj mus: [-0.9505611487329007, -0.9654015955746174, -0.9272055489361286, -0.879125350068525, -0.7992972433567047]
RDN obj sigmas: [0.029274364629173533, 0.0258325586860837, 0.04744620586241186, 0.050863625945390484, 0.04834527848079402]
2149 : 0.0
LR:  0.001
replay buffer size:  23022
Time Taken :  115.0  mins 44.870147943496704  seconds
[[[6452. 2859.  565.    2.    2.    1.    0.    0.]
  [3116. 1212.   12.   10.    2.    0.    0.    0.]
  [1144.  288.   38.   27.    6.    0.    0.    0.]
  [ 516.  209.   92.   34.    3.    0.    0.    0.]
  [ 275.  170.   62.   21.    0.    0.    0.    0.]
  [ 197.   88.   35.    0.    3.    1.    0.    0.]
  [ 131.   80.   17.    2.    4.    0.    0.    0.]
  [ 106.   79.   18.    5.    2.    1.    0.    0.]]]
line 193 mcts: sample exp_bonus -0.05350404605269432
siam score:  -0.69337773
siam score:  -0.6935155
siam score:  -0.7120645
siam score:  -0.7142376
siam score:  -0.718184
siam score:  -0.71050394
siam score:  -0.6724848
siam score:  -0.6709636
siam score:  -0.7143062
first move QE:  -0.1430183184741561
siam score:  -0.71167326
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 2.89987165719472e-20
0.0 2.5776636952841954e-20
0.0 2.094351752418409e-20
0.0 3.8664955429262935e-20
0.0 2.4165597143289333e-20
0.0 0.0
0.0 2.094351752418409e-20
0.0 -9.730354616090835e-12
0.0 9.021822933494684e-20
0.0 0.0
siam score:  -0.70836204
first move QE:  -0.1427538945000809
siam score:  -0.6792295
siam score:  -0.6681414
siam score:  -0.63893455
first move QE:  -0.13538834467148816
UNIT TEST: sample policy line 217 mcts : [0.167 0.375 0.25  0.125 0.083]
siam score:  -0.70571685
siam score:  -0.71425766
siam score:  -0.71198
UNIT TEST: sample policy line 217 mcts : [0.125 0.375 0.208 0.125 0.167]
siam score:  -0.69115365
siam score:  -0.70821035
siam score:  -0.6900902
first move QE:  -0.12845245357219753
line 193 mcts: sample exp_bonus 0.024541168093681336
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 3248, frames: 30192, time: 10855.54641008377
training steps:  3520
RDN obj mus: [-0.9496670013606548, -0.9597911682248116, -0.94239932102561, -0.8748334732692061, -0.7992972433567047]
RDN obj sigmas: [0.023930861172584087, 0.03278401135247226, 0.043843888545808306, 0.05211278928845546, 0.04834527848079402]
3248 : 0.0
LR:  0.001
replay buffer size:  33371
Time Taken :  180.0  mins 55.54626202583313  seconds
[[[9965. 4293.  859.    8.    2.    1.    0.    0.]
  [4683. 1831.   15.   17.    2.    0.    0.    0.]
  [1771.  436.   52.   37.    7.    0.    0.    0.]
  [ 759.  304.  135.   42.    7.    0.    0.    0.]
  [ 379.  231.   88.   27.    0.    0.    0.    0.]
  [ 243.  122.   51.    0.    3.    1.    0.    0.]
  [ 169.  117.   28.    2.    4.    0.    0.    0.]
  [ 124.   97.   23.    6.    2.    1.    0.    0.]]]
siam score:  -0.709486
first move QE:  -0.13041357668922543
siam score:  -0.6993597
siam score:  -0.6513219
line 193 mcts: sample exp_bonus 1.7701659080029044
siam score:  -0.71154475
siam score:  -0.69859725
siam score:  -0.6983441
siam score:  -0.68636847
siam score:  -0.68901277
line 193 mcts: sample exp_bonus 0.471371031412959
siam score:  -0.70517
line 193 mcts: sample exp_bonus 3.9622494464778897
line 193 mcts: sample exp_bonus 5.093390028536319
siam score:  -0.704425
siam score:  -0.6897085
start point for exploration sampling:  30000
line 193 mcts: sample exp_bonus 0.14609953043460844
siam score:  -0.7178475
first move QE:  -0.10299732941080678
start point for exploration sampling:  30000
first move QE:  -0.09732370349506647
start point for exploration sampling:  30000
start point for exploration sampling:  30000
siam score:  -0.7063234
UNIT TEST: sample policy line 217 mcts : [0.292 0.292 0.083 0.125 0.208]
siam score:  -0.7107768
first move QE:  -0.055888337516438666
siam score:  -0.6979372
line 193 mcts: sample exp_bonus 4.747372333942706
siam score:  -0.69464564
start point for exploration sampling:  30000
line 193 mcts: sample exp_bonus 2.7174582853341103
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 3863, frames: 40154, time: 13495.61730313301
training steps:  4266
RDN obj mus: [-0.9289312870442867, -0.9398235513269901, -0.9359446351706981, -0.9145980102241039, -0.9038128982847232]
RDN obj sigmas: [0.038005072635310846, 0.02951011580434084, 0.031091348366721543, 0.03888105714545783, 0.03818098414331711]
3863 : 0.0
LR:  0.001
replay buffer size:  43537
Time Taken :  224.0  mins 55.61715507507324  seconds
[[[12203.  5250.  1018.    69.    16.    10.     1.     1.]
  [ 5417.  2104.    70.   114.    19.    27.     3.     2.]
  [ 2182.   503.   233.   197.    32.     1.     0.     0.]
  [ 1159.   565.   380.   142.    15.     0.     0.     0.]
  [  640.   395.   145.    47.     0.     0.     0.     0.]
  [  485.   260.    66.     0.     4.     1.     0.     0.]
  [  464.   273.    54.    24.    23.     2.     0.     0.]
  [  729.   457.   228.   203.    50.     8.     0.     0.]]]
siam score:  -0.6856712
siam score:  -0.70318365
line 193 mcts: sample exp_bonus -0.14850153962397575
line 193 mcts: sample exp_bonus 6.942096595668793
siam score:  -0.6795588
start point for exploration sampling:  30000
siam score:  -0.6940341
siam score:  -0.6998493
siam score:  -0.7176922
first move QE:  -0.03874199575256726
siam score:  -0.71537787
siam score:  -0.72320414
siam score:  -0.7168103
siam score:  -0.71396416
siam score:  -0.7219911
siam score:  -0.7131759
siam score:  -0.7121261
siam score:  -0.7276584
line 193 mcts: sample exp_bonus 4.822199291436225
siam score:  -0.7279416
start point for exploration sampling:  30000
line 193 mcts: sample exp_bonus 6.101680087212663
siam score:  -0.72954094
Starting evaluation
line 193 mcts: sample exp_bonus 4.22156449131128
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 4342, frames: 50165, time: 15595.329303979874
training steps:  4822
RDN obj mus: [-0.9468432193100452, -0.9659284084022045, -0.9624090170562267, -0.9450323832690716, -0.927455482840538]
RDN obj sigmas: [0.020526232873868507, 0.015215624613386903, 0.016634124492752915, 0.02183442855836057, 0.026905951899574607]
4342 : 0.0
LR:  0.001
replay buffer size:  53802
Time Taken :  259.0  mins 55.32916283607483  seconds
[[[13790.  5989.  1139.   278.   489.   524.   342.   196.]
  [ 6075.  2273.   102.   281.   147.   184.   184.    32.]
  [ 2608.   553.   324.   328.    54.    31.    35.     0.]
  [ 1548.   803.   597.   257.    19.     4.    11.     0.]
  [  804.   490.   188.    58.     0.     0.     0.     0.]
  [  576.   316.    75.     6.    15.     3.     0.     0.]
  [  564.   339.    73.    98.   123.     8.     0.     0.]
  [  855.   607.   520.   617.   271.    20.     0.     0.]]]
siam score:  -0.7205599
line 193 mcts: sample exp_bonus 5.88508022504425
first move QE:  -0.007694811321660759
siam score:  -0.7137925
siam score:  -0.7121704
siam score:  -0.709384
start point for exploration sampling:  30000
line 193 mcts: sample exp_bonus 8.520503171831937
siam score:  -0.7262943
siam score:  -0.7351128
siam score:  -0.74219483
first move QE:  -0.0017693642857750508
siam score:  -0.7418556
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 9.666238857315733e-20
0.0 5.155327390568391e-20
0.0 0.0
0.0 3.3831836000605063e-20
0.0 7.410783123942061e-20
0.0 1.7721437905078842e-20
0.0 0.0
0.0 9.666238857315733e-20
0.0 0.0
line 193 mcts: sample exp_bonus 4.825829809617996
first move QE:  -0.005783380366786608
UNIT TEST: sample policy line 217 mcts : [0.333 0.25  0.083 0.25  0.083]
UNIT TEST: sample policy line 217 mcts : [0. 1. 0. 0. 0.]
UNIT TEST: sample policy line 217 mcts : [0.    0.583 0.25  0.042 0.125]
start point for exploration sampling:  30000
siam score:  -0.7569335
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.667 0.    0.25 ]
siam score:  -0.755509
line 193 mcts: sample exp_bonus 2.8825420413662193
siam score:  -0.7622272
siam score:  -0.7744763
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 4769, frames: 60222, time: 17532.357743024826
training steps:  5318
RDN obj mus: [-0.9364429778873921, -0.9271274799585343, -0.9549027257084847, -0.9532955599784851, -0.9282849687755108]
RDN obj sigmas: [0.047885736988260943, 0.05764300810366231, 0.03613160753934568, 0.02393064205363104, 0.042913974109152056]
4769 : 0.0
LR:  0.001
replay buffer size:  64042
Time Taken :  292.0  mins 12.357653856277466  seconds
[[[14794.  6417.  1250.   636.  1090.  1099.   619.   294.]
  [ 6634.  2414.   176.   487.   304.   362.   264.    48.]
  [ 2962.   596.   435.   462.    70.    50.    58.     2.]
  [ 1892.   988.   734.   317.    22.     4.    13.     0.]
  [  987.   584.   241.    64.     4.     0.     0.     0.]
  [  752.   386.    87.    25.    85.     7.     0.     0.]
  [  734.   439.   105.   396.   397.    14.     0.     0.]
  [ 1059.   823.   805.  1189.   750.    27.     0.     0.]]]
siam score:  -0.7712234
first move QE:  0.01310212892874186
line 193 mcts: sample exp_bonus 1.7862316824942082
line 193 mcts: sample exp_bonus 3.0058462282938785
siam score:  -0.7761479
siam score:  -0.7741114
siam score:  -0.77197266
start point for exploration sampling:  30000
siam score:  -0.7761595
siam score:  -0.77858436
first move QE:  0.02369342344813935
start point for exploration sampling:  30000
siam score:  -0.7966835
siam score:  -0.7958508
UNIT TEST: sample policy line 217 mcts : [0.    0.792 0.167 0.    0.042]
start point for exploration sampling:  30000
siam score:  -0.7959941
first move QE:  0.02958996266093606
siam score:  -0.7991449
first move QE:  0.028327140171570894
siam score:  -0.79600525
line 193 mcts: sample exp_bonus 4.1050195928349495
start point for exploration sampling:  30000
siam score:  -0.8024591
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 5152, frames: 70150, time: 19278.699161052704
training steps:  5839
RDN obj mus: [-0.9640317939221859, -0.9742959004104137, -0.969619049680233, -0.9641911681354046, -0.9490596345722675]
RDN obj sigmas: [0.01751420585007958, 0.015476812542014091, 0.018069578732313785, 0.021673206181233414, 0.03267041590137259]
5152 : 0.0
LR:  0.001
replay buffer size:  74149
Time Taken :  321.0  mins 18.699070930480957  seconds
[[[15654.  6781.  1313.   792.  1363.  1380.   724.   308.]
  [ 7197.  2561.   234.   678.   429.   552.   309.    51.]
  [ 3308.   633.   518.   564.    80.   124.    76.     3.]
  [ 2208.  1100.   832.   354.    26.    23.    16.     0.]
  [ 1209.   679.   296.    76.     9.     3.     0.     0.]
  [ 1024.   495.   107.    76.   198.    16.     0.     0.]
  [ 1271.   678.   124.   741.   662.    20.     0.     0.]
  [ 1728.  1267.  1150.  1835.  1107.    36.     0.     0.]]]
line 193 mcts: sample exp_bonus 5.313154667014017
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.375 0.208 0.292]
siam score:  -0.81499255
siam score:  -0.8151142
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.667 0.208 0.042]
first move QE:  0.03498952918620254
first move QE:  0.0352117743330231
siam score:  -0.8177176
siam score:  -0.818091
start point for exploration sampling:  30000
line 193 mcts: sample exp_bonus -0.24017405377149578
line 193 mcts: sample exp_bonus 4.053724549043179
start point for exploration sampling:  30000
first move QE:  0.037406144770102186
line 193 mcts: sample exp_bonus 4.392269448282719
siam score:  -0.8302307
siam score:  -0.836221
line 193 mcts: sample exp_bonus 1.516430648808479
line 193 mcts: sample exp_bonus 3.7479165507912637
line 193 mcts: sample exp_bonus 2.250034559202847
first move QE:  0.040145204727431764
line 193 mcts: sample exp_bonus 3.110257106836513
line 193 mcts: sample exp_bonus 0.7753029907703399
siam score:  -0.849129
siam score:  -0.8523105
siam score:  -0.85238355
siam score:  -0.84884644
siam score:  -0.84785116
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0, '0.17': 0, '0.33': 0, '0.5': 0}
Episodes: 5531, frames: 80194, time: 21238.25110721588
training steps:  6417
RDN obj mus: [-0.9653867499709129, -0.9766415059626102, -0.9712318578720093, -0.9593678469896316, -0.9481651490151882]
RDN obj sigmas: [0.018981381746405228, 0.013618484558316555, 0.01785718340119512, 0.024595478466571324, 0.03194443726212776]
5531 : 0.0
LR:  0.001
replay buffer size:  84372
Time Taken :  353.0  mins 58.2509560585022  seconds
[[[16282.  7030.  1368.   842.  1430.  1434.   739.   308.]
  [ 7736.  2665.   267.   797.   477.   630.   326.    51.]
  [ 3757.   686.   619.   673.    93.   342.   134.     4.]
  [ 2624.  1366.  1017.   415.    35.    75.    22.     1.]
  [ 1514.   940.   403.   109.    23.     8.     2.     1.]
  [ 1281.   649.   125.   292.   353.    30.     0.     0.]
  [ 1887.   994.   156.  1037.   791.    23.     0.     0.]
  [ 2756.  1869.  1470.  2371.  1294.    40.     0.     0.]]]
