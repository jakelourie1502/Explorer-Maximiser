[['S' 'F' 'H' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'H' 'F']
 ['F' 'F' 'F' 'F' 'H' 'G']]
Adding thread: now have 13 threads
Starting evaluation
siam score:  0.0017124815
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 106, frames: 1408, time: 28.107798099517822
training steps:  3
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
106 : 0.0
LR:  2e-06
replay buffer size:  1639
Time Taken :  0.0  mins 28.10765290260315  seconds
[[[323. 130.  22.   0.   0.   0.]
  [272. 123.  27.   0.   0.   0.]
  [135.  69.  10.   0.   0.   0.]
  [ 60.  38.   0.   0.   0.   0.]
  [ 36.   8.   4.   0.   0.   0.]
  [ 20.  11.  10.   3.   1.   0.]]]
siam score:  -0.029793084
deleting a thread, now have 12 threads
Frames:  1551 train batches done:  22 episodes:  120
line 193 mcts: sample exp_bonus 0.0
deleting a thread, now have 11 threads
Frames:  1849 train batches done:  55 episodes:  139
deleting a thread, now have 10 threads
Frames:  2120 train batches done:  90 episodes:  158
siam score:  -0.5670935
siam score:  -0.6101969
deleting a thread, now have 9 threads
Frames:  2363 train batches done:  119 episodes:  179
siam score:  -0.6002243
siam score:  -0.5986081
deleting a thread, now have 8 threads
Frames:  2660 train batches done:  152 episodes:  198
siam score:  -0.28823182
siam score:  -0.41460416
siam score:  -0.43837097
siam score:  -0.49753648
siam score:  -0.5389019
siam score:  -0.52700186
siam score:  -0.64439034
siam score:  -0.6198874
siam score:  -0.6172275
siam score:  -0.5862545
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.59217906
siam score:  -0.617505
siam score:  -0.64669234
siam score:  -0.6089519
siam score:  -0.5783659
siam score:  -0.5625683
siam score:  -0.56404203
siam score:  -0.6159783
siam score:  -0.6268528
siam score:  -0.73827475
siam score:  -0.716614
siam score:  -0.71946394
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7213934
siam score:  -0.7200249
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.70060885
siam score:  -0.7108887
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.72659355
Starting evaluation
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 659, frames: 10162, time: 2178.090553045273
training steps:  1116
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
659 : 0.0
LR:  0.001
replay buffer size:  10783
Time Taken :  36.0  mins 18.09044575691223  seconds
[[[1737.  770.  113.    0.    0.    0.]
  [1564.  665.   92.    0.    0.    0.]
  [ 984.  402.   77.    0.    0.    0.]
  [ 617.  236.    2.    0.    0.    0.]
  [ 524.  121.   37.    8.    0.    0.]
  [ 936.  381.  178.   49.   10.    0.]]]
siam score:  -0.74172586
siam score:  -0.74261206
siam score:  -0.72643524
siam score:  -0.72898626
siam score:  -0.72585136
siam score:  -0.7338832
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.67284316
siam score:  -0.7307935
siam score:  -0.6910145
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 5.745362600559512e-07
0.0 5.782158477179536e-07
0.0 0.0
0.0 0.0
0.0 0.0
0.0 6.154155016206734e-07
0.0 5.817887690701554e-07
0.0 5.782158477179536e-07
0.0 6.20103672974899e-07
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7281112
siam score:  -0.7293382
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7481877
siam score:  -0.72995865
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7308347
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 1349, frames: 20132, time: 5092.692108154297
training steps:  2351
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
1349 : 0.0
LR:  0.001
replay buffer size:  21115
Time Taken :  84.0  mins 52.69199895858765  seconds
[[[3670. 1691.  262.    0.    0.    0.]
  [3236. 1448.  223.    0.    0.    0.]
  [1978.  865.  152.    0.    0.    0.]
  [1206.  460.    5.    2.    1.    0.]
  [ 950.  220.   51.   16.    0.    0.]
  [1457.  558.  242.   76.   14.    0.]]]
siam score:  -0.72407633
siam score:  -0.72962373
siam score:  -0.7298751
siam score:  -0.74814224
siam score:  -0.74753875
siam score:  -0.7425421
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.74847
siam score:  -0.7461136
UNIT TEST: sample policy line 217 mcts : [0.208 0.333 0.167 0.125 0.167]
siam score:  -0.74394727
siam score:  -0.7588109
siam score:  -0.7566081
UNIT TEST: sample policy line 217 mcts : [0.125 0.083 0.292 0.292 0.208]
siam score:  -0.73462313
UNIT TEST: sample policy line 217 mcts : [0.083 0.375 0.25  0.125 0.167]
siam score:  -0.74833435
siam score:  -0.74724805
siam score:  -0.72666246
siam score:  -0.7334297
siam score:  -0.72534144
siam score:  -0.74785596
siam score:  -0.7458488
step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.083 0.125 0.25  0.125 0.417]
siam score:  -0.74352515
siam score:  -0.7385001
Starting evaluation
siam score:  -0.7345218
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  1
Scores:  {'0.0': 1.0}
Episodes: 2006, frames: 30253, time: 7517.327985286713
training steps:  3531
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
2006 : 0.0
LR:  0.001
replay buffer size:  32266
Time Taken :  125.0  mins 17.32782483100891  seconds
[[[5533. 2585.  396.    0.    0.    0.]
  [5002. 2186.  334.    0.    0.    0.]
  [3049. 1344.  225.    0.    0.    0.]
  [1815.  684.   13.    7.    2.    2.]
  [1330.  318.   75.   25.    0.    1.]
  [2004.  817.  358.  120.   21.    1.]]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0004],
        [0.0000],
        [0.0000],
        [0.0004],
        [0.0003],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0002],
        [0.0000]], dtype=torch.float64)
0.0 0.0003894593489138265
0.0 0.0
0.0 0.0
0.0 0.00038728807481070305
0.0 0.00028373369261754913
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.00022284351289322986
0.0 0.0
siam score:  -0.7364644
siam score:  -0.7351245
siam score:  -0.7338036
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.73406565
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7350447
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.875 0.042 0.042]
UNIT TEST: sample policy line 217 mcts : [0.083 0.125 0.375 0.167 0.25 ]
siam score:  -0.7444152
siam score:  -0.7359752
siam score:  -0.7368632
siam score:  -0.7481679
siam score:  -0.7388408
UNIT TEST: sample policy line 217 mcts : [0.125 0.042 0.417 0.25  0.167]
siam score:  -0.74421906
siam score:  -0.74970156
siam score:  -0.7535686
siam score:  -0.74369097
siam score:  -0.7433992
siam score:  -0.7487485
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
Starting evaluation
siam score:  -0.75552523
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  1
Scores:  {'0.0': 1.0}
Episodes: 2559, frames: 40365, time: 9241.92731332779
training steps:  4703
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
2559 : 0.0
LR:  0.001
replay buffer size:  44378
Time Taken :  154.0  mins 1.927161693572998  seconds
[[[7435. 3255.  480.    0.    0.    0.]
  [6629. 2774.  398.    0.    0.    0.]
  [4032. 1743.  280.    0.    0.    0.]
  [2504.  898.   23.   10.    3.    2.]
  [1915.  429.  124.   37.    1.    1.]
  [2983. 1174.  488.  158.   29.    1.]]]
siam score:  -0.76812303
siam score:  -0.75746566
siam score:  -0.7637785
line 193 mcts: sample exp_bonus 0.0
UNIT TEST: sample policy line 217 mcts : [0.333 0.083 0.333 0.125 0.125]
siam score:  -0.7719242
siam score:  -0.7661116
siam score:  -0.7661646
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.5   0.167 0.25 ]
siam score:  -0.7595717
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7625342
siam score:  -0.7718407
siam score:  -0.77198744
siam score:  -0.7714131
siam score:  -0.7665321
siam score:  -0.7663052
siam score:  -0.7777862
siam score:  -0.8145735
siam score:  -0.80825824
Starting evaluation
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0
siam score:  -0.8034169
siam score:  -0.80174756
Test reward:  0.04
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  3
Scores:  {'0.0': 1.0}
Episodes: 2874, frames: 50500, time: 10720.631289243698
training steps:  5688
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
2874 : 0.0
LR:  0.001
replay buffer size:  56450
Time Taken :  178.0  mins 40.63113284111023  seconds
[[[9752. 3792.  520.    0.    0.    0.]
  [8365. 3189.  436.    0.    0.    0.]
  [5023. 1974.  294.    0.    0.    1.]
  [3153.  987.   34.   15.    7.   14.]
  [2490.  511.  170.   50.    4.    8.]
  [4201. 1673.  715.  211.   36.    1.]]]
siam score:  -0.78149915
UNIT TEST: sample policy line 217 mcts : [0.25  0.083 0.042 0.292 0.333]
siam score:  -0.78088486
siam score:  -0.7827345
siam score:  -0.7747304
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.7830007
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.7706997
siam score:  -0.77019083
siam score:  -0.76725423
siam score:  -0.7637596
siam score:  -0.7582094
UNIT TEST: sample policy line 217 mcts : [0.208 0.042 0.333 0.208 0.208]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0005],
        [0.0004],
        [0.0006],
        [0.0004],
        [0.0000],
        [0.0000],
        [0.0004],
        [0.0003],
        [0.0005],
        [0.0005]], dtype=torch.float64)
0.0 0.0004910064478816324
0.0 0.00040333562866067826
0.0 0.0005607024852874949
0.0 0.000363903626267641
0.0 0.0
0.0 0.0
0.0 0.00040550461770894275
0.0 0.00034854375871201605
0.0 0.0004659342563104234
0.0 0.0005301842075250586
siam score:  -0.767773
siam score:  -0.76631665
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.79036707
siam score:  -0.79003614
siam score:  -0.7916952
siam score:  -0.7967679
siam score:  -0.8153912
siam score:  -0.82019186
siam score:  -0.8136137
siam score:  -0.81047213
step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
siam score:  -0.7809235
Test reward:  0.0
Q value #end_state_>_threshold:  1
Q value #non_end_state_>threshold:  4
Scores:  {'0.0': 4.0}
Episodes: 3104, frames: 60229, time: 11882.697726249695
training steps:  6432
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
3104 : 0.008333333333333333
LR:  0.001
replay buffer size:  66996
Time Taken :  198.0  mins 2.6975700855255127  seconds
[[[12824.  4246.   544.     0.     0.     0.]
  [10248.  3482.   452.     0.     0.     0.]
  [ 5760.  2085.   307.     2.     3.     4.]
  [ 3541.  1028.    64.    33.    21.    23.]
  [ 2990.   574.   238.    72.     7.    22.]
  [ 5300.  2053.   900.   257.    41.     4.]]]
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7696155
siam score:  -0.7735118
siam score:  -0.7688385
step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.7530078
siam score:  -0.75077456
line 193 mcts: sample exp_bonus 0.0
UNIT TEST: sample policy line 217 mcts : [0.125 0.042 0.167 0.333 0.333]
siam score:  -0.7451622
siam score:  -0.7465731
step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.7375464
step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.7103102
siam score:  -0.7119286
siam score:  -0.7156258
Starting evaluation
line 193 mcts: sample exp_bonus 0.0
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0
line 193 mcts: sample exp_bonus 0.0
step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0
siam score:  -0.7248546
step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0
Test reward:  0.32
Q value #end_state_>_threshold:  29
Q value #non_end_state_>threshold:  5
Scores:  {'0.0': 7.0}
Episodes: 3339, frames: 70375, time: 13085.463176250458
training steps:  7179
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
3339 : 0.016666666666666666
LR:  0.001
replay buffer size:  78451
Time Taken :  218.0  mins 5.463036775588989  seconds
[[[15933.  4749.   577.     0.     0.     0.]
  [12223.  3825.   471.     0.     1.     0.]
  [ 6702.  2213.   320.     5.     6.     7.]
  [ 4071.  1076.    88.    43.    32.    32.]
  [ 3395.   636.   276.    81.    10.    30.]
  [ 6470.  2394.  1019.   296.    48.     7.]]]
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.125 0.5   0.167 0.167]
line 193 mcts: sample exp_bonus 0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.375 0.042 0.208 0.167 0.208]
step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.74801904
siam score:  -0.7478508
siam score:  -0.7447739
siam score:  -0.7401045
siam score:  -0.73415864
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus 0.0
step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.73759633
step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus 0.0
step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.78669375
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.79137444
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.803445
siam score:  -0.8034637
siam score:  -0.8038655
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.81012225
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8027365
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8157484
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8161998
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.80781555
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.81820524
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8155109
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8117235
siam score:  -0.8141582
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0
Test reward:  1.0
Q value #end_state_>_threshold:  547
Q value #non_end_state_>threshold:  215
Scores:  {'0.0': 91.0}
Episodes: 3704, frames: 80106, time: 14719.208986282349
training steps:  8327
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
3704 : 0.25833333333333336
LR:  0.001
replay buffer size:  88652
Time Taken :  245.0  mins 19.208829879760742  seconds
[[[17738.  5164.   599.     1.     0.     0.]
  [13418.  4051.   480.     2.     3.     0.]
  [ 7534.  2327.   336.    22.    29.    42.]
  [ 4856.  1149.   240.   177.   162.   213.]
  [ 4178.   761.   449.    94.    23.   150.]
  [ 7628.  2862.  1255.   316.    52.    91.]]]
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
